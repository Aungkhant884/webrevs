{"files":[{"patch":"@@ -2,1 +2,1 @@\n-project=jdk\n+project=lilliput\n@@ -7,1 +7,1 @@\n-error=author,committer,reviewers,merge,issues,executable,symlink,message,hg-tag,whitespace,problemlists\n+error=author,committer,reviewers,issues,executable,symlink,message,hg-tag,whitespace,problemlists\n@@ -21,4 +21,1 @@\n-[checks \"merge\"]\n-message=Merge\n-\n-reviewers=1\n+committers=1\n","filename":".jcheck\/conf","additions":3,"deletions":6,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -7323,1 +7323,1 @@\n-  predicate(!needs_acquiring_load(n));\n+  predicate(!needs_acquiring_load(n) && !UseCompactObjectHeaders);\n@@ -7333,0 +7333,26 @@\n+instruct loadNKlassLilliput(iRegNNoSp dst, memory4 mem, rFlagsReg cr)\n+%{\n+  match(Set dst (LoadNKlass mem));\n+  effect(TEMP_DEF dst, KILL cr);\n+  predicate(!needs_acquiring_load(n) && UseCompactObjectHeaders);\n+\n+  ins_cost(4 * INSN_COST);\n+  format %{ \"ldrw  $dst, $mem\\t# compressed class ptr\" %}\n+  ins_encode %{\n+    assert($mem$$disp == oopDesc::klass_offset_in_bytes(), \"expect correct offset\");\n+    assert($mem$$index$$Register == noreg, \"expect no index\");\n+    Register dst = $dst$$Register;\n+    Register obj = $mem$$base$$Register;\n+    C2LoadNKlassStub* stub = new (Compile::current()->comp_arena()) C2LoadNKlassStub(dst);\n+    Compile::current()->output()->add_stub(stub);\n+    __ ldr(dst, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    \/\/ NOTE: We can't use tbnz here, because the target is sometimes too far away\n+    \/\/ and cannot be encoded.\n+    __ tst(dst, markWord::monitor_value);\n+    __ br(Assembler::NE, stub->entry());\n+    __ bind(stub->continuation());\n+    __ lsr(dst, dst, markWord::klass_shift);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":27,"deletions":1,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"oops\/compressedKlass.hpp\"\n@@ -1232,1 +1233,1 @@\n-                      arrayOopDesc::header_size(op->type()),\n+                      arrayOopDesc::base_offset_in_bytes(op->type()),\n@@ -2355,2 +2356,2 @@\n-        __ ldrw(tmp, src_klass_addr);\n-        __ ldrw(rscratch1, dst_klass_addr);\n+        __ load_nklass(tmp, src);\n+        __ load_nklass(rscratch1, dst);\n@@ -2359,2 +2360,2 @@\n-        __ ldr(tmp, src_klass_addr);\n-        __ ldr(rscratch1, dst_klass_addr);\n+        __ ldr(tmp, Address(src, oopDesc::klass_offset_in_bytes()));\n+        __ ldr(rscratch1, Address(dst, oopDesc::klass_offset_in_bytes()));\n@@ -2484,3 +2485,0 @@\n-    if (UseCompressedClassPointers) {\n-      __ encode_klass_not_null(tmp);\n-    }\n@@ -2489,8 +2487,1 @@\n-\n-      if (UseCompressedClassPointers) {\n-        __ ldrw(rscratch1, dst_klass_addr);\n-        __ cmpw(tmp, rscratch1);\n-      } else {\n-        __ ldr(rscratch1, dst_klass_addr);\n-        __ cmp(tmp, rscratch1);\n-      }\n+      __ cmp_klass(dst, tmp, rscratch1);\n@@ -2498,7 +2489,1 @@\n-      if (UseCompressedClassPointers) {\n-        __ ldrw(rscratch1, src_klass_addr);\n-        __ cmpw(tmp, rscratch1);\n-      } else {\n-        __ ldr(rscratch1, src_klass_addr);\n-        __ cmp(tmp, rscratch1);\n-      }\n+      __ cmp_klass(src, tmp, rscratch1);\n@@ -2507,7 +2492,1 @@\n-      if (UseCompressedClassPointers) {\n-        __ ldrw(rscratch1, dst_klass_addr);\n-        __ cmpw(tmp, rscratch1);\n-      } else {\n-        __ ldr(rscratch1, dst_klass_addr);\n-        __ cmp(tmp, rscratch1);\n-      }\n+      __ cmp_klass(dst, tmp, rscratch1);\n@@ -2594,1 +2573,12 @@\n-    __ ldrw(result, Address (obj, oopDesc::klass_offset_in_bytes()));\n+    if (UseCompactObjectHeaders) {\n+      \/\/ Check if we can take the (common) fast path, if obj is unlocked.\n+      __ ldr(result, Address(obj, oopDesc::mark_offset_in_bytes()));\n+      __ tst(result, markWord::monitor_value);\n+      __ br(Assembler::NE, *op->stub()->entry());\n+      __ bind(*op->stub()->continuation());\n+\n+      \/\/ Shift to get proper narrow Klass*.\n+      __ lsr(result, result, markWord::klass_shift);\n+    } else {\n+      __ ldrw(result, Address (obj, oopDesc::klass_offset_in_bytes()));\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":21,"deletions":31,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -183,7 +183,3 @@\n-  \/\/ This assumes that all prototype bits fit in an int32_t\n-  mov(t1, (int32_t)(intptr_t)markWord::prototype().value());\n-  str(t1, Address(obj, oopDesc::mark_offset_in_bytes()));\n-\n-  if (UseCompressedClassPointers) { \/\/ Take care not to kill klass\n-    encode_klass_not_null(t1, klass);\n-    strw(t1, Address(obj, oopDesc::klass_offset_in_bytes()));\n+  if (UseCompactObjectHeaders) {\n+    ldr(t1, Address(klass, Klass::prototype_header_offset()));\n+    str(t1, Address(obj, oopDesc::mark_offset_in_bytes()));\n@@ -191,1 +187,10 @@\n-    str(klass, Address(obj, oopDesc::klass_offset_in_bytes()));\n+    \/\/ This assumes that all prototype bits fit in an int32_t\n+    mov(t1, (int32_t)(intptr_t)markWord::prototype().value());\n+    str(t1, Address(obj, oopDesc::mark_offset_in_bytes()));\n+\n+    if (UseCompressedClassPointers) { \/\/ Take care not to kill klass\n+      encode_klass_not_null(t1, klass);\n+      strw(t1, Address(obj, oopDesc::klass_offset_in_bytes()));\n+    } else {\n+      str(klass, Address(obj, oopDesc::klass_offset_in_bytes()));\n+    }\n@@ -196,1 +201,1 @@\n-  } else if (UseCompressedClassPointers) {\n+  } else if (UseCompressedClassPointers && !UseCompactObjectHeaders) {\n@@ -215,0 +220,6 @@\n+  \/\/ Zero first 4 bytes, if start offset is not word aligned.\n+  if (!is_aligned(hdr_size_in_bytes, BytesPerWord)) {\n+    strw(zr, Address(obj, hdr_size_in_bytes));\n+    hdr_size_in_bytes += BytesPerInt;\n+  }\n+\n@@ -274,1 +285,1 @@\n-void C1_MacroAssembler::allocate_array(Register obj, Register len, Register t1, Register t2, int header_size, int f, Register klass, Label& slow_case) {\n+void C1_MacroAssembler::allocate_array(Register obj, Register len, Register t1, Register t2, int base_offset_in_bytes, int f, Register klass, Label& slow_case) {\n@@ -287,1 +298,1 @@\n-  mov(arr_size, (int32_t)header_size * BytesPerWord + MinObjAlignmentInBytesMask);\n+  mov(arr_size, (int32_t)base_offset_in_bytes + MinObjAlignmentInBytesMask);\n@@ -296,1 +307,1 @@\n-  initialize_body(obj, arr_size, header_size * BytesPerWord, t1, t2);\n+  initialize_body(obj, arr_size, base_offset_in_bytes, t1, t2);\n@@ -316,1 +327,5 @@\n-  assert(!MacroAssembler::needs_explicit_null_check(oopDesc::klass_offset_in_bytes()), \"must add explicit null check\");\n+  if (UseCompactObjectHeaders) {\n+    assert(!MacroAssembler::needs_explicit_null_check(oopDesc::mark_offset_in_bytes()), \"must add explicit null check\");\n+  } else {\n+    assert(!MacroAssembler::needs_explicit_null_check(oopDesc::klass_offset_in_bytes()), \"must add explicit null check\");\n+  }\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_MacroAssembler_aarch64.cpp","additions":28,"deletions":13,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -94,0 +94,11 @@\n+int C2LoadNKlassStub::max_size() const {\n+  return 8;\n+}\n+\n+void C2LoadNKlassStub::emit(C2_MacroAssembler& masm) {\n+  __ bind(entry());\n+  Register d = dst();\n+  __ ldr(d, Address(d, OM_OFFSET_NO_MONITOR_VALUE_TAG(header)));\n+  __ b(continuation());\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_CodeStubs_aarch64.cpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -48,0 +48,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n@@ -4316,0 +4317,24 @@\n+\/\/ Loads the obj's Klass* into dst.\n+\/\/ Preserves all registers (incl src, rscratch1 and rscratch2).\n+void MacroAssembler::load_nklass(Register dst, Register src) {\n+  assert(UseCompressedClassPointers, \"expects UseCompressedClassPointers\");\n+\n+  if (!UseCompactObjectHeaders) {\n+    ldrw(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+    return;\n+  }\n+\n+  Label fast;\n+\n+  \/\/ Check if we can take the (common) fast path, if obj is unlocked.\n+  ldr(dst, Address(src, oopDesc::mark_offset_in_bytes()));\n+  tbz(dst, exact_log2(markWord::monitor_value), fast);\n+\n+  \/\/ Fetch displaced header\n+  ldr(dst, Address(dst, OM_OFFSET_NO_MONITOR_VALUE_TAG(header)));\n+\n+  \/\/ Fast-path: shift and decode Klass*.\n+  bind(fast);\n+  lsr(dst, dst, markWord::klass_shift);\n+}\n+\n@@ -4318,1 +4343,5 @@\n-    ldrw(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+    if (UseCompactObjectHeaders) {\n+      load_nklass(dst, src);\n+    } else {\n+      ldrw(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+    }\n@@ -4356,0 +4385,1 @@\n+  assert_different_registers(oop, trial_klass, tmp);\n@@ -4357,1 +4387,5 @@\n-    ldrw(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));\n+    if (UseCompactObjectHeaders) {\n+      load_nklass(tmp, oop);\n+    } else {\n+      ldrw(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));\n+    }\n@@ -4526,0 +4560,14 @@\n+\/\/ Returns a static string\n+const char* MacroAssembler::describe_klass_decode_mode(MacroAssembler::KlassDecodeMode mode) {\n+  switch (mode) {\n+  case KlassDecodeNone: return \"none\";\n+  case KlassDecodeZero: return \"zero\";\n+  case KlassDecodeXor:  return \"xor\";\n+  case KlassDecodeMovk: return \"movk\";\n+  default:\n+    ShouldNotReachHere();\n+  }\n+  return NULL;\n+}\n+\n+\/\/ Return the current narrow Klass pointer decode mode.\n@@ -4527,2 +4575,4 @@\n-  assert(UseCompressedClassPointers, \"not using compressed class pointers\");\n-  assert(Metaspace::initialized(), \"metaspace not initialized yet\");\n+  if (_klass_decode_mode == KlassDecodeNone) {\n+    \/\/ First time initialization\n+    assert(UseCompressedClassPointers, \"not using compressed class pointers\");\n+    assert(Metaspace::initialized(), \"metaspace not initialized yet\");\n@@ -4530,2 +4580,5 @@\n-  if (_klass_decode_mode != KlassDecodeNone) {\n-    return _klass_decode_mode;\n+    _klass_decode_mode = klass_decode_mode_for_base(CompressedKlassPointers::base());\n+    guarantee(_klass_decode_mode != KlassDecodeNone,\n+              PTR_FORMAT \" is not a valid encoding base on aarch64\",\n+              p2i(CompressedKlassPointers::base()));\n+    log_info(metaspace)(\"klass decode mode initialized: %s\", describe_klass_decode_mode(_klass_decode_mode));\n@@ -4533,0 +4586,6 @@\n+  return _klass_decode_mode;\n+}\n+\n+\/\/ Given an arbitrary base address, return the KlassDecodeMode that would be used. Return KlassDecodeNone\n+\/\/ if base address is not valid for encoding.\n+MacroAssembler::KlassDecodeMode MacroAssembler::klass_decode_mode_for_base(address base) {\n@@ -4534,2 +4593,1 @@\n-  assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift()\n-         || 0 == CompressedKlassPointers::shift(), \"decode alg wrong\");\n+  const uint64_t base_u64 = (uint64_t) base;\n@@ -4537,2 +4595,2 @@\n-  if (CompressedKlassPointers::base() == nullptr) {\n-    return (_klass_decode_mode = KlassDecodeZero);\n+  if (base_u64 == 0) {\n+    return KlassDecodeZero;\n@@ -4541,7 +4599,3 @@\n-  if (operand_valid_for_logical_immediate(\n-        \/*is32*\/false, (uint64_t)CompressedKlassPointers::base())) {\n-    const uint64_t range_mask =\n-      (1ULL << log2i(CompressedKlassPointers::range())) - 1;\n-    if (((uint64_t)CompressedKlassPointers::base() & range_mask) == 0) {\n-      return (_klass_decode_mode = KlassDecodeXor);\n-    }\n+  if (operand_valid_for_logical_immediate(false, base_u64) &&\n+      ((base_u64 & (KlassEncodingMetaspaceMax - 1)) == 0)) {\n+    return KlassDecodeXor;\n@@ -4550,4 +4604,4 @@\n-  const uint64_t shifted_base =\n-    (uint64_t)CompressedKlassPointers::base() >> CompressedKlassPointers::shift();\n-  guarantee((shifted_base & 0xffff0000ffffffff) == 0,\n-            \"compressed class base bad alignment\");\n+  const uint64_t shifted_base = base_u64 >> LogKlassAlignmentInBytes;\n+  if ((shifted_base & 0xffff0000ffffffff) == 0) {\n+    return KlassDecodeMovk;\n+  }\n@@ -4555,1 +4609,1 @@\n-  return (_klass_decode_mode = KlassDecodeMovk);\n+  return KlassDecodeNone;\n@@ -4559,0 +4613,2 @@\n+  assert (UseCompressedClassPointers, \"should only be used for compressed headers\");\n+  assert(CompressedKlassPointers::shift() != 0, \"not lilliput?\");\n@@ -4561,5 +4617,1 @@\n-    if (CompressedKlassPointers::shift() != 0) {\n-      lsr(dst, src, LogKlassAlignmentInBytes);\n-    } else {\n-      if (dst != src) mov(dst, src);\n-    }\n+    lsr(dst, src, LogKlassAlignmentInBytes);\n@@ -4569,6 +4621,2 @@\n-    if (CompressedKlassPointers::shift() != 0) {\n-      eor(dst, src, (uint64_t)CompressedKlassPointers::base());\n-      lsr(dst, dst, LogKlassAlignmentInBytes);\n-    } else {\n-      eor(dst, src, (uint64_t)CompressedKlassPointers::base());\n-    }\n+    eor(dst, src, (uint64_t)CompressedKlassPointers::base());\n+    lsr(dst, dst, LogKlassAlignmentInBytes);\n@@ -4578,5 +4626,1 @@\n-    if (CompressedKlassPointers::shift() != 0) {\n-      ubfx(dst, src, LogKlassAlignmentInBytes, 32);\n-    } else {\n-      movw(dst, src);\n-    }\n+    ubfx(dst, src, LogKlassAlignmentInBytes, MaxNarrowKlassPointerBits);\n@@ -4598,0 +4642,2 @@\n+  assert(CompressedKlassPointers::shift() != 0, \"not lilliput?\");\n+\n@@ -4600,5 +4646,1 @@\n-    if (CompressedKlassPointers::shift() != 0) {\n-      lsl(dst, src, LogKlassAlignmentInBytes);\n-    } else {\n-      if (dst != src) mov(dst, src);\n-    }\n+    if (dst != src) mov(dst, src);\n@@ -4608,6 +4650,2 @@\n-    if (CompressedKlassPointers::shift() != 0) {\n-      lsl(dst, src, LogKlassAlignmentInBytes);\n-      eor(dst, dst, (uint64_t)CompressedKlassPointers::base());\n-    } else {\n-      eor(dst, src, (uint64_t)CompressedKlassPointers::base());\n-    }\n+    lsl(dst, src, LogKlassAlignmentInBytes);\n+    eor(dst, dst, (uint64_t)CompressedKlassPointers::base());\n@@ -4620,0 +4658,3 @@\n+    \/\/ Invalid base should have been gracefully handled via klass_decode_mode() in VM initialization.\n+    assert((shifted_base & 0xffff0000ffffffff) == 0, \"incompatible base\");\n+\n@@ -4622,5 +4663,1 @@\n-\n-    if (CompressedKlassPointers::shift() != 0) {\n-      lsl(dst, dst, LogKlassAlignmentInBytes);\n-    }\n-\n+    lsl(dst, dst, LogKlassAlignmentInBytes);\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":91,"deletions":54,"binary":false,"changes":145,"status":"modified"},{"patch":"@@ -89,0 +89,2 @@\n+ public:\n+\n@@ -96,1 +98,9 @@\n-  KlassDecodeMode klass_decode_mode();\n+  \/\/ Return the current narrow Klass pointer decode mode. Initialized on first call.\n+  static KlassDecodeMode klass_decode_mode();\n+\n+  \/\/ Given an arbitrary base address, return the KlassDecodeMode that would be used. Return KlassDecodeNone\n+  \/\/ if base address is not valid for encoding.\n+  static KlassDecodeMode klass_decode_mode_for_base(address base);\n+\n+  \/\/ Returns a static string\n+  static const char* describe_klass_decode_mode(KlassDecodeMode mode);\n@@ -852,0 +862,1 @@\n+  void load_nklass(Register dst, Register src);\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":12,"deletions":1,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -3572,1 +3572,1 @@\n-    __ sub(r3, r3, sizeof(oopDesc));\n+    __ sub(r3, r3, oopDesc::base_offset_in_bytes());\n@@ -3577,1 +3577,6 @@\n-      __ add(r2, r0, sizeof(oopDesc));\n+      __ add(r2, r0, oopDesc::base_offset_in_bytes());\n+      if (!is_aligned(oopDesc::base_offset_in_bytes(), BytesPerLong)) {\n+        __ strw(zr, Address(__ post(r2, BytesPerInt)));\n+        __ sub(r3, r3, BytesPerInt);\n+        __ cbz(r3, initialize_header);\n+      }\n@@ -3587,5 +3592,8 @@\n-    __ mov(rscratch1, (intptr_t)markWord::prototype().value());\n-    __ str(rscratch1, Address(r0, oopDesc::mark_offset_in_bytes()));\n-    __ store_klass_gap(r0, zr);  \/\/ zero klass gap for compressed oops\n-    __ store_klass(r0, r4);      \/\/ store klass last\n-\n+    if (UseCompactObjectHeaders) {\n+      __ ldr(rscratch1, Address(r4, Klass::prototype_header_offset()));\n+      __ str(rscratch1, Address(r0, oopDesc::mark_offset_in_bytes()));\n+    } else {\n+      __ mov(rscratch1, (intptr_t)markWord::prototype().value());\n+      __ str(rscratch1, Address(r0, oopDesc::mark_offset_in_bytes()));\n+      __ store_klass(r0, r4);      \/\/ store klass last\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/templateTable_aarch64.cpp","additions":15,"deletions":7,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -166,1 +166,1 @@\n-                                       int header_size, int element_size,\n+                                       int header_size_in_bytes, int element_size,\n@@ -169,1 +169,0 @@\n-  const int header_size_in_bytes = header_size * BytesPerWord;\n","filename":"src\/hotspot\/cpu\/arm\/c1_MacroAssembler_arm.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n","filename":"src\/hotspot\/cpu\/ppc\/macroAssembler_ppc.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -4589,9 +4589,0 @@\n-void MacroAssembler::test_bit(Register Rd, Register Rs, uint32_t bit_pos, Register tmp) {\n-  assert(bit_pos < 64, \"invalid bit range\");\n-  if (UseZbs) {\n-    bexti(Rd, Rs, bit_pos);\n-    return;\n-  }\n-  andi(Rd, Rs, 1UL << bit_pos, tmp);\n-}\n-\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.cpp","additions":0,"deletions":9,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -1684,1 +1684,1 @@\n-      __ cmpxchg_obj_header(x10, lock_reg, obj_reg, t0, count, \/*fallthrough*\/nullptr);\n+      __ cmpxchg_obj_header(x10, lock_reg, obj_reg, t0, count, \/*fallthrough*\/NULL);\n","filename":"src\/hotspot\/cpu\/riscv\/sharedRuntime_riscv.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n","filename":"src\/hotspot\/cpu\/s390\/macroAssembler_s390.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1638,1 +1638,1 @@\n-                      arrayOopDesc::header_size(op->type()),\n+                      arrayOopDesc::base_offset_in_bytes(op->type()),\n@@ -3073,0 +3073,1 @@\n+  Register tmp2 = UseCompactObjectHeaders ? rscratch2 : noreg;\n@@ -3264,7 +3265,1 @@\n-      if (UseCompressedClassPointers) {\n-        __ movl(tmp, src_klass_addr);\n-        __ cmpl(tmp, dst_klass_addr);\n-      } else {\n-        __ movptr(tmp, src_klass_addr);\n-        __ cmpptr(tmp, dst_klass_addr);\n-      }\n+      __ cmp_klass(src, dst, tmp, tmp2);\n@@ -3430,4 +3425,1 @@\n-\n-\n-      if (UseCompressedClassPointers)          __ cmpl(tmp, dst_klass_addr);\n-      else                   __ cmpptr(tmp, dst_klass_addr);\n+      __ cmp_klass(tmp, dst, tmp2);\n@@ -3436,2 +3428,1 @@\n-      if (UseCompressedClassPointers)          __ cmpl(tmp, src_klass_addr);\n-      else                   __ cmpptr(tmp, src_klass_addr);\n+      __ cmp_klass(tmp, src, tmp2);\n@@ -3440,2 +3431,1 @@\n-      if (UseCompressedClassPointers)          __ cmpl(tmp, dst_klass_addr);\n-      else                   __ cmpptr(tmp, dst_klass_addr);\n+      __ cmp_klass(tmp, dst, tmp2);\n@@ -3537,0 +3527,14 @@\n+  if (UseCompactObjectHeaders) {\n+    Register tmp = rscratch1;\n+    assert_different_registers(tmp, obj);\n+    assert_different_registers(tmp, result);\n+\n+    \/\/ Check if we can take the (common) fast path, if obj is unlocked.\n+    __ movq(result, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    __ testb(result, markWord::monitor_value);\n+    __ jcc(Assembler::notZero, *op->stub()->entry());\n+    __ bind(*op->stub()->continuation());\n+    \/\/ Fast-path: shift and decode Klass*.\n+    __ shrq(result, markWord::klass_shift);\n+    __ decode_klass_not_null(result, tmp);\n+  } else\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":20,"deletions":16,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -166,2 +166,6 @@\n-  assert_different_registers(obj, klass, len);\n-  movptr(Address(obj, oopDesc::mark_offset_in_bytes()), checked_cast<int32_t>(markWord::prototype().value()));\n+  assert_different_registers(obj, klass, len, t1, t2);\n+  if (UseCompactObjectHeaders) {\n+    movptr(t1, Address(klass, Klass::prototype_header_offset()));\n+    movptr(Address(obj, oopDesc::mark_offset_in_bytes()), t1);\n+  } else {\n+    movptr(Address(obj, oopDesc::mark_offset_in_bytes()), checked_cast<int32_t>(markWord::prototype().value()));\n@@ -169,5 +173,5 @@\n-  if (UseCompressedClassPointers) { \/\/ Take care not to kill klass\n-    movptr(t1, klass);\n-    encode_klass_not_null(t1, rscratch1);\n-    movl(Address(obj, oopDesc::klass_offset_in_bytes()), t1);\n-  } else\n+    if (UseCompressedClassPointers) { \/\/ Take care not to kill klass\n+      movptr(t1, klass);\n+      encode_klass_not_null(t1, rscratch1);\n+      movl(Address(obj, oopDesc::klass_offset_in_bytes()), t1);\n+    } else\n@@ -175,2 +179,3 @@\n-  {\n-    movptr(Address(obj, oopDesc::klass_offset_in_bytes()), klass);\n+    {\n+      movptr(Address(obj, oopDesc::klass_offset_in_bytes()), klass);\n+    }\n@@ -178,1 +183,0 @@\n-\n@@ -183,1 +187,1 @@\n-  else if (UseCompressedClassPointers) {\n+  else if (UseCompressedClassPointers && !UseCompactObjectHeaders) {\n@@ -225,0 +229,1 @@\n+    int hdr_size_aligned = align_up(hdr_size_in_bytes, BytesPerWord); \/\/ klass gap is already cleared by init_header().\n@@ -227,1 +232,1 @@\n-      initialize_body(obj, index, hdr_size_in_bytes, t1_zero);\n+      initialize_body(obj, index, hdr_size_aligned, t1_zero);\n@@ -232,1 +237,1 @@\n-      for (int i = hdr_size_in_bytes; i < con_size_in_bytes; i += BytesPerWord)\n+      for (int i = hdr_size_aligned; i < con_size_in_bytes; i += BytesPerWord)\n@@ -234,1 +239,1 @@\n-    } else if (con_size_in_bytes > hdr_size_in_bytes) {\n+    } else if (con_size_in_bytes > hdr_size_aligned) {\n@@ -239,1 +244,1 @@\n-      movptr(index, (con_size_in_bytes - hdr_size_in_bytes) >> 3);\n+      movptr(index, (con_size_in_bytes - hdr_size_aligned) >> 3);\n@@ -241,1 +246,1 @@\n-      if (((con_size_in_bytes - hdr_size_in_bytes) & 4) != 0)\n+      if (((con_size_in_bytes - hdr_size_aligned) & 4) != 0)\n@@ -246,1 +251,1 @@\n-        movptr(Address(obj, index, Address::times_8, hdr_size_in_bytes - (1*BytesPerWord)),\n+        movptr(Address(obj, index, Address::times_8, hdr_size_aligned - (1*BytesPerWord)),\n@@ -248,1 +253,1 @@\n-        NOT_LP64(movptr(Address(obj, index, Address::times_8, hdr_size_in_bytes - (2*BytesPerWord)),\n+        NOT_LP64(movptr(Address(obj, index, Address::times_8, hdr_size_aligned - (2*BytesPerWord)),\n@@ -264,1 +269,1 @@\n-void C1_MacroAssembler::allocate_array(Register obj, Register len, Register t1, Register t2, int header_size, Address::ScaleFactor f, Register klass, Label& slow_case) {\n+void C1_MacroAssembler::allocate_array(Register obj, Register len, Register t1, Register t2, int base_offset_in_bytes, Address::ScaleFactor f, Register klass, Label& slow_case) {\n@@ -277,1 +282,1 @@\n-  movptr(arr_size, header_size * BytesPerWord + MinObjAlignmentInBytesMask);\n+  movptr(arr_size, (int32_t)base_offset_in_bytes + MinObjAlignmentInBytesMask);\n@@ -287,1 +292,1 @@\n-  initialize_body(obj, arr_size, header_size * BytesPerWord, len_zero);\n+  initialize_body(obj, arr_size, base_offset_in_bytes, len_zero);\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":26,"deletions":21,"binary":false,"changes":47,"status":"modified"},{"patch":"@@ -666,1 +666,1 @@\n-  cmpxchgptr(scrReg, Address(boxReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));\n+  cmpxchgptr(thread, Address(boxReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));\n@@ -668,7 +668,0 @@\n-  \/\/ If we weren't able to swing _owner from null to the BasicLock\n-  \/\/ then take the slow path.\n-  jccb  (Assembler::notZero, NO_COUNT);\n-  \/\/ update _owner from BasicLock to thread\n-  get_thread (scrReg);                    \/\/ beware: clobbers ICCs\n-  movptr(Address(boxReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), scrReg);\n-  xorptr(boxReg, boxReg);                 \/\/ set icc.ZFlag = 1 to indicate success\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":1,"deletions":8,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n@@ -4172,1 +4173,1 @@\n-  assert((offset_in_bytes & (BytesPerWord - 1)) == 0, \"offset must be a multiple of BytesPerWord\");\n+  assert((offset_in_bytes & (BytesPerInt - 1)) == 0, \"offset must be a multiple of BytesPerInt\");\n@@ -4178,0 +4179,13 @@\n+  \/\/ Emit single 32bit store to clear leading bytes, if necessary.\n+  xorptr(temp, temp);    \/\/ use _zero reg to clear memory (shorter code)\n+#ifdef _LP64\n+  if (!is_aligned(offset_in_bytes, BytesPerWord)) {\n+    movl(Address(address, offset_in_bytes), temp);\n+    offset_in_bytes += BytesPerInt;\n+    decrement(length_in_bytes, BytesPerInt);\n+  }\n+  assert((offset_in_bytes & (BytesPerWord - 1)) == 0, \"offset must be a multiple of BytesPerWord\");\n+  testptr(length_in_bytes, length_in_bytes);\n+  jcc(Assembler::zero, done);\n+#endif\n+\n@@ -4190,1 +4204,0 @@\n-  xorptr(temp, temp);    \/\/ use _zero reg to clear memory (shorter code)\n@@ -5241,0 +5254,22 @@\n+#ifdef _LP64\n+void MacroAssembler::load_nklass(Register dst, Register src) {\n+  assert(UseCompressedClassPointers, \"expect compressed class pointers\");\n+\n+  if (!UseCompactObjectHeaders) {\n+    movl(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+    return;\n+  }\n+\n+  Label fast;\n+  movq(dst, Address(src, oopDesc::mark_offset_in_bytes()));\n+  testb(dst, markWord::monitor_value);\n+  jccb(Assembler::zero, fast);\n+\n+  \/\/ Fetch displaced header\n+  movq(dst, Address(dst, OM_OFFSET_NO_MONITOR_VALUE_TAG(header)));\n+\n+  bind(fast);\n+  shrq(dst, markWord::klass_shift);\n+}\n+#endif\n+\n@@ -5246,1 +5281,1 @@\n-    movl(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+    load_nklass(dst, src);\n@@ -5254,0 +5289,1 @@\n+  assert(!UseCompactObjectHeaders, \"not with compact headers\");\n@@ -5262,1 +5298,40 @@\n-    movptr(Address(dst, oopDesc::klass_offset_in_bytes()), src);\n+   movptr(Address(dst, oopDesc::klass_offset_in_bytes()), src);\n+}\n+\n+void MacroAssembler::cmp_klass(Register klass, Register obj, Register tmp) {\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    \/\/ NOTE: We need to deal with possible ObjectMonitor in object header.\n+    \/\/ Eventually we might be able to do simple movl & cmpl like in\n+    \/\/ the CCP path below.\n+    load_nklass(tmp, obj);\n+    cmpl(klass, tmp);\n+  } else if (UseCompressedClassPointers) {\n+    cmpl(klass, Address(obj, oopDesc::klass_offset_in_bytes()));\n+  } else\n+#endif\n+  {\n+    cmpptr(klass, Address(obj, oopDesc::klass_offset_in_bytes()));\n+  }\n+}\n+\n+void MacroAssembler::cmp_klass(Register src, Register dst, Register tmp1, Register tmp2) {\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    \/\/ NOTE: We need to deal with possible ObjectMonitor in object header.\n+    \/\/ Eventually we might be able to do simple movl & cmpl like in\n+    \/\/ the CCP path below.\n+    assert(tmp2 != noreg, \"need tmp2\");\n+    assert_different_registers(src, dst, tmp1, tmp2);\n+    load_nklass(tmp1, src);\n+    load_nklass(tmp2, dst);\n+    cmpl(tmp1, tmp2);\n+  } else if (UseCompressedClassPointers) {\n+    movl(tmp1, Address(src, oopDesc::klass_offset_in_bytes()));\n+    cmpl(tmp1, Address(dst, oopDesc::klass_offset_in_bytes()));\n+  } else\n+#endif\n+  {\n+    movptr(tmp1, Address(src, oopDesc::klass_offset_in_bytes()));\n+    cmpptr(tmp1, Address(dst, oopDesc::klass_offset_in_bytes()));\n+  }\n@@ -5469,0 +5544,61 @@\n+MacroAssembler::KlassDecodeMode MacroAssembler::_klass_decode_mode = KlassDecodeNone;\n+\n+\/\/ Returns a static string\n+const char* MacroAssembler::describe_klass_decode_mode(MacroAssembler::KlassDecodeMode mode) {\n+  switch (mode) {\n+  case KlassDecodeNone: return \"none\";\n+  case KlassDecodeZero: return \"zero\";\n+  case KlassDecodeXor:  return \"xor\";\n+  case KlassDecodeAdd:  return \"add\";\n+  default:\n+    ShouldNotReachHere();\n+  }\n+  return NULL;\n+}\n+\n+\/\/ Return the current narrow Klass pointer decode mode.\n+MacroAssembler::KlassDecodeMode MacroAssembler::klass_decode_mode() {\n+  if (_klass_decode_mode == KlassDecodeNone) {\n+    \/\/ First time initialization\n+    assert(UseCompressedClassPointers, \"not using compressed class pointers\");\n+    assert(Metaspace::initialized(), \"metaspace not initialized yet\");\n+\n+    _klass_decode_mode = klass_decode_mode_for_base(CompressedKlassPointers::base());\n+    guarantee(_klass_decode_mode != KlassDecodeNone,\n+              PTR_FORMAT \" is not a valid encoding base on aarch64\",\n+              p2i(CompressedKlassPointers::base()));\n+    log_info(metaspace)(\"klass decode mode initialized: %s\", describe_klass_decode_mode(_klass_decode_mode));\n+  }\n+  return _klass_decode_mode;\n+}\n+\n+\/\/ Given an arbitrary base address, return the KlassDecodeMode that would be used. Return KlassDecodeNone\n+\/\/ if base address is not valid for encoding.\n+MacroAssembler::KlassDecodeMode MacroAssembler::klass_decode_mode_for_base(address base) {\n+\n+  const uint64_t base_u64 = (uint64_t) base;\n+\n+  if (base_u64 == 0) {\n+    return KlassDecodeZero;\n+  }\n+\n+  if ((base_u64 & (KlassEncodingMetaspaceMax - 1)) == 0) {\n+    return KlassDecodeXor;\n+  }\n+\n+  \/\/ Note that there is no point in optimizing for shift=3 since lilliput\n+  \/\/ will use larger shifts\n+\n+  \/\/ The add+shift mode for decode_and_move_klass_not_null() requires the base to be\n+  \/\/  shiftable-without-loss. So, this is the minimum restriction on x64 for a valid\n+  \/\/  encoding base. This does not matter in reality since the shift values we use for\n+  \/\/  Lilliput, while large, won't be larger than a page size. And the encoding base\n+  \/\/  will be quite likely page aligned since it usually falls to the beginning of\n+  \/\/  either CDS or CCS.\n+  if ((base_u64 & (KlassAlignmentInBytes - 1)) == 0) {\n+    return KlassDecodeAdd;\n+  }\n+\n+  return KlassDecodeNone;\n+}\n+\n@@ -5471,1 +5607,12 @@\n-  if (CompressedKlassPointers::base() != nullptr) {\n+  switch (klass_decode_mode()) {\n+  case KlassDecodeZero: {\n+    shrq(r, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  case KlassDecodeXor: {\n+    mov64(tmp, (int64_t)CompressedKlassPointers::base());\n+    xorq(r, tmp);\n+    shrq(r, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  case KlassDecodeAdd: {\n@@ -5474,0 +5621,2 @@\n+    shrq(r, CompressedKlassPointers::shift());\n+    break;\n@@ -5475,3 +5624,2 @@\n-  if (CompressedKlassPointers::shift() != 0) {\n-    assert (LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-    shrq(r, LogKlassAlignmentInBytes);\n+  default:\n+    ShouldNotReachHere();\n@@ -5483,1 +5631,13 @@\n-  if (CompressedKlassPointers::base() != nullptr) {\n+  switch (klass_decode_mode()) {\n+  case KlassDecodeZero: {\n+    movptr(dst, src);\n+    shrq(dst, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  case KlassDecodeXor: {\n+    mov64(dst, (int64_t)CompressedKlassPointers::base());\n+    xorq(dst, src);\n+    shrq(dst, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  case KlassDecodeAdd: {\n@@ -5486,2 +5646,2 @@\n-  } else {\n-    movptr(dst, src);\n+    shrq(dst, CompressedKlassPointers::shift());\n+    break;\n@@ -5489,3 +5649,2 @@\n-  if (CompressedKlassPointers::shift() != 0) {\n-    assert (LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-    shrq(dst, LogKlassAlignmentInBytes);\n+  default:\n+    ShouldNotReachHere();\n@@ -5497,8 +5656,5 @@\n-  \/\/ Note: it will change flags\n-  assert(UseCompressedClassPointers, \"should only be used for compressed headers\");\n-  \/\/ Cannot assert, unverified entry point counts instructions (see .ad file)\n-  \/\/ vtableStubs also counts instructions in pd_code_size_limit.\n-  \/\/ Also do not verify_oop as this is called by verify_oop.\n-  if (CompressedKlassPointers::shift() != 0) {\n-    assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-    shlq(r, LogKlassAlignmentInBytes);\n+  const uint64_t base_u64 = (uint64_t)CompressedKlassPointers::base();\n+  switch (klass_decode_mode()) {\n+  case KlassDecodeZero: {\n+    shlq(r, CompressedKlassPointers::shift());\n+    break;\n@@ -5506,2 +5662,11 @@\n-  if (CompressedKlassPointers::base() != nullptr) {\n-    mov64(tmp, (int64_t)CompressedKlassPointers::base());\n+  case KlassDecodeXor: {\n+    assert((base_u64 & (KlassEncodingMetaspaceMax - 1)) == 0,\n+           \"base \" UINT64_FORMAT_X \" invalid for xor mode\", base_u64); \/\/ should have been handled at VM init.\n+    shlq(r, CompressedKlassPointers::shift());\n+    mov64(tmp, base_u64);\n+    xorq(r, tmp);\n+    break;\n+  }\n+  case KlassDecodeAdd: {\n+    shlq(r, CompressedKlassPointers::shift());\n+    mov64(tmp, base_u64);\n@@ -5509,0 +5674,4 @@\n+    break;\n+  }\n+  default:\n+    ShouldNotReachHere();\n@@ -5514,3 +5683,1 @@\n-  \/\/ Note: it will change flags\n-  assert (UseCompressedClassPointers, \"should only be used for compressed headers\");\n-  \/\/ Cannot assert, unverified entry point counts instructions (see .ad file)\n+  \/\/ Note: Cannot assert, unverified entry point counts instructions (see .ad file)\n@@ -5520,18 +5687,28 @@\n-  if (CompressedKlassPointers::base() == nullptr &&\n-      CompressedKlassPointers::shift() == 0) {\n-    \/\/ The best case scenario is that there is no base or shift. Then it is already\n-    \/\/ a pointer that needs nothing but a register rename.\n-    movl(dst, src);\n-  } else {\n-    if (CompressedKlassPointers::base() != nullptr) {\n-      mov64(dst, (int64_t)CompressedKlassPointers::base());\n-    } else {\n-      xorq(dst, dst);\n-    }\n-    if (CompressedKlassPointers::shift() != 0) {\n-      assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-      assert(LogKlassAlignmentInBytes == Address::times_8, \"klass not aligned on 64bits?\");\n-      leaq(dst, Address(dst, src, Address::times_8, 0));\n-    } else {\n-      addq(dst, src);\n-    }\n+  const uint64_t base_u64 = (uint64_t)CompressedKlassPointers::base();\n+\n+  switch (klass_decode_mode()) {\n+  case KlassDecodeZero: {\n+    movq(dst, src);\n+    shlq(dst, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  case KlassDecodeXor: {\n+    assert((base_u64 & (KlassEncodingMetaspaceMax - 1)) == 0,\n+           \"base \" UINT64_FORMAT_X \" invalid for xor mode\", base_u64); \/\/ should have been handled at VM init.\n+    const uint64_t base_right_shifted = base_u64 >> CompressedKlassPointers::shift();\n+    mov64(dst, base_right_shifted);\n+    xorq(dst, src);\n+    shlq(dst, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  case KlassDecodeAdd: {\n+    assert((base_u64 & (KlassAlignmentInBytes - 1)) == 0,\n+           \"base \" UINT64_FORMAT_X \" invalid for add mode\", base_u64); \/\/ should have been handled at VM init.\n+    const uint64_t base_right_shifted = base_u64 >> CompressedKlassPointers::shift();\n+    mov64(dst, base_right_shifted);\n+    addq(dst, src);\n+    shlq(dst, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  default:\n+    ShouldNotReachHere();\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":222,"deletions":45,"binary":false,"changes":267,"status":"modified"},{"patch":"@@ -82,0 +82,22 @@\n+ public:\n+\n+  enum KlassDecodeMode {\n+    KlassDecodeNone,\n+    KlassDecodeZero,\n+    KlassDecodeXor,\n+    KlassDecodeAdd\n+  };\n+\n+  \/\/ Return the current narrow Klass pointer decode mode. Initialized on first call.\n+  static KlassDecodeMode klass_decode_mode();\n+\n+  \/\/ Given an arbitrary base address, return the KlassDecodeMode that would be used. Return KlassDecodeNone\n+  \/\/ if base address is not valid for encoding.\n+  static KlassDecodeMode klass_decode_mode_for_base(address base);\n+\n+  \/\/ Returns a static string\n+  static const char* describe_klass_decode_mode(KlassDecodeMode mode);\n+\n+ private:\n+  static KlassDecodeMode _klass_decode_mode;\n+\n@@ -366,0 +388,3 @@\n+#ifdef _LP64\n+  void load_nklass(Register dst, Register src);\n+#endif\n@@ -369,0 +394,8 @@\n+  \/\/ Compares the Klass pointer of an object to a given Klass (which might be narrow,\n+  \/\/ depending on UseCompressedClassPointers).\n+  void cmp_klass(Register klass, Register dst, Register tmp);\n+\n+  \/\/ Compares the Klass pointer of two objects o1 and o2. Result is in the condition flags.\n+  \/\/ Uses t1 and t2 as temporary registers.\n+  void cmp_klass(Register src, Register dst, Register tmp1, Register tmp2);\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":33,"deletions":0,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -71,2 +71,7 @@\n-  __ shrptr(result, markWord::hash_shift);\n-  __ andptr(result, markWord::hash_mask);\n+  if (UseCompactObjectHeaders) {\n+    __ shrptr(result, markWord::hash_shift_compact);\n+    __ andptr(result, markWord::hash_mask_compact);\n+  } else {\n+    __ shrptr(result, markWord::hash_shift);\n+    __ andptr(result, markWord::hash_mask);\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86.cpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -4036,1 +4036,2 @@\n-    __ decrement(rdx, sizeof(oopDesc));\n+    int header_size = align_up(oopDesc::base_offset_in_bytes(), BytesPerLong);\n+    __ decrement(rdx, header_size);\n@@ -4058,2 +4059,2 @@\n-    __ movptr(Address(rax, rdx, Address::times_8, sizeof(oopDesc) - 1*oopSize), rcx);\n-    NOT_LP64(__ movptr(Address(rax, rdx, Address::times_8, sizeof(oopDesc) - 2*oopSize), rcx));\n+    __ movptr(Address(rax, rdx, Address::times_8, header_size - 1*oopSize), rcx);\n+    NOT_LP64(__ movptr(Address(rax, rdx, Address::times_8, header_size - 2*oopSize), rcx));\n@@ -4066,3 +4067,8 @@\n-    __ movptr(Address(rax, oopDesc::mark_offset_in_bytes()),\n-              (intptr_t)markWord::prototype().value()); \/\/ header\n-    __ pop(rcx);   \/\/ get saved klass back in the register.\n+    if (UseCompactObjectHeaders) {\n+      __ pop(rcx);   \/\/ get saved klass back in the register.\n+      __ movptr(rbx, Address(rcx, Klass::prototype_header_offset()));\n+      __ movptr(Address(rax, oopDesc::mark_offset_in_bytes ()), rbx);\n+    } else {\n+      __ movptr(Address(rax, oopDesc::mark_offset_in_bytes()),\n+                (intptr_t)markWord::prototype().value()); \/\/ header\n+      __ pop(rcx);   \/\/ get saved klass back in the register.\n@@ -4070,2 +4076,2 @@\n-    __ xorl(rsi, rsi); \/\/ use zero reg to clear memory (shorter code)\n-    __ store_klass_gap(rax, rsi);  \/\/ zero klass gap for compressed oops\n+      __ xorl(rsi, rsi); \/\/ use zero reg to clear memory (shorter code)\n+      __ store_klass_gap(rax, rsi);  \/\/ zero klass gap for compressed oops\n@@ -4073,1 +4079,2 @@\n-    __ store_klass(rax, rcx, rscratch1);  \/\/ klass\n+      __ store_klass(rax, rcx, rscratch1);  \/\/ klass\n+    }\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.cpp","additions":16,"deletions":9,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -5317,0 +5317,1 @@\n+  predicate(!UseCompactObjectHeaders);\n@@ -5327,0 +5328,23 @@\n+instruct loadNKlassLilliput(rRegN dst, indOffset8 mem, rFlagsReg cr)\n+%{\n+  predicate(UseCompactObjectHeaders);\n+  match(Set dst (LoadNKlass mem));\n+  effect(TEMP_DEF dst, KILL cr);\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movl    $dst, $mem\\t# compressed klass ptr\" %}\n+  ins_encode %{\n+    assert($mem$$disp == oopDesc::klass_offset_in_bytes(), \"expect correct offset 4, but got: %d\", $mem$$disp);\n+    assert($mem$$index == 4, \"expect no index register: %d\", $mem$$index);\n+    Register dst = $dst$$Register;\n+    Register obj = $mem$$base$$Register;\n+    C2LoadNKlassStub* stub = new (Compile::current()->comp_arena()) C2LoadNKlassStub(dst);\n+    Compile::current()->output()->add_stub(stub);\n+    __ movq(dst, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    __ testb(dst, markWord::monitor_value);\n+    __ jcc(Assembler::notZero, stub->entry());\n+    __ bind(stub->continuation());\n+    __ shrq(dst, markWord::klass_shift);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n@@ -12645,0 +12669,1 @@\n+  predicate(!UseCompactObjectHeaders);\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":25,"deletions":0,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -584,0 +584,18 @@\n+class LoadKlassStub: public CodeStub {\n+private:\n+  LIR_Opr          _result;\n+\n+public:\n+  LoadKlassStub(LIR_Opr result) :\n+    CodeStub(), _result(result) {};\n+\n+  virtual void emit_code(LIR_Assembler* e);\n+  virtual void visit(LIR_OpVisitState* visitor) {\n+    visitor->do_temp(_result);\n+    visitor->do_output(_result);\n+  }\n+#ifndef PRODUCT\n+virtual void print_name(outputStream* out) const { out->print(\"LoadKlassStub\"); }\n+#endif \/\/ PRODUCT\n+};\n+\n","filename":"src\/hotspot\/share\/c1\/c1_CodeStubs.hpp","additions":18,"deletions":0,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -175,0 +175,1 @@\n+\n","filename":"src\/hotspot\/share\/c1\/c1_Compilation.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -893,0 +893,1 @@\n+      if (opLoadKlass->_stub) do_stub(opLoadKlass->_stub);\n@@ -1073,0 +1074,3 @@\n+  if (stub()) {\n+    masm->append_code_stub(stub());\n+  }\n@@ -2049,0 +2053,3 @@\n+  if (stub()) {\n+    out->print(\"[lbl:\" INTPTR_FORMAT \"]\", p2i(stub()->entry()));\n+  }\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -1906,0 +1906,1 @@\n+  CodeStub* _stub;\n@@ -1907,1 +1908,1 @@\n-  LIR_OpLoadKlass(LIR_Opr obj, LIR_Opr result, CodeEmitInfo* info)\n+  LIR_OpLoadKlass(LIR_Opr obj, LIR_Opr result, CodeEmitInfo* info, CodeStub* stub)\n@@ -1910,1 +1911,1 @@\n-    {}\n+    , _stub(stub) {}\n@@ -1913,0 +1914,1 @@\n+  CodeStub* stub()     const { return _stub; }\n@@ -2378,1 +2380,1 @@\n-  void load_klass(LIR_Opr obj, LIR_Opr result, CodeEmitInfo* info) { append(new LIR_OpLoadKlass(obj, result, info)); }\n+  void load_klass(LIR_Opr obj, LIR_Opr result, CodeEmitInfo* info, CodeStub* stub) { append(new LIR_OpLoadKlass(obj, result, info, stub)); }\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.hpp","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1244,1 +1244,2 @@\n-  __ load_klass(obj, klass, null_check_info);\n+  CodeStub* slow_path = UseCompactObjectHeaders ? new LoadKlassStub(klass) : nullptr;\n+  __ load_klass(obj, klass, null_check_info, slow_path);\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n@@ -44,0 +45,1 @@\n+#include \"oops\/klass.inline.hpp\"\n@@ -213,2 +215,4 @@\n-    \/\/ See RunTimeClassInfo::get_for()\n-    _estimated_metaspaceobj_bytes += align_up(BytesPerWord, SharedSpaceObjectAlignment);\n+    \/\/ See ArchiveBuilder::make_shallow_copies: make sure we have enough space for both maximum\n+    \/\/ Klass alignment as well as the RuntimeInfo* pointer we will embed in front of a Klass.\n+    _estimated_metaspaceobj_bytes += align_up(BytesPerWord, KlassAlignmentInBytes) +\n+        align_up(sizeof(void*), SharedSpaceObjectAlignment);\n@@ -614,0 +618,3 @@\n+    dest = dump_region->allocate(bytes, KlassAlignmentInBytes);\n+  } else {\n+    dest = dump_region->allocate(bytes);\n@@ -615,1 +622,0 @@\n-  dest = dump_region->allocate(bytes);\n@@ -634,1 +640,2 @@\n-  log_trace(cds)(\"Copy: \" PTR_FORMAT \" ==> \" PTR_FORMAT \" %d\", p2i(src), p2i(dest), bytes);\n+  log_trace(cds)(\"Copy: \" PTR_FORMAT \" ==> \" PTR_FORMAT \" %d (%s)\", p2i(src), p2i(dest), bytes,\n+                 MetaspaceObj::type_name(src_info->msotype()));\n@@ -638,0 +645,2 @@\n+\n+  DEBUG_ONLY(_alloc_stats.verify((int)dump_region->used(), src_info->read_only()));\n@@ -697,0 +706,7 @@\n+#ifdef _LP64\n+    if (UseCompactObjectHeaders) {\n+      Klass* requested_k = to_requested(k);\n+      narrowKlass nk = CompressedKlassPointers::encode_not_null(requested_k, _requested_static_archive_bottom);\n+      k->set_prototype_header(markWord::prototype().set_narrow_klass(nk));\n+    }\n+#endif \/\/_LP64\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.cpp","additions":20,"deletions":4,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"memory\/metaspace\/metaspaceAlignment.hpp\"\n@@ -46,3 +47,13 @@\n-\/\/ Metaspace::allocate() requires that all blocks must be aligned with KlassAlignmentInBytes.\n-\/\/ We enforce the same alignment rule in blocks allocated from the shared space.\n-const int SharedSpaceObjectAlignment = KlassAlignmentInBytes;\n+\/\/ CDS has three alignments to deal with:\n+\/\/ - SharedSpaceObjectAlignment, always 8 bytes: used for placing arbitrary structures.\n+\/\/   These may contain 64-bit members (not larger, we know that much). Therefore we\n+\/\/   need to use 64-bit alignment on both 32-bit and 64-bit platforms. We reuse metaspace\n+\/\/   minimal alignment for this, which follows the same logic.\n+\/\/ - With CompressedClassPointers=1, we need to store Klass structures with a large\n+\/\/   alignment (Lilliput specific narrow Klass pointer encoding) - KlassAlignmentInBytes.\n+\/\/ - Header data and tags are squeezed in with word alignment, which happens to be 4 bytes\n+\/\/   on 32-bit. See ReadClosure::do_xxx() and DumpRegion::append_intptr().\n+const int SharedSpaceObjectAlignment = metaspace::MetaspaceMinAlignmentBytes;\n+\n+\/\/ standard alignment should be sufficient for storing 64-bit values.\n+STATIC_ASSERT(SharedSpaceObjectAlignment >= sizeof(uint64_t));\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.hpp","additions":14,"deletions":3,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -209,3 +209,13 @@\n-char* DumpRegion::allocate(size_t num_bytes) {\n-  char* p = (char*)align_up(_top, (size_t)SharedSpaceObjectAlignment);\n-  char* newtop = p + align_up(num_bytes, (size_t)SharedSpaceObjectAlignment);\n+char* DumpRegion::allocate(size_t num_bytes, size_t alignment) {\n+  \/\/ We align the starting address of each allocation.\n+  char* p = (char*)align_up(_top, alignment);\n+  char* newtop = p + num_bytes;\n+  \/\/ Leave _top always SharedSpaceObjectAlignment aligned. But not more -\n+  \/\/  if we allocate with large alignments, lets not waste the gaps.\n+  \/\/ Ideally we would not need to align _top to anything here but CDS has\n+  \/\/  a number of implicit alignment assumptions. Leaving this unaligned\n+  \/\/  here will trip of at least ReadClosure (assuming word alignment) and\n+  \/\/  DumpAllocStats (will get confused about counting bytes on 32-bit\n+  \/\/  platforms if we align to anything less than SharedSpaceObjectAlignment\n+  \/\/  here).\n+  newtop = align_up(newtop, SharedSpaceObjectAlignment);\n@@ -213,1 +223,1 @@\n-  memset(p, 0, newtop - p);\n+  memset(p, 0, newtop - p); \/\/ todo: needed? debug_only?\n@@ -217,0 +227,4 @@\n+char* DumpRegion::allocate(size_t num_bytes) {\n+  return allocate(num_bytes, SharedSpaceObjectAlignment);\n+}\n+\n@@ -333,1 +347,1 @@\n-  assert(tag == old_tag, \"old tag doesn't match\");\n+  assert(tag == old_tag, \"tag doesn't match (%d, expected %d)\", old_tag, tag);\n","filename":"src\/hotspot\/share\/cds\/archiveUtils.cpp","additions":19,"deletions":5,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -149,0 +149,1 @@\n+  \/\/ Allocate with default alignment (SharedSpaceObjectAlignment)\n@@ -150,0 +151,2 @@\n+  \/\/ Allocate with an arbitrary alignment.\n+  char* allocate(size_t num_bytes, size_t alignment);\n","filename":"src\/hotspot\/share\/cds\/archiveUtils.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -59,0 +59,1 @@\n+#include \"runtime\/safepoint.hpp\"\n","filename":"src\/hotspot\/share\/cds\/heapShared.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -61,0 +61,1 @@\n+#include \"oops\/compressedKlass.hpp\"\n@@ -1313,0 +1314,5 @@\n+#ifdef ASSERT\n+    if (UseCompressedClassPointers) {\n+      CompressedKlassPointers::verify_klass_pointer(record->_klass);\n+    }\n+#endif\n@@ -1314,1 +1320,0 @@\n-    assert(check_alignment(record->_klass), \"Address not aligned\");\n","filename":"src\/hotspot\/share\/classfile\/systemDictionaryShared.cpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -79,0 +79,1 @@\n+#include \"gc\/shared\/gcForwarding.hpp\"\n@@ -1520,0 +1521,2 @@\n+  GCForwarding::initialize(heap_rs.region(), HeapRegion::GrainWords);\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -210,1 +210,1 @@\n-    obj = cast_to_oop(m.decode_pointer());\n+    obj = obj->forwardee(m);\n@@ -224,1 +224,0 @@\n-  assert(from_obj->is_objArray(), \"must be obj array\");\n@@ -254,1 +253,0 @@\n-  assert(from_obj->is_objArray(), \"precondition\");\n@@ -381,1 +379,1 @@\n-                                                  oop const old, size_t word_sz, uint age,\n+                                                  oop const old, Klass* klass, size_t word_sz, uint age,\n@@ -385,1 +383,1 @@\n-    _g1h->gc_tracer_stw()->report_promotion_in_new_plab_event(old->klass(), word_sz * HeapWordSize, age,\n+    _g1h->gc_tracer_stw()->report_promotion_in_new_plab_event(klass, word_sz * HeapWordSize, age,\n@@ -389,1 +387,1 @@\n-    _g1h->gc_tracer_stw()->report_promotion_outside_plab_event(old->klass(), word_sz * HeapWordSize, age,\n+    _g1h->gc_tracer_stw()->report_promotion_outside_plab_event(klass, word_sz * HeapWordSize, age,\n@@ -397,0 +395,1 @@\n+                                                   Klass* klass,\n@@ -419,1 +418,1 @@\n-      report_promotion_event(*dest_attr, old, word_sz, age, obj_ptr, node_index);\n+      report_promotion_event(*dest_attr, old, klass, word_sz, age, obj_ptr, node_index);\n@@ -454,0 +453,4 @@\n+  if (old_mark.is_marked()) {\n+    \/\/ Already forwarded by somebody else, return forwardee.\n+    return old->forwardee(old_mark);\n+  }\n@@ -456,1 +459,9 @@\n-  Klass* klass = old->klass();\n+  Klass* klass;\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    klass = old_mark.safe_klass();\n+  } else\n+#endif\n+  {\n+    klass = old->klass();\n+  }\n@@ -469,1 +480,1 @@\n-    obj_ptr = allocate_copy_slow(&dest_attr, old, word_sz, age, node_index);\n+    obj_ptr = allocate_copy_slow(&dest_attr, old, klass, word_sz, age, node_index);\n@@ -623,1 +634,1 @@\n-  oop forward_ptr = old->forward_to_atomic(old, m, memory_order_relaxed);\n+  oop forward_ptr = old->forward_to_self_atomic(m, memory_order_relaxed);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.cpp","additions":21,"deletions":10,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -165,0 +165,1 @@\n+                               Klass* klass,\n@@ -199,1 +200,1 @@\n-                              oop const old, size_t word_sz, uint age,\n+                              oop const old, Klass* klass, size_t word_sz, uint age,\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -846,1 +846,10 @@\n-    from()->set_next_compaction_space(to());\n+    \/\/ Ensure that compaction spaces are in address-order.\n+    if (from()->bottom() < to()->bottom()) {\n+      eden()->set_next_compaction_space(from());\n+      from()->set_next_compaction_space(to());\n+      to()->set_next_compaction_space(nullptr);\n+    } else {\n+      eden()->set_next_compaction_space(to());\n+      to()->set_next_compaction_space(from());\n+      from()->set_next_compaction_space(nullptr);\n+    }\n@@ -881,1 +890,15 @@\n-        obj->init_mark();\n+#ifdef _LP64\n+        if (UseCompactObjectHeaders) {\n+          oop forwardee = obj->forwardee();\n+          markWord header = forwardee->mark();\n+          if (header.has_displaced_mark_helper()) {\n+            header = header.displaced_mark_helper();\n+          }\n+          assert(UseCompressedClassPointers, \"assume +UseCompressedClassPointers\");\n+          narrowKlass nklass = header.narrow_klass();\n+          obj->set_mark(markWord::prototype().set_narrow_klass(nklass));\n+        } else\n+#endif\n+        {\n+          obj->init_mark();\n+        }\n@@ -905,1 +928,1 @@\n-  old->forward_to(old);\n+  old->forward_to_self();\n","filename":"src\/hotspot\/share\/gc\/serial\/defNewGeneration.cpp","additions":26,"deletions":3,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"gc\/shared\/gcForwarding.hpp\"\n@@ -91,0 +92,2 @@\n+  GCForwarding::begin();\n+\n@@ -103,0 +106,2 @@\n+  GCForwarding::end();\n+\n@@ -164,0 +169,3 @@\n+  _young_marked_objects = 0;\n+  _old_marked_objects = 0;\n+\n","filename":"src\/hotspot\/share\/gc\/serial\/genMarkSweep.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/shared\/genCollectedHeap.hpp\"\n@@ -167,0 +168,2 @@\n+  ContinuationGCSupport::transform_stack_chunk(obj);\n+\n@@ -170,3 +173,13 @@\n-  obj->set_mark(markWord::prototype().set_marked());\n-\n-  ContinuationGCSupport::transform_stack_chunk(obj);\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    markWord real_mark = mark;\n+    if (real_mark.has_displaced_mark_helper()) {\n+      real_mark = real_mark.displaced_mark_helper();\n+    }\n+    Klass* klass = real_mark.klass();\n+    obj->set_mark(klass->prototype_header().set_marked());\n+  } else\n+#endif\n+  {\n+    obj->set_mark(markWord::prototype().set_marked());\n+  }\n@@ -177,0 +190,6 @@\n+\n+  if (GenCollectedHeap::heap()->is_in_young(obj)) {\n+    _young_marked_objects++;\n+  } else {\n+    _old_marked_objects++;\n+  }\n@@ -225,0 +244,3 @@\n+size_t MarkSweep::_young_marked_objects = 0;\n+size_t MarkSweep::_old_marked_objects = 0;\n+\n","filename":"src\/hotspot\/share\/gc\/serial\/markSweep.cpp","additions":25,"deletions":3,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -119,1 +119,4 @@\n- public:\n+  static size_t _young_marked_objects;\n+  static size_t _old_marked_objects;\n+\n+public:\n","filename":"src\/hotspot\/share\/gc\/serial\/markSweep.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -231,1 +231,1 @@\n-  if (!Metaspace::contains(object->klass_raw())) {\n+  if (!UseCompactObjectHeaders && !Metaspace::contains(object->klass_raw())) {\n@@ -261,2 +261,4 @@\n-  _filler_array_max_size = align_object_size(filler_array_hdr_size() +\n-                                             max_len \/ elements_per_word);\n+  int header_size_in_bytes = arrayOopDesc::base_offset_in_bytes(T_INT);\n+  assert(header_size_in_bytes % sizeof(jint) == 0, \"must be aligned to int\");\n+  int header_size_in_ints = header_size_in_bytes \/ sizeof(jint);\n+  _filler_array_max_size = align_object_size((header_size_in_ints + max_len) \/ elements_per_word);\n@@ -412,1 +414,3 @@\n-  size_t max_int_size = typeArrayOopDesc::header_size(T_INT) +\n+  int header_size_in_bytes = typeArrayOopDesc::base_offset_in_bytes(T_INT);\n+  assert(header_size_in_bytes % sizeof(jint) == 0, \"header size must align to int\");\n+  size_t max_int_size = header_size_in_bytes \/ HeapWordSize +\n@@ -418,5 +422,2 @@\n-size_t CollectedHeap::filler_array_hdr_size() {\n-  return align_object_offset(arrayOopDesc::header_size(T_INT)); \/\/ align to Long\n-}\n-\n-  return align_object_size(filler_array_hdr_size()); \/\/ align to MinObjAlignment\n+  int aligned_header_size_words = align_up(arrayOopDesc::base_offset_in_bytes(T_INT), HeapWordSize) \/ HeapWordSize;\n+  return align_object_size(aligned_header_size_words); \/\/ align to MinObjAlignment\n@@ -427,2 +428,3 @@\n-  Copy::fill_to_words(start + filler_array_hdr_size(),\n-                      words - filler_array_hdr_size(), value);\n+  int payload_start = align_up(arrayOopDesc::base_offset_in_bytes(T_INT), HeapWordSize) \/ HeapWordSize;\n+  Copy::fill_to_words(start + payload_start,\n+                      words - payload_start, value);\n@@ -452,2 +454,3 @@\n-  const size_t payload_size = words - filler_array_hdr_size();\n-  const size_t len = payload_size * HeapWordSize \/ sizeof(jint);\n+  const size_t payload_size_bytes = words * HeapWordSize - arrayOopDesc::base_offset_in_bytes(T_INT);\n+  assert(payload_size_bytes % sizeof(jint) == 0, \"must be int aligned\");\n+  const size_t len = payload_size_bytes \/ sizeof(jint);\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.cpp","additions":16,"deletions":13,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -167,1 +167,0 @@\n-  static inline size_t filler_array_hdr_size();\n@@ -311,1 +310,1 @@\n-  static constexpr size_t min_dummy_object_size() {\n+  static size_t min_dummy_object_size() {\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+#include \"gc\/shared\/gcForwarding.hpp\"\n@@ -134,0 +135,2 @@\n+  GCForwarding::initialize(_reserved, SpaceAlignment);\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/genCollectedHeap.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -383,1 +383,3 @@\n-  oopDesc::set_klass_gap(mem, 0);\n+  if (!UseCompactObjectHeaders) {\n+    oopDesc::set_klass_gap(mem, 0);\n+  }\n@@ -389,2 +391,0 @@\n-  \/\/ May be bootstrapping\n-  oopDesc::set_mark(mem, markWord::prototype());\n@@ -394,1 +394,6 @@\n-  oopDesc::release_set_klass(mem, _klass);\n+  if (UseCompactObjectHeaders) {\n+    oopDesc::release_set_mark(mem, _klass->prototype_header());\n+  } else {\n+    oopDesc::set_mark(mem, markWord::prototype());\n+    oopDesc::release_set_klass(mem, _klass);\n+  }\n@@ -408,1 +413,1 @@\n-  const size_t hs = arrayOopDesc::header_size(array_klass->element_type());\n+  const size_t hs = align_up(arrayOopDesc::base_offset_in_bytes(array_klass->element_type()), HeapWordSize) \/ HeapWordSize;\n","filename":"src\/hotspot\/share\/gc\/shared\/memAllocator.cpp","additions":10,"deletions":5,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"gc\/shared\/gcForwarding.inline.hpp\"\n@@ -45,2 +46,2 @@\n-  if (obj->is_forwarded()) {\n-    elem->set_oop(obj->forwardee());\n+  if (GCForwarding::is_forwarded(obj)) {\n+    elem->set_oop(GCForwarding::forwardee(obj));\n","filename":"src\/hotspot\/share\/gc\/shared\/preservedMarks.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"gc\/shared\/gcForwarding.hpp\"\n@@ -406,0 +407,2 @@\n+  GCForwarding::initialize(_heap_region, ShenandoahHeapRegion::region_size_words());\n+\n@@ -953,1 +956,1 @@\n-    if (!p->is_forwarded()) {\n+    if (!ShenandoahForwarding::is_forwarded(p)) {\n@@ -1298,0 +1301,1 @@\n+    shenandoah_assert_not_in_cset_except(NULL, obj, cancelled_gc());\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/shenandoah\/shenandoahObjectUtils.inline.hpp\"\n@@ -101,1 +102,1 @@\n-      if (is_instance_ref_klass(obj->klass())) {\n+      if (is_instance_ref_klass(ShenandoahObjectUtils::klass(obj))) {\n@@ -128,1 +129,1 @@\n-    Klass* obj_klass = obj->klass_or_null();\n+    Klass* obj_klass = ShenandoahObjectUtils::klass(obj);\n@@ -143,1 +144,1 @@\n-        check(ShenandoahAsserts::_safe_unknown, obj, (obj_addr + obj->size()) <= obj_reg->top(),\n+        check(ShenandoahAsserts::_safe_unknown, obj, (obj_addr + ShenandoahObjectUtils::size(obj)) <= obj_reg->top(),\n@@ -147,1 +148,1 @@\n-        size_t humongous_end = humongous_start + (obj->size() >> ShenandoahHeapRegion::region_size_words_shift());\n+        size_t humongous_end = humongous_start + (ShenandoahObjectUtils::size(obj) >> ShenandoahHeapRegion::region_size_words_shift());\n@@ -164,1 +165,1 @@\n-          Atomic::add(&_ld[obj_reg->index()], (uint) obj->size(), memory_order_relaxed);\n+          Atomic::add(&_ld[obj_reg->index()], (uint) ShenandoahObjectUtils::size(obj), memory_order_relaxed);\n@@ -205,1 +206,1 @@\n-      check(ShenandoahAsserts::_safe_oop, obj, (fwd_addr + fwd->size()) <= fwd_reg->top(),\n+      check(ShenandoahAsserts::_safe_oop, obj, (fwd_addr + ShenandoahObjectUtils::size(fwd)) <= fwd_reg->top(),\n@@ -308,1 +309,2 @@\n-    obj->oop_iterate(this);\n+    Klass* klass = ShenandoahObjectUtils::klass(obj);\n+    obj->oop_iterate_backwards(this, klass);\n@@ -588,1 +590,1 @@\n-    if (!is_instance_ref_klass(obj->klass())) {\n+    if (!is_instance_ref_klass(ShenandoahObjectUtils::klass(obj))) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.cpp","additions":10,"deletions":8,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -301,1 +301,1 @@\n-        assert(!UseCompressedClassPointers, \"should only happen without compressed class pointers\");\n+        assert(!UseCompressedClassPointers || UseCompactObjectHeaders, \"should only happen without compressed class pointers or with compact object headers\");\n","filename":"src\/hotspot\/share\/gc\/x\/c2\/xBarrierSetC2.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -53,2 +53,9 @@\n-  const size_t header = arrayOopDesc::header_size(element_type);\n-  const size_t payload_size = _word_size - header;\n+int base_offset = arrayOopDesc::base_offset_in_bytes(element_type);\n+\n+  \/\/ Clear leading 32 bit, if necessary.\n+  if (!is_aligned(base_offset, HeapWordSize)) {\n+    assert(is_aligned(base_offset, BytesPerInt), \"array base must be 32 bit aligned\");\n+    *reinterpret_cast<jint*>(reinterpret_cast<char*>(mem) + base_offset) = 0;\n+    base_offset += BytesPerInt;\n+  }\n+  assert(is_aligned(base_offset, HeapWordSize), \"remaining array base must be 64 bit aligned\");\n@@ -56,0 +63,2 @@\n+  const size_t header = heap_word_size(base_offset);\n+  const size_t payload_size = _word_size - header;\n@@ -66,2 +75,6 @@\n-  arrayOopDesc::set_mark(mem, markWord::prototype());\n-  arrayOopDesc::release_set_klass(mem, _klass);\n+  if (UseCompactObjectHeaders) {\n+    oopDesc::release_set_mark(mem, _klass->prototype_header());\n+  } else {\n+    arrayOopDesc::set_mark(mem, markWord::prototype());\n+    arrayOopDesc::release_set_klass(mem, _klass);\n+  }\n","filename":"src\/hotspot\/share\/gc\/x\/xObjArrayAllocator.cpp","additions":17,"deletions":4,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -483,1 +483,1 @@\n-        assert(!UseCompressedClassPointers, \"should only happen without compressed class pointers\");\n+        assert(!UseCompressedClassPointers || UseCompactObjectHeaders, \"should only happen without compressed class pointers or with compact object headers\");\n","filename":"src\/hotspot\/share\/gc\/z\/c2\/zBarrierSetC2.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -53,2 +53,9 @@\n-  const size_t header = arrayOopDesc::header_size(element_type);\n-  const size_t payload_size = _word_size - header;\n+  int base_offset = arrayOopDesc::base_offset_in_bytes(element_type);\n+\n+  \/\/ Clear leading 32 bit, if necessary.\n+  if (!is_aligned(base_offset, HeapWordSize)) {\n+    assert(is_aligned(base_offset, BytesPerInt), \"array base must be 32 bit aligned\");\n+    *reinterpret_cast<jint*>(reinterpret_cast<char*>(mem) + base_offset) = 0;\n+    base_offset += BytesPerInt;\n+  }\n+  assert(is_aligned(base_offset, HeapWordSize), \"remaining array base must be 64 bit aligned\");\n@@ -56,0 +63,2 @@\n+  const size_t header = heap_word_size(base_offset);\n+  const size_t payload_size = _word_size - header;\n@@ -69,2 +78,6 @@\n-  arrayOopDesc::set_mark(mem, markWord::prototype().set_marked());\n-  arrayOopDesc::release_set_klass(mem, _klass);\n+  if (UseCompactObjectHeaders) {\n+    oopDesc::release_set_mark(mem, _klass->prototype_header().set_marked());\n+  } else {\n+    arrayOopDesc::set_mark(mem, markWord::prototype().set_marked());\n+    arrayOopDesc::release_set_klass(mem, _klass);\n+  }\n","filename":"src\/hotspot\/share\/gc\/z\/zObjArrayAllocator.cpp","additions":17,"deletions":4,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -897,0 +897,4 @@\n+\n+    if (SuspendibleThreadSet::should_yield()) {\n+      SuspendibleThreadSet::yield();\n+    }\n@@ -1097,0 +1101,1 @@\n+    SuspendibleThreadSetJoiner sts_joiner;\n","filename":"src\/hotspot\/share\/gc\/z\/zRelocate.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2003,0 +2003,3 @@\n+#ifdef _LP64\n+              oopDesc::release_set_mark(result, ik->prototype_header());\n+#else\n@@ -2004,2 +2007,1 @@\n-              oopDesc::set_klass_gap(result, 0);\n-\n+#endif\n","filename":"src\/hotspot\/share\/interpreter\/zero\/bytecodeInterpreter.cpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2376,1 +2376,1 @@\n-  return arrayOopDesc::header_size(type) * HeapWordSize;\n+  return arrayOopDesc::base_offset_in_bytes(type);\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -296,1 +296,0 @@\n-  volatile_nonstatic_field(oopDesc,            _metadata._klass,                              Klass*)                                \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"memory\/metaspace\/metaspaceAlignment.hpp\"\n@@ -591,0 +592,7 @@\n+  \/\/ Note: code below is broken and needs rethinking since it confuses encoding base\n+  \/\/ with compressed class space attach address; both don't have be the same.\n+  \/\/ That is also the reason why we atm don't get zero-based encoding for aarch.\n+  \/\/ Comment is also wrong, at least for 9-bit shift.\n+\n+  \/\/ Will be fixed. For now it works well enough.\n+\n@@ -620,7 +628,9 @@\n-    assert(CompressedKlassPointers::is_valid_base(a), \"Sanity\");\n-    while (a < search_ranges[i].to) {\n-      ReservedSpace rs(size, Metaspace::reserve_alignment(),\n-                       os::vm_page_size(), (char*)a);\n-      if (rs.is_reserved()) {\n-        assert(a == (address)rs.base(), \"Sanity\");\n-        return rs;\n+    if (CompressedKlassPointers::is_valid_base(a)) {\n+      while (a < search_ranges[i].to) {\n+        ReservedSpace rs(size, Metaspace::reserve_alignment(),\n+                         os::vm_page_size(), (char*)a);\n+        if (rs.is_reserved()) {\n+          assert(a == (address)rs.base(), \"Sanity\");\n+          return rs;\n+        }\n+        a +=  search_ranges[i].increment;\n@@ -628,1 +638,0 @@\n-      a +=  search_ranges[i].increment;\n@@ -867,1 +876,15 @@\n-  return metaspace::chunklevel::MAX_CHUNK_WORD_SIZE;\n+  return metaspace::chunklevel::MAX_CHUNK_WORD_SIZE - KlassAlignmentInWords;\n+}\n+\n+#ifdef _LP64\n+\/\/ The largest allowed size for class space\n+size_t Metaspace::max_class_space_size() {\n+  assert(KlassEncodingMetaspaceMax > 0, \"too early.\");\n+  \/\/ This is a bit fuzzy. Max value of class space size depends on narrow klass pointer\n+  \/\/ encoding range size and CDS, since class space shares encoding range with CDS. CDS\n+  \/\/ archives are usually pretty small though, so to keep matters simple, for now we\n+  \/\/ just assume a reasonable default (this is hackish; improve!).\n+  const size_t slice_for_cds = M * 128;\n+  assert(KlassEncodingMetaspaceMax >= (slice_for_cds * 2), \"rethink this\");\n+  const size_t max_class_space_size = KlassEncodingMetaspaceMax - slice_for_cds;\n+  return max_class_space_size;\n@@ -869,0 +892,1 @@\n+#endif \/\/ _LP64\n@@ -1031,4 +1055,3 @@\n-bool Metaspace::contains_non_shared(const void* ptr) {\n-  if (using_class_space() && VirtualSpaceList::vslist_class()->contains((MetaWord*)ptr)) {\n-     return true;\n-  }\n+bool Metaspace::class_space_contains(const void* ptr) {\n+  return using_class_space() && VirtualSpaceList::vslist_class()->contains((MetaWord*)ptr);\n+}\n@@ -1036,1 +1059,3 @@\n-  return VirtualSpaceList::vslist_nonclass()->contains((MetaWord*)ptr);\n+bool Metaspace::contains_non_shared(const void* ptr) {\n+  return class_space_contains(ptr) ||\n+         VirtualSpaceList::vslist_nonclass()->contains((MetaWord*)ptr);\n","filename":"src\/hotspot\/share\/memory\/metaspace.cpp","additions":39,"deletions":14,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"oops\/klass.hpp\"    \/\/ just for sizeof(Klass)\n@@ -34,0 +35,1 @@\n+#include \"memory\/metaspace\/metaspaceAlignment.hpp\"\n@@ -53,0 +55,2 @@\n+constexpr size_t min_reasonable_klass_size = align_up(sizeof(Klass), BytesPerWord) \/ BytesPerWord;\n+\n@@ -123,0 +127,1 @@\n+\n@@ -216,0 +221,82 @@\n+MetaWord* MetaspaceArena::allocate_from_freeblocks_only(size_t word_size) {\n+  MutexLocker cl(lock(), Mutex::_no_safepoint_check_flag);\n+  const size_t raw_word_size = get_raw_word_size_for_requested_word_size(word_size);\n+  return allocate_from_freeblocks_locked(raw_word_size);\n+}\n+\n+MetaWord* MetaspaceArena::allocate_for_klass(size_t word_size) {\n+  MutexLocker cl(lock(), Mutex::_no_safepoint_check_flag);\n+  assert(word_size >= min_reasonable_klass_size,\n+         \"Suspiciously small for Klass: \" SIZE_FORMAT, word_size);\n+\n+  UL2(trace, \"Allocate for Klass: \" SIZE_FORMAT \" words.\", word_size);\n+\n+  \/\/ Klass alignment must be <= smallest chunk size to guarantee that a newly created chunk\n+  \/\/ will always allocate at a Klass-suitable address.\n+  assert((size_t)KlassAlignmentInWords <= chunklevel::MIN_CHUNK_WORD_SIZE, \"Must be\");\n+\n+  size_t alignment_padding_needed = 0;\n+  if (current_chunk() != nullptr) {\n+    const size_t used_words = current_chunk()->used_words();\n+    const size_t next_klass_offset = align_up(used_words, KlassAlignmentInWords);\n+    alignment_padding_needed = next_klass_offset - used_words;\n+  }\n+  const size_t total_word_size = alignment_padding_needed + word_size;\n+\n+  MetaWord* k = nullptr;\n+  MetaWord* all = allocate_inner(total_word_size);\n+\n+  if (all != nullptr) {\n+\n+    MetaWord* salvage = nullptr;\n+    size_t salvage_words = alignment_padding_needed;\n+\n+    if (is_aligned(all, KlassAlignmentInBytes)) {\n+      \/\/ The new allocation is already properly aligned. Happens if the allocation caused\n+      \/\/ a new chunk to be created. Salvage the trailing alignment gap.\n+      k = all;\n+      salvage = k + word_size;\n+    } else {\n+      \/\/ The new allocation is not aligned; we expanded the current chunk. Salvage the leading\n+      \/\/ alignment padding.\n+      k = all + alignment_padding_needed;\n+      salvage = all;\n+    }\n+\n+    \/\/ If alignment gap is large enough to be reused, squirrel it away\n+    if (alignment_padding_needed > FreeBlocks::MinWordSize) {\n+      UL2(trace, \"Save off alignment gap \" PTR_FORMAT \", \" SIZE_FORMAT \" words.\",\n+          p2i(salvage), alignment_padding_needed);\n+      DEBUG_ONLY(InternalStats::inc_num_klass_alignment_splinters_added();)\n+      \/\/ Note: Don't use deallocate here; it is only for external use since it does size\n+      \/\/ adjustment\n+      add_allocation_to_fbl(salvage, alignment_padding_needed);\n+    }\n+\n+  }\n+\n+  assert_is_aligned(k, KlassAlignmentInWords);\n+  UL2(trace, \"Returning \" PTR_FORMAT \".\", p2i(k));\n+  return k;\n+}\n+\n+\/\/ Allocate from freeblocks\n+MetaWord* MetaspaceArena::allocate_from_freeblocks_locked(size_t word_size) {\n+  DEBUG_ONLY(verify_locked();)\n+  MetaWord* p = nullptr;\n+  assert_lock_strong(lock());\n+  UL2(trace, \"Attempt to take requested \" SIZE_FORMAT \" words from fbl.\", word_size);\n+  if (_fbl != nullptr && !_fbl->is_empty()) {\n+    p = _fbl->remove_block(word_size);\n+    if (p != nullptr) {\n+      DEBUG_ONLY(InternalStats::inc_num_allocs_from_deallocated_blocks();)\n+      UL2(trace, \"taken from fbl (now: %d, \" SIZE_FORMAT \").\",\n+          _fbl->count(), _fbl->total_size());\n+      \/\/ Note: free blocks in freeblock dictionary still count as \"used\" as far as statistics go;\n+      \/\/ therefore we have no need to adjust any usage counters (see epilogue of allocate_inner())\n+      \/\/ and can just return here.\n+    }\n+  }\n+  return p;\n+}\n+\n@@ -230,11 +317,3 @@\n-  if (_fbl != nullptr && !_fbl->is_empty()) {\n-    p = _fbl->remove_block(raw_word_size);\n-    if (p != nullptr) {\n-      DEBUG_ONLY(InternalStats::inc_num_allocs_from_deallocated_blocks();)\n-      UL2(trace, \"taken from fbl (now: %d, \" SIZE_FORMAT \").\",\n-          _fbl->count(), _fbl->total_size());\n-      \/\/ Note: free blocks in freeblock dictionary still count as \"used\" as far as statistics go;\n-      \/\/ therefore we have no need to adjust any usage counters (see epilogue of allocate_inner())\n-      \/\/ and can just return here.\n-      return p;\n-    }\n+  p = allocate_from_freeblocks_locked(raw_word_size);\n+  if (p != nullptr) {\n+    return p;\n@@ -478,2 +557,2 @@\n-  st->print_cr(\"sm %s: %d chunks, total word size: \" SIZE_FORMAT \", committed word size: \" SIZE_FORMAT, _name,\n-               _chunks.count(), _chunks.calc_word_size(), _chunks.calc_committed_word_size());\n+  st->print_cr(\"Arena @\" PTR_FORMAT \" (%s): %d chunks, total word size: \" SIZE_FORMAT \", committed word size: \" SIZE_FORMAT,\n+               p2i(this), _name, _chunks.count(), _chunks.calc_word_size(), _chunks.calc_committed_word_size());\n@@ -482,2 +561,8 @@\n-  st->print_cr(\"growth-policy \" PTR_FORMAT \", lock \" PTR_FORMAT \", cm \" PTR_FORMAT \", fbl \" PTR_FORMAT,\n-                p2i(_growth_policy), p2i(_lock), p2i(_chunk_manager), p2i(_fbl));\n+  st->print(\"growth-policy \" PTR_FORMAT \", lock \" PTR_FORMAT \", cm \" PTR_FORMAT \", fbl \" PTR_FORMAT,\n+             p2i(_growth_policy), p2i(_lock), p2i(_chunk_manager), p2i(_fbl));\n+  if (_fbl) {\n+    st->print(\" (\");\n+    _fbl->print_on(st);\n+    st->print(\")\");\n+  }\n+  st->cr();\n","filename":"src\/hotspot\/share\/memory\/metaspace\/metaspaceArena.cpp","additions":100,"deletions":15,"binary":false,"changes":115,"status":"modified"},{"patch":"@@ -232,1 +232,2 @@\n-  UL2(debug, \"born (word_size \" SIZE_FORMAT \").\", _word_size);\n+  UL2(debug, \"born: [\" PTR_FORMAT \"..\" PTR_FORMAT \"), (word_size \" SIZE_FORMAT \").\",\n+      p2i(_rs.base()), p2i(_rs.end()), _word_size);\n","filename":"src\/hotspot\/share\/memory\/metaspace\/virtualSpaceNode.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -550,0 +550,4 @@\n+    \/\/ Note Lilliput: the advantages of this strategy were questionable before\n+    \/\/  (since CDS=off + Compressed oops + heap large enough to suffocate us out of lower 32g\n+    \/\/  is rare) and with Lilliput the encoding range drastically shrank. We may just do away\n+    \/\/  with this altogether.\n","filename":"src\/hotspot\/share\/memory\/virtualspace.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -53,0 +53,1 @@\n+#include \"runtime\/globals.hpp\"\n@@ -54,0 +55,1 @@\n+#include \"utilities\/align.hpp\"\n@@ -195,1 +197,15 @@\n-  return Metaspace::allocate(loader_data, word_size, MetaspaceObj::ClassType, THREAD);\n+  MetaWord* p = Metaspace::allocate(loader_data, word_size, MetaspaceObj::ClassType, THREAD);\n+  assert(is_aligned(p, KlassAlignmentInBytes),\n+         \"metaspace returned badly aligned memory (\" PTR_FORMAT \"), alignment required: %u\",\n+         p2i(p), (unsigned)KlassAlignmentInBytes);\n+  return p;\n+}\n+\n+static markWord make_prototype(Klass* kls) {\n+  markWord prototype = markWord::prototype();\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    prototype = prototype.set_klass(kls);\n+  }\n+#endif\n+  return prototype;\n@@ -203,0 +219,1 @@\n+                           _prototype_header(make_prototype(this)),\n@@ -747,0 +764,4 @@\n+     if (UseCompactObjectHeaders) {\n+       st->print(BULLET\"prototype_header: \" INTPTR_FORMAT, _prototype_header.value());\n+       st->cr();\n+     }\n@@ -770,0 +791,4 @@\n+  if (UseCompressedClassPointers) {\n+    assert(is_aligned(this, KlassAlignmentInBytes), \"misaligned Klass structure\");\n+  }\n+\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":26,"deletions":1,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -170,0 +170,2 @@\n+  markWord _prototype_header;   \/\/ Used to initialize objects' header\n+\n@@ -679,0 +681,7 @@\n+  markWord prototype_header() const      {\n+    assert(UseCompactObjectHeaders, \"only use with compact object headers\");\n+    return _prototype_header;\n+  }\n+  inline void set_prototype_header(markWord header);\n+  static ByteSize prototype_header_offset() { return in_ByteSize(offset_of(Klass, _prototype_header)); }\n+\n","filename":"src\/hotspot\/share\/oops\/klass.hpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n@@ -37,0 +38,1 @@\n+#include \"runtime\/interfaceSupport.inline.hpp\"\n@@ -158,2 +160,3 @@\n-  \/\/ Only has a klass gap when compressed class pointers are used.\n-  return UseCompressedClassPointers;\n+  \/\/ Only has a klass gap when compressed class pointers are used, but\n+  \/\/ only if not using compact headers..\n+  return UseCompressedClassPointers && !UseCompactObjectHeaders;\n@@ -166,1 +169,5 @@\n-  _metadata._compressed_klass = nk;\n+  if (UseCompactObjectHeaders) {\n+    set_mark(mark().set_narrow_klass(nk));\n+  } else {\n+    _metadata._compressed_klass = nk;\n+  }\n@@ -171,7 +178,8 @@\n-  if (UseCompressedClassPointers) {\n-    narrowKlass narrow_klass = obj->_metadata._compressed_klass;\n-    if (narrow_klass == 0) return nullptr;\n-    return (void*)CompressedKlassPointers::decode_raw(narrow_klass);\n-  } else {\n-    return obj->_metadata._klass;\n-  }\n+  \/\/ TODO: Remove method altogether and replace with calls to obj->klass() ?\n+  \/\/ OTOH, we may eventually get rid of locking in header, and then no\n+  \/\/ longer have to deal with that anymore.\n+#ifdef _LP64\n+  return obj->klass();\n+#else\n+  return obj->_metadata._klass;\n+#endif\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":18,"deletions":10,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"oops\/compressedKlass.hpp\"\n@@ -82,0 +83,2 @@\n+  inline markWord resolve_mark() const;\n+\n@@ -100,1 +103,8 @@\n-  static constexpr int header_size() { return sizeof(oopDesc)\/HeapWordSize; }\n+  static int header_size() {\n+#ifdef _LP64\n+    if (UseCompactObjectHeaders) {\n+      return sizeof(markWord) \/ HeapWordSize;\n+    } else\n+#endif\n+    return sizeof(oopDesc)\/HeapWordSize;\n+  }\n@@ -262,0 +272,1 @@\n+  inline void forward_to_self();\n@@ -268,0 +279,1 @@\n+  inline oop forward_to_self_atomic(markWord compare, atomic_memory_order order = memory_order_conservative);\n@@ -270,0 +282,1 @@\n+  inline oop forwardee(markWord header) const;\n@@ -313,1 +326,0 @@\n-  static int klass_offset_in_bytes()     { return (int)offset_of(oopDesc, _metadata._klass); }\n@@ -316,0 +328,1 @@\n+    assert(!UseCompactObjectHeaders, \"don't use klass_offset_in_bytes() with compact headers\");\n@@ -319,0 +332,24 @@\n+  static int klass_offset_in_bytes()     {\n+#ifdef _LP64\n+    if (UseCompactObjectHeaders) {\n+      STATIC_ASSERT(markWord::klass_shift % 8 == 0);\n+      return mark_offset_in_bytes() + markWord::klass_shift \/ 8;\n+    } else\n+#endif\n+    return offset_of(oopDesc, _metadata._klass);\n+  }\n+\n+  static int base_offset_in_bytes() {\n+#ifdef _LP64\n+    if (UseCompactObjectHeaders) {\n+      \/\/ With compact headers, the Klass* field is not used for the Klass*\n+      \/\/ and is used for the object fields instead.\n+      assert(sizeof(markWord) == 8, \"sanity\");\n+      return sizeof(markWord);\n+    } else if (UseCompressedClassPointers) {\n+      return sizeof(markWord) + sizeof(narrowKlass);\n+    } else\n+#endif\n+    return sizeof(oopDesc);\n+  }\n+\n","filename":"src\/hotspot\/share\/oops\/oop.hpp","additions":39,"deletions":2,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n@@ -37,1 +38,1 @@\n-#include \"oops\/markWord.hpp\"\n+#include \"oops\/markWord.inline.hpp\"\n@@ -41,0 +42,3 @@\n+#include \"runtime\/objectMonitor.inline.hpp\"\n+#include \"runtime\/safepoint.hpp\"\n+#include \"runtime\/synchronizer.hpp\"\n@@ -85,0 +89,10 @@\n+markWord oopDesc::resolve_mark() const {\n+  assert(LockingMode != LM_LEGACY, \"Not safe with legacy stack-locking\");\n+  markWord hdr = mark();\n+  if (hdr.has_monitor()) {\n+    ObjectMonitor* monitor = hdr.monitor();\n+    return monitor->header();\n+  }\n+  return hdr;\n+}\n+\n@@ -86,0 +100,7 @@\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    markWord header = resolve_mark();\n+    assert(CompressedKlassPointers::use_compressed_class_pointers(), \"expect compressed klass pointers\");\n+    set_mark(markWord((header.value() & markWord::klass_mask_in_place) | markWord::prototype().value()));\n+  } else\n+#endif\n@@ -90,1 +111,6 @@\n-  if (UseCompressedClassPointers) {\n+#ifdef _LP64\n+  if (CompressedKlassPointers::use_compact_object_headers()) {\n+    assert(CompressedKlassPointers::use_compressed_class_pointers(), \"only with compressed class pointers\");\n+    markWord header = resolve_mark();\n+    return header.klass();\n+  } else if (CompressedKlassPointers::use_compressed_class_pointers()) {\n@@ -92,3 +118,3 @@\n-  } else {\n-    return _metadata._klass;\n-  }\n+  } else\n+#endif\n+  return _metadata._klass;\n@@ -98,1 +124,6 @@\n-  if (UseCompressedClassPointers) {\n+#ifdef _LP64\n+  if (CompressedKlassPointers::use_compact_object_headers()) {\n+    assert(CompressedKlassPointers::use_compressed_class_pointers(), \"only with compressed class pointers\");\n+    markWord header = resolve_mark();\n+    return header.klass_or_null();\n+  } else if (CompressedKlassPointers::use_compressed_class_pointers()) {\n@@ -100,3 +131,3 @@\n-  } else {\n-    return _metadata._klass;\n-  }\n+  } else\n+#endif\n+  return _metadata._klass;\n@@ -106,6 +137,14 @@\n-  if (UseCompressedClassPointers) {\n-    narrowKlass nklass = Atomic::load_acquire(&_metadata._compressed_klass);\n-    return CompressedKlassPointers::decode(nklass);\n-  } else {\n-    return Atomic::load_acquire(&_metadata._klass);\n-  }\n+#ifdef _LP64\n+  if (CompressedKlassPointers::use_compact_object_headers()) {\n+    assert(CompressedKlassPointers::use_compressed_class_pointers(), \"only with compressed class pointers\");\n+    markWord header = mark_acquire();\n+    if (header.has_monitor()) {\n+      header = header.monitor()->header();\n+    }\n+    return header.klass_or_null();\n+  } else if (CompressedKlassPointers::use_compressed_class_pointers()) {\n+     narrowKlass nklass = Atomic::load_acquire(&_metadata._compressed_klass);\n+     return CompressedKlassPointers::decode(nklass);\n+  } else\n+#endif\n+  return Atomic::load_acquire(&_metadata._klass);\n@@ -115,5 +154,1 @@\n-  if (UseCompressedClassPointers) {\n-    return CompressedKlassPointers::decode_raw(_metadata._compressed_klass);\n-  } else {\n-    return _metadata._klass;\n-  }\n+  return klass();\n@@ -123,2 +158,3 @@\n-  assert(Universe::is_bootstrapping() || (k != nullptr && k->is_klass()), \"incorrect Klass\");\n-  if (UseCompressedClassPointers) {\n+  assert(Universe::is_bootstrapping() || (k != NULL && k->is_klass()), \"incorrect Klass\");\n+  assert(!UseCompactObjectHeaders, \"don't set Klass* with compact headers\");\n+  if (CompressedKlassPointers::use_compressed_class_pointers()) {\n@@ -132,1 +168,2 @@\n-  assert(Universe::is_bootstrapping() || (k != nullptr && k->is_klass()), \"incorrect Klass\");\n+  assert(Universe::is_bootstrapping() || (k != NULL && k->is_klass()), \"incorrect Klass\");\n+  assert(!UseCompactObjectHeaders, \"don't set Klass* with compact headers\");\n@@ -134,1 +171,1 @@\n-  if (UseCompressedClassPointers) {\n+  if (CompressedKlassPointers::use_compressed_class_pointers()) {\n@@ -143,1 +180,2 @@\n-  if (UseCompressedClassPointers) {\n+  assert(!UseCompactObjectHeaders, \"don't set Klass* gap with compact headers\");\n+  if (CompressedKlassPointers::use_compressed_class_pointers()) {\n@@ -274,1 +312,1 @@\n-  assert(m.decode_pointer() == p, \"encoding must be reversible\");\n+  assert(forwardee(m) == p, \"encoding must be reversable\");\n@@ -278,0 +316,16 @@\n+void oopDesc::forward_to_self() {\n+#ifdef _LP64\n+  markWord m = mark();\n+  \/\/ If mark is displaced, we need to preserve the Klass* from real header.\n+  assert(SafepointSynchronize::is_at_safepoint(), \"we can only safely fetch the displaced header at safepoint\");\n+  if (m.has_displaced_mark_helper()) {\n+    m = m.displaced_mark_helper();\n+  }\n+  m = m.set_self_forwarded();\n+  assert(forwardee(m) == cast_to_oop(this), \"encoding must be reversable\");\n+  set_mark(m);\n+#else\n+  forward_to(oop(this));\n+#endif\n+}\n+\n@@ -280,1 +334,1 @@\n-  assert(m.decode_pointer() == p, \"encoding must be reversible\");\n+  assert(forwardee(m) == p, \"encoding must be reversable\");\n@@ -285,1 +339,20 @@\n-    return cast_to_oop(old_mark.decode_pointer());\n+    return forwardee(old_mark);\n+  }\n+}\n+\n+oop oopDesc::forward_to_self_atomic(markWord compare, atomic_memory_order order) {\n+#ifdef _LP64\n+  markWord m = compare;\n+  \/\/ If mark is displaced, we need to preserve the Klass* from real header.\n+  assert(SafepointSynchronize::is_at_safepoint(), \"we can only safely fetch the displaced header at safepoint\");\n+  if (m.has_displaced_mark_helper()) {\n+    m = m.displaced_mark_helper();\n+  }\n+  m = m.set_self_forwarded();\n+  assert(forwardee(m) == cast_to_oop(this), \"encoding must be reversable\");\n+  markWord old_mark = cas_set_mark(m, compare, order);\n+  if (old_mark == compare) {\n+    return NULL;\n+  } else {\n+    assert(old_mark.is_marked(), \"must be marked here\");\n+    return forwardee(old_mark);\n@@ -287,0 +360,3 @@\n+#else\n+  return forward_to_atomic(oop(this), compare, order);\n+#endif\n@@ -293,2 +369,14 @@\n-  assert(is_forwarded(), \"only decode when actually forwarded\");\n-  return cast_to_oop(mark().decode_pointer());\n+  return forwardee(mark());\n+}\n+\n+oop oopDesc::forwardee(markWord header) const {\n+  assert(header.is_marked(), \"must be forwarded\");\n+#ifdef _LP64\n+  if (header.self_forwarded()) {\n+    return cast_to_oop(this);\n+  } else\n+#endif\n+  {\n+    assert(header.is_marked(), \"only decode when actually forwarded\");\n+    return cast_to_oop(header.decode_pointer());\n+  }\n@@ -349,1 +437,0 @@\n-  assert(k == klass(), \"wrong klass\");\n","filename":"src\/hotspot\/share\/oops\/oop.inline.hpp","additions":118,"deletions":31,"binary":false,"changes":149,"status":"modified"},{"patch":"@@ -40,3 +40,0 @@\n-\/\/ If compressed klass pointers then use narrowKlass.\n-typedef juint  narrowKlass;\n-\n","filename":"src\/hotspot\/share\/oops\/oopsHierarchy.hpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -113,0 +113,10 @@\n+\n+class C2LoadNKlassStub : public C2CodeStub {\n+private:\n+  Register _dst;\n+public:\n+  C2LoadNKlassStub(Register dst) : C2CodeStub(), _dst(dst) {}\n+  Register dst() { return _dst; }\n+  int max_size() const;\n+  void emit(C2_MacroAssembler& masm);\n+};\n","filename":"src\/hotspot\/share\/opto\/c2_CodeStubs.hpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -1576,2 +1576,8 @@\n-  \/\/ For now only enable fast locking for non-array types\n-  mark_node = phase->MakeConX(markWord::prototype().value());\n+  if (UseCompactObjectHeaders) {\n+    Node* klass_node = in(AllocateNode::KlassNode);\n+    Node* proto_adr = phase->transform(new AddPNode(klass_node, klass_node, phase->MakeConX(in_bytes(Klass::prototype_header_offset()))));\n+    mark_node = LoadNode::make(*phase, control, mem, proto_adr, TypeRawPtr::BOTTOM, TypeX_X, TypeX_X->basic_type(), MemNode::unordered);\n+  } else {\n+    \/\/ For now only enable fast locking for non-array types\n+    mark_node = phase->MakeConX(markWord::prototype().value());\n+  }\n","filename":"src\/hotspot\/share\/opto\/callnode.cpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -1685,0 +1685,4 @@\n+      if (UseCompactObjectHeaders) {\n+        if (flat->offset() == in_bytes(Klass::prototype_header_offset()))\n+          alias_type(idx)->set_rewritable(false);\n+      }\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -4388,2 +4388,2 @@\n-  Node *hash_mask      = _gvn.intcon(markWord::hash_mask);\n-  Node *hash_shift     = _gvn.intcon(markWord::hash_shift);\n+  Node *hash_mask      = _gvn.intcon(UseCompactObjectHeaders ? markWord::hash_mask_compact  : markWord::hash_mask);\n+  Node *hash_shift     = _gvn.intcon(UseCompactObjectHeaders ? markWord::hash_shift_compact : markWord::hash_shift);\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1872,0 +1872,7 @@\n+  if (UseCompactObjectHeaders) {\n+    if (tkls->offset() == in_bytes(Klass::prototype_header_offset())) {\n+      \/\/ The field is Klass::_prototype_header.  Return its (constant) value.\n+      assert(this->Opcode() == Op_LoadX, \"must load a proper type from _prototype_header\");\n+      return TypeX::make(klass->prototype_header());\n+    }\n+  }\n@@ -2044,0 +2051,7 @@\n+      if (UseCompactObjectHeaders) {\n+        if (tkls->offset() == in_bytes(Klass::prototype_header_offset())) {\n+          \/\/ The field is Klass::_prototype_header. Return its (constant) value.\n+          assert(this->Opcode() == Op_LoadX, \"must load a proper type from _prototype_header\");\n+          return TypeX::make(klass->prototype_header());\n+        }\n+      }\n@@ -2134,1 +2148,1 @@\n-  if (alloc != nullptr) {\n+  if (!UseCompactObjectHeaders && alloc != nullptr) {\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":15,"deletions":1,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -323,1 +323,1 @@\n-    const size_t hs = arrayOopDesc::header_size(elem_type);\n+    const size_t hs_bytes = arrayOopDesc::base_offset_in_bytes(elem_type);\n@@ -325,1 +325,1 @@\n-    const size_t aligned_hs = align_object_offset(hs);\n+    const size_t aligned_hs_bytes = align_up(hs_bytes, BytesPerLong);\n@@ -327,2 +327,2 @@\n-    if (aligned_hs > hs) {\n-      Copy::zero_to_words(obj+hs, aligned_hs-hs);\n+    if (aligned_hs_bytes > hs_bytes) {\n+      Copy::zero_to_bytes(obj + hs_bytes, aligned_hs_bytes - hs_bytes);\n@@ -331,0 +331,1 @@\n+    const size_t aligned_hs = aligned_hs_bytes \/ HeapWordSize;\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -5153,1 +5153,2 @@\n-    int header_size = objArrayOopDesc::header_size() * wordSize;\n+    BasicType basic_elem_type = elem()->basic_type();\n+    int header_size = arrayOopDesc::base_offset_in_bytes(basic_elem_type);\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -70,1 +70,1 @@\n-  ( arrayOopDesc::header_size(T_DOUBLE) * HeapWordSize \\\n+  ( arrayOopDesc::base_offset_in_bytes(T_DOUBLE) \\\n","filename":"src\/hotspot\/share\/prims\/unsafe.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1487,0 +1487,26 @@\n+\n+  if (UseCompactObjectHeaders) {\n+    \/\/ 512 byte alignment, 22-bit values (Lilliput)\n+    LogKlassAlignmentInBytes = 9;\n+    MaxNarrowKlassPointerBits = 22;\n+  } else {\n+    \/\/ Traditional: 8 byte alignment, 32-bit values\n+    LogKlassAlignmentInBytes = 3;\n+    MaxNarrowKlassPointerBits = 32;\n+  }\n+\n+  KlassAlignmentInBytes = 1 << LogKlassAlignmentInBytes;\n+  assert(is_aligned(KlassAlignmentInBytes, BytesPerWord), \"Must be at least word-sized\");\n+  KlassAlignmentInWords = KlassAlignmentInBytes \/ BytesPerWord;\n+  NarrowKlassPointerBitMask = ((((uint64_t)1) << MaxNarrowKlassPointerBits) - 1);\n+  KlassEncodingMetaspaceMax = UCONST64(1) << (MaxNarrowKlassPointerBits + LogKlassAlignmentInBytes);\n+\n+  \/\/ Assert validity of compressed class space size. User arg should have been checked at this point\n+  \/\/ (see CompressedClassSpaceSizeConstraintFunc()), so no need to be nice about it, this fires in\n+  \/\/ case the default is wrong.\n+  \/\/ TODO: This is placed wrong. The CompressedClassSpaceSizeFunc is done after ergo, but this\n+  \/\/ assert is during ergo.\n+  \/\/ assert(CompressedClassSpaceSize <= Metaspace::max_class_space_size(),\n+  \/\/        \"CompressedClassSpaceSize \" SIZE_FORMAT \" too large (max: \" SIZE_FORMAT \")\",\n+  \/\/        CompressedClassSpaceSize, Metaspace::max_class_space_size());\n+\n@@ -3098,0 +3124,16 @@\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders && FLAG_IS_CMDLINE(UseCompressedClassPointers) && !UseCompressedClassPointers) {\n+    \/\/ If user specifies -UseCompressedClassPointers, disable compact headers with a warning.\n+    warning(\"Compact object headers require compressed class pointers. Disabling compact object headers.\");\n+    FLAG_SET_DEFAULT(UseCompactObjectHeaders, false);\n+  }\n+\n+  if (UseCompactObjectHeaders && LockingMode == LM_LEGACY) {\n+    FLAG_SET_DEFAULT(LockingMode, LM_LIGHTWEIGHT);\n+  }\n+\n+  if (!UseCompactObjectHeaders) {\n+    FLAG_SET_DEFAULT(UseSharedSpaces, false);\n+  }\n+#endif\n+\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":42,"deletions":0,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -132,0 +132,3 @@\n+  product(bool, UseCompactObjectHeaders, true, EXPERIMENTAL,                \\\n+                \"Use 64-bit object headers instead of 96-bit headers\")      \\\n+                                                                            \\\n@@ -149,0 +152,1 @@\n+const bool UseCompactObjectHeaders = false;\n@@ -1052,1 +1056,1 @@\n-  develop(bool, UseHeavyMonitors, false,                                    \\\n+  product(bool, UseHeavyMonitors, false, DIAGNOSTIC,                        \\\n@@ -1409,1 +1413,1 @@\n-          range(1*M, 3*G)                                                   \\\n+          constraint(CompressedClassSpaceSizeConstraintFunc, AfterErgo)     \\\n@@ -1411,1 +1415,1 @@\n-  develop(size_t, CompressedClassSpaceBaseAddress, 0,                       \\\n+  product(size_t, CompressedClassSpaceBaseAddress, 0, DIAGNOSTIC,           \\\n@@ -1972,0 +1976,6 @@\n+  product(bool, HeapObjectStats, false, DIAGNOSTIC,                         \\\n+             \"Enable gathering of heap object statistics\")                  \\\n+                                                                            \\\n+  product(size_t, HeapObjectStatsSamplingInterval, 500, DIAGNOSTIC,         \\\n+             \"Heap object statistics sampling interval (ms)\")               \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":13,"deletions":3,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -220,0 +220,1 @@\n+  static ByteSize header_offset()      { return byte_offset_of(ObjectMonitor, _header); }\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+#include \"oops\/compressedKlass.hpp\"\n","filename":"src\/hotspot\/share\/runtime\/os.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shared\/suspendibleThreadSet.hpp\"\n@@ -904,1 +905,1 @@\n-  value &= markWord::hash_mask;\n+  value &= UseCompactObjectHeaders ? markWord::hash_mask_compact : markWord::hash_mask;\n@@ -1710,0 +1711,3 @@\n+      \/\/ Also, we sync and desync GC threads around the handshake, so that they can\n+      \/\/ safely read the mark-word and look-through to the object-monitor, without\n+      \/\/ being afraid that the object-monitor is going away.\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -93,0 +93,1 @@\n+  template(HeapObjectStatistics)                  \\\n","filename":"src\/hotspot\/share\/runtime\/vmOperation.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -382,2 +382,2 @@\n-     static_field(CompressedKlassPointers,     _narrow_klass._base,                           address)                               \\\n-     static_field(CompressedKlassPointers,     _narrow_klass._shift,                          int)                                   \\\n+     static_field(CompressedKlassPointers,     _base_copy,                           address)                                        \\\n+     static_field(CompressedKlassPointers,     _shift_copy,                          int)                                            \\\n@@ -2587,0 +2587,1 @@\n+  declare_constant(markWord::hash_bits_compact)                           \\\n@@ -2591,0 +2592,2 @@\n+  declare_constant(markWord::hash_shift_compact)                          \\\n+  LP64_ONLY(declare_constant(markWord::klass_shift))                      \\\n@@ -2598,0 +2601,2 @@\n+  declare_constant(markWord::hash_mask_compact)                           \\\n+  declare_constant(markWord::hash_mask_compact_in_place)                  \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -599,5 +599,0 @@\n-const int LogKlassAlignmentInBytes = 3;\n-const int LogKlassAlignment        = LogKlassAlignmentInBytes - LogHeapWordSize;\n-const int KlassAlignmentInBytes    = 1 << LogKlassAlignmentInBytes;\n-const int KlassAlignment           = KlassAlignmentInBytes \/ HeapWordSize;\n-\n@@ -611,5 +606,0 @@\n-\/\/ Maximal size of compressed class space. Above this limit compression is not possible.\n-\/\/ Also upper bound for placement of zero based class space. (Class space is further limited\n-\/\/ to be < 3G, see arguments.cpp.)\n-const  uint64_t KlassEncodingMetaspaceMax = (uint64_t(max_juint) + 1) << LogKlassAlignmentInBytes;\n-\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":0,"deletions":10,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"oops\/compressedKlass.hpp\"\n","filename":"src\/hotspot\/share\/utilities\/vmError.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -72,1 +72,2 @@\n-    final int hubOffset = getFieldOffset(\"oopDesc::_metadata._klass\", Integer.class, \"Klass*\");\n+    \/\/ TODO: Lilliput. Probably ok.\n+    final int hubOffset = 4; \/\/ getFieldOffset(\"oopDesc::_metadata._klass\", Integer.class, \"Klass*\");\n","filename":"src\/jdk.internal.vm.ci\/share\/classes\/jdk\/vm\/ci\/hotspot\/HotSpotVMConfig.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -176,0 +176,13 @@\n+\n+Lilliput temporary:\n+compiler\/c2\/irTests\/TestVectorizationNotRun.java 8301785 generic-all\n+runtime\/CompressedOops\/CompressedClassPointerEncoding.java#x64-area-beyond-encoding-range-use-xor 8302094 windows-all,macosx-all\n+runtime\/CompressedOops\/CompressedClassPointerEncoding.java#x64-area-partly-within-encoding-range-use-add 8302094 windows-all,macosx-all\n+runtime\/CompressedOops\/CompressedClassPointerEncoding.java#x64-area-within-encoding-range-use-zero 8302094 windows-all,macosx-all\n+runtime\/CompressedOops\/CompressedClassPointerEncoding.java#x64-area-far-out-no-low-bits-use-xor 8302094 windows-all,macosx-all\n+runtime\/CompressedOops\/CompressedClassPointerEncoding.java#x64-area-far-out-with-low-bits-use-add 8302094 windows-all,macosx-all\n+runtime\/CompressedOops\/CompressedClassPointerEncoding.java#aarch64-xor 8302094 windows-all,macosx-all\n+runtime\/CompressedOops\/CompressedClassPointerEncoding.java#aarch64-movk-1 8302094 windows-all,macosx-all\n+runtime\/CompressedOops\/CompressedClassPointerEncoding.java#aarch64-movk-2 8302094 windows-all,macosx-all\n+runtime\/CompressedOops\/CompressedClassPointerEncoding.java#aarch64-movk-3 8302094 windows-all,macosx-all\n+runtime\/CompressedOops\/CompressedClassSpaceSize.java 8302094 generic-all\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":13,"deletions":0,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -388,2 +388,0 @@\n- -runtime\/Metaspace\/elastic\/TestMetaspaceAllocationMT1.java \\\n- -runtime\/Metaspace\/elastic\/TestMetaspaceAllocationMT2.java \\\n","filename":"test\/hotspot\/jtreg\/TEST.groups","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -62,0 +62,34 @@\n+\n+\n+\/* @test id=ccs-on-compact-headers-on\n+ * @summary Run with +UseCCS and +UseCompactObjectHeaders\n+ * @library \/test\/lib\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.xml\n+ * @requires vm.bits == 64\n+ * @requires vm.flagless\n+ * @run main\/native GTestWrapper --gtest_filter=metaspace* -Xlog:metaspace* -XX:+UnlockExperimentalVMOptions -XX:+UseCompactObjectHeaders -XX:+UseCompressedClassPointers -XX:VerifyMetaspaceInterval=3\n+ *\/\n+\n+\/* @test id=ccs-on-compact-headers-off\n+ * @summary Run with +UseCCS and -UseCompactObjectHeaders\n+ * @library \/test\/lib\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.xml\n+ * @requires vm.bits == 64\n+ * @requires vm.flagless\n+ * @run main\/native GTestWrapper --gtest_filter=metaspace* -Xlog:metaspace* -XX:+UnlockExperimentalVMOptions -XX:-UseCompactObjectHeaders -XX:+UseCompressedClassPointers -XX:VerifyMetaspaceInterval=3\n+ *\/\n+\n+\/* @test id=ccs-off\n+ * @summary Run with -UseCCS\n+ * @library \/test\/lib\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.xml\n+ * @requires vm.bits == 64\n+ * @requires vm.flagless\n+ * @run main\/native GTestWrapper --gtest_filter=metaspace* -Xlog:metaspace* -XX:-UseCompressedClassPointers -XX:VerifyMetaspaceInterval=3\n+ *\/\n+\n+\n+\n","filename":"test\/hotspot\/jtreg\/gtest\/MetaspaceGtests.java","additions":34,"deletions":0,"binary":false,"changes":34,"status":"modified"}]}