{"files":[{"patch":"@@ -2,1 +2,1 @@\n-project=jdk\n+project=lilliput\n@@ -7,1 +7,1 @@\n-error=author,committer,reviewers,merge,issues,executable,symlink,message,hg-tag,whitespace,problemlists\n+error=author,committer,reviewers,executable,symlink,message,hg-tag,whitespace,problemlists\n@@ -21,4 +21,1 @@\n-[checks \"merge\"]\n-message=Merge\n-\n-reviewers=1\n+committers=1\n@@ -31,3 +28,0 @@\n-[checks \"issues\"]\n-pattern=^([124-8][0-9]{6}): (\\S.*)$\n-\n","filename":".jcheck\/conf","additions":3,"deletions":9,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -7708,1 +7708,1 @@\n-instruct loadNKlass(iRegNNoSp dst, memory4 mem)\n+instruct loadNKlass(iRegNNoSp dst, memory4 mem, rFlagsReg cr)\n@@ -7711,0 +7711,1 @@\n+  effect(TEMP_DEF dst, KILL cr);\n@@ -7715,4 +7716,6 @@\n-\n-  ins_encode(aarch64_enc_ldrw(dst, mem));\n-\n-  ins_pipe(iload_reg_mem);\n+  ins_encode %{\n+    assert($mem$$disp == oopDesc::klass_offset_in_bytes(), \"expect correct offset\");\n+    assert($mem$$index$$Register == noreg, \"expect no index\");\n+    __ load_nklass($dst$$Register, $mem$$base$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":8,"deletions":5,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -256,0 +256,25 @@\n+void LoadKlassStub::emit_code(LIR_Assembler* ce) {\n+  __ bind(_entry);\n+  Register res = _result->as_register();\n+  ce->store_parameter(_obj->as_register(), 0);\n+  if (res != r0) {\n+    \/\/ Note: we cannot push\/pop r0 around the call, because that\n+    \/\/ would mess with the stack pointer sp, and we need that to\n+    \/\/ remain intact for store_paramater\/load_argument to work correctly.\n+    \/\/ We swap r0 and res instead, which preserves current r0 in res.\n+    \/\/ The preserved value is later saved and restored around the\n+    \/\/ call in Runtime1::load_klass_id.\n+    __ mov(rscratch1, r0);\n+    __ mov(r0, res);\n+    __ mov(res, rscratch1);\n+  }\n+  __ far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::load_klass_id)));\n+  if (res != r0) {\n+    \/\/ Swap back r0 and res. This brings the call return value\n+    \/\/ from r0 into res, and the preserved value in res back into r0.\n+    __ mov(rscratch1, r0);\n+    __ mov(r0, res);\n+    __ mov(res, rscratch1);\n+  }\n+  __ b(_continuation);\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_CodeStubs_aarch64.cpp","additions":25,"deletions":0,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -1232,1 +1232,1 @@\n-                      arrayOopDesc::header_size(op->type()),\n+                      arrayOopDesc::base_offset_in_bytes(op->type()),\n@@ -2290,2 +2290,0 @@\n-  Address src_klass_addr = Address(src, oopDesc::klass_offset_in_bytes());\n-  Address dst_klass_addr = Address(dst, oopDesc::klass_offset_in_bytes());\n@@ -2352,9 +2350,4 @@\n-      if (UseCompressedClassPointers) {\n-        __ ldrw(tmp, src_klass_addr);\n-        __ ldrw(rscratch1, dst_klass_addr);\n-        __ cmpw(tmp, rscratch1);\n-      } else {\n-        __ ldr(tmp, src_klass_addr);\n-        __ ldr(rscratch1, dst_klass_addr);\n-        __ cmp(tmp, rscratch1);\n-      }\n+      assert(UseCompressedClassPointers, \"Lilliput\");\n+      __ load_nklass(tmp, src);\n+      __ load_nklass(rscratch1, dst);\n+      __ cmpw(tmp, rscratch1);\n@@ -2375,2 +2368,4 @@\n-      __ load_klass(src, src);\n-      __ load_klass(dst, dst);\n+      __ load_klass(tmp, src);\n+      __ mov(src, tmp);\n+      __ load_klass(tmp, dst);\n+      __ mov(dst, tmp);\n@@ -2486,0 +2481,1 @@\n+    assert(UseCompressedClassPointers, \"Lilliput\");\n@@ -2487,8 +2483,2 @@\n-\n-      if (UseCompressedClassPointers) {\n-        __ ldrw(rscratch1, dst_klass_addr);\n-        __ cmpw(tmp, rscratch1);\n-      } else {\n-        __ ldr(rscratch1, dst_klass_addr);\n-        __ cmp(tmp, rscratch1);\n-      }\n+      __ load_nklass(rscratch1, dst);\n+      __ cmpw(tmp, rscratch1);\n@@ -2496,7 +2486,2 @@\n-      if (UseCompressedClassPointers) {\n-        __ ldrw(rscratch1, src_klass_addr);\n-        __ cmpw(tmp, rscratch1);\n-      } else {\n-        __ ldr(rscratch1, src_klass_addr);\n-        __ cmp(tmp, rscratch1);\n-      }\n+      __ load_nklass(rscratch1, src);\n+      __ cmpw(tmp, rscratch1);\n@@ -2505,7 +2490,2 @@\n-      if (UseCompressedClassPointers) {\n-        __ ldrw(rscratch1, dst_klass_addr);\n-        __ cmpw(tmp, rscratch1);\n-      } else {\n-        __ ldr(rscratch1, dst_klass_addr);\n-        __ cmp(tmp, rscratch1);\n-      }\n+      __ load_nklass(rscratch1, dst);\n+      __ cmpw(tmp, rscratch1);\n@@ -2585,0 +2565,1 @@\n+  Register tmp = rscratch1;\n@@ -2591,6 +2572,14 @@\n-  if (UseCompressedClassPointers) {\n-    __ ldrw(result, Address (obj, oopDesc::klass_offset_in_bytes()));\n-    __ decode_klass_not_null(result);\n-  } else {\n-    __ ldr(result, Address (obj, oopDesc::klass_offset_in_bytes()));\n-  }\n+  assert(UseCompressedClassPointers, \"expects UseCompressedClassPointers\");\n+\n+  \/\/ Check if we can take the (common) fast path, if obj is unlocked.\n+  __ ldr(tmp, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  __ eor(tmp, tmp, markWord::unlocked_value);\n+  __ tst(tmp, markWord::lock_mask_in_place);\n+  __ br(Assembler::NE, *op->stub()->entry());\n+\n+  \/\/ Fast-path: shift and decode Klass*.\n+  __ mov(result, tmp);\n+  __ lsr(result, result, markWord::klass_shift);\n+\n+  __ bind(*op->stub()->continuation());\n+  __ decode_klass_not_null(result);\n@@ -2657,1 +2646,2 @@\n-      __ load_klass(recv, recv);\n+      __ load_klass(rscratch1, recv);\n+      __ mov(recv, rscratch1);\n@@ -2751,1 +2741,2 @@\n-      __ load_klass(tmp, tmp);\n+      __ load_klass(rscratch1, tmp);\n+      __ mov(tmp, rscratch1);\n@@ -2764,1 +2755,2 @@\n-          __ load_klass(tmp, tmp);\n+          __ load_klass(rscratch1, tmp);\n+          __ mov(tmp, rscratch1);\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":37,"deletions":45,"binary":false,"changes":82,"status":"modified"},{"patch":"@@ -166,2 +166,1 @@\n-  \/\/ This assumes that all prototype bits fit in an int32_t\n-  mov(t1, (int32_t)(intptr_t)markWord::prototype().value());\n+  ldr(t1, Address(klass, Klass::prototype_header_offset()));\n@@ -170,7 +169,0 @@\n-  if (UseCompressedClassPointers) { \/\/ Take care not to kill klass\n-    encode_klass_not_null(t1, klass);\n-    strw(t1, Address(obj, oopDesc::klass_offset_in_bytes()));\n-  } else {\n-    str(klass, Address(obj, oopDesc::klass_offset_in_bytes()));\n-  }\n-\n@@ -179,2 +171,0 @@\n-  } else if (UseCompressedClassPointers) {\n-    store_klass_gap(obj, zr);\n@@ -198,0 +188,6 @@\n+  \/\/ Zero first 4 bytes, if start offset is not word aligned.\n+  if (!is_aligned(hdr_size_in_bytes, BytesPerWord)) {\n+    strw(zr, Address(obj, hdr_size_in_bytes));\n+    hdr_size_in_bytes += BytesPerInt;\n+  }\n+\n@@ -247,1 +243,1 @@\n-void C1_MacroAssembler::allocate_array(Register obj, Register len, Register t1, Register t2, int header_size, int f, Register klass, Label& slow_case) {\n+void C1_MacroAssembler::allocate_array(Register obj, Register len, Register t1, Register t2, int base_offset_in_bytes, int f, Register klass, Label& slow_case) {\n@@ -260,1 +256,1 @@\n-  mov(arr_size, (int32_t)header_size * BytesPerWord + MinObjAlignmentInBytesMask);\n+  mov(arr_size, (int32_t)base_offset_in_bytes + MinObjAlignmentInBytesMask);\n@@ -269,1 +265,1 @@\n-  initialize_body(obj, arr_size, header_size * BytesPerWord, t1, t2);\n+  initialize_body(obj, arr_size, base_offset_in_bytes, t1, t2);\n@@ -286,1 +282,1 @@\n-  assert(!MacroAssembler::needs_explicit_null_check(oopDesc::klass_offset_in_bytes()), \"must add explicit null check\");\n+  assert(!MacroAssembler::needs_explicit_null_check(oopDesc::mark_offset_in_bytes()), \"must add explicit null check\");\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_MacroAssembler_aarch64.cpp","additions":11,"deletions":15,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -276,2 +276,2 @@\n-static OopMap* save_live_registers(StubAssembler* sasm,\n-                                   bool save_fpu_registers = true) {\n+static void save_live_registers_no_oop_map(StubAssembler* sasm,\n+                                              bool save_fpu_registers = true) {\n@@ -291,0 +291,1 @@\n+}\n@@ -292,0 +293,3 @@\n+static OopMap* save_live_registers(StubAssembler* sasm,\n+                                   bool save_fpu_registers = true) {\n+  save_live_registers_no_oop_map(sasm, save_fpu_registers);\n@@ -674,0 +678,10 @@\n+    case load_klass_id:\n+      {\n+        StubFrame f(sasm, \"load_klass\", dont_gc_arguments);\n+        save_live_registers_no_oop_map(sasm, true);\n+        f.load_argument(0, r0); \/\/ obj\n+        __ call_VM_leaf(CAST_FROM_FN_PTR(address, oopDesc::load_nklass_runtime), r0);\n+        restore_live_registers_except_r0(sasm, true);\n+      }\n+      break;\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_Runtime1_aarch64.cpp","additions":16,"deletions":2,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -1638,1 +1638,2 @@\n-  load_klass(obj, obj);\n+  load_klass(rscratch1, obj);\n+  mov(obj, rscratch1);\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -44,0 +44,1 @@\n+#include \"logging\/log.hpp\"\n@@ -48,0 +49,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n@@ -4039,6 +4041,32 @@\n-void MacroAssembler::load_klass(Register dst, Register src) {\n-  if (UseCompressedClassPointers) {\n-    ldrw(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n-    decode_klass_not_null(dst);\n-  } else {\n-    ldr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+\/\/ Loads the obj's Klass* into dst.\n+\/\/ src and dst must be distinct registers\n+\/\/ Preserves all registers (incl src, rscratch1 and rscratch2), but clobbers condition flags\n+void MacroAssembler::load_nklass(Register dst, Register src) {\n+  assert(UseCompressedClassPointers, \"expects UseCompressedClassPointers\");\n+\n+  assert_different_registers(src, dst);\n+\n+  Label slow, done;\n+\n+  \/\/ Check if we can take the (common) fast path, if obj is unlocked.\n+  ldr(dst, Address(src, oopDesc::mark_offset_in_bytes()));\n+  eor(dst, dst, markWord::unlocked_value);\n+  tst(dst, markWord::lock_mask_in_place);\n+  br(Assembler::NE, slow);\n+\n+  \/\/ Fast-path: shift and decode Klass*.\n+  lsr(dst, dst, markWord::klass_shift);\n+  b(done);\n+\n+  bind(slow);\n+  RegSet saved_regs = RegSet::of(lr);\n+  \/\/ We need r0 as argument and return register for the call. Preserve it, if necessary.\n+  if (dst != r0) {\n+    saved_regs += RegSet::of(r0);\n+  }\n+  push(saved_regs, sp);\n+  mov(r0, src);\n+  assert(StubRoutines::load_nklass() != NULL, \"Must have stub\");\n+  far_call(RuntimeAddress(StubRoutines::load_nklass()));\n+  if (dst != r0) {\n+    mov(dst, r0);\n@@ -4046,0 +4074,7 @@\n+  pop(saved_regs, sp);\n+  bind(done);\n+}\n+\n+void MacroAssembler::load_klass(Register dst, Register src) {\n+  load_nklass(dst, src);\n+  decode_klass_not_null(dst);\n@@ -4080,14 +4115,10 @@\n-  if (UseCompressedClassPointers) {\n-    ldrw(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));\n-    if (CompressedKlassPointers::base() == NULL) {\n-      cmp(trial_klass, tmp, LSL, CompressedKlassPointers::shift());\n-      return;\n-    } else if (((uint64_t)CompressedKlassPointers::base() & 0xffffffff) == 0\n-               && CompressedKlassPointers::shift() == 0) {\n-      \/\/ Only the bottom 32 bits matter\n-      cmpw(trial_klass, tmp);\n-      return;\n-    }\n-    decode_klass_not_null(tmp);\n-  } else {\n-    ldr(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));\n+  assert(UseCompressedClassPointers, \"Lilliput\");\n+  load_nklass(tmp, oop);\n+  if (CompressedKlassPointers::base() == NULL) {\n+    cmp(trial_klass, tmp, LSL, CompressedKlassPointers::shift());\n+    return;\n+  } else if (((uint64_t)CompressedKlassPointers::base() & 0xffffffff) == 0\n+             && CompressedKlassPointers::shift() == 0) {\n+    \/\/ Only the bottom 32 bits matter\n+    cmpw(trial_klass, tmp);\n+    return;\n@@ -4095,0 +4126,1 @@\n+  decode_klass_not_null(tmp);\n@@ -4098,18 +4130,0 @@\n-void MacroAssembler::store_klass(Register dst, Register src) {\n-  \/\/ FIXME: Should this be a store release?  concurrent gcs assumes\n-  \/\/ klass length is valid if klass field is not null.\n-  if (UseCompressedClassPointers) {\n-    encode_klass_not_null(src);\n-    strw(src, Address(dst, oopDesc::klass_offset_in_bytes()));\n-  } else {\n-    str(src, Address(dst, oopDesc::klass_offset_in_bytes()));\n-  }\n-}\n-\n-void MacroAssembler::store_klass_gap(Register dst, Register src) {\n-  if (UseCompressedClassPointers) {\n-    \/\/ Store to klass gap in destination\n-    strw(src, Address(dst, oopDesc::klass_gap_offset_in_bytes()));\n-  }\n-}\n-\n@@ -4250,0 +4264,14 @@\n+\/\/ Returns a static string\n+const char* MacroAssembler::describe_klass_decode_mode(MacroAssembler::KlassDecodeMode mode) {\n+  switch (mode) {\n+  case KlassDecodeNone: return \"none\";\n+  case KlassDecodeZero: return \"zero\";\n+  case KlassDecodeXor:  return \"xor\";\n+  case KlassDecodeMovk: return \"movk\";\n+  default:\n+    ShouldNotReachHere();\n+  }\n+  return NULL;\n+}\n+\n+\/\/ Return the current narrow Klass pointer decode mode.\n@@ -4251,2 +4279,4 @@\n-  assert(UseCompressedClassPointers, \"not using compressed class pointers\");\n-  assert(Metaspace::initialized(), \"metaspace not initialized yet\");\n+  if (_klass_decode_mode == KlassDecodeNone) {\n+    \/\/ First time initialization\n+    assert(UseCompressedClassPointers, \"not using compressed class pointers\");\n+    assert(Metaspace::initialized(), \"metaspace not initialized yet\");\n@@ -4254,2 +4284,5 @@\n-  if (_klass_decode_mode != KlassDecodeNone) {\n-    return _klass_decode_mode;\n+    _klass_decode_mode = klass_decode_mode_for_base(CompressedKlassPointers::base());\n+    guarantee(_klass_decode_mode != KlassDecodeNone,\n+              PTR_FORMAT \" is not a valid encoding base on aarch64\",\n+              p2i(CompressedKlassPointers::base()));\n+    log_info(metaspace)(\"klass decode mode initialized: %s\", describe_klass_decode_mode(_klass_decode_mode));\n@@ -4257,0 +4290,2 @@\n+  return _klass_decode_mode;\n+}\n@@ -4258,2 +4293,4 @@\n-  assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift()\n-         || 0 == CompressedKlassPointers::shift(), \"decode alg wrong\");\n+\/\/ Given an arbitrary base address, return the KlassDecodeMode that would be used. Return KlassDecodeNone\n+\/\/ if base address is not valid for encoding.\n+MacroAssembler::KlassDecodeMode MacroAssembler::klass_decode_mode_for_base(address base) {\n+  assert(CompressedKlassPointers::shift() != 0, \"not lilliput?\");\n@@ -4261,2 +4298,4 @@\n-  if (CompressedKlassPointers::base() == NULL) {\n-    return (_klass_decode_mode = KlassDecodeZero);\n+  const uint64_t base_u64 = (uint64_t) base;\n+\n+  if (base_u64 == 0) {\n+    return KlassDecodeZero;\n@@ -4265,7 +4304,3 @@\n-  if (operand_valid_for_logical_immediate(\n-        \/*is32*\/false, (uint64_t)CompressedKlassPointers::base())) {\n-    const uint64_t range_mask =\n-      (1ULL << log2i(CompressedKlassPointers::range())) - 1;\n-    if (((uint64_t)CompressedKlassPointers::base() & range_mask) == 0) {\n-      return (_klass_decode_mode = KlassDecodeXor);\n-    }\n+  if (operand_valid_for_logical_immediate(false, base_u64) &&\n+      ((base_u64 & (KlassEncodingMetaspaceMax - 1)) == 0)) {\n+    return KlassDecodeXor;\n@@ -4274,4 +4309,4 @@\n-  const uint64_t shifted_base =\n-    (uint64_t)CompressedKlassPointers::base() >> CompressedKlassPointers::shift();\n-  guarantee((shifted_base & 0xffff0000ffffffff) == 0,\n-            \"compressed class base bad alignment\");\n+  const uint64_t shifted_base = base_u64 >> CompressedKlassPointers::shift();\n+  if ((shifted_base & 0xffff0000ffffffff) == 0) {\n+    return KlassDecodeMovk;\n+  }\n@@ -4279,1 +4314,1 @@\n-  return (_klass_decode_mode = KlassDecodeMovk);\n+  return KlassDecodeNone;\n@@ -4283,0 +4318,2 @@\n+  assert (UseCompressedClassPointers, \"should only be used for compressed headers\");\n+  assert(CompressedKlassPointers::shift() != 0, \"not lilliput?\");\n@@ -4285,5 +4322,1 @@\n-    if (CompressedKlassPointers::shift() != 0) {\n-      lsr(dst, src, LogKlassAlignmentInBytes);\n-    } else {\n-      if (dst != src) mov(dst, src);\n-    }\n+    lsr(dst, src, LogKlassAlignmentInBytes);\n@@ -4293,6 +4326,2 @@\n-    if (CompressedKlassPointers::shift() != 0) {\n-      eor(dst, src, (uint64_t)CompressedKlassPointers::base());\n-      lsr(dst, dst, LogKlassAlignmentInBytes);\n-    } else {\n-      eor(dst, src, (uint64_t)CompressedKlassPointers::base());\n-    }\n+    eor(dst, src, (uint64_t)CompressedKlassPointers::base());\n+    lsr(dst, dst, LogKlassAlignmentInBytes);\n@@ -4302,5 +4331,1 @@\n-    if (CompressedKlassPointers::shift() != 0) {\n-      ubfx(dst, src, LogKlassAlignmentInBytes, 32);\n-    } else {\n-      movw(dst, src);\n-    }\n+    ubfx(dst, src, LogKlassAlignmentInBytes, MaxNarrowKlassPointerBits);\n@@ -4322,0 +4347,2 @@\n+  assert(CompressedKlassPointers::shift() != 0, \"not lilliput?\");\n+\n@@ -4324,5 +4351,1 @@\n-    if (CompressedKlassPointers::shift() != 0) {\n-      lsl(dst, src, LogKlassAlignmentInBytes);\n-    } else {\n-      if (dst != src) mov(dst, src);\n-    }\n+    if (dst != src) mov(dst, src);\n@@ -4332,6 +4355,2 @@\n-    if (CompressedKlassPointers::shift() != 0) {\n-      lsl(dst, src, LogKlassAlignmentInBytes);\n-      eor(dst, dst, (uint64_t)CompressedKlassPointers::base());\n-    } else {\n-      eor(dst, src, (uint64_t)CompressedKlassPointers::base());\n-    }\n+    lsl(dst, src, LogKlassAlignmentInBytes);\n+    eor(dst, dst, (uint64_t)CompressedKlassPointers::base());\n@@ -4344,0 +4363,3 @@\n+    \/\/ Invalid base should have been gracefully handled via klass_decode_mode() in VM initialization.\n+    assert((shifted_base & 0xffff0000ffffffff) == 0, \"incompatible base\");\n+\n@@ -4346,5 +4368,1 @@\n-\n-    if (CompressedKlassPointers::shift() != 0) {\n-      lsl(dst, dst, LogKlassAlignmentInBytes);\n-    }\n-\n+    lsl(dst, dst, LogKlassAlignmentInBytes);\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":108,"deletions":90,"binary":false,"changes":198,"status":"modified"},{"patch":"@@ -89,0 +89,2 @@\n+ public:\n+\n@@ -96,1 +98,9 @@\n-  KlassDecodeMode klass_decode_mode();\n+  \/\/ Return the current narrow Klass pointer decode mode. Initialized on first call.\n+  static KlassDecodeMode klass_decode_mode();\n+\n+  \/\/ Given an arbitrary base address, return the KlassDecodeMode that would be used. Return KlassDecodeNone\n+  \/\/ if base address is not valid for encoding.\n+  static KlassDecodeMode klass_decode_mode_for_base(address base);\n+\n+  \/\/ Returns a static string\n+  static const char* describe_klass_decode_mode(KlassDecodeMode mode);\n@@ -99,0 +109,1 @@\n+\n@@ -838,0 +849,1 @@\n+  void load_nklass(Register dst, Register src);\n@@ -839,1 +851,0 @@\n-  void store_klass(Register dst, Register src);\n@@ -865,2 +876,0 @@\n-  void store_klass_gap(Register dst, Register src);\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":13,"deletions":4,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -325,1 +325,1 @@\n-        __ null_check(receiver_reg, oopDesc::klass_offset_in_bytes());\n+        __ null_check(receiver_reg, oopDesc::mark_offset_in_bytes());\n","filename":"src\/hotspot\/cpu\/aarch64\/methodHandles_aarch64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -597,2 +597,12 @@\n-    __ load_klass(r0, r0);  \/\/ get klass\n-    __ cbz(r0, error);      \/\/ if klass is NULL it is broken\n+    \/\/ NOTE: We used to load the Klass* here, and compare that to zero.\n+    \/\/ However, with current Lilliput implementation, that would require\n+    \/\/ checking the locking bits and calling into the runtime, which\n+    \/\/ clobbers the condition flags, which may be live around this call.\n+    \/\/ OTOH, this is a simple NULL-check, and we can simply load the upper\n+    \/\/ 32bit of the header as narrowKlass, and compare that to 0. The\n+    \/\/ worst that can happen (rarely) is that the object is locked and\n+    \/\/ we have lock pointer bits in the upper 32bits. We can't get a false\n+    \/\/ negative.\n+    assert(oopDesc::klass_offset_in_bytes() % 4 == 0, \"must be 4 byte aligned\");\n+    __ ldrw(r0, Address(r0, oopDesc::klass_offset_in_bytes()));  \/\/ get klass\n+    __ cbzw(r0, error);      \/\/ if klass is NULL it is broken\n@@ -6615,0 +6625,23 @@\n+  \/\/ Pass object argument in r0 (which has to be preserved outside this stub)\n+  \/\/ Pass back result in r0\n+  \/\/ Clobbers rscratch1\n+  address generate_load_nklass() {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", \"load_nklass\");\n+\n+    address start = __ pc();\n+\n+    __ set_last_Java_frame(sp, rfp, lr, rscratch1);\n+    __ enter();\n+    __ push(RegSet::of(rscratch1, rscratch2), sp);\n+    __ push_call_clobbered_registers_except(r0);\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, oopDesc::load_nklass_runtime), 1);\n+    __ pop_call_clobbered_registers_except(r0);\n+    __ pop(RegSet::of(rscratch1, rscratch2), sp);\n+    __ leave();\n+    __ reset_last_Java_frame(true);\n+    __ ret(lr);\n+\n+    return start;\n+  }\n+\n@@ -7844,0 +7877,2 @@\n+\n+    StubRoutines::_load_nklass = generate_load_nklass();\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":37,"deletions":2,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -3238,1 +3238,1 @@\n-  __ null_check(recv, oopDesc::klass_offset_in_bytes());\n+  __ null_check(recv, oopDesc::mark_offset_in_bytes());\n@@ -3328,1 +3328,1 @@\n-  __ null_check(r2, oopDesc::klass_offset_in_bytes());\n+  __ null_check(r2, oopDesc::mark_offset_in_bytes());\n@@ -3345,1 +3345,1 @@\n-  __ null_check(r2, oopDesc::klass_offset_in_bytes());\n+  __ null_check(r2, oopDesc::mark_offset_in_bytes());\n@@ -3533,1 +3533,1 @@\n-    __ mov(rscratch1, (intptr_t)markWord::prototype().value());\n+    __ ldr(rscratch1, Address(r4, Klass::prototype_header_offset()));\n@@ -3535,2 +3535,0 @@\n-    __ store_klass_gap(r0, zr);  \/\/ zero klass gap for compressed oops\n-    __ store_klass(r0, r4);      \/\/ store klass last\n@@ -3666,1 +3664,2 @@\n-  __ load_klass(r3, r3);\n+  __ load_klass(rscratch1, r3);\n+  __ mov(r3, rscratch1);\n","filename":"src\/hotspot\/cpu\/aarch64\/templateTable_aarch64.cpp","additions":6,"deletions":7,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -974,1 +974,1 @@\n-                      arrayOopDesc::header_size(op->type()),\n+                      arrayOopDesc::base_offset_in_bytes(op->type()),\n","filename":"src\/hotspot\/cpu\/arm\/c1_LIRAssembler_arm.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -160,1 +160,1 @@\n-                                       int header_size, int element_size,\n+                                       int header_size_in_bytes, int element_size,\n@@ -163,1 +163,0 @@\n-  const int header_size_in_bytes = header_size * BytesPerWord;\n","filename":"src\/hotspot\/cpu\/arm\/c1_MacroAssembler_arm.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n","filename":"src\/hotspot\/cpu\/ppc\/macroAssembler_ppc.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -273,0 +273,5 @@\n+void LoadKlassStub::emit_code(LIR_Assembler* ce) {\n+  \/\/ Currently not needed.\n+  Unimplemented();\n+}\n+\n","filename":"src\/hotspot\/cpu\/s390\/c1_CodeStubs_s390.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n","filename":"src\/hotspot\/cpu\/s390\/macroAssembler_s390.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1636,1 +1636,1 @@\n-                      arrayOopDesc::header_size(op->type()),\n+                      arrayOopDesc::base_offset_in_bytes(op->type()),\n@@ -3066,0 +3066,1 @@\n+  Register tmp2 = LP64_ONLY(rscratch2) NOT_LP64(noreg);\n@@ -3190,0 +3191,1 @@\n+#ifndef _LP64\n@@ -3192,1 +3194,1 @@\n-\n+#endif\n@@ -3257,7 +3259,8 @@\n-      if (UseCompressedClassPointers) {\n-        __ movl(tmp, src_klass_addr);\n-        __ cmpl(tmp, dst_klass_addr);\n-      } else {\n-        __ movptr(tmp, src_klass_addr);\n-        __ cmpptr(tmp, dst_klass_addr);\n-      }\n+#ifdef _LP64\n+      __ load_nklass(tmp, src);\n+      __ load_nklass(tmp2, dst);\n+      __ cmpl(tmp, tmp2);\n+#else\n+      __ movptr(tmp, src_klass_addr);\n+      __ cmpptr(tmp, dst_klass_addr);\n+#endif\n@@ -3419,5 +3422,2 @@\n-    if (UseCompressedClassPointers) {\n-      __ encode_klass_not_null(tmp, rscratch1);\n-    }\n-#endif\n-\n+    assert(UseCompressedClassPointers, \"Lilliput\");\n+    __ encode_klass_not_null(tmp, rscratch1);\n@@ -3425,3 +3425,12 @@\n-\n-      if (UseCompressedClassPointers)          __ cmpl(tmp, dst_klass_addr);\n-      else                   __ cmpptr(tmp, dst_klass_addr);\n+      __ load_nklass(tmp2, dst);\n+      __ cmpl(tmp, tmp2);\n+      __ jcc(Assembler::notEqual, halt);\n+      __ load_nklass(tmp2, src);\n+      __ cmpl(tmp, tmp2);\n+      __ jcc(Assembler::equal, known_ok);\n+    } else {\n+      __ load_nklass(tmp2, dst);\n+      __ cmpl(tmp, tmp2);\n+#else\n+    if (basic_type != T_OBJECT) {\n+      __ cmpptr(tmp, dst_klass_addr);\n@@ -3429,2 +3438,1 @@\n-      if (UseCompressedClassPointers)          __ cmpl(tmp, src_klass_addr);\n-      else                   __ cmpptr(tmp, src_klass_addr);\n+      __ cmpptr(tmp, src_klass_addr);\n@@ -3433,2 +3441,2 @@\n-      if (UseCompressedClassPointers)          __ cmpl(tmp, dst_klass_addr);\n-      else                   __ cmpptr(tmp, dst_klass_addr);\n+      __ cmpptr(tmp, dst_klass_addr);\n+#endif\n@@ -3523,3 +3531,2 @@\n-  CodeEmitInfo* info = op->info();\n-  if (info != NULL) {\n-    add_debug_info_for_null_check_here(info);\n+  if (op->info() != NULL) {\n+    add_debug_info_for_null_check_here(op->info());\n@@ -3527,5 +3534,20 @@\n-\n-  if (UseCompressedClassPointers) {\n-    __ movl(result, Address(obj, oopDesc::klass_offset_in_bytes()));\n-    __ decode_klass_not_null(result, rscratch1);\n-  } else\n+  Register tmp = rscratch1;\n+  assert_different_registers(tmp, obj);\n+  assert_different_registers(tmp, result);\n+\n+  \/\/ Check if we can take the (common) fast path, if obj is unlocked.\n+  __ movq(tmp, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  __ xorq(tmp, markWord::unlocked_value);\n+  __ testb(tmp, markWord::lock_mask_in_place);\n+  __ jcc(Assembler::notZero, *op->stub()->entry());\n+\n+  \/\/ Fast-path: shift and decode Klass*.\n+  __ movq(result, tmp);\n+  __ shrq(result, markWord::klass_shift);\n+\n+  __ bind(*op->stub()->continuation());\n+  __ decode_klass_not_null(result, tmp);\n+#else\n+  __ movptr(result, Address(obj, oopDesc::klass_offset_in_bytes()));\n+  \/\/ Not really needed, but bind the label anyway to make compiler happy.\n+  __ bind(*op->stub()->continuation());\n@@ -3534,1 +3556,0 @@\n-    __ movptr(result, Address(obj, oopDesc::klass_offset_in_bytes()));\n@@ -3640,4 +3661,0 @@\n-#ifndef ASSERT\n-      __ jmpb(next);\n-    }\n-#else\n@@ -3646,0 +3663,1 @@\n+#ifdef ASSERT\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":52,"deletions":34,"binary":false,"changes":86,"status":"modified"},{"patch":"@@ -147,9 +147,5 @@\n-  assert_different_registers(obj, klass, len);\n-  \/\/ This assumes that all prototype bits fit in an int32_t\n-  movptr(Address(obj, oopDesc::mark_offset_in_bytes ()), (int32_t)(intptr_t)markWord::prototype().value());\n-#ifdef _LP64\n-  if (UseCompressedClassPointers) { \/\/ Take care not to kill klass\n-    movptr(t1, klass);\n-    encode_klass_not_null(t1, tmp_encode_klass);\n-    movl(Address(obj, oopDesc::klass_offset_in_bytes()), t1);\n-  } else\n+  assert_different_registers(obj, klass, len, t1, t2);\n+  movptr(t1, Address(klass, Klass::prototype_header_offset()));\n+  movptr(Address(obj, oopDesc::mark_offset_in_bytes()), t1);\n+#ifndef _LP64\n+  movptr(Address(obj, oopDesc::klass_offset_in_bytes()), klass);\n@@ -158,3 +154,0 @@\n-  {\n-    movptr(Address(obj, oopDesc::klass_offset_in_bytes()), klass);\n-  }\n@@ -165,6 +158,0 @@\n-#ifdef _LP64\n-  else if (UseCompressedClassPointers) {\n-    xorptr(t1, t1);\n-    store_klass_gap(obj, t1);\n-  }\n-#endif\n@@ -248,1 +235,1 @@\n-void C1_MacroAssembler::allocate_array(Register obj, Register len, Register t1, Register t2, int header_size, Address::ScaleFactor f, Register klass, Label& slow_case) {\n+void C1_MacroAssembler::allocate_array(Register obj, Register len, Register t1, Register t2, int base_offset_in_bytes, Address::ScaleFactor f, Register klass, Label& slow_case) {\n@@ -261,1 +248,1 @@\n-  movptr(arr_size, (int32_t)header_size * BytesPerWord + MinObjAlignmentInBytesMask);\n+  movptr(arr_size, (int32_t)base_offset_in_bytes + MinObjAlignmentInBytesMask);\n@@ -271,1 +258,1 @@\n-  initialize_body(obj, arr_size, header_size * BytesPerWord, len_zero);\n+  initialize_body(obj, arr_size, base_offset_in_bytes, len_zero);\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":8,"deletions":21,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -1047,1 +1047,11 @@\n-\n+#ifdef _LP64\n+    case load_klass_id:\n+      {\n+        StubFrame f(sasm, \"load_klass\", dont_gc_arguments);\n+        sasm->save_live_registers_no_oop_map(true);\n+        f.load_argument(0, c_rarg0); \/\/ obj\n+        __ call_VM_leaf(CAST_FROM_FN_PTR(address, oopDesc::load_nklass_runtime), c_rarg0);\n+        sasm->restore_live_registers_except_rax(true);\n+      }\n+      break;\n+#endif\n","filename":"src\/hotspot\/cpu\/x86\/c1_Runtime1_x86.cpp","additions":11,"deletions":1,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -74,6 +74,0 @@\n-#if INCLUDE_JVMCI\n-#define COMPRESSED_CLASS_POINTERS_DEPENDS_ON_COMPRESSED_OOPS EnableJVMCI\n-#else\n-#define COMPRESSED_CLASS_POINTERS_DEPENDS_ON_COMPRESSED_OOPS false\n-#endif\n-\n","filename":"src\/hotspot\/cpu\/x86\/globalDefinitions_x86.hpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -59,1 +59,1 @@\n-  jmpb(next);\n+  jmp(next);\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"logging\/log.hpp\"\n@@ -40,0 +41,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n@@ -3926,1 +3928,1 @@\n-  assert((offset_in_bytes & (BytesPerWord - 1)) == 0, \"offset must be a multiple of BytesPerWord\");\n+  assert((offset_in_bytes & (BytesPerInt - 1)) == 0, \"offset must be a multiple of BytesPerInt\");\n@@ -3932,0 +3934,13 @@\n+  \/\/ Emit single 32bit store to clear leading bytes, if necessary.\n+  xorptr(temp, temp);    \/\/ use _zero reg to clear memory (shorter code)\n+#ifdef _LP64\n+  if (!is_aligned(offset_in_bytes, BytesPerWord)) {\n+    movl(Address(address, offset_in_bytes), temp);\n+    offset_in_bytes += BytesPerInt;\n+    decrement(length_in_bytes, BytesPerInt);\n+  }\n+  assert((offset_in_bytes & (BytesPerWord - 1)) == 0, \"offset must be a multiple of BytesPerWord\");\n+  testptr(length_in_bytes, length_in_bytes);\n+  jcc(Assembler::zero, done);\n+#endif\n+\n@@ -3944,1 +3959,0 @@\n-  xorptr(temp, temp);    \/\/ use _zero reg to clear memory (shorter code)\n@@ -4878,9 +4892,30 @@\n-void MacroAssembler::load_klass(Register dst, Register src, Register tmp) {\n-  assert_different_registers(src, tmp);\n-  assert_different_registers(dst, tmp);\n-  if (UseCompressedClassPointers) {\n-    movl(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n-    decode_klass_not_null(dst, tmp);\n-  } else\n-#endif\n-    movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+void MacroAssembler::load_nklass(Register dst, Register src) {\n+  assert_different_registers(src, dst);\n+  assert(UseCompressedClassPointers, \"expect compressed class pointers\");\n+\n+  Label slow, done;\n+  movq(dst, Address(src, oopDesc::mark_offset_in_bytes()));\n+  \/\/ NOTE: While it would seem nice to use xorb instead (for which we don't have an encoding in our assembler),\n+  \/\/ the encoding for xorq uses the signed version (0x81\/6) of xor, which encodes as compact as xorb would,\n+  \/\/ and does't make a difference performance-wise.\n+  xorq(dst, markWord::unlocked_value);\n+  testb(dst, markWord::lock_mask_in_place);\n+  jccb(Assembler::notZero, slow);\n+\n+  shrq(dst, markWord::klass_shift);\n+  jmp(done);\n+  bind(slow);\n+\n+  if (dst != rax) {\n+    push(rax);\n+  }\n+  if (src != rax) {\n+    mov(rax, src);\n+  }\n+  call(RuntimeAddress(StubRoutines::load_nklass()));\n+  if (dst != rax) {\n+    mov(dst, rax);\n+    pop(rax);\n+  }\n+\n+  bind(done);\n@@ -4889,0 +4924,1 @@\n+#endif\n@@ -4890,1 +4926,1 @@\n-void MacroAssembler::store_klass(Register dst, Register src, Register tmp) {\n+void MacroAssembler::load_klass(Register dst, Register src, Register tmp, bool null_check_src) {\n@@ -4894,4 +4930,18 @@\n-  if (UseCompressedClassPointers) {\n-    encode_klass_not_null(src, tmp);\n-    movl(Address(dst, oopDesc::klass_offset_in_bytes()), src);\n-  } else\n+  assert(UseCompressedClassPointers, \"expect compressed class pointers\");\n+  Register d = dst;\n+  if (src == dst) {\n+    d = tmp;\n+  }\n+  if (null_check_src) {\n+    null_check(src, oopDesc::mark_offset_in_bytes());\n+  }\n+  load_nklass(d, src);\n+  if (src == dst) {\n+    mov(dst, d);\n+  }\n+  decode_klass_not_null(dst, tmp);\n+#else\n+  if (null_check_src) {\n+    null_check(src, oopDesc::klass_offset_in_bytes());\n+  }\n+  movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n@@ -4899,1 +4949,0 @@\n-    movptr(Address(dst, oopDesc::klass_offset_in_bytes()), src);\n@@ -4902,0 +4951,6 @@\n+#ifndef _LP64\n+void MacroAssembler::store_klass(Register dst, Register src) {\n+  movptr(Address(dst, oopDesc::klass_offset_in_bytes()), src);\n+}\n+#endif\n+\n@@ -4948,7 +5003,0 @@\n-void MacroAssembler::store_klass_gap(Register dst, Register src) {\n-  if (UseCompressedClassPointers) {\n-    \/\/ Store to klass gap in destination\n-    movl(Address(dst, oopDesc::klass_gap_offset_in_bytes()), src);\n-  }\n-}\n-\n@@ -5107,0 +5155,62 @@\n+MacroAssembler::KlassDecodeMode MacroAssembler::_klass_decode_mode = KlassDecodeNone;\n+\n+\/\/ Returns a static string\n+const char* MacroAssembler::describe_klass_decode_mode(MacroAssembler::KlassDecodeMode mode) {\n+  switch (mode) {\n+  case KlassDecodeNone: return \"none\";\n+  case KlassDecodeZero: return \"zero\";\n+  case KlassDecodeXor:  return \"xor\";\n+  case KlassDecodeAdd:  return \"add\";\n+  default:\n+    ShouldNotReachHere();\n+  }\n+  return NULL;\n+}\n+\n+\/\/ Return the current narrow Klass pointer decode mode.\n+MacroAssembler::KlassDecodeMode MacroAssembler::klass_decode_mode() {\n+  if (_klass_decode_mode == KlassDecodeNone) {\n+    \/\/ First time initialization\n+    assert(UseCompressedClassPointers, \"not using compressed class pointers\");\n+    assert(Metaspace::initialized(), \"metaspace not initialized yet\");\n+\n+    _klass_decode_mode = klass_decode_mode_for_base(CompressedKlassPointers::base());\n+    guarantee(_klass_decode_mode != KlassDecodeNone,\n+              PTR_FORMAT \" is not a valid encoding base on aarch64\",\n+              p2i(CompressedKlassPointers::base()));\n+    log_info(metaspace)(\"klass decode mode initialized: %s\", describe_klass_decode_mode(_klass_decode_mode));\n+  }\n+  return _klass_decode_mode;\n+}\n+\n+\/\/ Given an arbitrary base address, return the KlassDecodeMode that would be used. Return KlassDecodeNone\n+\/\/ if base address is not valid for encoding.\n+MacroAssembler::KlassDecodeMode MacroAssembler::klass_decode_mode_for_base(address base) {\n+  assert(CompressedKlassPointers::shift() != 0, \"not lilliput?\");\n+\n+  const uint64_t base_u64 = (uint64_t) base;\n+\n+  if (base_u64 == 0) {\n+    return KlassDecodeZero;\n+  }\n+\n+  if ((base_u64 & (KlassEncodingMetaspaceMax - 1)) == 0) {\n+    return KlassDecodeXor;\n+  }\n+\n+  \/\/ Note that there is no point in optimizing for shift=3 since lilliput\n+  \/\/ will use larger shifts\n+\n+  \/\/ The add+shift mode for decode_and_move_klass_not_null() requires the base to be\n+  \/\/  shiftable-without-loss. So, this is the minimum restriction on x64 for a valid\n+  \/\/  encoding base. This does not matter in reality since the shift values we use for\n+  \/\/  Lilliput, while large, won't be larger than a page size. And the encoding base\n+  \/\/  will be quite likely page aligned since it usually falls to the beginning of\n+  \/\/  either CDS or CCS.\n+  if ((base_u64 & (KlassAlignmentInBytes - 1)) == 0) {\n+    return KlassDecodeAdd;\n+  }\n+\n+  return KlassDecodeNone;\n+}\n+\n@@ -5109,1 +5219,12 @@\n-  if (CompressedKlassPointers::base() != NULL) {\n+  switch (klass_decode_mode()) {\n+  case KlassDecodeZero: {\n+    shrq(r, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  case KlassDecodeXor: {\n+    mov64(tmp, (int64_t)CompressedKlassPointers::base());\n+    xorq(r, tmp);\n+    shrq(r, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  case KlassDecodeAdd: {\n@@ -5112,0 +5233,2 @@\n+    shrq(r, CompressedKlassPointers::shift());\n+    break;\n@@ -5113,3 +5236,2 @@\n-  if (CompressedKlassPointers::shift() != 0) {\n-    assert (LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-    shrq(r, LogKlassAlignmentInBytes);\n+  default:\n+    ShouldNotReachHere();\n@@ -5121,1 +5243,13 @@\n-  if (CompressedKlassPointers::base() != NULL) {\n+  switch (klass_decode_mode()) {\n+  case KlassDecodeZero: {\n+    movptr(dst, src);\n+    shrq(dst, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  case KlassDecodeXor: {\n+    mov64(dst, (int64_t)CompressedKlassPointers::base());\n+    xorq(dst, src);\n+    shrq(dst, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  case KlassDecodeAdd: {\n@@ -5124,2 +5258,2 @@\n-  } else {\n-    movptr(dst, src);\n+    shrq(dst, CompressedKlassPointers::shift());\n+    break;\n@@ -5127,3 +5261,2 @@\n-  if (CompressedKlassPointers::shift() != 0) {\n-    assert (LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-    shrq(dst, LogKlassAlignmentInBytes);\n+  default:\n+    ShouldNotReachHere();\n@@ -5135,8 +5268,5 @@\n-  \/\/ Note: it will change flags\n-  assert(UseCompressedClassPointers, \"should only be used for compressed headers\");\n-  \/\/ Cannot assert, unverified entry point counts instructions (see .ad file)\n-  \/\/ vtableStubs also counts instructions in pd_code_size_limit.\n-  \/\/ Also do not verify_oop as this is called by verify_oop.\n-  if (CompressedKlassPointers::shift() != 0) {\n-    assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-    shlq(r, LogKlassAlignmentInBytes);\n+  const uint64_t base_u64 = (uint64_t)CompressedKlassPointers::base();\n+  switch (klass_decode_mode()) {\n+  case KlassDecodeZero: {\n+    shlq(r, CompressedKlassPointers::shift());\n+    break;\n@@ -5144,2 +5274,11 @@\n-  if (CompressedKlassPointers::base() != NULL) {\n-    mov64(tmp, (int64_t)CompressedKlassPointers::base());\n+  case KlassDecodeXor: {\n+    assert((base_u64 & (KlassEncodingMetaspaceMax - 1)) == 0,\n+           \"base \" UINT64_FORMAT_X \" invalid for xor mode\", base_u64); \/\/ should have been handled at VM init.\n+    shlq(r, CompressedKlassPointers::shift());\n+    mov64(tmp, base_u64);\n+    xorq(r, tmp);\n+    break;\n+  }\n+  case KlassDecodeAdd: {\n+    shlq(r, CompressedKlassPointers::shift());\n+    mov64(tmp, base_u64);\n@@ -5147,0 +5286,4 @@\n+    break;\n+  }\n+  default:\n+    ShouldNotReachHere();\n@@ -5152,3 +5295,1 @@\n-  \/\/ Note: it will change flags\n-  assert (UseCompressedClassPointers, \"should only be used for compressed headers\");\n-  \/\/ Cannot assert, unverified entry point counts instructions (see .ad file)\n+  \/\/ Note: Cannot assert, unverified entry point counts instructions (see .ad file)\n@@ -5158,18 +5299,28 @@\n-  if (CompressedKlassPointers::base() == NULL &&\n-      CompressedKlassPointers::shift() == 0) {\n-    \/\/ The best case scenario is that there is no base or shift. Then it is already\n-    \/\/ a pointer that needs nothing but a register rename.\n-    movl(dst, src);\n-  } else {\n-    if (CompressedKlassPointers::base() != NULL) {\n-      mov64(dst, (int64_t)CompressedKlassPointers::base());\n-    } else {\n-      xorq(dst, dst);\n-    }\n-    if (CompressedKlassPointers::shift() != 0) {\n-      assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-      assert(LogKlassAlignmentInBytes == Address::times_8, \"klass not aligned on 64bits?\");\n-      leaq(dst, Address(dst, src, Address::times_8, 0));\n-    } else {\n-      addq(dst, src);\n-    }\n+  const uint64_t base_u64 = (uint64_t)CompressedKlassPointers::base();\n+\n+  switch (klass_decode_mode()) {\n+  case KlassDecodeZero: {\n+    movq(dst, src);\n+    shlq(dst, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  case KlassDecodeXor: {\n+    assert((base_u64 & (KlassEncodingMetaspaceMax - 1)) == 0,\n+           \"base \" UINT64_FORMAT_X \" invalid for xor mode\", base_u64); \/\/ should have been handled at VM init.\n+    const uint64_t base_right_shifted = base_u64 >> CompressedKlassPointers::shift();\n+    mov64(dst, base_right_shifted);\n+    xorq(dst, src);\n+    shlq(dst, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  case KlassDecodeAdd: {\n+    assert((base_u64 & (KlassAlignmentInBytes - 1)) == 0,\n+           \"base \" UINT64_FORMAT_X \" invalid for add mode\", base_u64); \/\/ should have been handled at VM init.\n+    const uint64_t base_right_shifted = base_u64 >> CompressedKlassPointers::shift();\n+    mov64(dst, base_right_shifted);\n+    addq(dst, src);\n+    shlq(dst, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  default:\n+    ShouldNotReachHere();\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":216,"deletions":65,"binary":false,"changes":281,"status":"modified"},{"patch":"@@ -82,0 +82,23 @@\n+ public:\n+\n+  enum KlassDecodeMode {\n+    KlassDecodeNone,\n+    KlassDecodeZero,\n+    KlassDecodeXor,\n+    KlassDecodeAdd\n+  };\n+\n+  \/\/ Return the current narrow Klass pointer decode mode. Initialized on first call.\n+  static KlassDecodeMode klass_decode_mode();\n+\n+  \/\/ Given an arbitrary base address, return the KlassDecodeMode that would be used. Return KlassDecodeNone\n+  \/\/ if base address is not valid for encoding.\n+  static KlassDecodeMode klass_decode_mode_for_base(address base);\n+\n+  \/\/ Returns a static string\n+  static const char* describe_klass_decode_mode(KlassDecodeMode mode);\n+\n+ private:\n+\n+  static KlassDecodeMode _klass_decode_mode;\n+\n@@ -348,2 +371,6 @@\n-  void load_klass(Register dst, Register src, Register tmp);\n-  void store_klass(Register dst, Register src, Register tmp);\n+  void load_klass(Register dst, Register src, Register tmp, bool null_check_src = false);\n+#ifdef _LP64\n+  void load_nklass(Register dst, Register src);\n+#else\n+  void store_klass(Register dst, Register src);\n+#endif\n@@ -368,2 +395,0 @@\n-  void store_klass_gap(Register dst, Register src);\n-\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":29,"deletions":4,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -381,2 +381,1 @@\n-        __ null_check(receiver_reg, oopDesc::klass_offset_in_bytes());\n-        __ load_klass(temp1_recv_klass, receiver_reg, temp2);\n+        __ load_klass(temp1_recv_klass, receiver_reg, temp2, true);\n","filename":"src\/hotspot\/cpu\/x86\/methodHandles_x86.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -7504,0 +7504,40 @@\n+  \/\/ Call stub to call runtime oopDesc::load_nklass_runtime().\n+  \/\/ rax: call argument (object)\n+  \/\/ rax: return object's narrowKlass\n+  \/\/ Preserves all caller-saved registers, except rax\n+#ifdef _LP64\n+  address generate_load_nklass() {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark(this, \"StubRoutines\", \"load_nklass\");\n+    address start = __ pc();\n+    __ enter(); \/\/ save rbp\n+\n+    __ andptr(rsp, -(StackAlignmentInBytes));    \/\/ Align stack\n+    __ push_FPU_state();\n+\n+    __ push(rdi);\n+    __ push(rsi);\n+    __ push(rdx);\n+    __ push(rcx);\n+    __ push(r8);\n+    __ push(r9);\n+    __ push(r10);\n+    __ push(r11);\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, oopDesc::load_nklass_runtime), rax);\n+    __ pop(r11);\n+    __ pop(r10);\n+    __ pop(r9);\n+    __ pop(r8);\n+    __ pop(rcx);\n+    __ pop(rdx);\n+    __ pop(rsi);\n+    __ pop(rdi);\n+\n+    __ pop_FPU_state();\n+\n+    __ leave();\n+    __ ret(0);\n+    return start;\n+  }\n+#endif \/\/ _LP64\n+\n@@ -7976,0 +8016,4 @@\n+\n+#ifdef _LP64\n+    StubRoutines::_load_nklass = generate_load_nklass();\n+#endif\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":44,"deletions":0,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -3661,2 +3661,1 @@\n-  __ null_check(recv, oopDesc::klass_offset_in_bytes());\n-  __ load_klass(rax, recv, tmp_load_klass);\n+  __ load_klass(rax, recv, tmp_load_klass, true);\n@@ -3754,2 +3753,1 @@\n-  __ null_check(rcx, oopDesc::klass_offset_in_bytes());\n-  __ load_klass(rlocals, rcx, tmp_load_klass);\n+  __ load_klass(rlocals, rcx, tmp_load_klass, true);\n@@ -3778,2 +3776,1 @@\n-  __ null_check(rcx, oopDesc::klass_offset_in_bytes());\n-  __ load_klass(rdx, rcx, tmp_load_klass);\n+  __ load_klass(rdx, rcx, tmp_load_klass, true);\n@@ -4001,5 +3998,4 @@\n-    __ movptr(Address(rax, oopDesc::mark_offset_in_bytes()),\n-              (intptr_t)markWord::prototype().value()); \/\/ header\n-#ifdef _LP64\n-    __ xorl(rsi, rsi); \/\/ use zero reg to clear memory (shorter code)\n-    __ store_klass_gap(rax, rsi);  \/\/ zero klass gap for compressed oops\n+    __ movptr(rbx, Address(rcx, Klass::prototype_header_offset()));\n+    __ movptr(Address(rax, oopDesc::mark_offset_in_bytes ()), rbx);\n+#ifndef _LP64\n+    __ store_klass(rax, rcx);  \/\/ klass\n@@ -4008,2 +4004,0 @@\n-    Register tmp_store_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n-    __ store_klass(rax, rcx, tmp_store_klass);  \/\/ klass\n@@ -4163,1 +4157,1 @@\n-  __ jmpb(resolved);\n+  __ jmp(resolved);\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.cpp","additions":8,"deletions":14,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -5310,1 +5310,1 @@\n-instruct loadNKlass(rRegN dst, memory mem)\n+instruct loadNKlass(rRegN dst, indOffset8 mem, rFlagsReg cr)\n@@ -5313,1 +5313,1 @@\n-\n+  effect(TEMP_DEF dst, KILL cr);\n@@ -5317,1 +5317,3 @@\n-    __ movl($dst$$Register, $mem$$Address);\n+    assert($mem$$disp == oopDesc::klass_offset_in_bytes(), \"expect correct offset 4, but got: %d\", $mem$$disp);\n+    assert($mem$$index == 4, \"expect no index register: %d\", $mem$$index);\n+    __ load_nklass($dst$$Register, $mem$$base$$Register);\n@@ -5319,1 +5321,1 @@\n-  ins_pipe(ialu_reg_mem); \/\/ XXX\n+  ins_pipe(pipe_slow); \/\/ XXX\n@@ -12907,0 +12909,3 @@\n+\/\/ Disabled because the compressed Klass* in header cannot be safely\n+\/\/ accessed. TODO: Re-enable it as soon as synchronization does not\n+\/\/ overload the upper header bits anymore.\n@@ -12909,0 +12914,1 @@\n+  predicate(false);\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":10,"deletions":4,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -557,0 +557,19 @@\n+class LoadKlassStub: public CodeStub {\n+private:\n+  LIR_Opr          _obj;\n+  LIR_Opr          _result;\n+\n+public:\n+  LoadKlassStub(LIR_Opr obj, LIR_Opr result) :\n+    CodeStub(), _obj(obj), _result(result) {};\n+\n+  virtual void emit_code(LIR_Assembler* e);\n+  virtual void visit(LIR_OpVisitState* visitor) {\n+    visitor->do_input(_obj);\n+    visitor->do_output(_result);\n+  }\n+#ifndef PRODUCT\n+virtual void print_name(outputStream* out) const { out->print(\"LoadKlassStub\"); }\n+#endif \/\/ PRODUCT\n+};\n+\n","filename":"src\/hotspot\/share\/c1\/c1_CodeStubs.hpp","additions":19,"deletions":0,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -1904,0 +1904,1 @@\n+  CodeStub* _stub;\n@@ -1905,1 +1906,1 @@\n-  LIR_OpLoadKlass(LIR_Opr obj, LIR_Opr result, CodeEmitInfo* info)\n+  LIR_OpLoadKlass(LIR_Opr obj, LIR_Opr result, CodeEmitInfo* info, CodeStub* stub)\n@@ -1908,1 +1909,1 @@\n-    {}\n+    , _stub(stub) {}\n@@ -1911,0 +1912,1 @@\n+  CodeStub* stub()     const { return _stub; }\n@@ -2374,1 +2376,1 @@\n-  void load_klass(LIR_Opr obj, LIR_Opr result, CodeEmitInfo* info) { append(new LIR_OpLoadKlass(obj, result, info)); }\n+  void load_klass(LIR_Opr obj, LIR_Opr result, CodeEmitInfo* info, CodeStub* stub) { append(new LIR_OpLoadKlass(obj, result, info, stub)); }\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.hpp","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1243,1 +1243,2 @@\n-  __ load_klass(obj, klass, null_check_info);\n+  CodeStub* slow_path = new LoadKlassStub(obj, klass);\n+  __ load_klass(obj, klass, null_check_info, slow_path);\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -247,0 +247,1 @@\n+  case load_klass_id:\n","filename":"src\/hotspot\/share\/c1\/c1_Runtime1.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n@@ -43,0 +44,1 @@\n+#include \"oops\/klass.inline.hpp\"\n@@ -221,2 +223,4 @@\n-    \/\/ See RunTimeClassInfo::get_for()\n-    _estimated_metaspaceobj_bytes += align_up(BytesPerWord, SharedSpaceObjectAlignment);\n+    \/\/ See ArchiveBuilder::make_shallow_copies: make sure we have enough space for both maximum\n+    \/\/ Klass alignment as well as the RuntimeInfo* pointer we will embed in front of a Klass.\n+    _estimated_metaspaceobj_bytes += align_up(BytesPerWord, KlassAlignmentInBytes) +\n+        align_up(sizeof(void*), SharedSpaceObjectAlignment);\n@@ -619,4 +623,5 @@\n-    \/\/ Save a pointer immediate in front of an InstanceKlass, so\n-    \/\/ we can do a quick lookup from InstanceKlass* -> RunTimeClassInfo*\n-    \/\/ without building another hashtable. See RunTimeClassInfo::get_for()\n-    \/\/ in systemDictionaryShared.cpp.\n+    \/\/ Reserve space for a pointer immediately in front of an InstanceKlass. That space will\n+    \/\/ later be used to store the RuntimeClassInfo* pointer directly in front of the archived\n+    \/\/ InstanceKlass, in order to have a quick lookup InstanceKlass* -> RunTimeClassInfo*\n+    \/\/ without building another hashtable. See RunTimeClassInfo::get_for()\/::set_for() for\n+    \/\/ details.\n@@ -628,0 +633,3 @@\n+    dest = dump_region->allocate(bytes, KlassAlignmentInBytes);\n+  } else {\n+    dest = dump_region->allocate(bytes);\n@@ -629,1 +637,0 @@\n-  dest = dump_region->allocate(bytes);\n@@ -640,1 +647,2 @@\n-  log_trace(cds)(\"Copy: \" PTR_FORMAT \" ==> \" PTR_FORMAT \" %d\", p2i(src), p2i(dest), bytes);\n+  log_trace(cds)(\"Copy: \" PTR_FORMAT \" ==> \" PTR_FORMAT \" %d (%s)\", p2i(src), p2i(dest), bytes,\n+                 MetaspaceObj::type_name(ref->msotype()));\n@@ -644,0 +652,2 @@\n+\n+  DEBUG_ONLY(_alloc_stats.verify((int)dump_region->used(), src_info->read_only()));\n@@ -733,0 +743,7 @@\n+    Klass* requested_k = to_requested(k);\n+#ifdef _LP64\n+    narrowKlass nk = CompressedKlassPointers::encode_not_null(requested_k, _requested_static_archive_bottom);\n+    k->set_prototype_header(markWord::prototype().set_narrow_klass(nk));\n+#else\n+    k->set_prototype_header(markWord::prototype());\n+#endif\n@@ -824,1 +841,3 @@\n-  o->set_narrow_klass(nk);\n+#ifdef _LP64\n+  o->set_mark(o->mark().set_narrow_klass(nk));\n+#endif\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.cpp","additions":28,"deletions":9,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -207,3 +207,13 @@\n-char* DumpRegion::allocate(size_t num_bytes) {\n-  char* p = (char*)align_up(_top, (size_t)SharedSpaceObjectAlignment);\n-  char* newtop = p + align_up(num_bytes, (size_t)SharedSpaceObjectAlignment);\n+char* DumpRegion::allocate(size_t num_bytes, size_t alignment) {\n+  \/\/ We align the starting address of each allocation.\n+  char* p = (char*)align_up(_top, alignment);\n+  char* newtop = p + num_bytes;\n+  \/\/ Leave _top always SharedSpaceObjectAlignment aligned. But not more -\n+  \/\/  if we allocate with large alignments, lets not waste the gaps.\n+  \/\/ Ideally we would not need to align _top to anything here but CDS has\n+  \/\/  a number of implicit alignment assumptions. Leaving this unaligned\n+  \/\/  here will trip of at least ReadClosure (assuming word alignment) and\n+  \/\/  DumpAllocStats (will get confused about counting bytes on 32-bit\n+  \/\/  platforms if we align to anything less than SharedSpaceObjectAlignment\n+  \/\/  here).\n+  newtop = align_up(newtop, SharedSpaceObjectAlignment);\n@@ -211,1 +221,1 @@\n-  memset(p, 0, newtop - p);\n+  memset(p, 0, newtop - p); \/\/ todo: needed? debug_only?\n@@ -215,0 +225,4 @@\n+char* DumpRegion::allocate(size_t num_bytes) {\n+  return allocate(num_bytes, SharedSpaceObjectAlignment);\n+}\n+\n@@ -311,1 +325,1 @@\n-  assert(tag == old_tag, \"old tag doesn't match\");\n+  assert(tag == old_tag, \"tag doesn't match (%d, expected %d)\", old_tag, tag);\n","filename":"src\/hotspot\/share\/cds\/archiveUtils.cpp","additions":19,"deletions":5,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -63,0 +63,1 @@\n+#include \"runtime\/safepoint.hpp\"\n@@ -333,1 +334,8 @@\n-    archived_oop->set_mark(markWord::prototype().copy_set_hash(hash_original));\n+\n+    assert(SafepointSynchronize::is_at_safepoint(), \"resolving displaced headers only at safepoint\");\n+    markWord mark = obj->mark();\n+    if (mark.has_displaced_mark_helper()) {\n+      mark = mark.displaced_mark_helper();\n+    }\n+    narrowKlass nklass = mark.narrow_klass();\n+    archived_oop->set_mark(markWord::prototype().copy_set_hash(hash_original) LP64_ONLY(.set_narrow_klass(nklass)));\n@@ -573,1 +581,2 @@\n-    oopDesc::set_mark(mem, markWord::prototype());\n+    oopDesc::set_mark(mem, k->prototype_header());\n+#ifndef _LP64\n@@ -575,0 +584,1 @@\n+#endif\n","filename":"src\/hotspot\/share\/cds\/heapShared.cpp","additions":12,"deletions":2,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -61,0 +61,1 @@\n+#include \"oops\/compressedKlass.hpp\"\n@@ -1375,0 +1376,5 @@\n+#ifdef ASSERT\n+    if (UseCompressedClassPointers) {\n+      CompressedKlassPointers::verify_klass_pointer(record->_klass);\n+    }\n+#endif\n@@ -1376,1 +1382,0 @@\n-    assert(check_alignment(record->_klass), \"Address not aligned\");\n","filename":"src\/hotspot\/share\/classfile\/systemDictionaryShared.cpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -90,0 +90,2 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n+#include \"gc\/shared\/suspendibleThreadSet.hpp\"\n@@ -1586,0 +1588,2 @@\n+  _forwarding = new SlidingForwarding(heap_rs.region(), HeapRegion::LogOfHRGrainBytes - LogHeapWordSize);\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -88,0 +88,1 @@\n+class SlidingForwarding;\n@@ -232,0 +233,2 @@\n+  SlidingForwarding* _forwarding;\n+\n@@ -245,0 +248,4 @@\n+  SlidingForwarding* forwarding() const {\n+    return _forwarding;\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n@@ -316,0 +317,2 @@\n+  _heap->forwarding()->clear();\n+\n@@ -323,3 +326,4 @@\n-  if (!has_free_compaction_targets) {\n-    phase2c_prepare_serial_compaction();\n-  }\n+  \/\/ TODO: Disabled for now because it violates sliding-forwarding assumption.\n+  \/\/ if (!has_free_compaction_targets) {\n+  \/\/   phase2c_prepare_serial_compaction();\n+  \/\/ }\n@@ -345,30 +349,31 @@\n-  GCTraceTime(Debug, gc, phases) debug(\"Phase 2: Prepare serial compaction\", scope()->timer());\n-  \/\/ At this point we know that after parallel compaction there will be no\n-  \/\/ completely free regions. That means that the last region of\n-  \/\/ all compaction queues still have data in them. We try to compact\n-  \/\/ these regions in serial to avoid a premature OOM when the mutator wants\n-  \/\/ to allocate the first eden region after gc.\n-  for (uint i = 0; i < workers(); i++) {\n-    G1FullGCCompactionPoint* cp = compaction_point(i);\n-    if (cp->has_regions()) {\n-      serial_compaction_point()->add(cp->remove_last());\n-    }\n-  }\n-\n-  \/\/ Update the forwarding information for the regions in the serial\n-  \/\/ compaction point.\n-  G1FullGCCompactionPoint* cp = serial_compaction_point();\n-  for (GrowableArrayIterator<HeapRegion*> it = cp->regions()->begin(); it != cp->regions()->end(); ++it) {\n-    HeapRegion* current = *it;\n-    if (!cp->is_initialized()) {\n-      \/\/ Initialize the compaction point. Nothing more is needed for the first heap region\n-      \/\/ since it is already prepared for compaction.\n-      cp->initialize(current);\n-    } else {\n-      assert(!current->is_humongous(), \"Should be no humongous regions in compaction queue\");\n-      G1SerialRePrepareClosure re_prepare(cp, current);\n-      current->set_compaction_top(current->bottom());\n-      current->apply_to_marked_objects(mark_bitmap(), &re_prepare);\n-    }\n-  }\n-  cp->update();\n+  ShouldNotReachHere(); \/\/ Disabled in Lilliput.\n+\/\/  GCTraceTime(Debug, gc, phases) debug(\"Phase 2: Prepare serial compaction\", scope()->timer());\n+\/\/  \/\/ At this point we know that after parallel compaction there will be no\n+\/\/  \/\/ completely free regions. That means that the last region of\n+\/\/  \/\/ all compaction queues still have data in them. We try to compact\n+\/\/  \/\/ these regions in serial to avoid a premature OOM when the mutator wants\n+\/\/  \/\/ to allocate the first eden region after gc.\n+\/\/  for (uint i = 0; i < workers(); i++) {\n+\/\/    G1FullGCCompactionPoint* cp = compaction_point(i);\n+\/\/    if (cp->has_regions()) {\n+\/\/      serial_compaction_point()->add(cp->remove_last());\n+\/\/    }\n+\/\/  }\n+\/\/\n+\/\/  \/\/ Update the forwarding information for the regions in the serial\n+\/\/  \/\/ compaction point.\n+\/\/  G1FullGCCompactionPoint* cp = serial_compaction_point();\n+\/\/  for (GrowableArrayIterator<HeapRegion*> it = cp->regions()->begin(); it != cp->regions()->end(); ++it) {\n+\/\/    HeapRegion* current = *it;\n+\/\/    if (!cp->is_initialized()) {\n+\/\/      \/\/ Initialize the compaction point. Nothing more is needed for the first heap region\n+\/\/      \/\/ since it is already prepared for compaction.\n+\/\/      cp->initialize(current);\n+\/\/    } else {\n+\/\/      assert(!current->is_humongous(), \"Should be no humongous regions in compaction queue\");\n+\/\/      G1SerialRePrepareClosure re_prepare(cp, current);\n+\/\/      current->set_compaction_top(current->bottom());\n+\/\/      current->apply_to_marked_objects(mark_bitmap(), &re_prepare);\n+\/\/    }\n+\/\/  }\n+\/\/  cp->update();\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullCollector.cpp","additions":38,"deletions":33,"binary":false,"changes":71,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -79,1 +80,1 @@\n-    HeapWord* destination = cast_from_oop<HeapWord*>(obj->forwardee());\n+    HeapWord* destination = cast_from_oop<HeapWord*>(_forwarding->forwardee(obj));\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactTask.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+class SlidingForwarding;\n@@ -53,0 +54,1 @@\n+    const SlidingForwarding* const _forwarding;\n@@ -55,1 +57,3 @@\n-    G1CompactRegionClosure(G1CMBitMap* bitmap) : _bitmap(bitmap) { }\n+    G1CompactRegionClosure(G1CMBitMap* bitmap) :\n+      _bitmap(bitmap),\n+      _forwarding(G1CollectedHeap::heap()->forwarding()) { }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactTask.hpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"gc\/g1\/g1CollectedHeap.hpp\"\n@@ -35,0 +36,1 @@\n+class SlidingForwarding;\n@@ -84,0 +86,1 @@\n+  const SlidingForwarding* const _forwarding;\n@@ -87,1 +90,3 @@\n-  G1AdjustClosure(G1FullCollector* collector) : _collector(collector) { }\n+  G1AdjustClosure(G1FullCollector* collector) :\n+    _collector(collector),\n+    _forwarding(G1CollectedHeap::heap()->forwarding()) { }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCOopClosures.hpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -89,1 +90,1 @@\n-    oop forwardee = obj->forwardee();\n+    oop forwardee = _forwarding->forwardee(obj);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCOopClosures.inline.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -146,1 +147,1 @@\n-    _cp(cp) { }\n+    _cp(cp), _forwarding(G1CollectedHeap::heap()->forwarding()) { }\n@@ -150,1 +151,1 @@\n-  _cp->forward(object, size);\n+  _cp->forward(_forwarding, object, size);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCPrepareTask.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+class SlidingForwarding;\n@@ -111,0 +112,1 @@\n+    SlidingForwarding* const _forwarding;\n@@ -120,11 +122,11 @@\n-class G1SerialRePrepareClosure : public StackObj {\n-  G1FullGCCompactionPoint* _cp;\n-  HeapRegion* _current;\n-\n-public:\n-  G1SerialRePrepareClosure(G1FullGCCompactionPoint* hrcp, HeapRegion* hr) :\n-    _cp(hrcp),\n-    _current(hr) { }\n-\n-  inline size_t apply(oop obj);\n-};\n+\/\/class G1SerialRePrepareClosure : public StackObj {\n+\/\/  G1FullGCCompactionPoint* _cp;\n+\/\/  HeapRegion* _current;\n+\/\/\n+\/\/public:\n+\/\/  G1SerialRePrepareClosure(G1FullGCCompactionPoint* hrcp, HeapRegion* hr) :\n+\/\/    _cp(hrcp),\n+\/\/    _current(hr) { }\n+\/\/\n+\/\/  inline size_t apply(oop obj);\n+\/\/};\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCPrepareTask.hpp","additions":13,"deletions":11,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -238,1 +238,1 @@\n-      forwardee = cast_to_oop(m.decode_pointer());\n+      forwardee = obj->forwardee(m);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1OopClosures.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -209,1 +209,1 @@\n-    obj = cast_to_oop(m.decode_pointer());\n+    obj = obj->forwardee(m);\n@@ -223,1 +223,0 @@\n-  assert(from_obj->is_objArray(), \"must be obj array\");\n@@ -253,1 +252,0 @@\n-  assert(from_obj->is_objArray(), \"precondition\");\n@@ -380,1 +378,1 @@\n-                                                  oop const old, size_t word_sz, uint age,\n+                                                  oop const old, Klass* klass, size_t word_sz, uint age,\n@@ -384,1 +382,1 @@\n-    _g1h->gc_tracer_stw()->report_promotion_in_new_plab_event(old->klass(), word_sz * HeapWordSize, age,\n+    _g1h->gc_tracer_stw()->report_promotion_in_new_plab_event(klass, word_sz * HeapWordSize, age,\n@@ -388,1 +386,1 @@\n-    _g1h->gc_tracer_stw()->report_promotion_outside_plab_event(old->klass(), word_sz * HeapWordSize, age,\n+    _g1h->gc_tracer_stw()->report_promotion_outside_plab_event(klass, word_sz * HeapWordSize, age,\n@@ -396,0 +394,1 @@\n+                                                   Klass* klass,\n@@ -418,1 +417,1 @@\n-      report_promotion_event(*dest_attr, old, word_sz, age, obj_ptr, node_index);\n+      report_promotion_event(*dest_attr, old, klass, word_sz, age, obj_ptr, node_index);\n@@ -453,0 +452,4 @@\n+  if (old_mark.is_marked()) {\n+    \/\/ Already forwarded by somebody else, return forwardee.\n+    return old->forwardee(old_mark);\n+  }\n@@ -455,0 +458,3 @@\n+#ifdef _LP64\n+  Klass* klass = old_mark.safe_klass();\n+#else\n@@ -456,0 +462,1 @@\n+#endif\n@@ -468,1 +475,1 @@\n-    obj_ptr = allocate_copy_slow(&dest_attr, old, word_sz, age, node_index);\n+    obj_ptr = allocate_copy_slow(&dest_attr, old, klass, word_sz, age, node_index);\n@@ -621,1 +628,1 @@\n-  oop forward_ptr = old->forward_to_atomic(old, m, memory_order_relaxed);\n+  oop forward_ptr = old->forward_to_self_atomic(m, memory_order_relaxed);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.cpp","additions":16,"deletions":9,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -112,1 +112,1 @@\n-              touched_words = MIN2((size_t)align_object_size(typeArrayOopDesc::header_size(T_INT)),\n+              touched_words = MIN2((size_t)align_object_size(align_up(typeArrayOopDesc::base_offset_in_bytes(T_INT), HeapWordSize) \/ HeapWordSize),\n","filename":"src\/hotspot\/share\/gc\/parallel\/mutableNUMASpace.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -296,1 +296,0 @@\n-  assert(old->is_objArray(), \"invariant\");\n@@ -334,1 +333,1 @@\n-  if (obj->forward_to_atomic(obj, obj_mark) == NULL) {\n+  if (obj->forward_to_self_atomic(obj_mark) == NULL) {\n","filename":"src\/hotspot\/share\/gc\/parallel\/psPromotionManager.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -66,1 +66,1 @@\n-inline void PSPromotionManager::promotion_trace_event(oop new_obj, oop old_obj,\n+inline void PSPromotionManager::promotion_trace_event(oop new_obj, oop old_obj, Klass* klass,\n@@ -79,1 +79,1 @@\n-        gc_tracer->report_promotion_in_new_plab_event(old_obj->klass(), obj_bytes,\n+        gc_tracer->report_promotion_in_new_plab_event(klass, obj_bytes,\n@@ -86,1 +86,1 @@\n-        gc_tracer->report_promotion_outside_plab_event(old_obj->klass(), obj_bytes,\n+        gc_tracer->report_promotion_outside_plab_event(klass, obj_bytes,\n@@ -150,1 +150,1 @@\n-    return cast_to_oop(m.decode_pointer());\n+    return o->forwardee(m);\n@@ -166,1 +166,6 @@\n-  size_t new_obj_size = o->size();\n+#ifdef _LP64\n+  Klass* klass = test_mark.safe_klass();\n+#else\n+  Klass* klass = o->klass();\n+#endif\n+  size_t new_obj_size = o->size_given_klass(klass);\n@@ -181,1 +186,1 @@\n-          promotion_trace_event(new_obj, o, new_obj_size, age, false, NULL);\n+          promotion_trace_event(new_obj, o, klass, new_obj_size, age, false, NULL);\n@@ -191,1 +196,1 @@\n-            promotion_trace_event(new_obj, o, new_obj_size, age, false, &_young_lab);\n+            promotion_trace_event(new_obj, o, klass, new_obj_size, age, false, &_young_lab);\n@@ -217,1 +222,1 @@\n-          promotion_trace_event(new_obj, o, new_obj_size, age, true, NULL);\n+          promotion_trace_event(new_obj, o, klass, new_obj_size, age, true, NULL);\n@@ -227,1 +232,1 @@\n-            promotion_trace_event(new_obj, o, new_obj_size, age, true, &_old_lab);\n+            promotion_trace_event(new_obj, o, klass, new_obj_size, age, true, &_old_lab);\n@@ -250,3 +255,5 @@\n-  \/\/ Parallel GC claims with a release - so other threads might access this object\n-  \/\/ after claiming and they should see the \"completed\" object.\n-  ContinuationGCSupport::transform_stack_chunk(new_obj);\n+  if (!new_obj->mark().is_marked()) {\n+    \/\/ Parallel GC claims with a release - so other threads might access this object\n+    \/\/ after claiming and they should see the \"completed\" object.\n+    ContinuationGCSupport::transform_stack_chunk(new_obj);\n+  }\n","filename":"src\/hotspot\/share\/gc\/parallel\/psPromotionManager.inline.hpp","additions":19,"deletions":12,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -678,0 +678,10 @@\n+#ifdef _LP64\n+        oop forwardee = obj->forwardee();\n+        markWord header = forwardee->mark();\n+        if (header.has_displaced_mark_helper()) {\n+          header = header.displaced_mark_helper();\n+        }\n+        assert(UseCompressedClassPointers, \"assume +UseCompressedClassPointers\");\n+        narrowKlass nklass = header.narrow_klass();\n+        obj->set_mark(markWord::prototype().set_narrow_klass(nklass));\n+#else\n@@ -679,0 +689,1 @@\n+#endif\n@@ -702,1 +713,1 @@\n-  old->forward_to(old);\n+  old->forward_to_self();\n","filename":"src\/hotspot\/share\/gc\/serial\/defNewGeneration.cpp","additions":12,"deletions":1,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -266,0 +266,2 @@\n+  AdjustPointerClosure adjust_pointer_closure(gch->forwarding());\n+  CLDToOopClosure adjust_cld_closure(&adjust_pointer_closure, ClassLoaderData::_claim_strong);\n","filename":"src\/hotspot\/share\/gc\/serial\/genMarkSweep.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -44,0 +44,1 @@\n+class SlidingForwarding;\n@@ -131,2 +132,0 @@\n-  static AdjustPointerClosure adjust_pointer_closure;\n-  static CLDToOopClosure      adjust_cld_closure;\n@@ -149,1 +148,1 @@\n-  static size_t adjust_pointers(oop obj);\n+  static size_t adjust_pointers(const SlidingForwarding* const forwarding, oop obj);\n@@ -157,1 +156,1 @@\n-  template <class T> static inline void adjust_pointer(T* p);\n+  template <class T> static inline void adjust_pointer(const SlidingForwarding* const forwarding, T* p);\n@@ -195,0 +194,2 @@\n+private:\n+  const SlidingForwarding* const _forwarding;\n@@ -196,0 +197,1 @@\n+  AdjustPointerClosure(const SlidingForwarding* forwarding) : _forwarding(forwarding) {}\n@@ -209,1 +211,1 @@\n-  void adjust_pointer();\n+  void adjust_pointer(const SlidingForwarding* const forwarding);\n","filename":"src\/hotspot\/share\/gc\/serial\/markSweep.hpp","additions":7,"deletions":5,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -54,1 +55,1 @@\n-  obj->set_mark(markWord::prototype().set_marked());\n+  obj->set_mark(obj->klass()->prototype_header().set_marked());\n@@ -92,1 +93,1 @@\n-template <class T> inline void MarkSweep::adjust_pointer(T* p) {\n+template <class T> inline void MarkSweep::adjust_pointer(const SlidingForwarding* const forwarding, T* p) {\n@@ -98,2 +99,4 @@\n-    if (obj->is_forwarded()) {\n-      oop new_obj = obj->forwardee();\n+    markWord header = obj->mark();\n+    if (header.is_marked()) {\n+      oop new_obj = forwarding->forwardee(obj);\n+      assert(new_obj != NULL, \"must be forwarded\");\n@@ -107,1 +110,1 @@\n-void AdjustPointerClosure::do_oop_work(T* p)           { MarkSweep::adjust_pointer(p); }\n+void AdjustPointerClosure::do_oop_work(T* p)           { MarkSweep::adjust_pointer(_forwarding, p); }\n@@ -112,2 +115,3 @@\n-inline size_t MarkSweep::adjust_pointers(oop obj) {\n-  return obj->oop_iterate_size(&MarkSweep::adjust_pointer_closure);\n+inline size_t MarkSweep::adjust_pointers(const SlidingForwarding* const forwarding, oop obj) {\n+  AdjustPointerClosure cl(forwarding);\n+  return obj->oop_iterate_size(&cl);\n","filename":"src\/hotspot\/share\/gc\/serial\/markSweep.inline.hpp","additions":11,"deletions":7,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -226,4 +226,0 @@\n-  if (is_in(object->klass_or_null())) {\n-    return false;\n-  }\n-\n@@ -256,2 +252,4 @@\n-  _filler_array_max_size = align_object_size(filler_array_hdr_size() +\n-                                             max_len \/ elements_per_word);\n+  int header_size_in_bytes = arrayOopDesc::base_offset_in_bytes(T_INT);\n+  assert(header_size_in_bytes % sizeof(jint) == 0, \"must be aligned to int\");\n+  int header_size_in_ints = header_size_in_bytes \/ sizeof(jint);\n+  _filler_array_max_size = align_object_size((header_size_in_ints + max_len) \/ elements_per_word);\n@@ -418,1 +416,3 @@\n-  size_t max_int_size = typeArrayOopDesc::header_size(T_INT) +\n+  int header_size_in_bytes = typeArrayOopDesc::base_offset_in_bytes(T_INT);\n+  assert(header_size_in_bytes % sizeof(jint) == 0, \"header size must align to int\");\n+  size_t max_int_size = header_size_in_bytes \/ HeapWordSize +\n@@ -424,5 +424,2 @@\n-size_t CollectedHeap::filler_array_hdr_size() {\n-  return align_object_offset(arrayOopDesc::header_size(T_INT)); \/\/ align to Long\n-}\n-\n-  return align_object_size(filler_array_hdr_size()); \/\/ align to MinObjAlignment\n+  int aligned_header_size_words = align_up(arrayOopDesc::base_offset_in_bytes(T_INT), HeapWordSize) \/ HeapWordSize;\n+  return align_object_size(aligned_header_size_words); \/\/ align to MinObjAlignment\n@@ -433,2 +430,3 @@\n-  Copy::fill_to_words(start + filler_array_hdr_size(),\n-                      words - filler_array_hdr_size(), value);\n+  int payload_start = align_up(arrayOopDesc::base_offset_in_bytes(T_INT), HeapWordSize) \/ HeapWordSize;\n+  Copy::fill_to_words(start + payload_start,\n+                      words - payload_start, value);\n@@ -458,2 +456,3 @@\n-  const size_t payload_size = words - filler_array_hdr_size();\n-  const size_t len = payload_size * HeapWordSize \/ sizeof(jint);\n+  const size_t payload_size_bytes = words * HeapWordSize - arrayOopDesc::base_offset_in_bytes(T_INT);\n+  assert(payload_size_bytes % sizeof(jint) == 0, \"must be int aligned\");\n+  const size_t len = payload_size_bytes \/ sizeof(jint);\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.cpp","additions":15,"deletions":16,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -166,1 +166,0 @@\n-  static inline size_t filler_array_hdr_size();\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -56,0 +56,1 @@\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n@@ -126,0 +127,1 @@\n+  _forwarding = new SlidingForwarding(_reserved);\n@@ -1073,0 +1075,1 @@\n+  _forwarding->clear();\n","filename":"src\/hotspot\/share\/gc\/shared\/genCollectedHeap.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+class SlidingForwarding;\n@@ -90,0 +91,2 @@\n+  SlidingForwarding* _forwarding;\n+\n@@ -325,0 +328,4 @@\n+  SlidingForwarding* forwarding() const {\n+    return _forwarding;\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/genCollectedHeap.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"memory\/metaspace.hpp\"\n@@ -403,0 +404,15 @@\n+JVMFlag::Error CompressedClassSpaceSizeConstraintFunc(size_t value, bool verbose) {\n+#ifdef _LP64\n+  \/\/ There is no minimal value check, although class space will be transparently enlarged\n+  \/\/ to a multiple of metaspace root chunk size (4m).\n+  \/\/ The max. value of class space size depends on narrow klass pointer encoding range size\n+  \/\/ and CDS, see metaspace.cpp.\n+  if (value > Metaspace::max_class_space_size()) {\n+    JVMFlag::printError(verbose, \"CompressedClassSpaceSize \" SIZE_FORMAT \" too large (max: \" SIZE_FORMAT \")\\n\",\n+                        value, Metaspace::max_class_space_size());\n+    return JVMFlag::VIOLATES_CONSTRAINT;\n+  }\n+#endif\n+  return JVMFlag::SUCCESS;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/jvmFlagConstraintsGC.cpp","additions":16,"deletions":0,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -400,1 +400,0 @@\n-  oopDesc::set_klass_gap(mem, 0);\n@@ -406,2 +405,0 @@\n-  \/\/ May be bootstrapping\n-  oopDesc::set_mark(mem, markWord::prototype());\n@@ -411,0 +408,4 @@\n+#ifdef _LP64\n+  oopDesc::release_set_mark(mem, _klass->prototype_header());\n+#else\n+  oopDesc::set_mark(mem, _klass->prototype_header());\n@@ -412,0 +413,1 @@\n+#endif\n@@ -425,1 +427,1 @@\n-  const size_t hs = arrayOopDesc::header_size(array_klass->element_type());\n+  const size_t hs = align_up(arrayOopDesc::base_offset_in_bytes(array_klass->element_type()), HeapWordSize) \/ HeapWordSize;\n","filename":"src\/hotspot\/share\/gc\/shared\/memAllocator.cpp","additions":6,"deletions":4,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -350,1 +351,1 @@\n-                                    CompactPoint* cp, HeapWord* compact_top) {\n+                                    CompactPoint* cp, HeapWord* compact_top, SlidingForwarding* const forwarding) {\n@@ -373,1 +374,1 @@\n-    q->forward_to(cast_to_oop(compact_top));\n+    forwarding->forward_to(q, cast_to_oop(compact_top));\n@@ -421,0 +422,1 @@\n+  SlidingForwarding* const forwarding = GenCollectedHeap::heap()->forwarding();\n@@ -426,1 +428,1 @@\n-      compact_top = cp->space->forward(cast_to_oop(cur_obj), size, cp, compact_top);\n+      compact_top = cp->space->forward(cast_to_oop(cur_obj), size, cp, compact_top, forwarding);\n@@ -442,1 +444,1 @@\n-        compact_top = cp->space->forward(obj, obj->size(), cp, compact_top);\n+        compact_top = cp->space->forward(obj, obj->size(), cp, compact_top, forwarding);\n@@ -485,0 +487,1 @@\n+  const SlidingForwarding* const forwarding = GenCollectedHeap::heap()->forwarding();\n@@ -496,1 +499,1 @@\n-      size_t size = MarkSweep::adjust_pointers(cast_to_oop(cur_obj));\n+      size_t size = MarkSweep::adjust_pointers(forwarding, cast_to_oop(cur_obj));\n@@ -537,0 +540,2 @@\n+  const SlidingForwarding* const forwarding = GenCollectedHeap::heap()->forwarding();\n+\n@@ -550,1 +555,1 @@\n-      HeapWord* compaction_top = cast_from_oop<HeapWord*>(cast_to_oop(cur_obj)->forwardee());\n+      HeapWord* compaction_top = cast_from_oop<HeapWord*>(forwarding->forwardee(cast_to_oop(cur_obj)));\n","filename":"src\/hotspot\/share\/gc\/shared\/space.cpp","additions":11,"deletions":6,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -53,0 +53,1 @@\n+class SlidingForwarding;\n@@ -395,1 +396,1 @@\n-                    HeapWord* compact_top);\n+                    HeapWord* compact_top, SlidingForwarding* const forwarding);\n","filename":"src\/hotspot\/share\/gc\/shared\/space.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -79,1 +79,4 @@\n-  markWord old_mark = obj->mark();\n+\n+  markWord old_mark = ObjectSynchronizer::read_stable_mark(obj);\n+  assert(!old_mark.is_being_inflated(), \"must not see INFLATING marker here\");\n+\n@@ -84,0 +87,4 @@\n+  \/\/ Ensure that the copy has the correct mark-word, in case it happened to copy with\n+  \/\/ INFLATING marker.\n+  update->set_mark(old_mark);\n+\n@@ -85,5 +92,12 @@\n-  markWord prev_mark = obj->cas_set_mark(new_mark, old_mark, memory_order_conservative);\n-  if (prev_mark == old_mark) {\n-    return update;\n-  } else {\n-    return cast_to_oop(prev_mark.clear_lock_bits().to_pointer());\n+  while (true) {\n+    markWord prev_mark = obj->cas_set_mark(new_mark, old_mark, memory_order_conservative);\n+    if (prev_mark == old_mark) {\n+      return update;\n+    } else if (prev_mark == markWord::INFLATING()) {\n+      \/\/ This happens when we encounter a stack-locked object in from-space.\n+      \/\/ Busy-wait for completion.\n+      SpinPause();\n+    } else {\n+      assert(prev_mark.is_marked(), \"must be forwarded\");\n+      return cast_to_oop(prev_mark.clear_lock_bits().to_pointer());\n+    }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahForwarding.inline.hpp","additions":20,"deletions":6,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -186,0 +187,1 @@\n+    heap->forwarding()->clear();\n@@ -296,2 +298,3 @@\n-  PreservedMarks*          const _preserved_marks;\n-  ShenandoahHeap*          const _heap;\n+  PreservedMarks*    const _preserved_marks;\n+  SlidingForwarding* const _forwarding;\n+  ShenandoahHeap*    const _heap;\n@@ -309,0 +312,1 @@\n+    _forwarding(ShenandoahHeap::heap()->forwarding()),\n@@ -362,1 +366,1 @@\n-    p->forward_to(cast_to_oop(_compact_point));\n+    _forwarding->forward_to(p, cast_to_oop(_compact_point));\n@@ -436,0 +440,1 @@\n+  SlidingForwarding* forwarding = heap->forwarding();\n@@ -470,1 +475,1 @@\n-        old_obj->forward_to(cast_to_oop(heap->get_region(start)->bottom()));\n+        forwarding->forward_to(old_obj, cast_to_oop(heap->get_region(start)->bottom()));\n@@ -721,1 +726,2 @@\n-  ShenandoahHeap* const _heap;\n+  ShenandoahHeap*           const _heap;\n+  const SlidingForwarding*  const _forwarding;\n@@ -731,1 +737,1 @@\n-        oop forw = obj->forwardee();\n+        oop forw = _forwarding->forwardee(obj);\n@@ -740,0 +746,1 @@\n+    _forwarding(_heap->forwarding()),\n@@ -801,1 +808,2 @@\n-    _preserved_marks->get(worker_id)->adjust_during_full_gc();\n+    const SlidingForwarding* const forwarding = ShenandoahHeap::heap()->forwarding();\n+    _preserved_marks->get(worker_id)->adjust_during_full_gc(forwarding);\n@@ -831,2 +839,3 @@\n-  ShenandoahHeap* const _heap;\n-  uint            const _worker_id;\n+  ShenandoahHeap*          const _heap;\n+  const SlidingForwarding* const _forwarding;\n+  uint                     const _worker_id;\n@@ -836,1 +845,1 @@\n-    _heap(ShenandoahHeap::heap()), _worker_id(worker_id) {}\n+    _heap(ShenandoahHeap::heap()), _forwarding(_heap->forwarding()), _worker_id(worker_id) {}\n@@ -843,1 +852,1 @@\n-      HeapWord* compact_to = cast_from_oop<HeapWord*>(p->forwardee());\n+      HeapWord* compact_to = cast_from_oop<HeapWord*>(_forwarding->forwardee(p));\n@@ -940,0 +949,1 @@\n+  const SlidingForwarding* const forwarding = heap->forwarding();\n@@ -954,1 +964,1 @@\n-      size_t new_start = heap->heap_region_index_containing(old_obj->forwardee());\n+      size_t new_start = heap->heap_region_index_containing(forwarding->forwardee(old_obj));\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":22,"deletions":12,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n@@ -195,0 +196,2 @@\n+  _forwarding = new SlidingForwarding(_heap_region, ShenandoahHeapRegion::region_size_words_shift());\n+\n@@ -951,1 +954,1 @@\n-    if (!p->is_forwarded()) {\n+    if (!ShenandoahForwarding::is_forwarded(p)) {\n@@ -1239,1 +1242,4 @@\n-      obj = ShenandoahBarrierSet::resolve_forwarded_not_null(obj);\n+\n+      \/\/ We must not expose from-space oops to the rest of runtime, or else it\n+      \/\/ will call klass() on it, which might fail because of unexpected header.\n+      obj = ShenandoahBarrierSet::barrier_set()->load_reference_barrier(obj);\n@@ -1295,0 +1301,1 @@\n+    shenandoah_assert_not_in_cset_except(NULL, obj, cancelled_gc());\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":9,"deletions":2,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -64,0 +64,1 @@\n+class SlidingForwarding;\n@@ -230,0 +231,1 @@\n+  SlidingForwarding* _forwarding;\n@@ -246,0 +248,2 @@\n+  SlidingForwarding* forwarding() const { return _forwarding; }\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -45,0 +45,1 @@\n+#include \"gc\/shenandoah\/shenandoahObjectUtils.inline.hpp\"\n@@ -300,1 +301,1 @@\n-  size_t size = p->size();\n+  size_t size = ShenandoahObjectUtils::size(p);\n@@ -338,2 +339,7 @@\n-  ContinuationGCSupport::relativize_stack_chunk(copy_val);\n-\n+  if (!copy_val->mark().is_marked()) {\n+    \/\/ If we copied a mark-word that indicates 'forwarded' state, then\n+    \/\/ another thread beat us, and this new copy will never be published.\n+    \/\/ ContinuationGCSupport would get a corrupt Klass* in that case,\n+    \/\/ so don't even attempt it.\n+    ContinuationGCSupport::relativize_stack_chunk(copy_val);\n+  }\n@@ -519,1 +525,1 @@\n-    size_t size = obj->size();\n+    size_t size = ShenandoahObjectUtils::size(obj);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":10,"deletions":4,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/shenandoah\/shenandoahObjectUtils.inline.hpp\"\n@@ -101,1 +102,1 @@\n-      if (is_instance_ref_klass(obj->klass())) {\n+      if (is_instance_ref_klass(ShenandoahObjectUtils::klass(obj))) {\n@@ -128,1 +129,1 @@\n-    Klass* obj_klass = obj->klass_or_null();\n+    Klass* obj_klass = ShenandoahObjectUtils::klass(obj);\n@@ -143,1 +144,1 @@\n-        check(ShenandoahAsserts::_safe_unknown, obj, (obj_addr + obj->size()) <= obj_reg->top(),\n+        check(ShenandoahAsserts::_safe_unknown, obj, (obj_addr + ShenandoahObjectUtils::size(obj)) <= obj_reg->top(),\n@@ -147,1 +148,1 @@\n-        size_t humongous_end = humongous_start + (obj->size() >> ShenandoahHeapRegion::region_size_words_shift());\n+        size_t humongous_end = humongous_start + (ShenandoahObjectUtils::size(obj) >> ShenandoahHeapRegion::region_size_words_shift());\n@@ -164,1 +165,1 @@\n-          Atomic::add(&_ld[obj_reg->index()], (uint) obj->size(), memory_order_relaxed);\n+          Atomic::add(&_ld[obj_reg->index()], (uint) ShenandoahObjectUtils::size(obj), memory_order_relaxed);\n@@ -205,1 +206,1 @@\n-      check(ShenandoahAsserts::_safe_oop, obj, (fwd_addr + fwd->size()) <= fwd_reg->top(),\n+      check(ShenandoahAsserts::_safe_oop, obj, (fwd_addr + ShenandoahObjectUtils::size(fwd)) <= fwd_reg->top(),\n@@ -308,1 +309,2 @@\n-    obj->oop_iterate(this);\n+    Klass* klass = ShenandoahObjectUtils::klass(obj);\n+    obj->oop_iterate_backwards(this, klass);\n@@ -588,1 +590,1 @@\n-    if (!is_instance_ref_klass(obj->klass())) {\n+    if (!is_instance_ref_klass(ShenandoahObjectUtils::klass(obj))) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.cpp","additions":10,"deletions":8,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -1967,0 +1967,3 @@\n+#ifdef _LP64\n+              oopDesc::release_set_mark(result, ik->prototype_header());\n+#else\n@@ -1968,2 +1971,1 @@\n-              oopDesc::set_klass_gap(result, 0);\n-\n+#endif\n","filename":"src\/hotspot\/share\/interpreter\/zero\/bytecodeInterpreter.cpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2207,1 +2207,1 @@\n-  return arrayOopDesc::header_size(type) * HeapWordSize;\n+  return arrayOopDesc::base_offset_in_bytes(type);\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -269,1 +269,0 @@\n-  volatile_nonstatic_field(oopDesc,            _metadata._klass,                              Klass*)                                \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"memory\/metaspace\/metaspaceAlignment.hpp\"\n@@ -100,1 +101,2 @@\n-    arena = new MetaspaceArena(_context->cm(), growth_policy, lock, &_used_words_counter, _name);\n+    arena = new MetaspaceArena(_context->cm(), growth_policy, MetaspaceMinAlignmentWords,\n+                               lock, &_used_words_counter, _name);\n","filename":"src\/hotspot\/share\/memory\/metaspace\/testHelpers.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -126,1 +126,1 @@\n-  static void trace_reference_gc(const char *s, oop obj) NOT_DEBUG_RETURN;\n+  void trace_reference_gc(const char *s, oop obj) NOT_DEBUG_RETURN;\n","filename":"src\/hotspot\/share\/oops\/instanceRefKlass.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -189,1 +189,1 @@\n-  if (java_lang_ref_Reference::is_phantom(obj)) {\n+  if (reference_type() == REF_PHANTOM) {\n","filename":"src\/hotspot\/share\/oops\/instanceRefKlass.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -168,0 +168,2 @@\n+  markWord _prototype_header;   \/\/ Used to initialize objects' header\n+\n@@ -666,0 +668,4 @@\n+  markWord prototype_header() const      { return _prototype_header; }\n+  inline void set_prototype_header(markWord header);\n+  static ByteSize prototype_header_offset() { return in_ByteSize(offset_of(Klass, _prototype_header)); }\n+\n","filename":"src\/hotspot\/share\/oops\/klass.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -55,0 +55,4 @@\n+inline void Klass::set_prototype_header(markWord header) {\n+  _prototype_header = header;\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/klass.inline.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"oops\/compressedOops.inline.hpp\"\n","filename":"src\/hotspot\/share\/oops\/markWord.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -0,0 +1,68 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_OOPS_MARKWORD_INLINE_HPP\n+#define SHARE_OOPS_MARKWORD_INLINE_HPP\n+\n+#include \"oops\/compressedKlass.inline.hpp\"\n+#include \"oops\/compressedOops.inline.hpp\"\n+#include \"oops\/markWord.hpp\"\n+#include \"runtime\/safepoint.hpp\"\n+\n+#ifdef _LP64\n+narrowKlass markWord::narrow_klass() const {\n+  return narrowKlass(value() >> klass_shift);\n+}\n+\n+Klass* markWord::klass() const {\n+  assert(!CompressedKlassPointers::is_null(narrow_klass()), \"narrow klass must not be null: \" INTPTR_FORMAT, value());\n+  return CompressedKlassPointers::decode_not_null(narrow_klass());\n+}\n+\n+Klass* markWord::klass_or_null() const {\n+  return CompressedKlassPointers::decode(narrow_klass());\n+}\n+\n+markWord markWord::set_narrow_klass(const narrowKlass nklass) const {\n+  return markWord((value() & ~klass_mask_in_place) | ((uintptr_t) nklass << klass_shift));\n+}\n+\n+Klass* markWord::safe_klass() const {\n+  assert(SafepointSynchronize::is_at_safepoint(), \"only call at safepoint\");\n+  markWord m = *this;\n+  if (m.has_displaced_mark_helper()) {\n+    m = m.displaced_mark_helper();\n+  }\n+  return CompressedKlassPointers::decode_not_null(m.narrow_klass());\n+}\n+\n+markWord markWord::set_klass(const Klass* klass) const {\n+  assert(UseCompressedClassPointers, \"expect compressed klass pointers\");\n+  \/\/ TODO: Don't cast to non-const, change CKP::encode() to accept const Klass* instead.\n+  narrowKlass nklass = CompressedKlassPointers::encode(const_cast<Klass*>(klass));\n+  return set_narrow_klass(nklass);\n+}\n+#endif\n+\n+#endif \/\/ SHARE_OOPS_MARKWORD_INLINE_HPP\n","filename":"src\/hotspot\/share\/oops\/markWord.inline.hpp","additions":68,"deletions":0,"binary":false,"changes":68,"status":"added"},{"patch":"@@ -73,1 +73,0 @@\n-  assert (obj->is_array(), \"obj must be array\");\n","filename":"src\/hotspot\/share\/oops\/objArrayKlass.inline.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -54,23 +54,3 @@\n-  \/\/ Give size of objArrayOop in HeapWords minus the header\n-  static int array_size(int length) {\n-    const uint OopsPerHeapWord = HeapWordSize\/heapOopSize;\n-    assert(OopsPerHeapWord >= 1 && (HeapWordSize % heapOopSize == 0),\n-           \"Else the following (new) computation would be in error\");\n-    uint res = ((uint)length + OopsPerHeapWord - 1)\/OopsPerHeapWord;\n-#ifdef ASSERT\n-    \/\/ The old code is left in for sanity-checking; it'll\n-    \/\/ go away pretty soon. XXX\n-    \/\/ Without UseCompressedOops, this is simply:\n-    \/\/ oop->length() * HeapWordsPerOop;\n-    \/\/ With narrowOops, HeapWordsPerOop is 1\/2 or equal 0 as an integer.\n-    \/\/ The oop elements are aligned up to wordSize\n-    const uint HeapWordsPerOop = heapOopSize\/HeapWordSize;\n-    uint old_res;\n-    if (HeapWordsPerOop > 0) {\n-      old_res = length * HeapWordsPerOop;\n-    } else {\n-      old_res = align_up((uint)length, OopsPerHeapWord)\/OopsPerHeapWord;\n-    }\n-    assert(res == old_res, \"Inconsistency between old and new.\");\n-#endif  \/\/ ASSERT\n-    return res;\n+  \/\/ Give size of objArrayOop in bytes minus the header\n+  static size_t array_size_in_bytes(int length) {\n+    return (size_t)length * heapOopSize;\n@@ -96,1 +76,0 @@\n-  static int header_size()    { return arrayOopDesc::header_size(T_OBJECT); }\n@@ -101,5 +80,5 @@\n-    uint asz = array_size(length);\n-    uint osz = align_object_size(header_size() + asz);\n-    assert(osz >= asz,   \"no overflow\");\n-    assert((int)osz > 0, \"no overflow\");\n-    return (size_t)osz;\n+    size_t asz = array_size_in_bytes(length);\n+    size_t size_words = align_up(base_offset_in_bytes() + asz, HeapWordSize) \/ HeapWordSize;\n+    size_t osz = align_object_size(size_words);\n+    assert(osz < max_jint, \"no overflow\");\n+    return osz;\n","filename":"src\/hotspot\/share\/oops\/objArrayOop.hpp","additions":8,"deletions":29,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n@@ -37,0 +38,1 @@\n+#include \"runtime\/interfaceSupport.inline.hpp\"\n@@ -149,20 +151,8 @@\n-bool oopDesc::has_klass_gap() {\n-  \/\/ Only has a klass gap when compressed class pointers are used.\n-  return UseCompressedClassPointers;\n-}\n-\n-#if INCLUDE_CDS_JAVA_HEAP\n-void oopDesc::set_narrow_klass(narrowKlass nk) {\n-  assert(DumpSharedSpaces, \"Used by CDS only. Do not abuse!\");\n-  assert(UseCompressedClassPointers, \"must be\");\n-  _metadata._compressed_klass = nk;\n-}\n-#endif\n-\n-  if (UseCompressedClassPointers) {\n-    narrowKlass narrow_klass = obj->_metadata._compressed_klass;\n-    if (narrow_klass == 0) return NULL;\n-    return (void*)CompressedKlassPointers::decode_raw(narrow_klass);\n-  } else {\n-    return obj->_metadata._klass;\n-  }\n+  \/\/ TODO: Remove method altogether and replace with calls to obj->klass() ?\n+  \/\/ OTOH, we may eventually get rid of locking in header, and then no\n+  \/\/ longer have to deal with that anymore.\n+#ifdef _LP64\n+  return obj->klass();\n+#else\n+  return obj->_klass;\n+#endif\n@@ -183,0 +173,15 @@\n+#ifdef _LP64\n+JRT_LEAF(narrowKlass, oopDesc::load_nklass_runtime(oopDesc* o))\n+  assert(o != NULL, \"null-check\");\n+  oop obj = oop(o);\n+  assert(oopDesc::is_oop(obj), \"need a valid oop here: \" PTR_FORMAT, p2i(o));\n+  markWord header = obj->mark();\n+  if (!header.is_neutral()) {\n+    header = ObjectSynchronizer::stable_mark(obj);\n+  }\n+  assert(header.is_neutral(), \"expect neutral header here\");\n+  narrowKlass nklass = header.narrow_klass();\n+  return nklass;\n+JRT_END\n+#endif\n+\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":25,"deletions":20,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"oops\/compressedKlass.hpp\"\n@@ -57,4 +58,3 @@\n-  union _metadata {\n-    Klass*      _klass;\n-    narrowKlass _compressed_klass;\n-  } _metadata;\n+#ifndef _LP64\n+  Klass*            _klass;\n+#endif\n@@ -78,0 +78,1 @@\n+  static inline void release_set_mark(HeapWord* mem, markWord m);\n@@ -89,1 +90,1 @@\n-  void set_narrow_klass(narrowKlass nk) NOT_CDS_JAVA_HEAP_RETURN;\n+#ifndef _LP64\n@@ -92,3 +93,1 @@\n-\n-  \/\/ For klass field compression\n-  static inline void set_klass_gap(HeapWord* mem, int z);\n+#endif\n@@ -257,0 +256,1 @@\n+  inline void forward_to_self();\n@@ -263,0 +263,1 @@\n+  inline oop forward_to_self_atomic(markWord compare, atomic_memory_order order = memory_order_conservative);\n@@ -265,0 +266,1 @@\n+  inline oop forwardee(markWord header) const;\n@@ -303,2 +305,0 @@\n-  static bool has_klass_gap();\n-\n@@ -307,4 +307,7 @@\n-  static int klass_offset_in_bytes()     { return offset_of(oopDesc, _metadata._klass); }\n-  static int klass_gap_offset_in_bytes() {\n-    assert(has_klass_gap(), \"only applicable to compressed klass pointers\");\n-    return klass_offset_in_bytes() + sizeof(narrowKlass);\n+  static int klass_offset_in_bytes()     {\n+#ifdef _LP64\n+    STATIC_ASSERT(markWord::klass_shift % 8 == 0);\n+    return mark_offset_in_bytes() + markWord::klass_shift \/ 8;\n+#else\n+    return offset_of(oopDesc, _klass);\n+#endif\n@@ -317,0 +320,5 @@\n+  \/\/ Runtime entry\n+#ifdef _LP64\n+  static narrowKlass load_nklass_runtime(oopDesc* o);\n+#endif\n+\n","filename":"src\/hotspot\/share\/oops\/oop.hpp","additions":22,"deletions":14,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n@@ -36,1 +37,1 @@\n-#include \"oops\/markWord.hpp\"\n+#include \"oops\/markWord.inline.hpp\"\n@@ -40,0 +41,2 @@\n+#include \"runtime\/safepoint.hpp\"\n+#include \"runtime\/synchronizer.hpp\"\n@@ -72,0 +75,4 @@\n+void oopDesc::release_set_mark(HeapWord* mem, markWord m) {\n+  Atomic::release_store((markWord*)(((char*)mem) + mark_offset_in_bytes()), m);\n+}\n+\n@@ -81,1 +88,8 @@\n-  set_mark(markWord::prototype());\n+#ifdef _LP64\n+  markWord header = ObjectSynchronizer::stable_mark(cast_to_oop(this));\n+  assert(UseCompressedClassPointers, \"expect compressed klass pointers\");\n+  header = markWord((header.value() & markWord::klass_mask_in_place) | markWord::prototype().value());\n+#else\n+  markWord header = markWord::prototype();\n+#endif\n+  set_mark(header);\n@@ -85,4 +99,5 @@\n-  if (UseCompressedClassPointers) {\n-    return CompressedKlassPointers::decode_not_null(_metadata._compressed_klass);\n-  } else {\n-    return _metadata._klass;\n+#ifdef _LP64\n+  assert(UseCompressedClassPointers, \"only with compressed class pointers\");\n+  markWord header = mark();\n+  if (!header.is_neutral()) {\n+    header = ObjectSynchronizer::stable_mark(cast_to_oop(this));\n@@ -90,0 +105,4 @@\n+  return header.klass();\n+#else\n+  return _klass;\n+#endif\n@@ -93,4 +112,5 @@\n-  if (UseCompressedClassPointers) {\n-    return CompressedKlassPointers::decode(_metadata._compressed_klass);\n-  } else {\n-    return _metadata._klass;\n+#ifdef _LP64\n+  assert(UseCompressedClassPointers, \"only with compressed class pointers\");\n+  markWord header = mark();\n+  if (!header.is_neutral()) {\n+    header = ObjectSynchronizer::stable_mark(cast_to_oop(this));\n@@ -98,0 +118,4 @@\n+  return header.klass_or_null();\n+#else\n+  return _klass;\n+#endif\n@@ -101,5 +125,5 @@\n-  if (UseCompressedClassPointers) {\n-    narrowKlass nklass = Atomic::load_acquire(&_metadata._compressed_klass);\n-    return CompressedKlassPointers::decode(nklass);\n-  } else {\n-    return Atomic::load_acquire(&_metadata._klass);\n+#ifdef _LP64\n+  assert(UseCompressedClassPointers, \"only with compressed class pointers\");\n+  markWord header = mark_acquire();\n+  if (!header.is_neutral()) {\n+    header = ObjectSynchronizer::stable_mark(cast_to_oop(this));\n@@ -107,0 +131,4 @@\n+  return header.klass_or_null();\n+#else\n+  return Atomic::load_acquire(&_klass);\n+#endif\n@@ -109,0 +137,1 @@\n+#ifndef _LP64\n@@ -111,5 +140,1 @@\n-  if (UseCompressedClassPointers) {\n-    _metadata._compressed_klass = CompressedKlassPointers::encode_not_null(k);\n-  } else {\n-    _metadata._klass = k;\n-  }\n+  _klass = k;\n@@ -128,6 +153,1 @@\n-\n-void oopDesc::set_klass_gap(HeapWord* mem, int v) {\n-  if (UseCompressedClassPointers) {\n-    *(int*)(((char*)mem) + klass_gap_offset_in_bytes()) = v;\n-  }\n-}\n+#endif\n@@ -260,1 +280,1 @@\n-  assert(m.decode_pointer() == p, \"encoding must be reversible\");\n+  assert(forwardee(m) == p, \"encoding must be reversable\");\n@@ -264,0 +284,17 @@\n+void oopDesc::forward_to_self() {\n+#ifdef _LP64\n+  verify_forwardee(this);\n+  markWord m = mark();\n+  \/\/ If mark is displaced, we need to preserve the Klass* from real header.\n+  assert(SafepointSynchronize::is_at_safepoint(), \"we can only safely fetch the displaced header at safepoint\");\n+  if (m.has_displaced_mark_helper()) {\n+    m = m.displaced_mark_helper();\n+  }\n+  m = m.set_self_forwarded();\n+  assert(forwardee(m) == cast_to_oop(this), \"encoding must be reversable\");\n+  set_mark(m);\n+#else\n+  forward_to(oop(this));\n+#endif\n+}\n+\n@@ -267,1 +304,20 @@\n-  assert(m.decode_pointer() == p, \"encoding must be reversible\");\n+  assert(forwardee(m) == p, \"encoding must be reversable\");\n+  markWord old_mark = cas_set_mark(m, compare, order);\n+  if (old_mark == compare) {\n+    return NULL;\n+  } else {\n+    return forwardee(old_mark);\n+  }\n+}\n+\n+oop oopDesc::forward_to_self_atomic(markWord compare, atomic_memory_order order) {\n+#ifdef _LP64\n+  verify_forwardee(this);\n+  markWord m = compare;\n+  \/\/ If mark is displaced, we need to preserve the Klass* from real header.\n+  assert(SafepointSynchronize::is_at_safepoint(), \"we can only safely fetch the displaced header at safepoint\");\n+  if (m.has_displaced_mark_helper()) {\n+    m = m.displaced_mark_helper();\n+  }\n+  m = m.set_self_forwarded();\n+  assert(forwardee(m) == cast_to_oop(this), \"encoding must be reversable\");\n@@ -272,1 +328,2 @@\n-    return cast_to_oop(old_mark.decode_pointer());\n+    assert(old_mark.is_marked(), \"must be marked here\");\n+    return forwardee(old_mark);\n@@ -274,0 +331,3 @@\n+#else\n+  return forward_to_atomic(oop(this), compare, order);\n+#endif\n@@ -280,2 +340,14 @@\n-  assert(is_forwarded(), \"only decode when actually forwarded\");\n-  return cast_to_oop(mark().decode_pointer());\n+  return forwardee(mark());\n+}\n+\n+oop oopDesc::forwardee(markWord header) const {\n+  assert(header.is_marked(), \"must be forwarded\");\n+#ifdef _LP64\n+  if (header.self_forwarded()) {\n+    return cast_to_oop(this);\n+  } else\n+#endif\n+  {\n+    assert(header.is_marked(), \"only decode when actually forwarded\");\n+    return cast_to_oop(header.decode_pointer());\n+  }\n@@ -336,1 +408,0 @@\n-  assert(k == klass(), \"wrong klass\");\n","filename":"src\/hotspot\/share\/oops\/oop.inline.hpp","additions":103,"deletions":32,"binary":false,"changes":135,"status":"modified"},{"patch":"@@ -39,3 +39,0 @@\n-\/\/ If compressed klass pointers then use narrowKlass.\n-typedef juint  narrowKlass;\n-\n","filename":"src\/hotspot\/share\/oops\/oopsHierarchy.hpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1595,3 +1595,3 @@\n-  Node* mark_node = NULL;\n-  \/\/ For now only enable fast locking for non-array types\n-  mark_node = phase->MakeConX(markWord::prototype().value());\n+  Node* klass_node = in(AllocateNode::KlassNode);\n+  Node* proto_adr = phase->transform(new AddPNode(klass_node, klass_node, phase->MakeConX(in_bytes(Klass::prototype_header_offset()))));\n+  Node* mark_node = LoadNode::make(*phase, control, mem, proto_adr, TypeRawPtr::BOTTOM, TypeX_X, TypeX_X->basic_type(), MemNode::unordered);\n","filename":"src\/hotspot\/share\/opto\/callnode.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1667,0 +1667,1 @@\n+#ifndef _LP64\n@@ -1668,0 +1669,1 @@\n+#endif\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -310,1 +310,1 @@\n-    const size_t hs = arrayOopDesc::header_size(elem_type);\n+    const size_t hs_bytes = arrayOopDesc::base_offset_in_bytes(elem_type);\n@@ -312,1 +312,1 @@\n-    const size_t aligned_hs = align_object_offset(hs);\n+    const size_t aligned_hs_bytes = align_up(hs_bytes, BytesPerLong);\n@@ -314,2 +314,2 @@\n-    if (aligned_hs > hs) {\n-      Copy::zero_to_words(obj+hs, aligned_hs-hs);\n+    if (aligned_hs_bytes > hs_bytes) {\n+      Copy::zero_to_bytes(obj + hs_bytes, aligned_hs_bytes - hs_bytes);\n@@ -318,0 +318,1 @@\n+    const size_t aligned_hs = aligned_hs_bytes \/ HeapWordSize;\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -4841,1 +4841,2 @@\n-    int header_size = objArrayOopDesc::header_size() * wordSize;\n+    BasicType basic_elem_type = elem()->basic_type();\n+    int header_size = arrayOopDesc::base_offset_in_bytes(basic_elem_type);\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -70,1 +70,1 @@\n-  ( arrayOopDesc::header_size(T_DOUBLE) * HeapWordSize \\\n+  ( arrayOopDesc::base_offset_in_bytes(T_DOUBLE) \\\n","filename":"src\/hotspot\/share\/prims\/unsafe.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1607,3 +1607,0 @@\n-      if (COMPRESSED_CLASS_POINTERS_DEPENDS_ON_COMPRESSED_OOPS) {\n-        FLAG_SET_DEFAULT(UseCompressedClassPointers, false);\n-      }\n@@ -1615,1 +1612,0 @@\n-\n@@ -1620,25 +1616,14 @@\n-  \/\/ On some architectures, the use of UseCompressedClassPointers implies the use of\n-  \/\/ UseCompressedOops. The reason is that the rheap_base register of said platforms\n-  \/\/ is reused to perform some optimized spilling, in order to use rheap_base as a\n-  \/\/ temp register. But by treating it as any other temp register, spilling can typically\n-  \/\/ be completely avoided instead. So it is better not to perform this trick. And by\n-  \/\/ not having that reliance, large heaps, or heaps not supporting compressed oops,\n-  \/\/ can still use compressed class pointers.\n-  if (COMPRESSED_CLASS_POINTERS_DEPENDS_ON_COMPRESSED_OOPS && !UseCompressedOops) {\n-    if (UseCompressedClassPointers) {\n-      warning(\"UseCompressedClassPointers requires UseCompressedOops\");\n-    }\n-    FLAG_SET_DEFAULT(UseCompressedClassPointers, false);\n-  } else {\n-    \/\/ Turn on UseCompressedClassPointers too\n-    if (FLAG_IS_DEFAULT(UseCompressedClassPointers)) {\n-      FLAG_SET_ERGO(UseCompressedClassPointers, true);\n-    }\n-    \/\/ Check the CompressedClassSpaceSize to make sure we use compressed klass ptrs.\n-    if (UseCompressedClassPointers) {\n-      if (CompressedClassSpaceSize > KlassEncodingMetaspaceMax) {\n-        warning(\"CompressedClassSpaceSize is too large for UseCompressedClassPointers\");\n-        FLAG_SET_DEFAULT(UseCompressedClassPointers, false);\n-      }\n-    }\n-  }\n+  if (!UseCompressedClassPointers) {\n+    \/\/ Lilliput requires compressed class pointers. Default shall reflect that.\n+    \/\/ If user specifies -UseCompressedClassPointers, it should be reverted with\n+    \/\/ a warning.\n+    assert(!FLAG_IS_DEFAULT(UseCompressedClassPointers), \"Wrong default for UseCompressedClassPointers\");\n+    warning(\"Lilliput reqires compressed class pointers.\");\n+    FLAG_SET_ERGO(UseCompressedClassPointers, true);\n+  }\n+  \/\/ Assert validity of compressed class space size. User arg should have been checked at this point\n+  \/\/ (see CompressedClassSpaceSizeConstraintFunc()), so no need to be nice about it, this fires in\n+  \/\/ case the default is wrong.\n+  assert(CompressedClassSpaceSize <= Metaspace::max_class_space_size(),\n+         \"CompressedClassSpaceSize \" SIZE_FORMAT \" too large (max: \" SIZE_FORMAT \")\",\n+         CompressedClassSpaceSize, Metaspace::max_class_space_size());\n@@ -1807,3 +1792,0 @@\n-          if (COMPRESSED_CLASS_POINTERS_DEPENDS_ON_COMPRESSED_OOPS) {\n-            FLAG_SET_ERGO(UseCompressedClassPointers, false);\n-          }\n@@ -4208,0 +4190,1 @@\n+\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":15,"deletions":32,"binary":false,"changes":47,"status":"modified"},{"patch":"@@ -128,1 +128,1 @@\n-  product(bool, UseCompressedClassPointers, false,                          \\\n+  product(bool, UseCompressedClassPointers, true,                           \\\n@@ -1069,1 +1069,1 @@\n-  develop(bool, UseHeavyMonitors, false,                                    \\\n+  product(bool, UseHeavyMonitors, false, DIAGNOSTIC,                        \\\n@@ -1426,1 +1426,1 @@\n-          range(1*M, 3*G)                                                   \\\n+          constraint(CompressedClassSpaceSizeConstraintFunc,AtParse)        \\\n@@ -1428,1 +1428,1 @@\n-  develop(size_t, CompressedClassSpaceBaseAddress, 0,                       \\\n+  product(size_t, CompressedClassSpaceBaseAddress, 0, DIAGNOSTIC,           \\\n@@ -2059,0 +2059,6 @@\n+  product(bool, HeapObjectStats, false, DIAGNOSTIC,                         \\\n+             \"Enable gathering of heap object statistics\")                  \\\n+                                                                            \\\n+  product(size_t, HeapObjectStatsSamplingInterval, 500, DIAGNOSTIC,         \\\n+             \"Heap object statistics sampling interval (ms)\")               \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":10,"deletions":4,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -603,0 +603,16 @@\n+\/\/ We might access the dead object headers for parsable heap walk, make sure\n+\/\/ headers are in correct shape, e.g. monitors deflated.\n+void ObjectMonitor::maybe_deflate_dead(oop* p) {\n+  oop obj = *p;\n+  assert(obj != NULL, \"must not yet been cleared\");\n+  markWord mark = obj->mark();\n+  if (mark.has_monitor()) {\n+    ObjectMonitor* monitor = mark.monitor();\n+    if (p == monitor->_object.ptr_raw()) {\n+      assert(monitor->object_peek() == obj, \"lock object must match\");\n+      markWord dmw = monitor->header();\n+      obj->set_mark(dmw);\n+    }\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.cpp","additions":16,"deletions":0,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -334,0 +334,2 @@\n+  static void maybe_deflate_dead(oop* p);\n+\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+#include \"oops\/compressedKlass.hpp\"\n","filename":"src\/hotspot\/share\/runtime\/os.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -176,0 +176,1 @@\n+address StubRoutines::_load_nklass = NULL;\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -267,1 +267,3 @@\n- public:\n+  static address _load_nklass;\n+\n+public:\n@@ -433,0 +435,1 @@\n+  static address load_nklass()         { return _load_nklass; }\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"gc\/shared\/suspendibleThreadSet.hpp\"\n@@ -746,1 +747,1 @@\n-static markWord read_stable_mark(oop obj) {\n+markWord ObjectSynchronizer::read_stable_mark(oop obj) {\n@@ -806,0 +807,67 @@\n+\/\/ Safely load a mark word from an object, even with racing stack-locking or monitor inflation.\n+\/\/ The protocol is a partial inflation-protocol: it installs INFLATING into the object's mark\n+\/\/ word in order to prevent an stack-locks or inflations from interferring (or detect such\n+\/\/ interference and retry), but then, instead of creating and installing a monitor, simply\n+\/\/ read and return the real mark word.\n+markWord ObjectSynchronizer::stable_mark(oop object) {\n+  for (;;) {\n+    const markWord mark = read_stable_mark(object);\n+    assert(!mark.is_being_inflated(), \"read_stable_mark must prevent inflating mark\");\n+\n+    \/\/ The mark can be in one of the following states:\n+    \/\/ *  Inflated     - just return mark from inflated monitor\n+    \/\/ *  Stack-locked - coerce it to inflating, and then return displaced mark\n+    \/\/ *  Neutral      - return mark\n+    \/\/ *  Marked       - return mark\n+\n+    \/\/ CASE: inflated\n+    if (mark.has_monitor()) {\n+      ObjectMonitor* inf = mark.monitor();\n+      markWord dmw = inf->header();\n+      assert(dmw.is_neutral(), \"invariant: header=\" INTPTR_FORMAT, dmw.value());\n+      return dmw;\n+    }\n+\n+    \/\/ CASE: stack-locked\n+    \/\/ Could be stack-locked either by this thread or by some other thread.\n+    if (mark.has_locker()) {\n+      BasicLock* lock = mark.locker();\n+      if (Thread::current()->is_lock_owned((address)lock)) {\n+        \/\/ If locked by this thread, it is safe to access the displaced header.\n+        markWord dmw = lock->displaced_header();\n+        assert(dmw.is_neutral(), \"invariant: header=\" INTPTR_FORMAT, dmw.value());\n+        return dmw;\n+      }\n+\n+      \/\/ Otherwise, attempt to temporarily install INFLATING into the mark-word,\n+      \/\/ to prevent inflation or unlocking by competing thread.\n+      markWord cmp = object->cas_set_mark(markWord::INFLATING(), mark);\n+      if (cmp != mark) {\n+        continue;       \/\/ Interference -- just retry\n+      }\n+\n+      \/\/ fetch the displaced mark from the owner's stack.\n+      \/\/ The owner can't die or unwind past the lock while our INFLATING\n+      \/\/ object is in the mark.  Furthermore the owner can't complete\n+      \/\/ an unlock on the object, either.\n+      markWord dmw = mark.displaced_mark_helper();\n+      \/\/ Catch if the object's header is not neutral (not locked and\n+      \/\/ not marked is what we care about here).\n+      assert(dmw.is_neutral(), \"invariant: header=\" INTPTR_FORMAT, dmw.value());\n+\n+      \/\/ Must preserve store ordering. The monitor state must\n+      \/\/ be stable at the time of publishing the monitor address.\n+      assert(object->mark() == markWord::INFLATING(), \"invariant\");\n+      \/\/ Release semantics so that above set_object() is seen first.\n+      object->release_set_mark(mark);\n+\n+      return dmw;\n+    }\n+\n+    \/\/ CASE: neutral or marked (for GC)\n+    \/\/ Catch if the object's header is not neutral or marked (it must not be locked).\n+    assert(mark.is_neutral() || mark.is_marked(), \"invariant: header=\" INTPTR_FORMAT, mark.value());\n+    return mark;\n+  }\n+}\n+\n@@ -1474,0 +1542,10 @@\n+class VM_RendezvousGCThreads : public VM_Operation {\n+public:\n+  bool evaluate_at_safepoint() const override { return false; }\n+  VMOp_Type type() const override { return VMOp_RendezvousGCThreads; }\n+  void doit() override {\n+    SuspendibleThreadSet::synchronize();\n+    SuspendibleThreadSet::desynchronize();\n+  };\n+};\n+\n@@ -1526,0 +1604,3 @@\n+      \/\/ Also, we sync and desync GC threads around the handshake, so that they can\n+      \/\/ safely read the mark-word and look-through to the object-monitor, without\n+      \/\/ being afraid that the object-monitor is going away.\n@@ -1528,0 +1609,2 @@\n+      VM_RendezvousGCThreads sync_gc;\n+      VMThread::execute(&sync_gc);\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":84,"deletions":1,"binary":false,"changes":85,"status":"modified"},{"patch":"@@ -83,0 +83,1 @@\n+  template(HeapObjectStatistics)                  \\\n@@ -92,0 +93,1 @@\n+  template(RendezvousGCThreads)                   \\\n","filename":"src\/hotspot\/share\/runtime\/vmOperation.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -203,2 +203,1 @@\n-  volatile_nonstatic_field(oopDesc,            _metadata._klass,                              Klass*)                                \\\n-  volatile_nonstatic_field(oopDesc,            _metadata._compressed_klass,                   narrowKlass)                           \\\n+  NOT_LP64(volatile_nonstatic_field(oopDesc,   _klass,                                        Klass*))                               \\\n@@ -384,2 +383,2 @@\n-     static_field(CompressedKlassPointers,     _narrow_klass._base,                           address)                               \\\n-     static_field(CompressedKlassPointers,     _narrow_klass._shift,                          int)                                   \\\n+     static_field(CompressedKlassPointers,     _base,                           address)                                             \\\n+     static_field(CompressedKlassPointers,     _shift_copy,                          int)                                            \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":3,"deletions":4,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -0,0 +1,177 @@\n+\/*\n+ * Copyright (c) 2021, Red Hat, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/collectedHeap.hpp\"\n+#include \"logging\/logStream.hpp\"\n+#include \"logging\/logTag.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"memory\/iterator.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"memory\/universe.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"runtime\/mutexLocker.hpp\"\n+#include \"runtime\/vmThread.hpp\"\n+#include \"services\/heapObjectStatistics.hpp\"\n+#include \"utilities\/copy.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/ostream.hpp\"\n+\n+HeapObjectStatistics* HeapObjectStatistics::_instance = NULL;\n+\n+class HeapObjectStatsObjectClosure : public ObjectClosure {\n+private:\n+  HeapObjectStatistics* const _stats;\n+public:\n+  HeapObjectStatsObjectClosure() : _stats(HeapObjectStatistics::instance()) {}\n+  void do_object(oop obj) {\n+    _stats->visit_object(obj);\n+  }\n+};\n+\n+class VM_HeapObjectStatistics : public VM_Operation {\n+public:\n+  VMOp_Type type() const { return VMOp_HeapObjectStatistics; }\n+  bool doit_prologue() {\n+    Heap_lock->lock();\n+    return true;\n+  }\n+\n+  void doit_epilogue() {\n+    Heap_lock->unlock();\n+  }\n+\n+  void doit() {\n+    assert(SafepointSynchronize::is_at_safepoint(), \"all threads are stopped\");\n+    assert(Heap_lock->is_locked(), \"should have the Heap_lock\");\n+\n+    CollectedHeap* heap = Universe::heap();\n+    heap->ensure_parsability(false);\n+\n+    HeapObjectStatistics* stats = HeapObjectStatistics::instance();\n+    stats->begin_sample();\n+\n+    HeapObjectStatsObjectClosure cl;\n+    heap->object_iterate(&cl);\n+  }\n+};\n+\n+HeapObjectStatisticsTask::HeapObjectStatisticsTask() : PeriodicTask(HeapObjectStatsSamplingInterval) {}\n+\n+void HeapObjectStatisticsTask::task() {\n+  VM_HeapObjectStatistics vmop;\n+  VMThread::execute(&vmop);\n+}\n+\n+void HeapObjectStatistics::initialize() {\n+  assert(_instance == NULL, \"Don't init twice\");\n+  if (HeapObjectStats) {\n+    _instance = new HeapObjectStatistics();\n+    _instance->start();\n+  }\n+}\n+\n+void HeapObjectStatistics::shutdown() {\n+  if (HeapObjectStats) {\n+    assert(_instance != NULL, \"Must be initialized\");\n+    LogTarget(Info, heap, stats) lt;\n+    if (lt.is_enabled()) {\n+      LogStream ls(lt);\n+      ResourceMark rm;\n+      _instance->print(&ls);\n+    }\n+    _instance->stop();\n+    delete _instance;\n+    _instance = NULL;\n+  }\n+}\n+\n+HeapObjectStatistics* HeapObjectStatistics::instance() {\n+  assert(_instance != NULL, \"Must be initialized\");\n+  return _instance;\n+}\n+\n+void HeapObjectStatistics::increase_counter(uint64_t& counter, uint64_t val) {\n+  uint64_t oldval = counter;\n+  uint64_t newval = counter + val;\n+  if (newval < oldval) {\n+    log_warning(heap, stats)(\"HeapObjectStats counter overflow: resulting statistics will be useless\");\n+  }\n+  counter = newval;\n+}\n+\n+HeapObjectStatistics::HeapObjectStatistics() :\n+  _task(), _num_samples(0), _num_objects(0), _num_ihashed(0), _num_locked(0), _lds(0) { }\n+\n+void HeapObjectStatistics::start() {\n+  _task.enroll();\n+}\n+\n+void HeapObjectStatistics::stop() {\n+  _task.disenroll();\n+}\n+\n+void HeapObjectStatistics::begin_sample() {\n+  _num_samples++;\n+}\n+\n+void HeapObjectStatistics::visit_object(oop obj) {\n+  increase_counter(_num_objects);\n+  markWord mark = obj->mark();\n+  if (!mark.has_no_hash()) {\n+    increase_counter(_num_ihashed);\n+    if (mark.age() > 0) {\n+      increase_counter(_num_ihashed_moved);\n+    }\n+  }\n+  if (mark.is_locked()) {\n+    increase_counter(_num_locked);\n+  }\n+#ifdef ASSERT\n+#ifdef _LP64\n+  if (!mark.has_displaced_mark_helper()) {\n+    assert(mark.narrow_klass() == CompressedKlassPointers::encode(obj->klass_or_null()), \"upper 32 mark bits must be narrow klass: mark: \" INTPTR_FORMAT \", compressed-klass: \" INTPTR_FORMAT, (intptr_t)mark.narrow_klass(), (intptr_t)CompressedKlassPointers::encode(obj->klass_or_null()));\n+  }\n+#endif\n+#endif\n+  increase_counter(_lds, obj->size());\n+}\n+\n+void HeapObjectStatistics::print(outputStream* out) const {\n+  if (!HeapObjectStats) {\n+    return;\n+  }\n+  if (_num_samples == 0 || _num_objects == 0) {\n+    return;\n+  }\n+\n+  out->print_cr(\"Number of samples:  \" UINT64_FORMAT, _num_samples);\n+  out->print_cr(\"Average number of objects: \" UINT64_FORMAT, _num_objects \/ _num_samples);\n+  out->print_cr(\"Average object size: \" UINT64_FORMAT \" bytes, %.1f words\", (_lds * HeapWordSize) \/ _num_objects, (float) _lds \/ _num_objects);\n+  out->print_cr(\"Average number of hashed objects: \" UINT64_FORMAT \" (%.2f%%)\", _num_ihashed \/ _num_samples, (float) (_num_ihashed * 100.0) \/ _num_objects);\n+  out->print_cr(\"Average number of moved hashed objects: \" UINT64_FORMAT \" (%.2f%%)\", _num_ihashed_moved \/ _num_samples, (float) (_num_ihashed_moved * 100.0) \/ _num_objects);\n+  out->print_cr(\"Average number of locked objects: \" UINT64_FORMAT \" (%.2f%%)\", _num_locked \/ _num_samples, (float) (_num_locked * 100) \/ _num_objects);\n+  out->print_cr(\"Average LDS: \" UINT64_FORMAT \" bytes\", _lds * HeapWordSize \/ _num_samples);\n+  out->print_cr(\"Avg LDS with (assumed) 64bit header: \" UINT64_FORMAT \" bytes (%.1f%%)\", (_lds - _num_objects) * HeapWordSize \/ _num_samples, ((float) _lds - _num_objects) * 100.0 \/ _lds);\n+}\n","filename":"src\/hotspot\/share\/services\/heapObjectStatistics.cpp","additions":177,"deletions":0,"binary":false,"changes":177,"status":"added"},{"patch":"@@ -563,5 +563,0 @@\n-const int LogKlassAlignmentInBytes = 3;\n-const int LogKlassAlignment        = LogKlassAlignmentInBytes - LogHeapWordSize;\n-const int KlassAlignmentInBytes    = 1 << LogKlassAlignmentInBytes;\n-const int KlassAlignment           = KlassAlignmentInBytes \/ HeapWordSize;\n-\n@@ -575,5 +570,0 @@\n-\/\/ Maximal size of compressed class space. Above this limit compression is not possible.\n-\/\/ Also upper bound for placement of zero based class space. (Class space is further limited\n-\/\/ to be < 3G, see arguments.cpp.)\n-const  uint64_t KlassEncodingMetaspaceMax = (uint64_t(max_juint) + 1) << LogKlassAlignmentInBytes;\n-\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":0,"deletions":10,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"oops\/compressedKlass.hpp\"\n","filename":"src\/hotspot\/share\/utilities\/vmError.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -76,1 +76,2 @@\n-    final int hubOffset = getFieldOffset(\"oopDesc::_metadata._klass\", Integer.class, \"Klass*\");\n+    \/\/ TODO: Lilliput. Probably ok.\n+    final int hubOffset = 4; \/\/ getFieldOffset(\"oopDesc::_metadata._klass\", Integer.class, \"Klass*\");\n","filename":"src\/jdk.internal.vm.ci\/share\/classes\/jdk.vm.ci.hotspot\/src\/jdk\/vm\/ci\/hotspot\/HotSpotVMConfig.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -124,0 +124,34 @@\n+# Missing Lilliput support to load Klass*\n+serviceability\/sa\/CDSJMapClstats.java 1234567 generic-all\n+serviceability\/sa\/ClhsdbCDSCore.java 1234567 generic-all\n+serviceability\/sa\/ClhsdbCDSJstackPrintAll.java 1234567 generic-all\n+serviceability\/sa\/ClhsdbDumpheap.java 1234567 generic-all\n+serviceability\/sa\/ClhsdbFindPC.java#no-xcomp-core\n+serviceability\/sa\/ClhsdbFindPC.java#no-xcomp-process\n+serviceability\/sa\/ClhsdbFindPC.java#xcomp-core\n+serviceability\/sa\/ClhsdbFindPC.java#xcomp-process\n+serviceability\/sa\/ClhsdbInspect.java 1234567 generic-all\n+serviceability\/sa\/ClhsdbJdis.java 1234567 generic-all\n+serviceability\/sa\/ClhsdbJhisto.java 1234567 generic-all\n+serviceability\/sa\/ClhsdbJstack.java#id0 1234567 generic-all\n+serviceability\/sa\/ClhsdbJstack.java#id1 1234567 generic-all\n+serviceability\/sa\/ClhsdbPrintAs.java 1234567 generic-all\n+serviceability\/sa\/ClhsdbPstack.java#core 1234567 generic-all\n+serviceability\/sa\/ClhsdbPstack.java#process 1234567 generic-all\n+serviceability\/sa\/ClhsdbSource.java 1234567 generic-all\n+serviceability\/sa\/ClhsdbThread.java 1234567 generic-all\n+serviceability\/sa\/ClhsdbThreadContext.java 1234567 generic-all\n+serviceability\/sa\/ClhsdbWhere.java 1234567 generic-all\n+serviceability\/sa\/DeadlockDetectionTest.java 1234567 generic-all\n+serviceability\/sa\/JhsdbThreadInfoTest.java 1234567 generic-all\n+serviceability\/sa\/TestClhsdbJstackLock.java 1234567 generic-all\n+serviceability\/sa\/TestHeapDumpForInvokeDynamic.java 1234567 generic-all\n+serviceability\/sa\/TestJhsdbJstackLineNumbers.java 1234567 generic-all\n+serviceability\/sa\/TestJhsdbJstackLock.java 1234567 generic-all\n+serviceability\/sa\/TestJhsdbJstackMixed.java 1234567 generic-all\n+serviceability\/sa\/TestObjectMonitorIterate.java 1234567 generic-all\n+serviceability\/sa\/TestSysProps.java 1234567 generic-all\n+serviceability\/sa\/jmap-hprof\/JMapHProfLargeHeapTest.java 1234567 generic-all\n+serviceability\/sa\/sadebugd\/DebugdConnectTest.java 1234567 generic-all\n+serviceability\/sa\/sadebugd\/DisableRegistryTest.java 1234567 generic-all\n+\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":34,"deletions":0,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -113,1 +113,1 @@\n-    public static final String LOAD_KLASS  = START + \"LoadK\" + MID + END;\n+    public static final String LOAD_KLASS  = START + \"LoadN?K\" + MID + END;\n","filename":"test\/hotspot\/jtreg\/compiler\/lib\/ir_framework\/IRNode.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -129,1 +129,3 @@\n-    String compressedClassSpaceSizeArg = \"-XX:CompressedClassSpaceSize=\" + 2 * getCompressedClassSpaceSize();\n+    \/\/ Lilliput: do not assume a max. class space size, since that is subject to change. Instead, use a value slightly smaller\n+    \/\/  than what the parent VM runs with (which is the default size).\n+    String compressedClassSpaceSizeArg = \"-XX:CompressedClassSpaceSize=\" + (getCompressedClassSpaceSize() - 1);\n","filename":"test\/hotspot\/jtreg\/gc\/arguments\/TestUseCompressedOopsErgoTools.java","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -48,0 +48,1 @@\n+import jdk.test.lib.Platform;\n@@ -75,1 +76,1 @@\n-    private static final int OBJECT_SIZE_HIGH = 3250;\n+    private static final int OBJECT_SIZE_HIGH = Platform.is64bit() ? 3266 : 3258;\n","filename":"test\/hotspot\/jtreg\/gc\/g1\/plab\/TestPLABPromotion.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -268,1 +268,1 @@\n-        return Platform.is64bit() && InputArguments.contains(\"-XX:+UseCompressedClassPointers\");\n+        return Platform.is64bit();\n","filename":"test\/hotspot\/jtreg\/gc\/metaspace\/TestMetaspacePerfCounters.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -52,1 +52,1 @@\n- *   -Xmx1G -XX:G1HeapRegionSize=8m -XX:MaxGCPauseMillis=1000 gc.stress.TestMultiThreadStressRSet 60 16\n+ *   -Xmx1100M -XX:G1HeapRegionSize=8m -XX:MaxGCPauseMillis=1000 gc.stress.TestMultiThreadStressRSet 60 16\n","filename":"test\/hotspot\/jtreg\/gc\/stress\/TestMultiThreadStressRSet.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -48,1 +48,1 @@\n-      processArgs.add(\"-XX:MaxMetaspaceSize=3m\");\n+      processArgs.add(\"-XX:MaxMetaspaceSize=2m\");\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/MaxMetaspaceSize.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -134,2 +134,1 @@\n-        runCheck(new String[] {\"-XX:+IgnoreUnrecognizedVMOptions\", \"-XX:-UseCompressedClassPointers\"},\n-                 BadFailOnConstraint.create(Loads.class, \"load()\", 1, 1, \"Load\"),\n+        runCheck(BadFailOnConstraint.create(Loads.class, \"load()\", 1, 1, \"Load\"),\n","filename":"test\/hotspot\/jtreg\/testlibrary_tests\/ir_framework\/tests\/TestIRMatching.java","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -375,1 +375,1 @@\n-        long expected = roundUp(Platform.is64bit() ? 16 : 8, OBJ_ALIGN);\n+        long expected = roundUp(8, OBJ_ALIGN);\n@@ -382,1 +382,1 @@\n-        long expected = roundUp(Platform.is64bit() ? 16 : 8, OBJ_ALIGN);\n+        long expected = roundUp(8, OBJ_ALIGN);\n@@ -392,1 +392,1 @@\n-        long expected = roundUp(Platform.is64bit() ? 16 : 8, OBJ_ALIGN);\n+        long expected = roundUp(8, OBJ_ALIGN);\n","filename":"test\/jdk\/java\/lang\/instrument\/GetObjectSizeIntrinsicsTest.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -74,0 +74,3 @@\n+\n+\n+jdk\/jshell\/ToolTabSnippetTest.java 1234567 generic-all\n","filename":"test\/langtools\/ProblemList.txt","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"}]}