{"files":[{"patch":"@@ -4,1 +4,1 @@\n-version=19\n+version=20\n","filename":".jcheck\/conf","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1202,0 +1202,3 @@\n+reg_class p0_reg(P0);\n+reg_class p1_reg(P1);\n+\n@@ -1784,10 +1787,0 @@\n-int MachCallNativeNode::ret_addr_offset() {\n-  \/\/ This is implemented using aarch64_enc_java_to_runtime as above.\n-  CodeBlob *cb = CodeCache::find_blob(_entry_point);\n-  if (cb) {\n-    return 1 * NativeInstruction::instruction_size;\n-  } else {\n-    return 6 * NativeInstruction::instruction_size;\n-  }\n-}\n-\n@@ -1930,1 +1923,18 @@\n-    bs->nmethod_entry_barrier(&_masm);\n+    if (BarrierSet::barrier_set()->barrier_set_nmethod() != NULL) {\n+      \/\/ Dummy labels for just measuring the code size\n+      Label dummy_slow_path;\n+      Label dummy_continuation;\n+      Label dummy_guard;\n+      Label* slow_path = &dummy_slow_path;\n+      Label* continuation = &dummy_continuation;\n+      Label* guard = &dummy_guard;\n+      if (!Compile::current()->output()->in_scratch_emit_size()) {\n+        \/\/ Use real labels from actual stub when not emitting code for the purpose of measuring its size\n+        C2EntryBarrierStub* stub = Compile::current()->output()->entry_barrier_table()->add_entry_barrier();\n+        slow_path = &stub->slow_path();\n+        continuation = &stub->continuation();\n+        guard = &stub->guard();\n+      }\n+      \/\/ In the C2 code, we move the non-hot part of nmethod entry barriers out-of-line to a stub.\n+      bs->nmethod_entry_barrier(&_masm, slow_path, continuation, guard);\n+    }\n@@ -2442,0 +2452,13 @@\n+const bool Matcher::match_rule_supported_superword(int opcode, int vlen, BasicType bt) {\n+  if (UseSVE == 0) {\n+    \/\/ ConvD2I and ConvL2F are not profitable to be vectorized on NEON, because no direct\n+    \/\/ NEON instructions support them. But the match rule support for them is profitable for\n+    \/\/ Vector API intrinsics.\n+    if ((opcode == Op_VectorCastD2X && bt == T_INT) ||\n+        (opcode == Op_VectorCastL2X && bt == T_FLOAT)) {\n+      return false;\n+    }\n+  }\n+  return match_rule_supported_vector(opcode, vlen, bt);\n+}\n+\n@@ -2468,0 +2491,1 @@\n+    case Op_PopulateIndex:\n@@ -2475,0 +2499,1 @@\n+    case Op_VectorMaskGen:\n@@ -2477,0 +2502,4 @@\n+    case Op_CompressV:\n+    case Op_CompressM:\n+    case Op_ExpandV:\n+    case Op_VectorLongToMask:\n@@ -2494,0 +2523,5 @@\n+const bool Matcher::vector_needs_partial_operations(Node* node, const TypeVect* vt) {\n+  \/\/ Only SVE has partial vector operations\n+  return (UseSVE > 0) && partial_op_sve_needed(node, vt);\n+}\n+\n@@ -2705,2 +2739,7 @@\n-bool can_combine_with_imm(Node* binary_node, Node* replicate_node) {\n-  if (UseSVE == 0 || !VectorNode::is_invariant_vector(replicate_node)){\n+\/\/ Binary src (Replicate con)\n+bool is_valid_sve_arith_imm_pattern(Node* n, Node* m) {\n+  if (n == NULL || m == NULL) {\n+    return false;\n+  }\n+\n+  if (UseSVE == 0 || !VectorNode::is_invariant_vector(m)) {\n@@ -2709,1 +2748,2 @@\n-  Node* imm_node = replicate_node->in(1);\n+\n+  Node* imm_node = m->in(1);\n@@ -2719,1 +2759,1 @@\n-  switch (binary_node->Opcode()) {\n+  switch (n->Opcode()) {\n@@ -2723,1 +2763,1 @@\n-    Assembler::SIMD_RegVariant T = Assembler::elemType_to_regVariant(Matcher::vector_element_basic_type(binary_node));\n+    Assembler::SIMD_RegVariant T = Assembler::elemType_to_regVariant(Matcher::vector_element_basic_type(n));\n@@ -2739,1 +2779,3 @@\n-bool is_vector_arith_imm_pattern(Node* n, Node* m) {\n+\/\/ (XorV src (Replicate m1))\n+\/\/ (XorVMask src (MaskAll m1))\n+bool is_vector_bitwise_not_pattern(Node* n, Node* m) {\n@@ -2741,1 +2783,2 @@\n-    return can_combine_with_imm(n, m);\n+    return (n->Opcode() == Op_XorV || n->Opcode() == Op_XorVMask) &&\n+           VectorNode::is_all_ones_vector(m);\n@@ -2748,3 +2791,3 @@\n-  \/\/ ShiftV src (ShiftCntV con)\n-  \/\/ Binary src (Replicate con)\n-  if (is_vshift_con_pattern(n, m) || is_vector_arith_imm_pattern(n, m)) {\n+  if (is_vshift_con_pattern(n, m) ||\n+      is_vector_bitwise_not_pattern(n, m) ||\n+      is_valid_sve_arith_imm_pattern(n, m)) {\n@@ -2754,1 +2797,0 @@\n-\n@@ -2799,4 +2841,0 @@\n-bool Parse::do_one_bytecode_targeted() {\n-  return false;\n-}\n-\n@@ -3823,5 +3861,11 @@\n-      \/\/ Emit stub for static call\n-      address stub = CompiledStaticCall::emit_to_interp_stub(cbuf);\n-      if (stub == NULL) {\n-        ciEnv::current()->record_failure(\"CodeCache is full\");\n-        return;\n+      if (CodeBuffer::supports_shared_stubs() && _method->can_be_statically_bound()) {\n+        \/\/ Calls of the same statically bound method can share\n+        \/\/ a stub to the interpreter.\n+        cbuf.shared_stub_to_interp_for(_method, cbuf.insts()->mark_off());\n+      } else {\n+        \/\/ Emit stub for static call\n+        address stub = CompiledStaticCall::emit_to_interp_stub(cbuf);\n+        if (stub == NULL) {\n+          ciEnv::current()->record_failure(\"CodeCache is full\");\n+          return;\n+        }\n@@ -3831,0 +3875,3 @@\n+    _masm.clear_inst_mark();\n+    __ post_call_nop();\n+\n@@ -3844,1 +3891,4 @@\n-    } else if (Compile::current()->max_vector_size() > 0) {\n+    }\n+    _masm.clear_inst_mark();\n+    __ post_call_nop();\n+    if (Compile::current()->max_vector_size() > 0) {\n@@ -3872,0 +3922,2 @@\n+      _masm.clear_inst_mark();\n+      __ post_call_nop();\n@@ -3880,0 +3932,1 @@\n+      __ post_call_nop();\n@@ -3926,1 +3979,1 @@\n-    Label cas_failed;\n+    Label no_count;\n@@ -3962,3 +4015,0 @@\n-      __ bind(cas_failed);\n-      \/\/ We did not see an unlocked object so try the fast recursive case.\n-\n@@ -4009,0 +4059,5 @@\n+    __ br(Assembler::NE, no_count);\n+\n+    __ increment(Address(rthread, JavaThread::held_monitor_count_offset()));\n+\n+    __ bind(no_count);\n@@ -4019,0 +4074,1 @@\n+    Label no_count;\n@@ -4077,0 +4133,5 @@\n+    __ br(Assembler::NE, no_count);\n+\n+    __ decrement(Address(rthread, JavaThread::held_monitor_count_offset()));\n+\n+    __ bind(no_count);\n@@ -4441,0 +4502,10 @@\n+operand immI_positive()\n+%{\n+  predicate(n->get_int() > 0);\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n@@ -5727,0 +5798,18 @@\n+operand pRegGov_P0()\n+%{\n+  constraint(ALLOC_IN_RC(p0_reg));\n+  match(RegVectMask);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand pRegGov_P1()\n+%{\n+  constraint(ALLOC_IN_RC(p1_reg));\n+  match(RegVectMask);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n@@ -8647,1 +8736,0 @@\n-  predicate(UsePopCountInstruction);\n@@ -8669,1 +8757,0 @@\n-  predicate(UsePopCountInstruction);\n@@ -8692,1 +8779,0 @@\n-  predicate(UsePopCountInstruction);\n@@ -8712,1 +8798,0 @@\n-  predicate(UsePopCountInstruction);\n@@ -11121,1 +11206,1 @@\n-  format %{ \"smulh   $dst, $src1, $src2, \\t# mulhi\" %}\n+  format %{ \"smulh   $dst, $src1, $src2\\t# mulhi\" %}\n@@ -11132,0 +11217,16 @@\n+instruct umulHiL_rReg(iRegLNoSp dst, iRegL src1, iRegL src2, rFlagsReg cr)\n+%{\n+  match(Set dst (UMulHiL src1 src2));\n+\n+  ins_cost(INSN_COST * 7);\n+  format %{ \"umulh   $dst, $src1, $src2\\t# umulhi\" %}\n+\n+  ins_encode %{\n+    __ umulh(as_Register($dst$$reg),\n+             as_Register($src1$$reg),\n+             as_Register($src2$$reg));\n+  %}\n+\n+  ins_pipe(lmul_reg_reg);\n+%}\n+\n@@ -11330,1 +11431,1 @@\n-            \"msubw($dst, rscratch1, $src2, $src1\" %}\n+            \"msubw  $dst, rscratch1, $src2, $src1\" %}\n@@ -11343,1 +11444,1 @@\n-            \"msub($dst, rscratch1, $src2, $src1\" %}\n+            \"msub   $dst, rscratch1, $src2, $src1\" %}\n@@ -11349,0 +11450,64 @@\n+\/\/ Unsigned Integer Divide\n+\n+instruct UdivI_reg_reg(iRegINoSp dst, iRegIorL2I src1, iRegIorL2I src2) %{\n+  match(Set dst (UDivI src1 src2));\n+\n+  ins_cost(INSN_COST * 19);\n+  format %{ \"udivw  $dst, $src1, $src2\" %}\n+\n+  ins_encode %{\n+    __ udivw($dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+\n+  ins_pipe(idiv_reg_reg);\n+%}\n+\n+\/\/  Unsigned Long Divide\n+\n+instruct UdivL_reg_reg(iRegLNoSp dst, iRegL src1, iRegL src2) %{\n+  match(Set dst (UDivL src1 src2));\n+\n+  ins_cost(INSN_COST * 35);\n+  format %{ \"udiv   $dst, $src1, $src2\" %}\n+\n+  ins_encode %{\n+    __ udiv($dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+\n+  ins_pipe(ldiv_reg_reg);\n+%}\n+\n+\/\/ Unsigned Integer Remainder\n+\n+instruct UmodI_reg_reg(iRegINoSp dst, iRegIorL2I src1, iRegIorL2I src2) %{\n+  match(Set dst (UModI src1 src2));\n+\n+  ins_cost(INSN_COST * 22);\n+  format %{ \"udivw  rscratch1, $src1, $src2\\n\\t\"\n+            \"msubw  $dst, rscratch1, $src2, $src1\" %}\n+\n+  ins_encode %{\n+    __ udivw(rscratch1, $src1$$Register, $src2$$Register);\n+    __ msubw($dst$$Register, rscratch1, $src2$$Register, $src1$$Register);\n+  %}\n+\n+  ins_pipe(idiv_reg_reg);\n+%}\n+\n+\/\/ Unsigned Long Remainder\n+\n+instruct UModL_reg_reg(iRegLNoSp dst, iRegL src1, iRegL src2) %{\n+  match(Set dst (UModL src1 src2));\n+\n+  ins_cost(INSN_COST * 38);\n+  format %{ \"udiv   rscratch1, $src1, $src2\\n\"\n+            \"msub   $dst, rscratch1, $src2, $src1\" %}\n+\n+  ins_encode %{\n+    __ udiv(rscratch1, $src1$$Register, $src2$$Register);\n+    __ msub($dst$$Register, rscratch1, $src2$$Register, $src1$$Register);\n+  %}\n+\n+  ins_pipe(ldiv_reg_reg);\n+%}\n+\n@@ -15422,1 +15587,5 @@\n-    __ zero_words($base$$Register, (uint64_t)$cnt$$constant);\n+    address tpc = __ zero_words($base$$Register, (uint64_t)$cnt$$constant);\n+    if (tpc == NULL) {\n+      ciEnv::current()->record_failure(\"CodeCache is full\");\n+      return;\n+    }\n@@ -15474,1 +15643,1 @@\n-  format %{ \"cmn   $op1, $op2\\t# overflow check long\" %}\n+  format %{ \"adds  zr, $op1, $op2\\t# overflow check long\" %}\n@@ -15477,1 +15646,1 @@\n-    __ cmn($op1$$Register, $op2$$constant);\n+    __ adds(zr, $op1$$Register, $op2$$constant);\n@@ -16759,15 +16928,0 @@\n-instruct CallNativeDirect(method meth)\n-%{\n-  match(CallNative);\n-\n-  effect(USE meth);\n-\n-  ins_cost(CALL_COST);\n-\n-  format %{ \"CALL, native $meth\" %}\n-\n-  ins_encode( aarch64_enc_java_to_runtime(meth) );\n-\n-  ins_pipe(pipe_class_call);\n-%}\n-\n@@ -16906,1 +17060,1 @@\n-  predicate(((StrCompNode*)n)->encoding() == StrIntrinsicNode::UU);\n+  predicate((UseSVE == 0) && (((StrCompNode*)n)->encoding() == StrIntrinsicNode::UU));\n@@ -16916,1 +17070,1 @@\n-                      fnoreg, fnoreg, fnoreg, StrIntrinsicNode::UU);\n+                      fnoreg, fnoreg, fnoreg, pnoreg, pnoreg, StrIntrinsicNode::UU);\n@@ -16924,1 +17078,1 @@\n-  predicate(((StrCompNode*)n)->encoding() == StrIntrinsicNode::LL);\n+  predicate((UseSVE == 0) && (((StrCompNode*)n)->encoding() == StrIntrinsicNode::LL));\n@@ -16933,1 +17087,1 @@\n-                      fnoreg, fnoreg, fnoreg, StrIntrinsicNode::LL);\n+                      fnoreg, fnoreg, fnoreg, pnoreg, pnoreg, StrIntrinsicNode::LL);\n@@ -16942,1 +17096,1 @@\n-  predicate(((StrCompNode*)n)->encoding() == StrIntrinsicNode::UL);\n+  predicate((UseSVE == 0) && (((StrCompNode*)n)->encoding() == StrIntrinsicNode::UL));\n@@ -16953,1 +17107,1 @@\n-                      $vtmp3$$FloatRegister, StrIntrinsicNode::UL);\n+                      $vtmp3$$FloatRegister, pnoreg, pnoreg, StrIntrinsicNode::UL);\n@@ -16962,1 +17116,1 @@\n-  predicate(((StrCompNode*)n)->encoding() == StrIntrinsicNode::LU);\n+  predicate((UseSVE == 0) && (((StrCompNode*)n)->encoding() == StrIntrinsicNode::LU));\n@@ -16973,1 +17127,1 @@\n-                      $vtmp3$$FloatRegister,StrIntrinsicNode::LU);\n+                      $vtmp3$$FloatRegister, pnoreg, pnoreg, StrIntrinsicNode::LU);\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":223,"deletions":69,"binary":false,"changes":292,"status":"modified"},{"patch":"@@ -127,1 +127,1 @@\n-  __ far_call(Address(Runtime1::entry_for(Runtime1::throw_div0_exception_id), relocInfo::runtime_call_type));\n+  __ far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::throw_div0_exception_id)));\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_CodeStubs_aarch64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -285,1 +285,1 @@\n-      __ ldr(r19, Address(OSR_buf, slot_offset + 0));\n+      __ ldp(r19, r20, Address(OSR_buf, slot_offset));\n@@ -287,2 +287,1 @@\n-      __ ldr(r19, Address(OSR_buf, slot_offset + 1*BytesPerWord));\n-      __ str(r19, frame_map()->address_for_monitor_object(i));\n+      __ str(r20, frame_map()->address_for_monitor_object(i));\n@@ -381,7 +380,0 @@\n-  \/\/ if the last instruction is a call (typically to do a throw which\n-  \/\/ is coming at the end after block reordering) the return address\n-  \/\/ must still point into the code area in order to avoid assertion\n-  \/\/ failures when searching for the corresponding bci => add a nop\n-  \/\/ (was bug 5\/14\/1999 - gri)\n-  __ nop();\n-\n@@ -406,1 +398,2 @@\n-  __ far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::handle_exception_from_callee_id)));  __ should_not_reach_here();\n+  __ far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::handle_exception_from_callee_id)));\n+  __ should_not_reach_here();\n@@ -474,7 +467,0 @@\n-  \/\/ if the last instruction is a call (typically to do a throw which\n-  \/\/ is coming at the end after block reordering) the return address\n-  \/\/ must still point into the code area in order to avoid assertion\n-  \/\/ failures when searching for the corresponding bci => add a nop\n-  \/\/ (was bug 5\/14\/1999 - gri)\n-  __ nop();\n-\n@@ -980,1 +966,1 @@\n-         __ ldr(dest->as_register(), as_Address(from_addr));\n+        __ ldr(dest->as_register(), as_Address(from_addr));\n@@ -2053,0 +2039,1 @@\n+  __ post_call_nop();\n@@ -2063,0 +2050,1 @@\n+  __ post_call_nop();\n@@ -2552,0 +2540,4 @@\n+    if (op->info() != NULL) {\n+      add_debug_info_for_null_check_here(op->info());\n+      __ null_check(obj, -1);\n+    }\n@@ -2691,1 +2683,1 @@\n-  if (offset) __ add(res, res, offset);\n+  __ add(res, res, offset);\n@@ -2907,0 +2899,1 @@\n+  __ post_call_nop();\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":13,"deletions":20,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -119,0 +119,1 @@\n+  increment(Address(rthread, JavaThread::held_monitor_count_offset()));\n@@ -150,0 +151,1 @@\n+  decrement(Address(rthread, JavaThread::held_monitor_count_offset()));\n@@ -158,1 +160,1 @@\n-    eden_allocate(obj, var_size_in_bytes, con_size_in_bytes, t1, slow_case);\n+    b(slow_case);\n@@ -295,1 +297,1 @@\n-  bs->nmethod_entry_barrier(this);\n+  bs->nmethod_entry_barrier(this, NULL \/* slow_path *\/, NULL \/* continuation *\/, NULL \/* guard *\/);\n@@ -312,1 +314,1 @@\n-  \/\/ rbp, + 0: link\n+  \/\/ rfp, + 0: link\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_MacroAssembler_aarch64.cpp","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -663,50 +663,0 @@\n-        \/\/ If TLAB is disabled, see if there is support for inlining contiguous\n-        \/\/ allocations.\n-        \/\/ Otherwise, just go to the slow path.\n-        if ((id == fast_new_instance_id || id == fast_new_instance_init_check_id) &&\n-            !UseTLAB && Universe::heap()->supports_inline_contig_alloc()) {\n-          Label slow_path;\n-          Register obj_size = r19;\n-          Register t1       = r10;\n-          Register t2       = r11;\n-          assert_different_registers(klass, obj, obj_size, t1, t2);\n-\n-          __ stp(r19, zr, Address(__ pre(sp, -2 * wordSize)));\n-\n-          if (id == fast_new_instance_init_check_id) {\n-            \/\/ make sure the klass is initialized\n-            __ ldrb(rscratch1, Address(klass, InstanceKlass::init_state_offset()));\n-            __ cmpw(rscratch1, InstanceKlass::fully_initialized);\n-            __ br(Assembler::NE, slow_path);\n-          }\n-\n-#ifdef ASSERT\n-          \/\/ assert object can be fast path allocated\n-          {\n-            Label ok, not_ok;\n-            __ ldrw(obj_size, Address(klass, Klass::layout_helper_offset()));\n-            __ cmp(obj_size, (u1)0);\n-            __ br(Assembler::LE, not_ok);  \/\/ make sure it's an instance (LH > 0)\n-            __ tstw(obj_size, Klass::_lh_instance_slow_path_bit);\n-            __ br(Assembler::EQ, ok);\n-            __ bind(not_ok);\n-            __ stop(\"assert(can be fast path allocated)\");\n-            __ should_not_reach_here();\n-            __ bind(ok);\n-          }\n-#endif \/\/ ASSERT\n-\n-          \/\/ get the instance size (size is positive so movl is fine for 64bit)\n-          __ ldrw(obj_size, Address(klass, Klass::layout_helper_offset()));\n-\n-          __ eden_allocate(obj, obj_size, 0, t1, slow_path);\n-\n-          __ initialize_object(obj, klass, obj_size, 0, t1, t2, \/* is_tlab_allocated *\/ false);\n-          __ verify_oop(obj);\n-          __ ldp(r19, zr, Address(__ post(sp, 2 * wordSize)));\n-          __ ret(lr);\n-\n-          __ bind(slow_path);\n-          __ ldp(r19, zr, Address(__ post(sp, 2 * wordSize)));\n-        }\n-\n@@ -788,45 +738,0 @@\n-        \/\/ If TLAB is disabled, see if there is support for inlining contiguous\n-        \/\/ allocations.\n-        \/\/ Otherwise, just go to the slow path.\n-        if (!UseTLAB && Universe::heap()->supports_inline_contig_alloc()) {\n-          Register arr_size = r5;\n-          Register t1       = r10;\n-          Register t2       = r11;\n-          Label slow_path;\n-          assert_different_registers(length, klass, obj, arr_size, t1, t2);\n-\n-          \/\/ check that array length is small enough for fast path.\n-          __ mov(rscratch1, C1_MacroAssembler::max_array_allocation_length);\n-          __ cmpw(length, rscratch1);\n-          __ br(Assembler::HI, slow_path);\n-\n-          \/\/ get the allocation size: round_up(hdr + length << (layout_helper & 0x1F))\n-          \/\/ since size is positive ldrw does right thing on 64bit\n-          __ ldrw(t1, Address(klass, Klass::layout_helper_offset()));\n-          \/\/ since size is positive movw does right thing on 64bit\n-          __ movw(arr_size, length);\n-          __ lslvw(arr_size, length, t1);\n-          __ ubfx(t1, t1, Klass::_lh_header_size_shift,\n-                  exact_log2(Klass::_lh_header_size_mask + 1));\n-          __ add(arr_size, arr_size, t1);\n-          __ add(arr_size, arr_size, MinObjAlignmentInBytesMask); \/\/ align up\n-          __ andr(arr_size, arr_size, ~MinObjAlignmentInBytesMask);\n-\n-          __ eden_allocate(obj, arr_size, 0, t1, slow_path);  \/\/ preserves arr_size\n-\n-          __ initialize_header(obj, klass, length, t1, t2);\n-          __ ldrb(t1, Address(klass, in_bytes(Klass::layout_helper_offset()) + (Klass::_lh_header_size_shift \/ BitsPerByte)));\n-          assert(Klass::_lh_header_size_shift % BitsPerByte == 0, \"bytewise\");\n-          assert(Klass::_lh_header_size_mask <= 0xFF, \"bytewise\");\n-          __ andr(t1, t1, Klass::_lh_header_size_mask);\n-          __ sub(arr_size, arr_size, t1);  \/\/ body length\n-          __ add(t1, t1, obj);       \/\/ body start\n-          __ initialize_body(t1, arr_size, 0, t1, t2);\n-          __ membar(Assembler::StoreStore);\n-          __ verify_oop(obj);\n-\n-          __ ret(lr);\n-\n-          __ bind(slow_path);\n-        }\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_Runtime1_aarch64.cpp","additions":0,"deletions":95,"binary":false,"changes":95,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -45,1 +46,0 @@\n-#include \"runtime\/thread.inline.hpp\"\n@@ -173,1 +173,6 @@\n-  lea(rdispatch, Address(rdispatch, offset));\n+  \/\/ Use add() here after ARDP, rather than lea().\n+  \/\/ lea() does not generate anything if its offset is zero.\n+  \/\/ However, relocs expect to find either an ADD or a load\/store\n+  \/\/ insn after an ADRP.  add() always generates an ADD insn, even\n+  \/\/ for add(Rn, Rn, 0).\n+  add(rdispatch, rdispatch, offset);\n@@ -424,1 +429,1 @@\n-  mov(r13, sp);\n+  mov(r19_sender_sp, sp);\n@@ -733,1 +738,1 @@\n-    Label done;\n+    Label count, done;\n@@ -767,1 +772,1 @@\n-    cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, rscratch1, done, \/*fallthrough*\/NULL);\n+    cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, rscratch1, count, \/*fallthrough*\/NULL);\n@@ -804,1 +809,1 @@\n-    br(Assembler::EQ, done);\n+    br(Assembler::EQ, count);\n@@ -812,0 +817,4 @@\n+    b(done);\n+\n+    bind(count);\n+    increment(Address(rthread, JavaThread::held_monitor_count_offset()));\n@@ -836,1 +845,1 @@\n-    Label done;\n+    Label count, done;\n@@ -859,1 +868,1 @@\n-    cbz(header_reg, done);\n+    cbz(header_reg, count);\n@@ -862,1 +871,1 @@\n-    cmpxchg_obj_header(swap_reg, header_reg, obj_reg, rscratch1, done, \/*fallthrough*\/NULL);\n+    cmpxchg_obj_header(swap_reg, header_reg, obj_reg, rscratch1, count, \/*fallthrough*\/NULL);\n@@ -867,0 +876,1 @@\n+    b(done);\n@@ -868,1 +878,2 @@\n-    bind(done);\n+    bind(count);\n+    decrement(Address(rthread, JavaThread::held_monitor_count_offset()));\n@@ -870,0 +881,1 @@\n+    bind(done);\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.cpp","additions":22,"deletions":10,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"compiler\/oopMap.hpp\"\n@@ -51,0 +52,1 @@\n+#include \"runtime\/continuation.hpp\"\n@@ -53,0 +55,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -56,1 +59,0 @@\n-#include \"runtime\/thread.hpp\"\n@@ -76,65 +78,162 @@\n-\/\/ Patch any kind of instruction; there may be several instructions.\n-\/\/ Return the total length (in bytes) of the instructions.\n-int MacroAssembler::pd_patch_instruction_size(address branch, address target) {\n-  int instructions = 1;\n-  assert((uint64_t)target < (1ull << 48), \"48-bit overflow in address constant\");\n-  intptr_t offset = (target - branch) >> 2;\n-  unsigned insn = *(unsigned*)branch;\n-  if ((Instruction_aarch64::extract(insn, 29, 24) & 0b111011) == 0b011000) {\n-    \/\/ Load register (literal)\n-    Instruction_aarch64::spatch(branch, 23, 5, offset);\n-  } else if (Instruction_aarch64::extract(insn, 30, 26) == 0b00101) {\n-    \/\/ Unconditional branch (immediate)\n-    Instruction_aarch64::spatch(branch, 25, 0, offset);\n-  } else if (Instruction_aarch64::extract(insn, 31, 25) == 0b0101010) {\n-    \/\/ Conditional branch (immediate)\n-    Instruction_aarch64::spatch(branch, 23, 5, offset);\n-  } else if (Instruction_aarch64::extract(insn, 30, 25) == 0b011010) {\n-    \/\/ Compare & branch (immediate)\n-    Instruction_aarch64::spatch(branch, 23, 5, offset);\n-  } else if (Instruction_aarch64::extract(insn, 30, 25) == 0b011011) {\n-    \/\/ Test & branch (immediate)\n-    Instruction_aarch64::spatch(branch, 18, 5, offset);\n-  } else if (Instruction_aarch64::extract(insn, 28, 24) == 0b10000) {\n-    \/\/ PC-rel. addressing\n-    offset = target-branch;\n-    int shift = Instruction_aarch64::extract(insn, 31, 31);\n-    if (shift) {\n-      uint64_t dest = (uint64_t)target;\n-      uint64_t pc_page = (uint64_t)branch >> 12;\n-      uint64_t adr_page = (uint64_t)target >> 12;\n-      unsigned offset_lo = dest & 0xfff;\n-      offset = adr_page - pc_page;\n-\n-      \/\/ We handle 4 types of PC relative addressing\n-      \/\/   1 - adrp    Rx, target_page\n-      \/\/       ldr\/str Ry, [Rx, #offset_in_page]\n-      \/\/   2 - adrp    Rx, target_page\n-      \/\/       add     Ry, Rx, #offset_in_page\n-      \/\/   3 - adrp    Rx, target_page (page aligned reloc, offset == 0)\n-      \/\/       movk    Rx, #imm16<<32\n-      \/\/   4 - adrp    Rx, target_page (page aligned reloc, offset == 0)\n-      \/\/ In the first 3 cases we must check that Rx is the same in the adrp and the\n-      \/\/ subsequent ldr\/str, add or movk instruction. Otherwise we could accidentally end\n-      \/\/ up treating a type 4 relocation as a type 1, 2 or 3 just because it happened\n-      \/\/ to be followed by a random unrelated ldr\/str, add or movk instruction.\n-      \/\/\n-      unsigned insn2 = ((unsigned*)branch)[1];\n-      if (Instruction_aarch64::extract(insn2, 29, 24) == 0b111001 &&\n-                Instruction_aarch64::extract(insn, 4, 0) ==\n-                        Instruction_aarch64::extract(insn2, 9, 5)) {\n-        \/\/ Load\/store register (unsigned immediate)\n-        unsigned size = Instruction_aarch64::extract(insn2, 31, 30);\n-        Instruction_aarch64::patch(branch + sizeof (unsigned),\n-                                    21, 10, offset_lo >> size);\n-        guarantee(((dest >> size) << size) == dest, \"misaligned target\");\n-        instructions = 2;\n-      } else if (Instruction_aarch64::extract(insn2, 31, 22) == 0b1001000100 &&\n-                Instruction_aarch64::extract(insn, 4, 0) ==\n-                        Instruction_aarch64::extract(insn2, 4, 0)) {\n-        \/\/ add (immediate)\n-        Instruction_aarch64::patch(branch + sizeof (unsigned),\n-                                   21, 10, offset_lo);\n-        instructions = 2;\n-      } else if (Instruction_aarch64::extract(insn2, 31, 21) == 0b11110010110 &&\n-                   Instruction_aarch64::extract(insn, 4, 0) ==\n+#ifdef ASSERT\n+extern \"C\" void disnm(intptr_t p);\n+#endif\n+\/\/ Target-dependent relocation processing\n+\/\/\n+\/\/ Instruction sequences whose target may need to be retrieved or\n+\/\/ patched are distinguished by their leading instruction, sorting\n+\/\/ them into three main instruction groups and related subgroups.\n+\/\/\n+\/\/ 1) Branch, Exception and System (insn count = 1)\n+\/\/    1a) Unconditional branch (immediate):\n+\/\/      b\/bl imm19\n+\/\/    1b) Compare & branch (immediate):\n+\/\/      cbz\/cbnz Rt imm19\n+\/\/    1c) Test & branch (immediate):\n+\/\/      tbz\/tbnz Rt imm14\n+\/\/    1d) Conditional branch (immediate):\n+\/\/      b.cond imm19\n+\/\/\n+\/\/ 2) Loads and Stores (insn count = 1)\n+\/\/    2a) Load register literal:\n+\/\/      ldr Rt imm19\n+\/\/\n+\/\/ 3) Data Processing Immediate (insn count = 2 or 3)\n+\/\/    3a) PC-rel. addressing\n+\/\/      adr\/adrp Rx imm21; ldr\/str Ry Rx  #imm12\n+\/\/      adr\/adrp Rx imm21; add Ry Rx  #imm12\n+\/\/      adr\/adrp Rx imm21; movk Rx #imm16<<32; ldr\/str Ry, [Rx, #offset_in_page]\n+\/\/      adr\/adrp Rx imm21\n+\/\/      adr\/adrp Rx imm21; movk Rx #imm16<<32\n+\/\/      adr\/adrp Rx imm21; movk Rx #imm16<<32; add Ry, Rx, #offset_in_page\n+\/\/      The latter form can only happen when the target is an\n+\/\/      ExternalAddress, and (by definition) ExternalAddresses don't\n+\/\/      move. Because of that property, there is never any need to\n+\/\/      patch the last of the three instructions. However,\n+\/\/      MacroAssembler::target_addr_for_insn takes all three\n+\/\/      instructions into account and returns the correct address.\n+\/\/    3b) Move wide (immediate)\n+\/\/      movz Rx #imm16; movk Rx #imm16 << 16; movk Rx #imm16 << 32;\n+\/\/\n+\/\/ A switch on a subset of the instruction's bits provides an\n+\/\/ efficient dispatch to these subcases.\n+\/\/\n+\/\/ insn[28:26] -> main group ('x' == don't care)\n+\/\/   00x -> UNALLOCATED\n+\/\/   100 -> Data Processing Immediate\n+\/\/   101 -> Branch, Exception and System\n+\/\/   x1x -> Loads and Stores\n+\/\/\n+\/\/ insn[30:25] -> subgroup ('_' == group, 'x' == don't care).\n+\/\/ n.b. in some cases extra bits need to be checked to verify the\n+\/\/ instruction is as expected\n+\/\/\n+\/\/ 1) ... xx101x Branch, Exception and System\n+\/\/   1a)  00___x Unconditional branch (immediate)\n+\/\/   1b)  01___0 Compare & branch (immediate)\n+\/\/   1c)  01___1 Test & branch (immediate)\n+\/\/   1d)  10___0 Conditional branch (immediate)\n+\/\/        other  Should not happen\n+\/\/\n+\/\/ 2) ... xxx1x0 Loads and Stores\n+\/\/   2a)  xx1__00 Load\/Store register (insn[28] == 1 && insn[24] == 0)\n+\/\/   2aa) x01__00 Load register literal (i.e. requires insn[29] == 0)\n+\/\/                strictly should be 64 bit non-FP\/SIMD i.e.\n+\/\/       0101_000 (i.e. requires insn[31:24] == 01011000)\n+\/\/\n+\/\/ 3) ... xx100x Data Processing Immediate\n+\/\/   3a)  xx___00 PC-rel. addressing (n.b. requires insn[24] == 0)\n+\/\/   3b)  xx___101 Move wide (immediate) (n.b. requires insn[24:23] == 01)\n+\/\/                 strictly should be 64 bit movz #imm16<<0\n+\/\/       110___10100 (i.e. requires insn[31:21] == 11010010100)\n+\/\/\n+class RelocActions {\n+protected:\n+  typedef int (*reloc_insn)(address insn_addr, address &target);\n+\n+  virtual reloc_insn adrpMem() = 0;\n+  virtual reloc_insn adrpAdd() = 0;\n+  virtual reloc_insn adrpMovk() = 0;\n+\n+  const address _insn_addr;\n+  const uint32_t _insn;\n+\n+  static uint32_t insn_at(address insn_addr, int n) {\n+    return ((uint32_t*)insn_addr)[n];\n+  }\n+  uint32_t insn_at(int n) const {\n+    return insn_at(_insn_addr, n);\n+  }\n+\n+public:\n+\n+  RelocActions(address insn_addr) : _insn_addr(insn_addr), _insn(insn_at(insn_addr, 0)) {}\n+  RelocActions(address insn_addr, uint32_t insn)\n+    :  _insn_addr(insn_addr), _insn(insn) {}\n+\n+  virtual int unconditionalBranch(address insn_addr, address &target) = 0;\n+  virtual int conditionalBranch(address insn_addr, address &target) = 0;\n+  virtual int testAndBranch(address insn_addr, address &target) = 0;\n+  virtual int loadStore(address insn_addr, address &target) = 0;\n+  virtual int adr(address insn_addr, address &target) = 0;\n+  virtual int adrp(address insn_addr, address &target, reloc_insn inner) = 0;\n+  virtual int immediate(address insn_addr, address &target) = 0;\n+  virtual void verify(address insn_addr, address &target) = 0;\n+\n+  int ALWAYSINLINE run(address insn_addr, address &target) {\n+    int instructions = 1;\n+\n+    uint32_t dispatch = Instruction_aarch64::extract(_insn, 30, 25);\n+    switch(dispatch) {\n+      case 0b001010:\n+      case 0b001011: {\n+        instructions = unconditionalBranch(insn_addr, target);\n+        break;\n+      }\n+      case 0b101010:   \/\/ Conditional branch (immediate)\n+      case 0b011010: { \/\/ Compare & branch (immediate)\n+        instructions = conditionalBranch(insn_addr, target);\n+          break;\n+      }\n+      case 0b011011: {\n+        instructions = testAndBranch(insn_addr, target);\n+        break;\n+      }\n+      case 0b001100:\n+      case 0b001110:\n+      case 0b011100:\n+      case 0b011110:\n+      case 0b101100:\n+      case 0b101110:\n+      case 0b111100:\n+      case 0b111110: {\n+        \/\/ load\/store\n+        if ((Instruction_aarch64::extract(_insn, 29, 24) & 0b111011) == 0b011000) {\n+          \/\/ Load register (literal)\n+          instructions = loadStore(insn_addr, target);\n+          break;\n+        } else {\n+          \/\/ nothing to do\n+          assert(target == 0, \"did not expect to relocate target for polling page load\");\n+        }\n+        break;\n+      }\n+      case 0b001000:\n+      case 0b011000:\n+      case 0b101000:\n+      case 0b111000: {\n+        \/\/ adr\/adrp\n+        assert(Instruction_aarch64::extract(_insn, 28, 24) == 0b10000, \"must be\");\n+        int shift = Instruction_aarch64::extract(_insn, 31, 31);\n+        if (shift) {\n+          uint32_t insn2 = insn_at(1);\n+          if (Instruction_aarch64::extract(insn2, 29, 24) == 0b111001 &&\n+              Instruction_aarch64::extract(_insn, 4, 0) ==\n+              Instruction_aarch64::extract(insn2, 9, 5)) {\n+            instructions = adrp(insn_addr, target, adrpMem());\n+          } else if (Instruction_aarch64::extract(insn2, 31, 22) == 0b1001000100 &&\n+                     Instruction_aarch64::extract(_insn, 4, 0) ==\n+                     Instruction_aarch64::extract(insn2, 4, 0)) {\n+            instructions = adrp(insn_addr, target, adrpAdd());\n+          } else if (Instruction_aarch64::extract(insn2, 31, 21) == 0b11110010110 &&\n+                     Instruction_aarch64::extract(_insn, 4, 0) ==\n@@ -142,7 +241,18 @@\n-        \/\/ movk #imm16<<32\n-        Instruction_aarch64::patch(branch + 4, 20, 5, (uint64_t)target >> 32);\n-        uintptr_t dest = ((uintptr_t)target & 0xffffffffULL) | ((uintptr_t)branch & 0xffff00000000ULL);\n-        uintptr_t pc_page = (uintptr_t)branch >> 12;\n-        uintptr_t adr_page = (uintptr_t)dest >> 12;\n-        offset = adr_page - pc_page;\n-        instructions = 2;\n+            instructions = adrp(insn_addr, target, adrpMovk());\n+          } else {\n+            ShouldNotReachHere();\n+          }\n+        } else {\n+          instructions = adr(insn_addr, target);\n+        }\n+        break;\n+      }\n+      case 0b001001:\n+      case 0b011001:\n+      case 0b101001:\n+      case 0b111001: {\n+        instructions = immediate(insn_addr, target);\n+        break;\n+      }\n+      default: {\n+        ShouldNotReachHere();\n@@ -151,0 +261,40 @@\n+\n+    verify(insn_addr, target);\n+    return instructions * NativeInstruction::instruction_size;\n+  }\n+};\n+\n+class Patcher : public RelocActions {\n+  virtual reloc_insn adrpMem() { return &Patcher::adrpMem_impl; }\n+  virtual reloc_insn adrpAdd() { return &Patcher::adrpAdd_impl; }\n+  virtual reloc_insn adrpMovk() { return &Patcher::adrpMovk_impl; }\n+\n+public:\n+  Patcher(address insn_addr) : RelocActions(insn_addr) {}\n+\n+  virtual int unconditionalBranch(address insn_addr, address &target) {\n+    intptr_t offset = (target - insn_addr) >> 2;\n+    Instruction_aarch64::spatch(insn_addr, 25, 0, offset);\n+    return 1;\n+  }\n+  virtual int conditionalBranch(address insn_addr, address &target) {\n+    intptr_t offset = (target - insn_addr) >> 2;\n+    Instruction_aarch64::spatch(insn_addr, 23, 5, offset);\n+    return 1;\n+  }\n+  virtual int testAndBranch(address insn_addr, address &target) {\n+    intptr_t offset = (target - insn_addr) >> 2;\n+    Instruction_aarch64::spatch(insn_addr, 18, 5, offset);\n+    return 1;\n+  }\n+  virtual int loadStore(address insn_addr, address &target) {\n+    intptr_t offset = (target - insn_addr) >> 2;\n+    Instruction_aarch64::spatch(insn_addr, 23, 5, offset);\n+    return 1;\n+  }\n+  virtual int adr(address insn_addr, address &target) {\n+#ifdef ASSERT\n+    assert(Instruction_aarch64::extract(_insn, 28, 24) == 0b10000, \"must be\");\n+#endif\n+    \/\/ PC-rel. addressing\n+    ptrdiff_t offset = target - insn_addr;\n@@ -153,3 +303,48 @@\n-    Instruction_aarch64::spatch(branch, 23, 5, offset);\n-    Instruction_aarch64::patch(branch, 30, 29, offset_lo);\n-  } else if (Instruction_aarch64::extract(insn, 31, 21) == 0b11010010100) {\n+    Instruction_aarch64::spatch(insn_addr, 23, 5, offset);\n+    Instruction_aarch64::patch(insn_addr, 30, 29, offset_lo);\n+    return 1;\n+  }\n+  virtual int adrp(address insn_addr, address &target, reloc_insn inner) {\n+    int instructions = 1;\n+#ifdef ASSERT\n+    assert(Instruction_aarch64::extract(_insn, 28, 24) == 0b10000, \"must be\");\n+#endif\n+    ptrdiff_t offset = target - insn_addr;\n+    instructions = 2;\n+    precond(inner != nullptr);\n+    \/\/ Give the inner reloc a chance to modify the target.\n+    address adjusted_target = target;\n+    instructions = (*inner)(insn_addr, adjusted_target);\n+    uintptr_t pc_page = (uintptr_t)insn_addr >> 12;\n+    uintptr_t adr_page = (uintptr_t)adjusted_target >> 12;\n+    offset = adr_page - pc_page;\n+    int offset_lo = offset & 3;\n+    offset >>= 2;\n+    Instruction_aarch64::spatch(insn_addr, 23, 5, offset);\n+    Instruction_aarch64::patch(insn_addr, 30, 29, offset_lo);\n+    return instructions;\n+  }\n+  static int adrpMem_impl(address insn_addr, address &target) {\n+    uintptr_t dest = (uintptr_t)target;\n+    int offset_lo = dest & 0xfff;\n+    uint32_t insn2 = insn_at(insn_addr, 1);\n+    uint32_t size = Instruction_aarch64::extract(insn2, 31, 30);\n+    Instruction_aarch64::patch(insn_addr + sizeof (uint32_t), 21, 10, offset_lo >> size);\n+    guarantee(((dest >> size) << size) == dest, \"misaligned target\");\n+    return 2;\n+  }\n+  static int adrpAdd_impl(address insn_addr, address &target) {\n+    uintptr_t dest = (uintptr_t)target;\n+    int offset_lo = dest & 0xfff;\n+    Instruction_aarch64::patch(insn_addr + sizeof (uint32_t), 21, 10, offset_lo);\n+    return 2;\n+  }\n+  static int adrpMovk_impl(address insn_addr, address &target) {\n+    uintptr_t dest = uintptr_t(target);\n+    Instruction_aarch64::patch(insn_addr + sizeof (uint32_t), 20, 5, (uintptr_t)target >> 32);\n+    dest = (dest & 0xffffffffULL) | (uintptr_t(insn_addr) & 0xffff00000000ULL);\n+    target = address(dest);\n+    return 2;\n+  }\n+  virtual int immediate(address insn_addr, address &target) {\n+    assert(Instruction_aarch64::extract(_insn, 31, 21) == 0b11010010100, \"must be\");\n@@ -158,12 +353,6 @@\n-    assert(nativeInstruction_at(branch+4)->is_movk(), \"wrong insns in patch\");\n-    assert(nativeInstruction_at(branch+8)->is_movk(), \"wrong insns in patch\");\n-    Instruction_aarch64::patch(branch, 20, 5, dest & 0xffff);\n-    Instruction_aarch64::patch(branch+4, 20, 5, (dest >>= 16) & 0xffff);\n-    Instruction_aarch64::patch(branch+8, 20, 5, (dest >>= 16) & 0xffff);\n-    assert(target_addr_for_insn(branch) == target, \"should be\");\n-    instructions = 3;\n-  } else if (NativeInstruction::is_ldrw_to_zr(address(&insn))) {\n-    \/\/ nothing to do\n-    assert(target == 0, \"did not expect to relocate target for polling page load\");\n-  } else {\n-    ShouldNotReachHere();\n+    assert(nativeInstruction_at(insn_addr+4)->is_movk(), \"wrong insns in patch\");\n+    assert(nativeInstruction_at(insn_addr+8)->is_movk(), \"wrong insns in patch\");\n+    Instruction_aarch64::patch(insn_addr, 20, 5, dest & 0xffff);\n+    Instruction_aarch64::patch(insn_addr+4, 20, 5, (dest >>= 16) & 0xffff);\n+    Instruction_aarch64::patch(insn_addr+8, 20, 5, (dest >>= 16) & 0xffff);\n+    return 3;\n@@ -171,1 +360,147 @@\n-  return instructions * NativeInstruction::instruction_size;\n+  virtual void verify(address insn_addr, address &target) {\n+#ifdef ASSERT\n+    address address_is = MacroAssembler::target_addr_for_insn(insn_addr);\n+    if (!(address_is == target)) {\n+      tty->print_cr(\"%p at %p should be %p\", address_is, insn_addr, target);\n+      disnm((intptr_t)insn_addr);\n+      assert(address_is == target, \"should be\");\n+    }\n+#endif\n+  }\n+};\n+\n+\/\/ If insn1 and insn2 use the same register to form an address, either\n+\/\/ by an offsetted LDR or a simple ADD, return the offset. If the\n+\/\/ second instruction is an LDR, the offset may be scaled.\n+static bool offset_for(uint32_t insn1, uint32_t insn2, ptrdiff_t &byte_offset) {\n+  if (Instruction_aarch64::extract(insn2, 29, 24) == 0b111001 &&\n+      Instruction_aarch64::extract(insn1, 4, 0) ==\n+      Instruction_aarch64::extract(insn2, 9, 5)) {\n+    \/\/ Load\/store register (unsigned immediate)\n+    byte_offset = Instruction_aarch64::extract(insn2, 21, 10);\n+    uint32_t size = Instruction_aarch64::extract(insn2, 31, 30);\n+    byte_offset <<= size;\n+    return true;\n+  } else if (Instruction_aarch64::extract(insn2, 31, 22) == 0b1001000100 &&\n+             Instruction_aarch64::extract(insn1, 4, 0) ==\n+             Instruction_aarch64::extract(insn2, 4, 0)) {\n+    \/\/ add (immediate)\n+    byte_offset = Instruction_aarch64::extract(insn2, 21, 10);\n+    return true;\n+  }\n+  return false;\n+}\n+\n+class Decoder : public RelocActions {\n+  virtual reloc_insn adrpMem() { return &Decoder::adrpMem_impl; }\n+  virtual reloc_insn adrpAdd() { return &Decoder::adrpAdd_impl; }\n+  virtual reloc_insn adrpMovk() { return &Decoder::adrpMovk_impl; }\n+\n+public:\n+  Decoder(address insn_addr, uint32_t insn) : RelocActions(insn_addr, insn) {}\n+\n+  virtual int loadStore(address insn_addr, address &target) {\n+    intptr_t offset = Instruction_aarch64::sextract(_insn, 23, 5);\n+    target = insn_addr + (offset << 2);\n+    return 1;\n+  }\n+  virtual int unconditionalBranch(address insn_addr, address &target) {\n+    intptr_t offset = Instruction_aarch64::sextract(_insn, 25, 0);\n+    target = insn_addr + (offset << 2);\n+    return 1;\n+  }\n+  virtual int conditionalBranch(address insn_addr, address &target) {\n+    intptr_t offset = Instruction_aarch64::sextract(_insn, 23, 5);\n+    target = address(((uint64_t)insn_addr + (offset << 2)));\n+    return 1;\n+  }\n+  virtual int testAndBranch(address insn_addr, address &target) {\n+    intptr_t offset = Instruction_aarch64::sextract(_insn, 18, 5);\n+    target = address(((uint64_t)insn_addr + (offset << 2)));\n+    return 1;\n+  }\n+  virtual int adr(address insn_addr, address &target) {\n+    \/\/ PC-rel. addressing\n+    intptr_t offset = Instruction_aarch64::extract(_insn, 30, 29);\n+    offset |= Instruction_aarch64::sextract(_insn, 23, 5) << 2;\n+    target = address((uint64_t)insn_addr + offset);\n+    return 1;\n+  }\n+  virtual int adrp(address insn_addr, address &target, reloc_insn inner) {\n+    assert(Instruction_aarch64::extract(_insn, 28, 24) == 0b10000, \"must be\");\n+    intptr_t offset = Instruction_aarch64::extract(_insn, 30, 29);\n+    offset |= Instruction_aarch64::sextract(_insn, 23, 5) << 2;\n+    int shift = 12;\n+    offset <<= shift;\n+    uint64_t target_page = ((uint64_t)insn_addr) + offset;\n+    target_page &= ((uint64_t)-1) << shift;\n+    uint32_t insn2 = insn_at(1);\n+    target = address(target_page);\n+    precond(inner != nullptr);\n+    (*inner)(insn_addr, target);\n+    return 2;\n+  }\n+  static int adrpMem_impl(address insn_addr, address &target) {\n+    uint32_t insn2 = insn_at(insn_addr, 1);\n+    \/\/ Load\/store register (unsigned immediate)\n+    ptrdiff_t byte_offset = Instruction_aarch64::extract(insn2, 21, 10);\n+    uint32_t size = Instruction_aarch64::extract(insn2, 31, 30);\n+    byte_offset <<= size;\n+    target += byte_offset;\n+    return 2;\n+  }\n+  static int adrpAdd_impl(address insn_addr, address &target) {\n+    uint32_t insn2 = insn_at(insn_addr, 1);\n+    \/\/ add (immediate)\n+    ptrdiff_t byte_offset = Instruction_aarch64::extract(insn2, 21, 10);\n+    target += byte_offset;\n+    return 2;\n+  }\n+  static int adrpMovk_impl(address insn_addr, address &target) {\n+    uint32_t insn2 = insn_at(insn_addr, 1);\n+    uint64_t dest = uint64_t(target);\n+    dest = (dest & 0xffff0000ffffffff) |\n+      ((uint64_t)Instruction_aarch64::extract(insn2, 20, 5) << 32);\n+    target = address(dest);\n+\n+    \/\/ We know the destination 4k page. Maybe we have a third\n+    \/\/ instruction.\n+    uint32_t insn = insn_at(insn_addr, 0);\n+    uint32_t insn3 = insn_at(insn_addr, 2);\n+    ptrdiff_t byte_offset;\n+    if (offset_for(insn, insn3, byte_offset)) {\n+      target += byte_offset;\n+      return 3;\n+    } else {\n+      return 2;\n+    }\n+  }\n+  virtual int immediate(address insn_addr, address &target) {\n+    uint32_t *insns = (uint32_t *)insn_addr;\n+    assert(Instruction_aarch64::extract(_insn, 31, 21) == 0b11010010100, \"must be\");\n+    \/\/ Move wide constant: movz, movk, movk.  See movptr().\n+    assert(nativeInstruction_at(insns+1)->is_movk(), \"wrong insns in patch\");\n+    assert(nativeInstruction_at(insns+2)->is_movk(), \"wrong insns in patch\");\n+    target = address(uint64_t(Instruction_aarch64::extract(_insn, 20, 5))\n+                 + (uint64_t(Instruction_aarch64::extract(insns[1], 20, 5)) << 16)\n+                 + (uint64_t(Instruction_aarch64::extract(insns[2], 20, 5)) << 32));\n+    assert(nativeInstruction_at(insn_addr+4)->is_movk(), \"wrong insns in patch\");\n+    assert(nativeInstruction_at(insn_addr+8)->is_movk(), \"wrong insns in patch\");\n+    return 3;\n+  }\n+  virtual void verify(address insn_addr, address &target) {\n+  }\n+};\n+\n+address MacroAssembler::target_addr_for_insn(address insn_addr, uint32_t insn) {\n+  Decoder decoder(insn_addr, insn);\n+  address target;\n+  decoder.run(insn_addr, target);\n+  return target;\n+}\n+\n+\/\/ Patch any kind of instruction; there may be several instructions.\n+\/\/ Return the total length (in bytes) of the instructions.\n+int MacroAssembler::pd_patch_instruction_size(address insn_addr, address target) {\n+  Patcher patcher(insn_addr);\n+  return patcher.run(insn_addr, target);\n@@ -213,81 +548,0 @@\n-address MacroAssembler::target_addr_for_insn(address insn_addr, unsigned insn) {\n-  intptr_t offset = 0;\n-  if ((Instruction_aarch64::extract(insn, 29, 24) & 0b011011) == 0b00011000) {\n-    \/\/ Load register (literal)\n-    offset = Instruction_aarch64::sextract(insn, 23, 5);\n-    return address(((uint64_t)insn_addr + (offset << 2)));\n-  } else if (Instruction_aarch64::extract(insn, 30, 26) == 0b00101) {\n-    \/\/ Unconditional branch (immediate)\n-    offset = Instruction_aarch64::sextract(insn, 25, 0);\n-  } else if (Instruction_aarch64::extract(insn, 31, 25) == 0b0101010) {\n-    \/\/ Conditional branch (immediate)\n-    offset = Instruction_aarch64::sextract(insn, 23, 5);\n-  } else if (Instruction_aarch64::extract(insn, 30, 25) == 0b011010) {\n-    \/\/ Compare & branch (immediate)\n-    offset = Instruction_aarch64::sextract(insn, 23, 5);\n-   } else if (Instruction_aarch64::extract(insn, 30, 25) == 0b011011) {\n-    \/\/ Test & branch (immediate)\n-    offset = Instruction_aarch64::sextract(insn, 18, 5);\n-  } else if (Instruction_aarch64::extract(insn, 28, 24) == 0b10000) {\n-    \/\/ PC-rel. addressing\n-    offset = Instruction_aarch64::extract(insn, 30, 29);\n-    offset |= Instruction_aarch64::sextract(insn, 23, 5) << 2;\n-    int shift = Instruction_aarch64::extract(insn, 31, 31) ? 12 : 0;\n-    if (shift) {\n-      offset <<= shift;\n-      uint64_t target_page = ((uint64_t)insn_addr) + offset;\n-      target_page &= ((uint64_t)-1) << shift;\n-      \/\/ Return the target address for the following sequences\n-      \/\/   1 - adrp    Rx, target_page\n-      \/\/       ldr\/str Ry, [Rx, #offset_in_page]\n-      \/\/   2 - adrp    Rx, target_page\n-      \/\/       add     Ry, Rx, #offset_in_page\n-      \/\/   3 - adrp    Rx, target_page (page aligned reloc, offset == 0)\n-      \/\/       movk    Rx, #imm12<<32\n-      \/\/   4 - adrp    Rx, target_page (page aligned reloc, offset == 0)\n-      \/\/\n-      \/\/ In the first two cases  we check that the register is the same and\n-      \/\/ return the target_page + the offset within the page.\n-      \/\/ Otherwise we assume it is a page aligned relocation and return\n-      \/\/ the target page only.\n-      \/\/\n-      unsigned insn2 = ((unsigned*)insn_addr)[1];\n-      if (Instruction_aarch64::extract(insn2, 29, 24) == 0b111001 &&\n-                Instruction_aarch64::extract(insn, 4, 0) ==\n-                        Instruction_aarch64::extract(insn2, 9, 5)) {\n-        \/\/ Load\/store register (unsigned immediate)\n-        unsigned int byte_offset = Instruction_aarch64::extract(insn2, 21, 10);\n-        unsigned int size = Instruction_aarch64::extract(insn2, 31, 30);\n-        return address(target_page + (byte_offset << size));\n-      } else if (Instruction_aarch64::extract(insn2, 31, 22) == 0b1001000100 &&\n-                Instruction_aarch64::extract(insn, 4, 0) ==\n-                        Instruction_aarch64::extract(insn2, 4, 0)) {\n-        \/\/ add (immediate)\n-        unsigned int byte_offset = Instruction_aarch64::extract(insn2, 21, 10);\n-        return address(target_page + byte_offset);\n-      } else {\n-        if (Instruction_aarch64::extract(insn2, 31, 21) == 0b11110010110  &&\n-               Instruction_aarch64::extract(insn, 4, 0) ==\n-                 Instruction_aarch64::extract(insn2, 4, 0)) {\n-          target_page = (target_page & 0xffffffff) |\n-                         ((uint64_t)Instruction_aarch64::extract(insn2, 20, 5) << 32);\n-        }\n-        return (address)target_page;\n-      }\n-    } else {\n-      ShouldNotReachHere();\n-    }\n-  } else if (Instruction_aarch64::extract(insn, 31, 23) == 0b110100101) {\n-    uint32_t *insns = (uint32_t *)insn_addr;\n-    \/\/ Move wide constant: movz, movk, movk.  See movptr().\n-    assert(nativeInstruction_at(insns+1)->is_movk(), \"wrong insns in patch\");\n-    assert(nativeInstruction_at(insns+2)->is_movk(), \"wrong insns in patch\");\n-    return address(uint64_t(Instruction_aarch64::extract(insns[0], 20, 5))\n-                   + (uint64_t(Instruction_aarch64::extract(insns[1], 20, 5)) << 16)\n-                   + (uint64_t(Instruction_aarch64::extract(insns[2], 20, 5)) << 32));\n-  } else {\n-    ShouldNotReachHere();\n-  }\n-  return address(((uint64_t)insn_addr + (offset << 2)));\n-}\n-\n@@ -296,1 +550,1 @@\n-    return 0;\n+    return nullptr;\n@@ -301,1 +555,1 @@\n-void MacroAssembler::safepoint_poll(Label& slow_path, bool at_return, bool acquire, bool in_nmethod) {\n+void MacroAssembler::safepoint_poll(Label& slow_path, bool at_return, bool acquire, bool in_nmethod, Register tmp) {\n@@ -303,2 +557,2 @@\n-    lea(rscratch1, Address(rthread, JavaThread::polling_word_offset()));\n-    ldar(rscratch1, rscratch1);\n+    lea(tmp, Address(rthread, JavaThread::polling_word_offset()));\n+    ldar(tmp, tmp);\n@@ -306,1 +560,1 @@\n-    ldr(rscratch1, Address(rthread, JavaThread::polling_word_offset()));\n+    ldr(tmp, Address(rthread, JavaThread::polling_word_offset()));\n@@ -311,1 +565,1 @@\n-    cmp(in_nmethod ? sp : rfp, rscratch1);\n+    cmp(in_nmethod ? sp : rfp, tmp);\n@@ -314,1 +568,11 @@\n-    tbnz(rscratch1, log2i_exact(SafepointMechanism::poll_bit()), slow_path);\n+    tbnz(tmp, log2i_exact(SafepointMechanism::poll_bit()), slow_path);\n+  }\n+}\n+\n+void MacroAssembler::rt_call(address dest, Register tmp) {\n+  CodeBlob *cb = CodeCache::find_blob(dest);\n+  if (cb) {\n+    far_call(RuntimeAddress(dest));\n+  } else {\n+    lea(tmp, RuntimeAddress(dest));\n+    blr(tmp);\n@@ -318,0 +582,21 @@\n+void MacroAssembler::push_cont_fastpath(Register java_thread) {\n+  if (!Continuations::enabled()) return;\n+  Label done;\n+  ldr(rscratch1, Address(java_thread, JavaThread::cont_fastpath_offset()));\n+  cmp(sp, rscratch1);\n+  br(Assembler::LS, done);\n+  mov(rscratch1, sp); \/\/ we can't use sp as the source in str\n+  str(rscratch1, Address(java_thread, JavaThread::cont_fastpath_offset()));\n+  bind(done);\n+}\n+\n+void MacroAssembler::pop_cont_fastpath(Register java_thread) {\n+  if (!Continuations::enabled()) return;\n+  Label done;\n+  ldr(rscratch1, Address(java_thread, JavaThread::cont_fastpath_offset()));\n+  cmp(sp, rscratch1);\n+  br(Assembler::LO, done);\n+  str(zr, Address(java_thread, JavaThread::cont_fastpath_offset()));\n+  bind(done);\n+}\n+\n@@ -408,0 +693,3 @@\n+  assert(entry.rspec().type() == relocInfo::external_word_type\n+         || entry.rspec().type() == relocInfo::runtime_call_type\n+         || entry.rspec().type() == relocInfo::none, \"wrong entry relocInfo type\");\n@@ -426,0 +714,3 @@\n+  assert(entry.rspec().type() == relocInfo::external_word_type\n+         || entry.rspec().type() == relocInfo::runtime_call_type\n+         || entry.rspec().type() == relocInfo::none, \"wrong entry relocInfo type\");\n@@ -565,3 +856,1 @@\n-\n-address MacroAssembler::trampoline_call(Address entry, CodeBuffer* cbuf) {\n-  assert(JavaThread::current()->is_Compiler_thread(), \"just checking\");\n+address MacroAssembler::trampoline_call1(Address entry, CodeBuffer* cbuf, bool check_emit_size) {\n@@ -573,0 +862,15 @@\n+  bool need_trampoline = far_branches();\n+  if (!need_trampoline && entry.rspec().type() == relocInfo::runtime_call_type && !CodeCache::contains(entry.target())) {\n+    \/\/ If it is a runtime call of an address outside small CodeCache,\n+    \/\/ we need to check whether it is in range.\n+    address target = entry.target();\n+    assert(target < CodeCache::low_bound() || target >= CodeCache::high_bound(), \"target is inside CodeCache\");\n+    \/\/ Case 1: -------T-------L====CodeCache====H-------\n+    \/\/                ^-------longest branch---|\n+    \/\/ Case 2: -------L====CodeCache====H-------T-------\n+    \/\/                |-------longest branch ---^\n+    address longest_branch_start = (target < CodeCache::low_bound()) ? CodeCache::high_bound() - NativeInstruction::instruction_size\n+                                                                     : CodeCache::low_bound();\n+    need_trampoline = !reachable_from_branch_at(longest_branch_start, target);\n+  }\n+\n@@ -574,1 +878,1 @@\n-  if (far_branches()) {\n+  if (need_trampoline) {\n@@ -577,6 +881,8 @@\n-    \/\/ We don't want to emit a trampoline if C2 is generating dummy\n-    \/\/ code during its branch shortening phase.\n-    CompileTask* task = ciEnv::current()->task();\n-    in_scratch_emit_size =\n-      (task != NULL && is_c2_compile(task->comp_level()) &&\n-       Compile::current()->output()->in_scratch_emit_size());\n+    if (check_emit_size) {\n+      \/\/ We don't want to emit a trampoline if C2 is generating dummy\n+      \/\/ code during its branch shortening phase.\n+      CompileTask* task = ciEnv::current()->task();\n+      in_scratch_emit_size =\n+        (task != NULL && is_c2_compile(task->comp_level()) &&\n+         Compile::current()->output()->in_scratch_emit_size());\n+    }\n@@ -595,1 +901,1 @@\n-  if (!far_branches()) {\n+  if (!need_trampoline) {\n@@ -660,1 +966,1 @@\n-  \/\/ Jump to the entry point of the i2c stub.\n+  \/\/ Jump to the entry point of the c2i stub.\n@@ -789,0 +1095,11 @@\n+void MacroAssembler::post_call_nop() {\n+  if (!Continuations::enabled()) {\n+    return;\n+  }\n+  InstructionMark im(this);\n+  relocate(post_call_nop_Relocation::spec());\n+  nop();\n+  movk(zr, 0);\n+  movk(zr, 0);\n+}\n+\n@@ -1066,1 +1383,1 @@\n-  if (super_klass != r0 || UseCompressedOops) {\n+  if (super_klass != r0) {\n@@ -1580,2 +1897,1 @@\n-  if (VM_Version::supports_stxr_prefetch())\n-    prfm(Address(counter_addr), PSTL1STRM);\n+  prfm(Address(counter_addr), PSTL1STRM);\n@@ -2163,1 +2479,1 @@\n-  tbz(r0, 0, not_weak);    \/\/ Test for jweak tag.\n+  tbz(value, 0, not_weak);    \/\/ Test for jweak tag.\n@@ -2195,0 +2511,9 @@\n+void MacroAssembler::_assert_asm(Assembler::Condition cc, const char* msg) {\n+#ifdef ASSERT\n+  Label OK;\n+  br(cc, OK);\n+  stop(msg);\n+  bind(OK);\n+#endif\n+}\n+\n@@ -2197,1 +2522,1 @@\n-void MacroAssembler::wrap_add_sub_imm_insn(Register Rd, Register Rn, unsigned imm,\n+void MacroAssembler::wrap_add_sub_imm_insn(Register Rd, Register Rn, uint64_t imm,\n@@ -2199,1 +2524,2 @@\n-                                           add_sub_reg_insn insn2) {\n+                                           add_sub_reg_insn insn2,\n+                                           bool is32) {\n@@ -2201,1 +2527,2 @@\n-  if (operand_valid_for_add_sub_immediate((int)imm)) {\n+  bool fits = operand_valid_for_add_sub_immediate(is32 ? (int32_t)imm : imm);\n+  if (fits) {\n@@ -2209,1 +2536,1 @@\n-       mov(Rd, (uint64_t)imm);\n+       mov(Rd, imm);\n@@ -2217,4 +2544,6 @@\n-void MacroAssembler::wrap_adds_subs_imm_insn(Register Rd, Register Rn, unsigned imm,\n-                                           add_sub_imm_insn insn1,\n-                                           add_sub_reg_insn insn2) {\n-  if (operand_valid_for_add_sub_immediate((int)imm)) {\n+void MacroAssembler::wrap_adds_subs_imm_insn(Register Rd, Register Rn, uint64_t imm,\n+                                             add_sub_imm_insn insn1,\n+                                             add_sub_reg_insn insn2,\n+                                             bool is32) {\n+  bool fits = operand_valid_for_add_sub_immediate(is32 ? (int32_t)imm : imm);\n+  if (fits) {\n@@ -2225,1 +2554,1 @@\n-    mov(Rd, (uint64_t)imm);\n+    mov(Rd, imm);\n@@ -2302,2 +2631,1 @@\n-    if (VM_Version::supports_stxr_prefetch())\n-      prfm(Address(addr), PSTL1STRM);\n+    prfm(Address(addr), PSTL1STRM);\n@@ -2345,2 +2673,1 @@\n-    if (VM_Version::supports_stxr_prefetch())\n-      prfm(Address(addr), PSTL1STRM);\n+    prfm(Address(addr), PSTL1STRM);\n@@ -2387,2 +2714,1 @@\n-    if (VM_Version::supports_stxr_prefetch())\n-      prfm(Address(addr), PSTL1STRM);\n+    prfm(Address(addr), PSTL1STRM);\n@@ -2446,2 +2772,1 @@\n-  if (VM_Version::supports_stxr_prefetch())                             \\\n-    prfm(Address(addr), PSTL1STRM);                                     \\\n+  prfm(Address(addr), PSTL1STRM);                                       \\\n@@ -2477,2 +2802,1 @@\n-  if (VM_Version::supports_stxr_prefetch())                             \\\n-    prfm(Address(addr), PSTL1STRM);                                     \\\n+  prfm(Address(addr), PSTL1STRM);                                       \\\n@@ -3283,1 +3607,1 @@\n-    cmn(len, 32);\n+    cmn(len, (u1)32);\n@@ -3346,1 +3670,1 @@\n-    cmn(len, 128);\n+    cmn(len, (u1)128);\n@@ -3363,1 +3687,0 @@\n-  uint64_t offset;\n@@ -3372,2 +3695,5 @@\n-    adrp(table0, ExternalAddress(StubRoutines::crc_table_addr()), offset);\n-    if (offset) add(table0, table0, offset);\n+    {\n+      uint64_t offset;\n+      adrp(table0, ExternalAddress(StubRoutines::crc_table_addr()), offset);\n+      add(table0, table0, offset);\n+    }\n@@ -3580,1 +3906,1 @@\n-    cmn(len, 32);\n+    cmn(len, (u1)32);\n@@ -3643,1 +3969,1 @@\n-    cmn(len, 128);\n+    cmn(len, (u1)128);\n@@ -4163,1 +4489,2 @@\n-  if (BarrierSet::barrier_set()->barrier_set_nmethod() != NULL || !immediate) {\n+  BarrierSet* bs = BarrierSet::barrier_set();\n+  if ((bs->barrier_set_nmethod() != NULL && bs->barrier_set_assembler()->nmethod_patching_type() == NMethodPatchingType::conc_data_patch) || !immediate) {\n@@ -4206,10 +4533,0 @@\n-\/\/ Defines obj, preserves var_size_in_bytes\n-void MacroAssembler::eden_allocate(Register obj,\n-                                   Register var_size_in_bytes,\n-                                   int con_size_in_bytes,\n-                                   Register t1,\n-                                   Label& slow_case) {\n-  BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-  bs->eden_allocate(this, obj, var_size_in_bytes, con_size_in_bytes, t1, slow_case);\n-}\n-\n@@ -4777,1 +5094,0 @@\n-        assert(false, \"failed to allocate space for trampoline\");\n@@ -4811,1 +5127,1 @@\n-void MacroAssembler::zero_words(Register base, uint64_t cnt)\n+address MacroAssembler::zero_words(Register base, uint64_t cnt)\n@@ -4813,1 +5129,1 @@\n-  guarantee(zero_words_block_size < BlockZeroingLowLimit,\n+  assert(wordSize <= BlockZeroingLowLimit,\n@@ -4815,0 +5131,1 @@\n+  address result = nullptr;\n@@ -4848,0 +5165,1 @@\n+    result = pc();\n@@ -4850,1 +5168,1 @@\n-    zero_words(r10, r11);\n+    result = zero_words(r10, r11);\n@@ -4852,0 +5170,1 @@\n+  return result;\n@@ -5297,1 +5616,1 @@\n-void MacroAssembler::verify_sve_vector_length() {\n+void MacroAssembler::verify_sve_vector_length(Register tmp) {\n@@ -5301,3 +5620,3 @@\n-  movw(rscratch1, zr);\n-  sve_inc(rscratch1, B);\n-  subsw(zr, rscratch1, VM_Version::get_initial_sve_vector_length());\n+  movw(tmp, zr);\n+  sve_inc(tmp, B);\n+  subsw(zr, tmp, VM_Version::get_initial_sve_vector_length());\n@@ -5463,0 +5782,174 @@\n+\n+\/\/ The java_calling_convention describes stack locations as ideal slots on\n+\/\/ a frame with no abi restrictions. Since we must observe abi restrictions\n+\/\/ (like the placement of the register window) the slots must be biased by\n+\/\/ the following value.\n+static int reg2offset_in(VMReg r) {\n+  \/\/ Account for saved rfp and lr\n+  \/\/ This should really be in_preserve_stack_slots\n+  return (r->reg2stack() + 4) * VMRegImpl::stack_slot_size;\n+}\n+\n+static int reg2offset_out(VMReg r) {\n+  return (r->reg2stack() + SharedRuntime::out_preserve_stack_slots()) * VMRegImpl::stack_slot_size;\n+}\n+\n+\/\/ On 64bit we will store integer like items to the stack as\n+\/\/ 64bits items (AArch64 ABI) even though java would only store\n+\/\/ 32bits for a parameter. On 32bit it will simply be 32bits\n+\/\/ So this routine will do 32->32 on 32bit and 32->64 on 64bit\n+void MacroAssembler::move32_64(VMRegPair src, VMRegPair dst, Register tmp) {\n+  if (src.first()->is_stack()) {\n+    if (dst.first()->is_stack()) {\n+      \/\/ stack to stack\n+      ldr(tmp, Address(rfp, reg2offset_in(src.first())));\n+      str(tmp, Address(sp, reg2offset_out(dst.first())));\n+    } else {\n+      \/\/ stack to reg\n+      ldrsw(dst.first()->as_Register(), Address(rfp, reg2offset_in(src.first())));\n+    }\n+  } else if (dst.first()->is_stack()) {\n+    \/\/ reg to stack\n+    str(src.first()->as_Register(), Address(sp, reg2offset_out(dst.first())));\n+  } else {\n+    if (dst.first() != src.first()) {\n+      sxtw(dst.first()->as_Register(), src.first()->as_Register());\n+    }\n+  }\n+}\n+\n+\/\/ An oop arg. Must pass a handle not the oop itself\n+void MacroAssembler::object_move(\n+                        OopMap* map,\n+                        int oop_handle_offset,\n+                        int framesize_in_slots,\n+                        VMRegPair src,\n+                        VMRegPair dst,\n+                        bool is_receiver,\n+                        int* receiver_offset) {\n+\n+  \/\/ must pass a handle. First figure out the location we use as a handle\n+\n+  Register rHandle = dst.first()->is_stack() ? rscratch2 : dst.first()->as_Register();\n+\n+  \/\/ See if oop is NULL if it is we need no handle\n+\n+  if (src.first()->is_stack()) {\n+\n+    \/\/ Oop is already on the stack as an argument\n+    int offset_in_older_frame = src.first()->reg2stack() + SharedRuntime::out_preserve_stack_slots();\n+    map->set_oop(VMRegImpl::stack2reg(offset_in_older_frame + framesize_in_slots));\n+    if (is_receiver) {\n+      *receiver_offset = (offset_in_older_frame + framesize_in_slots) * VMRegImpl::stack_slot_size;\n+    }\n+\n+    ldr(rscratch1, Address(rfp, reg2offset_in(src.first())));\n+    lea(rHandle, Address(rfp, reg2offset_in(src.first())));\n+    \/\/ conditionally move a NULL\n+    cmp(rscratch1, zr);\n+    csel(rHandle, zr, rHandle, Assembler::EQ);\n+  } else {\n+\n+    \/\/ Oop is in an a register we must store it to the space we reserve\n+    \/\/ on the stack for oop_handles and pass a handle if oop is non-NULL\n+\n+    const Register rOop = src.first()->as_Register();\n+    int oop_slot;\n+    if (rOop == j_rarg0)\n+      oop_slot = 0;\n+    else if (rOop == j_rarg1)\n+      oop_slot = 1;\n+    else if (rOop == j_rarg2)\n+      oop_slot = 2;\n+    else if (rOop == j_rarg3)\n+      oop_slot = 3;\n+    else if (rOop == j_rarg4)\n+      oop_slot = 4;\n+    else if (rOop == j_rarg5)\n+      oop_slot = 5;\n+    else if (rOop == j_rarg6)\n+      oop_slot = 6;\n+    else {\n+      assert(rOop == j_rarg7, \"wrong register\");\n+      oop_slot = 7;\n+    }\n+\n+    oop_slot = oop_slot * VMRegImpl::slots_per_word + oop_handle_offset;\n+    int offset = oop_slot*VMRegImpl::stack_slot_size;\n+\n+    map->set_oop(VMRegImpl::stack2reg(oop_slot));\n+    \/\/ Store oop in handle area, may be NULL\n+    str(rOop, Address(sp, offset));\n+    if (is_receiver) {\n+      *receiver_offset = offset;\n+    }\n+\n+    cmp(rOop, zr);\n+    lea(rHandle, Address(sp, offset));\n+    \/\/ conditionally move a NULL\n+    csel(rHandle, zr, rHandle, Assembler::EQ);\n+  }\n+\n+  \/\/ If arg is on the stack then place it otherwise it is already in correct reg.\n+  if (dst.first()->is_stack()) {\n+    str(rHandle, Address(sp, reg2offset_out(dst.first())));\n+  }\n+}\n+\n+\/\/ A float arg may have to do float reg int reg conversion\n+void MacroAssembler::float_move(VMRegPair src, VMRegPair dst, Register tmp) {\n+ if (src.first()->is_stack()) {\n+    if (dst.first()->is_stack()) {\n+      ldrw(tmp, Address(rfp, reg2offset_in(src.first())));\n+      strw(tmp, Address(sp, reg2offset_out(dst.first())));\n+    } else {\n+      ldrs(dst.first()->as_FloatRegister(), Address(rfp, reg2offset_in(src.first())));\n+    }\n+  } else if (src.first() != dst.first()) {\n+    if (src.is_single_phys_reg() && dst.is_single_phys_reg())\n+      fmovs(dst.first()->as_FloatRegister(), src.first()->as_FloatRegister());\n+    else\n+      strs(src.first()->as_FloatRegister(), Address(sp, reg2offset_out(dst.first())));\n+  }\n+}\n+\n+\/\/ A long move\n+void MacroAssembler::long_move(VMRegPair src, VMRegPair dst, Register tmp) {\n+  if (src.first()->is_stack()) {\n+    if (dst.first()->is_stack()) {\n+      \/\/ stack to stack\n+      ldr(tmp, Address(rfp, reg2offset_in(src.first())));\n+      str(tmp, Address(sp, reg2offset_out(dst.first())));\n+    } else {\n+      \/\/ stack to reg\n+      ldr(dst.first()->as_Register(), Address(rfp, reg2offset_in(src.first())));\n+    }\n+  } else if (dst.first()->is_stack()) {\n+    \/\/ reg to stack\n+    \/\/ Do we really have to sign extend???\n+    \/\/ __ movslq(src.first()->as_Register(), src.first()->as_Register());\n+    str(src.first()->as_Register(), Address(sp, reg2offset_out(dst.first())));\n+  } else {\n+    if (dst.first() != src.first()) {\n+      mov(dst.first()->as_Register(), src.first()->as_Register());\n+    }\n+  }\n+}\n+\n+\n+\/\/ A double move\n+void MacroAssembler::double_move(VMRegPair src, VMRegPair dst, Register tmp) {\n+ if (src.first()->is_stack()) {\n+    if (dst.first()->is_stack()) {\n+      ldr(tmp, Address(rfp, reg2offset_in(src.first())));\n+      str(tmp, Address(sp, reg2offset_out(dst.first())));\n+    } else {\n+      ldrd(dst.first()->as_FloatRegister(), Address(rfp, reg2offset_in(src.first())));\n+    }\n+  } else if (src.first() != dst.first()) {\n+    if (src.is_single_phys_reg() && dst.is_single_phys_reg())\n+      fmovd(dst.first()->as_FloatRegister(), src.first()->as_FloatRegister());\n+    else\n+      strd(src.first()->as_FloatRegister(), Address(sp, reg2offset_out(dst.first())));\n+  }\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":731,"deletions":238,"binary":false,"changes":969,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"code\/vmreg.hpp\"\n@@ -35,0 +36,2 @@\n+class OopMap;\n+\n@@ -118,1 +121,2 @@\n-  void safepoint_poll(Label& slow_path, bool at_return, bool acquire, bool in_nmethod);\n+  void safepoint_poll(Label& slow_path, bool at_return, bool acquire, bool in_nmethod, Register tmp = rscratch1);\n+  void rt_call(address dest, Register tmp = rscratch1);\n@@ -207,2 +211,5 @@\n-  inline void cmnw(Register Rd, unsigned imm) { addsw(zr, Rd, imm); }\n-  inline void cmn(Register Rd, unsigned imm) { adds(zr, Rd, imm); }\n+  template<class T>\n+  inline void cmnw(Register Rd, T imm) { addsw(zr, Rd, imm); }\n+\n+  inline void cmn(Register Rd, unsigned char imm8)  { adds(zr, Rd, imm8); }\n+  inline void cmn(Register Rd, unsigned imm) = delete;\n@@ -226,1 +233,1 @@\n-      addw(Rd, Rn, 0U);\n+      Assembler::addw(Rd, Rn, 0U);\n@@ -235,1 +242,1 @@\n-      add(Rd, Rn, 0U);\n+      Assembler::add(Rd, Rn, 0U);\n@@ -701,0 +708,3 @@\n+  \/\/ nop\n+  void post_call_nop();\n+\n@@ -717,0 +727,14 @@\n+  \/\/ support for argument shuffling\n+  void move32_64(VMRegPair src, VMRegPair dst, Register tmp = rscratch1);\n+  void float_move(VMRegPair src, VMRegPair dst, Register tmp = rscratch1);\n+  void long_move(VMRegPair src, VMRegPair dst, Register tmp = rscratch1);\n+  void double_move(VMRegPair src, VMRegPair dst, Register tmp = rscratch1);\n+  void object_move(\n+                   OopMap* map,\n+                   int oop_handle_offset,\n+                   int framesize_in_slots,\n+                   VMRegPair src,\n+                   VMRegPair dst,\n+                   bool is_receiver,\n+                   int* receiver_offset);\n+\n@@ -886,0 +910,3 @@\n+  void push_cont_fastpath(Register java_thread);\n+  void pop_cont_fastpath(Register java_thread);\n+\n@@ -894,7 +921,0 @@\n-  void eden_allocate(\n-    Register obj,                      \/\/ result: pointer to object after successful allocation\n-    Register var_size_in_bytes,        \/\/ object size in bytes if unknown at compile time; invalid otherwise\n-    int      con_size_in_bytes,        \/\/ object size in bytes if   known at compile time\n-    Register t1,                       \/\/ temp register\n-    Label&   slow_case                 \/\/ continuation point if fast allocation fails\n-  );\n@@ -967,1 +987,1 @@\n-  void verify_sve_vector_length();\n+  void verify_sve_vector_length(Register tmp = rscratch1);\n@@ -1016,0 +1036,4 @@\n+  void _assert_asm(Condition cc, const char* msg);\n+#define assert_asm0(cc, msg) _assert_asm(cc, FILE_AND_LINE \": \" msg)\n+#define assert_asm(masm, command, cc, msg) DEBUG_ONLY(command; (masm)->_assert_asm(cc, FILE_AND_LINE \": \" #command \" \" #cc \": \" msg))\n+\n@@ -1094,3 +1118,71 @@\n-  \/\/ Calls\n-\n-  address trampoline_call(Address entry, CodeBuffer* cbuf = NULL);\n+  \/\/ AArch64 OpenJDK uses four different types of calls:\n+  \/\/   - direct call: bl pc_relative_offset\n+  \/\/     This is the shortest and the fastest, but the offset has the range:\n+  \/\/     +\/-128MB for the release build, +\/-2MB for the debug build.\n+  \/\/\n+  \/\/   - far call: adrp reg, pc_relative_offset; add; bl reg\n+  \/\/     This is longer than a direct call. The offset has\n+  \/\/     the range +\/-4GB. As the code cache size is limited to 4GB,\n+  \/\/     far calls can reach anywhere in the code cache. If a jump is\n+  \/\/     needed rather than a call, a far jump 'b reg' can be used instead.\n+  \/\/     All instructions are embedded at a call site.\n+  \/\/\n+  \/\/   - trampoline call:\n+  \/\/     This is only available in C1\/C2-generated code (nmethod). It is a combination\n+  \/\/     of a direct call, which is used if the destination of a call is in range,\n+  \/\/     and a register-indirect call. It has the advantages of reaching anywhere in\n+  \/\/     the AArch64 address space and being patchable at runtime when the generated\n+  \/\/     code is being executed by other threads.\n+  \/\/\n+  \/\/     [Main code section]\n+  \/\/       bl trampoline\n+  \/\/     [Stub code section]\n+  \/\/     trampoline:\n+  \/\/       ldr reg, pc + 8\n+  \/\/       br reg\n+  \/\/       <64-bit destination address>\n+  \/\/\n+  \/\/     If the destination is in range when the generated code is moved to the code\n+  \/\/     cache, 'bl trampoline' is replaced with 'bl destination' and the trampoline\n+  \/\/     is not used.\n+  \/\/     The optimization does not remove the trampoline from the stub section.\n+  \/\/     This is necessary because the trampoline may well be redirected later when\n+  \/\/     code is patched, and the new destination may not be reachable by a simple BR\n+  \/\/     instruction.\n+  \/\/\n+  \/\/   - indirect call: move reg, address; blr reg\n+  \/\/     This too can reach anywhere in the address space, but it cannot be\n+  \/\/     patched while code is running, so it must only be modified at a safepoint.\n+  \/\/     This form of call is most suitable for targets at fixed addresses, which\n+  \/\/     will never be patched.\n+  \/\/\n+  \/\/ The patching we do conforms to the \"Concurrent modification and\n+  \/\/ execution of instructions\" section of the Arm Architectural\n+  \/\/ Reference Manual, which only allows B, BL, BRK, HVC, ISB, NOP, SMC,\n+  \/\/ or SVC instructions to be modified while another thread is\n+  \/\/ executing them.\n+  \/\/\n+  \/\/ To patch a trampoline call when the BL can't reach, we first modify\n+  \/\/ the 64-bit destination address in the trampoline, then modify the\n+  \/\/ BL to point to the trampoline, then flush the instruction cache to\n+  \/\/ broadcast the change to all executing threads. See\n+  \/\/ NativeCall::set_destination_mt_safe for the details.\n+  \/\/\n+  \/\/ There is a benign race in that the other thread might observe the\n+  \/\/ modified BL before it observes the modified 64-bit destination\n+  \/\/ address. That does not matter because the destination method has been\n+  \/\/ invalidated, so there will be a trap at its start.\n+  \/\/ For this to work, the destination address in the trampoline is\n+  \/\/ always updated, even if we're not using the trampoline.\n+\n+  \/\/ Emit a direct call if the entry address will always be in range,\n+  \/\/ otherwise a trampoline call.\n+  \/\/ Supported entry.rspec():\n+  \/\/ - relocInfo::runtime_call_type\n+  \/\/ - relocInfo::opt_virtual_call_type\n+  \/\/ - relocInfo::static_call_type\n+  \/\/ - relocInfo::virtual_call_type\n+  \/\/\n+  \/\/ Return: NULL if CodeCache is full.\n+  address trampoline_call(Address entry, CodeBuffer* cbuf = NULL) { return trampoline_call1(entry, cbuf, true); }\n+  address trampoline_call1(Address entry, CodeBuffer* cbuf, bool check_emit_size = true);\n@@ -1107,2 +1199,11 @@\n-  \/\/ Jumps that can reach anywhere in the code cache.\n-  \/\/ Trashes tmp.\n+  \/\/ Emit a direct call\/jump if the entry address will always be in range,\n+  \/\/ otherwise a far call\/jump.\n+  \/\/ The address must be inside the code cache.\n+  \/\/ Supported entry.rspec():\n+  \/\/ - relocInfo::external_word_type\n+  \/\/ - relocInfo::runtime_call_type\n+  \/\/ - relocInfo::none\n+  \/\/ In the case of a far call\/jump, the entry address is put in the tmp register.\n+  \/\/ The tmp register is invalidated.\n+  \/\/\n+  \/\/ Far_jump returns the amount of the emitted code.\n@@ -1156,1 +1257,1 @@\n-  void wrap_add_sub_imm_insn(Register Rd, Register Rn, unsigned imm,\n+  void wrap_add_sub_imm_insn(Register Rd, Register Rn, uint64_t imm,\n@@ -1158,1 +1259,1 @@\n-                             add_sub_reg_insn insn2);\n+                             add_sub_reg_insn insn2, bool is32);\n@@ -1160,3 +1261,3 @@\n-  void wrap_adds_subs_imm_insn(Register Rd, Register Rn, unsigned imm,\n-                             add_sub_imm_insn insn1,\n-                             add_sub_reg_insn insn2);\n+  void wrap_adds_subs_imm_insn(Register Rd, Register Rn, uint64_t imm,\n+                               add_sub_imm_insn insn1,\n+                               add_sub_reg_insn insn2, bool is32);\n@@ -1164,3 +1265,3 @@\n-#define WRAP(INSN)                                                      \\\n-  void INSN(Register Rd, Register Rn, unsigned imm) {                   \\\n-    wrap_add_sub_imm_insn(Rd, Rn, imm, &Assembler::INSN, &Assembler::INSN); \\\n+#define WRAP(INSN, is32)                                                \\\n+  void INSN(Register Rd, Register Rn, uint64_t imm) {                   \\\n+    wrap_add_sub_imm_insn(Rd, Rn, imm, &Assembler::INSN, &Assembler::INSN, is32); \\\n@@ -1183,1 +1284,1 @@\n-  WRAP(add) WRAP(addw) WRAP(sub) WRAP(subw)\n+  WRAP(add, false) WRAP(addw, true) WRAP(sub, false) WRAP(subw, true)\n@@ -1186,3 +1287,3 @@\n-#define WRAP(INSN)                                                      \\\n-  void INSN(Register Rd, Register Rn, unsigned imm) {                   \\\n-    wrap_adds_subs_imm_insn(Rd, Rn, imm, &Assembler::INSN, &Assembler::INSN); \\\n+#define WRAP(INSN, is32)                                                \\\n+  void INSN(Register Rd, Register Rn, uint64_t imm) {                   \\\n+    wrap_adds_subs_imm_insn(Rd, Rn, imm, &Assembler::INSN, &Assembler::INSN, is32); \\\n@@ -1205,1 +1306,1 @@\n-  WRAP(adds) WRAP(addsw) WRAP(subs) WRAP(subsw)\n+  WRAP(adds, false) WRAP(addsw, true) WRAP(subs, false) WRAP(subsw, true)\n@@ -1278,1 +1379,1 @@\n-  void zero_words(Register base, uint64_t cnt);\n+  address zero_words(Register base, uint64_t cnt);\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":133,"deletions":32,"binary":false,"changes":165,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -188,1 +188,1 @@\n-  \/\/ r13: sender SP (must preserve; see prepare_to_jump_from_interpreted)\n+  \/\/ r19_sender_sp: sender SP (must preserve; see prepare_to_jump_from_interpreted)\n@@ -261,0 +261,15 @@\n+void MethodHandles::jump_to_native_invoker(MacroAssembler* _masm, Register nep_reg, Register temp_target) {\n+  BLOCK_COMMENT(\"jump_to_native_invoker {\");\n+  assert_different_registers(nep_reg, temp_target);\n+  assert(nep_reg != noreg, \"required register\");\n+\n+  \/\/ Load the invoker, as NEP -> .invoker\n+  __ verify_oop(nep_reg);\n+  __ access_load_at(T_ADDRESS, IN_HEAP, temp_target,\n+                    Address(nep_reg, NONZERO(jdk_internal_foreign_abi_NativeEntryPoint::downcall_stub_address_offset_in_bytes())),\n+                    noreg, noreg);\n+\n+  __ br(temp_target);\n+  BLOCK_COMMENT(\"} jump_to_native_invoker\");\n+}\n+\n@@ -271,1 +286,1 @@\n-  Register temp3 = r14;  \/\/ r13 is live by this point: it contains the sender SP\n+  Register temp3 = r14;\n@@ -273,1 +288,1 @@\n-    assert(receiver_reg == (iid == vmIntrinsics::_linkToStatic ? noreg : j_rarg0), \"only valid assignment\");\n+    assert(receiver_reg == (iid == vmIntrinsics::_linkToStatic || iid == vmIntrinsics::_linkToNative ? noreg : j_rarg0), \"only valid assignment\");\n@@ -282,4 +297,1 @@\n-  if (iid == vmIntrinsics::_invokeBasic || iid == vmIntrinsics::_linkToNative) {\n-    if (iid == vmIntrinsics::_linkToNative) {\n-      assert(for_compiler_entry, \"only compiler entry is supported\");\n-    }\n+  if (iid == vmIntrinsics::_invokeBasic) {\n@@ -289,0 +301,3 @@\n+  } else if (iid == vmIntrinsics::_linkToNative) {\n+    assert(for_compiler_entry, \"only compiler entry is supported\");\n+    jump_to_native_invoker(_masm, member_reg, temp1);\n@@ -344,1 +359,1 @@\n-    \/\/  r13 - interpreter linkage (if interpreted)  ??? FIXME\n+    \/\/  r19 - interpreter linkage (if interpreted)\n@@ -431,1 +446,1 @@\n-    \/\/ live at this point:  rmethod, r13 (if interpreted)\n+    \/\/ live at this point:  rmethod, r19_sender_sp (if interpreted)\n","filename":"src\/hotspot\/cpu\/aarch64\/methodHandles_aarch64.cpp","additions":25,"deletions":10,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -45,0 +45,2 @@\n+#include \"runtime\/continuation.hpp\"\n+#include \"runtime\/continuationEntry.inline.hpp\"\n@@ -47,0 +49,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -50,1 +53,1 @@\n-#include \"runtime\/thread.inline.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -76,0 +79,4 @@\n+OopMap* continuation_enter_setup(MacroAssembler* masm, int& stack_slots);\n+void fill_continuation_entry(MacroAssembler* masm);\n+void continuation_enter_cleanup(MacroAssembler* masm);\n+\n@@ -294,1 +301,1 @@\n-    \/\/      r13: sender sp\n+    \/\/      r19_sender_sp: sender sp\n@@ -296,1 +303,1 @@\n-    __ mov(r13, sp);\n+    __ mov(r19_sender_sp, sp);\n@@ -352,0 +359,2 @@\n+    __ pop_cont_fastpath(rthread);\n+\n@@ -5149,1 +5158,1 @@\n-    address generate_method_entry_barrier() {\n+  address generate_method_entry_barrier() {\n@@ -5157,0 +5166,14 @@\n+    BarrierSetAssembler* bs_asm = BarrierSet::barrier_set()->barrier_set_assembler();\n+\n+    if (bs_asm->nmethod_patching_type() == NMethodPatchingType::conc_instruction_and_data_patch) {\n+      BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n+      \/\/ We can get here despite the nmethod being good, if we have not\n+      \/\/ yet applied our cross modification fence (or data fence).\n+      Address thread_epoch_addr(rthread, in_bytes(bs_nm->thread_disarmed_offset()) + 4);\n+      __ lea(rscratch2, ExternalAddress(bs_asm->patching_epoch_addr()));\n+      __ ldrw(rscratch2, rscratch2);\n+      __ strw(rscratch2, thread_epoch_addr);\n+      __ isb();\n+      __ membar(__ LoadLoad);\n+    }\n+\n@@ -5314,0 +5337,112 @@\n+  enum string_compare_mode {\n+    LL,\n+    LU,\n+    UL,\n+    UU,\n+  };\n+\n+  \/\/ The following registers are declared in aarch64.ad\n+  \/\/ r0  = result\n+  \/\/ r1  = str1\n+  \/\/ r2  = cnt1\n+  \/\/ r3  = str2\n+  \/\/ r4  = cnt2\n+  \/\/ r10 = tmp1\n+  \/\/ r11 = tmp2\n+  \/\/ z0  = ztmp1\n+  \/\/ z1  = ztmp2\n+  \/\/ p0  = pgtmp1\n+  \/\/ p1  = pgtmp2\n+  address generate_compare_long_string_sve(string_compare_mode mode) {\n+    __ align(CodeEntryAlignment);\n+    address entry = __ pc();\n+    Register result = r0, str1 = r1, cnt1 = r2, str2 = r3, cnt2 = r4,\n+             tmp1 = r10, tmp2 = r11;\n+\n+    Label LOOP, DONE, MISMATCH;\n+    Register vec_len = tmp1;\n+    Register idx = tmp2;\n+    \/\/ The minimum of the string lengths has been stored in cnt2.\n+    Register cnt = cnt2;\n+    FloatRegister ztmp1 = z0, ztmp2 = z1;\n+    PRegister pgtmp1 = p0, pgtmp2 = p1;\n+\n+#define LOAD_PAIR(ztmp1, ztmp2, pgtmp1, src1, src2, idx)                       \\\n+    switch (mode) {                                                            \\\n+      case LL:                                                                 \\\n+        __ sve_ld1b(ztmp1, __ B, pgtmp1, Address(str1, idx));                  \\\n+        __ sve_ld1b(ztmp2, __ B, pgtmp1, Address(str2, idx));                  \\\n+        break;                                                                 \\\n+      case LU:                                                                 \\\n+        __ sve_ld1b(ztmp1, __ H, pgtmp1, Address(str1, idx));                  \\\n+        __ sve_ld1h(ztmp2, __ H, pgtmp1, Address(str2, idx, Address::lsl(1))); \\\n+        break;                                                                 \\\n+      case UL:                                                                 \\\n+        __ sve_ld1h(ztmp1, __ H, pgtmp1, Address(str1, idx, Address::lsl(1))); \\\n+        __ sve_ld1b(ztmp2, __ H, pgtmp1, Address(str2, idx));                  \\\n+        break;                                                                 \\\n+      case UU:                                                                 \\\n+        __ sve_ld1h(ztmp1, __ H, pgtmp1, Address(str1, idx, Address::lsl(1))); \\\n+        __ sve_ld1h(ztmp2, __ H, pgtmp1, Address(str2, idx, Address::lsl(1))); \\\n+        break;                                                                 \\\n+      default:                                                                 \\\n+        ShouldNotReachHere();                                                  \\\n+    }\n+\n+    const char* stubname;\n+    switch (mode) {\n+      case LL: stubname = \"compare_long_string_same_encoding LL\";      break;\n+      case LU: stubname = \"compare_long_string_different_encoding LU\"; break;\n+      case UL: stubname = \"compare_long_string_different_encoding UL\"; break;\n+      case UU: stubname = \"compare_long_string_same_encoding UU\";      break;\n+      default: ShouldNotReachHere();\n+    }\n+\n+    StubCodeMark mark(this, \"StubRoutines\", stubname);\n+\n+    __ mov(idx, 0);\n+    __ sve_whilelt(pgtmp1, mode == LL ? __ B : __ H, idx, cnt);\n+\n+    if (mode == LL) {\n+      __ sve_cntb(vec_len);\n+    } else {\n+      __ sve_cnth(vec_len);\n+    }\n+\n+    __ sub(rscratch1, cnt, vec_len);\n+\n+    __ bind(LOOP);\n+\n+      \/\/ main loop\n+      LOAD_PAIR(ztmp1, ztmp2, pgtmp1, src1, src2, idx);\n+      __ add(idx, idx, vec_len);\n+      \/\/ Compare strings.\n+      __ sve_cmp(Assembler::NE, pgtmp2, mode == LL ? __ B : __ H, pgtmp1, ztmp1, ztmp2);\n+      __ br(__ NE, MISMATCH);\n+      __ cmp(idx, rscratch1);\n+      __ br(__ LT, LOOP);\n+\n+    \/\/ post loop, last iteration\n+    __ sve_whilelt(pgtmp1, mode == LL ? __ B : __ H, idx, cnt);\n+\n+    LOAD_PAIR(ztmp1, ztmp2, pgtmp1, src1, src2, idx);\n+    __ sve_cmp(Assembler::NE, pgtmp2, mode == LL ? __ B : __ H, pgtmp1, ztmp1, ztmp2);\n+    __ br(__ EQ, DONE);\n+\n+    __ bind(MISMATCH);\n+\n+    \/\/ Crop the vector to find its location.\n+    __ sve_brkb(pgtmp2, pgtmp1, pgtmp2, false \/* isMerge *\/);\n+    \/\/ Extract the first different characters of each string.\n+    __ sve_lasta(rscratch1, mode == LL ? __ B : __ H, pgtmp2, ztmp1);\n+    __ sve_lasta(rscratch2, mode == LL ? __ B : __ H, pgtmp2, ztmp2);\n+\n+    \/\/ Compute the difference of the first different characters.\n+    __ sub(result, rscratch1, rscratch2);\n+\n+    __ bind(DONE);\n+    __ ret(lr);\n+#undef LOAD_PAIR\n+    return entry;\n+  }\n+\n@@ -5315,0 +5450,1 @@\n+    if (UseSVE == 0) {\n@@ -5323,0 +5459,10 @@\n+    } else {\n+      StubRoutines::aarch64::_compare_long_string_LL\n+          = generate_compare_long_string_sve(LL);\n+      StubRoutines::aarch64::_compare_long_string_UU\n+          = generate_compare_long_string_sve(UU);\n+      StubRoutines::aarch64::_compare_long_string_LU\n+          = generate_compare_long_string_sve(LU);\n+      StubRoutines::aarch64::_compare_long_string_UL\n+          = generate_compare_long_string_sve(UL);\n+    }\n@@ -6281,1 +6427,1 @@\n-#ifdef LINUX\n+#if defined (LINUX) && !defined (__ARM_FEATURE_ATOMICS)\n@@ -6502,0 +6648,248 @@\n+  RuntimeStub* generate_cont_doYield() {\n+    if (!Continuations::enabled()) return nullptr;\n+\n+    const char *name = \"cont_doYield\";\n+\n+    enum layout {\n+      rfp_off1,\n+      rfp_off2,\n+      lr_off,\n+      lr_off2,\n+      framesize \/\/ inclusive of return address\n+    };\n+    \/\/ assert(is_even(framesize\/2), \"sp not 16-byte aligned\");\n+\n+    int insts_size = 512;\n+    int locs_size  = 64;\n+    CodeBuffer code(name, insts_size, locs_size);\n+    OopMapSet* oop_maps  = new OopMapSet();\n+    MacroAssembler* masm = new MacroAssembler(&code);\n+    MacroAssembler* _masm = masm;\n+\n+    address start = __ pc();\n+\n+    __ enter();\n+\n+    __ mov(c_rarg1, sp);\n+\n+    int frame_complete = __ pc() - start;\n+    address the_pc = __ pc();\n+\n+    __ post_call_nop(); \/\/ this must be exactly after the pc value that is pushed into the frame info, we use this nop for fast CodeBlob lookup\n+\n+    __ mov(c_rarg0, rthread);\n+    __ set_last_Java_frame(sp, rfp, the_pc, rscratch1);\n+    __ call_VM_leaf(Continuation::freeze_entry(), 2);\n+    __ reset_last_Java_frame(true);\n+\n+    Label pinned;\n+\n+    __ cbnz(r0, pinned);\n+\n+    \/\/ We've succeeded, set sp to the ContinuationEntry\n+    __ ldr(rscratch1, Address(rthread, JavaThread::cont_entry_offset()));\n+    __ mov(sp, rscratch1);\n+    continuation_enter_cleanup(masm);\n+\n+    __ bind(pinned); \/\/ pinned -- return to caller\n+\n+    __ leave();\n+    __ ret(lr);\n+\n+    OopMap* map = new OopMap(framesize, 1);\n+    oop_maps->add_gc_map(the_pc - start, map);\n+\n+    RuntimeStub* stub = \/\/ codeBlob framesize is in words (not VMRegImpl::slot_size)\n+    RuntimeStub::new_runtime_stub(name,\n+                                  &code,\n+                                  frame_complete,\n+                                  (framesize >> (LogBytesPerWord - LogBytesPerInt)),\n+                                  oop_maps, false);\n+    return stub;\n+  }\n+\n+  address generate_cont_thaw(Continuation::thaw_kind kind) {\n+    bool return_barrier = Continuation::is_thaw_return_barrier(kind);\n+    bool return_barrier_exception = Continuation::is_thaw_return_barrier_exception(kind);\n+\n+    address start = __ pc();\n+\n+    if (return_barrier) {\n+      __ ldr(rscratch1, Address(rthread, JavaThread::cont_entry_offset()));\n+      __ mov(sp, rscratch1);\n+    }\n+    assert_asm(_masm, (__ ldr(rscratch1, Address(rthread, JavaThread::cont_entry_offset())), __ cmp(sp, rscratch1)), Assembler::EQ, \"incorrect sp\");\n+\n+    if (return_barrier) {\n+      \/\/ preserve possible return value from a method returning to the return barrier\n+      __ fmovd(rscratch1, v0);\n+      __ stp(rscratch1, r0, Address(__ pre(sp, -2 * wordSize)));\n+    }\n+\n+    __ movw(c_rarg1, (return_barrier ? 1 : 0));\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, Continuation::prepare_thaw), rthread, c_rarg1);\n+    __ mov(rscratch2, r0); \/\/ r0 contains the size of the frames to thaw, 0 if overflow or no more frames\n+\n+    if (return_barrier) {\n+      \/\/ restore return value (no safepoint in the call to thaw, so even an oop return value should be OK)\n+      __ ldp(rscratch1, r0, Address(__ post(sp, 2 * wordSize)));\n+      __ fmovd(v0, rscratch1);\n+    }\n+    assert_asm(_masm, (__ ldr(rscratch1, Address(rthread, JavaThread::cont_entry_offset())), __ cmp(sp, rscratch1)), Assembler::EQ, \"incorrect sp\");\n+\n+\n+    Label thaw_success;\n+    \/\/ rscratch2 contains the size of the frames to thaw, 0 if overflow or no more frames\n+    __ cbnz(rscratch2, thaw_success);\n+    __ lea(rscratch1, ExternalAddress(StubRoutines::throw_StackOverflowError_entry()));\n+    __ br(rscratch1);\n+    __ bind(thaw_success);\n+\n+    \/\/ make room for the thawed frames\n+    __ sub(rscratch1, sp, rscratch2);\n+    __ andr(rscratch1, rscratch1, -16); \/\/ align\n+    __ mov(sp, rscratch1);\n+\n+    if (return_barrier) {\n+      \/\/ save original return value -- again\n+      __ fmovd(rscratch1, v0);\n+      __ stp(rscratch1, r0, Address(__ pre(sp, -2 * wordSize)));\n+    }\n+\n+    \/\/ If we want, we can templatize thaw by kind, and have three different entries\n+    __ movw(c_rarg1, (uint32_t)kind);\n+\n+    __ call_VM_leaf(Continuation::thaw_entry(), rthread, c_rarg1);\n+    __ mov(rscratch2, r0); \/\/ r0 is the sp of the yielding frame\n+\n+    if (return_barrier) {\n+      \/\/ restore return value (no safepoint in the call to thaw, so even an oop return value should be OK)\n+      __ ldp(rscratch1, r0, Address(__ post(sp, 2 * wordSize)));\n+      __ fmovd(v0, rscratch1);\n+    } else {\n+      __ mov(r0, zr); \/\/ return 0 (success) from doYield\n+    }\n+\n+    \/\/ we're now on the yield frame (which is in an address above us b\/c rsp has been pushed down)\n+    __ sub(sp, rscratch2, 2*wordSize); \/\/ now pointing to rfp spill\n+    __ mov(rfp, sp);\n+\n+    if (return_barrier_exception) {\n+      __ ldr(c_rarg1, Address(rfp, wordSize)); \/\/ return address\n+      __ verify_oop(r0);\n+      __ mov(r19, r0); \/\/ save return value contaning the exception oop in callee-saved R19\n+\n+      __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::exception_handler_for_return_address), rthread, c_rarg1);\n+\n+      \/\/ Reinitialize the ptrue predicate register, in case the external runtime call clobbers ptrue reg, as we may return to SVE compiled code.\n+      \/\/ __ reinitialize_ptrue();\n+\n+      \/\/ see OptoRuntime::generate_exception_blob: r0 -- exception oop, r3 -- exception pc\n+\n+      __ mov(r1, r0); \/\/ the exception handler\n+      __ mov(r0, r19); \/\/ restore return value contaning the exception oop\n+      __ verify_oop(r0);\n+\n+      __ leave();\n+      __ mov(r3, lr);\n+      __ br(r1); \/\/ the exception handler\n+    } else {\n+      \/\/ We're \"returning\" into the topmost thawed frame; see Thaw::push_return_frame\n+      __ leave();\n+      __ ret(lr);\n+    }\n+\n+    return start;\n+  }\n+\n+  address generate_cont_thaw() {\n+    if (!Continuations::enabled()) return nullptr;\n+\n+    StubCodeMark mark(this, \"StubRoutines\", \"Cont thaw\");\n+    address start = __ pc();\n+    generate_cont_thaw(Continuation::thaw_top);\n+    return start;\n+  }\n+\n+  address generate_cont_returnBarrier() {\n+    if (!Continuations::enabled()) return nullptr;\n+\n+    \/\/ TODO: will probably need multiple return barriers depending on return type\n+    StubCodeMark mark(this, \"StubRoutines\", \"cont return barrier\");\n+    address start = __ pc();\n+\n+    generate_cont_thaw(Continuation::thaw_return_barrier);\n+\n+    return start;\n+  }\n+\n+  address generate_cont_returnBarrier_exception() {\n+    if (!Continuations::enabled()) return nullptr;\n+\n+    StubCodeMark mark(this, \"StubRoutines\", \"cont return barrier exception handler\");\n+    address start = __ pc();\n+\n+    generate_cont_thaw(Continuation::thaw_return_barrier_exception);\n+\n+    return start;\n+  }\n+\n+#if INCLUDE_JFR\n+\n+  static void jfr_prologue(address the_pc, MacroAssembler* _masm, Register thread) {\n+    __ set_last_Java_frame(sp, rfp, the_pc, rscratch1);\n+    __ mov(c_rarg0, thread);\n+  }\n+\n+  \/\/ The handle is dereferenced through a load barrier.\n+  static void jfr_epilogue(MacroAssembler* _masm, Register thread) {\n+    __ reset_last_Java_frame(true);\n+    Label null_jobject;\n+    __ cbz(r0, null_jobject);\n+    DecoratorSet decorators = ACCESS_READ | IN_NATIVE;\n+    BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+    bs->load_at(_masm, decorators, T_OBJECT, r0, Address(r0, 0), c_rarg0, thread);\n+    __ bind(null_jobject);\n+  }\n+\n+  \/\/ For c2: c_rarg0 is junk, call to runtime to write a checkpoint.\n+  \/\/ It returns a jobject handle to the event writer.\n+  \/\/ The handle is dereferenced and the return value is the event writer oop.\n+  static RuntimeStub* generate_jfr_write_checkpoint() {\n+    enum layout {\n+      rbp_off,\n+      rbpH_off,\n+      return_off,\n+      return_off2,\n+      framesize \/\/ inclusive of return address\n+    };\n+\n+    int insts_size = 512;\n+    int locs_size = 64;\n+    CodeBuffer code(\"jfr_write_checkpoint\", insts_size, locs_size);\n+    OopMapSet* oop_maps = new OopMapSet();\n+    MacroAssembler* masm = new MacroAssembler(&code);\n+    MacroAssembler* _masm = masm;\n+\n+    address start = __ pc();\n+    __ enter();\n+    int frame_complete = __ pc() - start;\n+    address the_pc = __ pc();\n+    jfr_prologue(the_pc, _masm, rthread);\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, JfrIntrinsicSupport::write_checkpoint), 1);\n+    jfr_epilogue(_masm, rthread);\n+    __ leave();\n+    __ ret(lr);\n+\n+    OopMap* map = new OopMap(framesize, 1); \/\/ rfp\n+    oop_maps->add_gc_map(the_pc - start, map);\n+\n+    RuntimeStub* stub = \/\/ codeBlob framesize is in words (not VMRegImpl::slot_size)\n+      RuntimeStub::new_runtime_stub(\"jfr_write_checkpoint\", &code, frame_complete,\n+                                    (framesize >> (LogBytesPerWord - LogBytesPerInt)),\n+                                    oop_maps, false);\n+    return stub;\n+  }\n+\n+#endif \/\/ INCLUDE_JFR\n+\n@@ -6557,1 +6951,1 @@\n-    __ sub(sp, rfp, ((unsigned)framesize-4) << LogBytesPerInt); \/\/ prolog\n+    __ sub(sp, rfp, ((uint64_t)framesize-4) << LogBytesPerInt); \/\/ prolog\n@@ -6601,1 +6995,0 @@\n-\n@@ -7488,0 +7881,13 @@\n+  void generate_phase1() {\n+    \/\/ Continuation stubs:\n+    StubRoutines::_cont_thaw          = generate_cont_thaw();\n+    StubRoutines::_cont_returnBarrier = generate_cont_returnBarrier();\n+    StubRoutines::_cont_returnBarrierExc = generate_cont_returnBarrier_exception();\n+    StubRoutines::_cont_doYield_stub = generate_cont_doYield();\n+    StubRoutines::_cont_doYield      = StubRoutines::_cont_doYield_stub == nullptr ? nullptr\n+                                        : StubRoutines::_cont_doYield_stub->entry_point();\n+\n+    JFR_ONLY(StubRoutines::_jfr_write_checkpoint_stub = generate_jfr_write_checkpoint();)\n+    JFR_ONLY(StubRoutines::_jfr_write_checkpoint = StubRoutines::_jfr_write_checkpoint_stub->entry_point();)\n+  }\n+\n@@ -7620,1 +8026,1 @@\n-#ifdef LINUX\n+#if defined (LINUX) && !defined (__ARM_FEATURE_ATOMICS)\n@@ -7630,4 +8036,2 @@\n-  StubGenerator(CodeBuffer* code, bool all) : StubCodeGenerator(code) {\n-    if (all) {\n-      generate_all();\n-    } else {\n+  StubGenerator(CodeBuffer* code, int phase) : StubCodeGenerator(code) {\n+    if (phase == 0) {\n@@ -7635,0 +8039,4 @@\n+    } else if (phase == 1) {\n+      generate_phase1(); \/\/ stubs that must be available for the interpreter\n+    } else {\n+      generate_all();\n@@ -7640,1 +8048,1 @@\n-void StubGenerator_generate(CodeBuffer* code, bool all) {\n+void StubGenerator_generate(CodeBuffer* code, int phase) {\n@@ -7644,1 +8052,1 @@\n-  StubGenerator g(code, all);\n+  StubGenerator g(code, phase);\n@@ -7648,1 +8056,1 @@\n-#ifdef LINUX\n+#if defined (LINUX)\n@@ -7679,0 +8087,72 @@\n+\n+\n+#undef __\n+#define __ masm->\n+\n+\/\/ on exit, sp points to the ContinuationEntry\n+OopMap* continuation_enter_setup(MacroAssembler* masm, int& stack_slots) {\n+  assert(ContinuationEntry::size() % VMRegImpl::stack_slot_size == 0, \"\");\n+  assert(in_bytes(ContinuationEntry::cont_offset())  % VMRegImpl::stack_slot_size == 0, \"\");\n+  assert(in_bytes(ContinuationEntry::chunk_offset()) % VMRegImpl::stack_slot_size == 0, \"\");\n+\n+  stack_slots += (int)ContinuationEntry::size()\/wordSize;\n+  __ sub(sp, sp, (int)ContinuationEntry::size()); \/\/ place Continuation metadata\n+\n+  OopMap* map = new OopMap(((int)ContinuationEntry::size() + wordSize)\/ VMRegImpl::stack_slot_size, 0 \/* arg_slots*\/);\n+  ContinuationEntry::setup_oopmap(map);\n+\n+  __ ldr(rscratch1, Address(rthread, JavaThread::cont_entry_offset()));\n+  __ str(rscratch1, Address(sp, ContinuationEntry::parent_offset()));\n+  __ mov(rscratch1, sp); \/\/ we can't use sp as the source in str\n+  __ str(rscratch1, Address(rthread, JavaThread::cont_entry_offset()));\n+\n+  return map;\n+}\n+\n+\/\/ on entry c_rarg1 points to the continuation\n+\/\/          sp points to ContinuationEntry\n+\/\/          c_rarg3 -- isVirtualThread\n+void fill_continuation_entry(MacroAssembler* masm) {\n+#ifdef ASSERT\n+  __ movw(rscratch1, ContinuationEntry::cookie_value());\n+  __ strw(rscratch1, Address(sp, ContinuationEntry::cookie_offset()));\n+#endif\n+\n+  __ str (c_rarg1, Address(sp, ContinuationEntry::cont_offset()));\n+  __ strw(c_rarg3, Address(sp, ContinuationEntry::flags_offset()));\n+  __ str (zr,      Address(sp, ContinuationEntry::chunk_offset()));\n+  __ strw(zr,      Address(sp, ContinuationEntry::argsize_offset()));\n+  __ strw(zr,      Address(sp, ContinuationEntry::pin_count_offset()));\n+\n+  __ ldr(rscratch1, Address(rthread, JavaThread::cont_fastpath_offset()));\n+  __ str(rscratch1, Address(sp, ContinuationEntry::parent_cont_fastpath_offset()));\n+  __ ldr(rscratch1, Address(rthread, JavaThread::held_monitor_count_offset()));\n+  __ str(rscratch1, Address(sp, ContinuationEntry::parent_held_monitor_count_offset()));\n+\n+  __ str(zr, Address(rthread, JavaThread::cont_fastpath_offset()));\n+  __ str(zr, Address(rthread, JavaThread::held_monitor_count_offset()));\n+}\n+\n+\/\/ on entry, sp points to the ContinuationEntry\n+\/\/ on exit, rfp points to the spilled rfp in the entry frame\n+void continuation_enter_cleanup(MacroAssembler* masm) {\n+#ifndef PRODUCT\n+  Label OK;\n+  __ ldr(rscratch1, Address(rthread, JavaThread::cont_entry_offset()));\n+  __ cmp(sp, rscratch1);\n+  __ br(Assembler::EQ, OK);\n+  __ stop(\"incorrect sp1\");\n+  __ bind(OK);\n+#endif\n+\n+  __ ldr(rscratch1, Address(sp, ContinuationEntry::parent_cont_fastpath_offset()));\n+  __ str(rscratch1, Address(rthread, JavaThread::cont_fastpath_offset()));\n+  __ ldr(rscratch1, Address(sp, ContinuationEntry::parent_held_monitor_count_offset()));\n+  __ str(rscratch1, Address(rthread, JavaThread::held_monitor_count_offset()));\n+\n+  __ ldr(rscratch2, Address(sp, ContinuationEntry::parent_offset()));\n+  __ str(rscratch2, Address(rthread, JavaThread::cont_entry_offset()));\n+  __ add(rfp, sp, (int)ContinuationEntry::size());\n+}\n+\n+#undef __\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":495,"deletions":15,"binary":false,"changes":510,"status":"modified"},{"patch":"@@ -3471,1 +3471,0 @@\n-  Label initialize_object; \/\/ including clearing the fields\n@@ -3504,5 +3503,0 @@\n-  \/\/  Else If inline contiguous allocations are enabled:\n-  \/\/    Try to allocate in eden.\n-  \/\/    If fails due to heap end, go to slow path.\n-  \/\/\n-  \/\/  If TLAB is enabled OR inline contiguous is enabled:\n@@ -3513,2 +3507,0 @@\n-  const bool allow_shared_alloc =\n-    Universe::heap()->supports_inline_contig_alloc();\n@@ -3522,14 +3514,0 @@\n-    } else {\n-      \/\/ initialize both the header and fields\n-      __ b(initialize_object);\n-  } else {\n-    \/\/ Allocation in the shared Eden, if allowed.\n-    \/\/\n-    \/\/ r3: instance size in bytes\n-    if (allow_shared_alloc) {\n-      __ eden_allocate(r0, r3, 0, r10, slow_case);\n-    }\n-  }\n-  \/\/ If UseTLAB or allow_shared_alloc are true, the object is created above and\n-  \/\/ there is an initialize need. Otherwise, skip and go to the slow path.\n-  if (UseTLAB || allow_shared_alloc) {\n@@ -3540,1 +3518,0 @@\n-    __ bind(initialize_object);\n@@ -3774,1 +3751,1 @@\n-\/\/ [saved rbp    ] <--- rbp\n+\/\/ [saved rfp    ] <--- rfp\n@@ -3829,0 +3806,6 @@\n+\n+    __ check_extended_sp();\n+    __ sub(sp, sp, entry_size);           \/\/ make room for the monitor\n+    __ mov(rscratch1, sp);\n+    __ str(rscratch1, Address(rfp, frame::interpreter_frame_extended_sp_offset * wordSize));\n+\n@@ -3835,2 +3818,0 @@\n-    __ sub(sp, sp, entry_size);           \/\/ make room for the monitor\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/templateTable_aarch64.cpp","additions":7,"deletions":26,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -202,3 +202,0 @@\n-  \/\/ TODO: ARM\n-  __ nop(); \/\/ See comments in other ports\n-\n@@ -1484,0 +1481,3 @@\n+        case T_METADATA:\n+          __ mov_metadata(result->as_register(), c->as_metadata(), acond);\n+          break;\n@@ -2435,0 +2435,4 @@\n+    if (op->info() != NULL) {\n+      add_debug_info_for_null_check_here(op->info());\n+      __ null_check(obj);\n+    }\n","filename":"src\/hotspot\/cpu\/arm\/c1_LIRAssembler_arm.cpp","additions":7,"deletions":3,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -84,1 +84,1 @@\n-    eden_allocate(obj, obj_end, tmp1, tmp2, size_expression, slow_case);\n+    b(slow_case);\n","filename":"src\/hotspot\/cpu\/arm\/c1_MacroAssembler_arm.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2080,12 +2080,0 @@\n-\/\/ allocation (for C1)\n-void MacroAssembler::eden_allocate(\n-  Register obj,                      \/\/ result: pointer to object after successful allocation\n-  Register var_size_in_bytes,        \/\/ object size in bytes if unknown at compile time; invalid otherwise\n-  int      con_size_in_bytes,        \/\/ object size in bytes if   known at compile time\n-  Register t1,                       \/\/ temp register\n-  Register t2,                       \/\/ temp register\n-  Label&   slow_case                 \/\/ continuation point if fast allocation fails\n-) {\n-  b(slow_case);\n-}\n-\n","filename":"src\/hotspot\/cpu\/ppc\/macroAssembler_ppc.cpp","additions":0,"deletions":12,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -381,1 +381,1 @@\n-  \/\/ we make the data look like a the following add instruction:\n+  \/\/ we make the data look like the following add instruction:\n","filename":"src\/hotspot\/cpu\/s390\/c1_CodeStubs_s390.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -3,1 +3,1 @@\n- * Copyright (c) 2016, 2019 SAP SE. All rights reserved.\n+ * Copyright (c) 2016, 2022 SAP SE. All rights reserved.\n@@ -4671,0 +4671,16 @@\n+void MacroAssembler::kmctr(Register dstBuff, Register ctrBuff, Register srcBuff) {\n+  \/\/ DstBuff and srcBuff are allowed to be the same register (encryption in-place).\n+  \/\/ DstBuff and srcBuff storage must not overlap destructively, and neither must overlap the parameter block.\n+  assert(srcBuff->encoding()     != 0, \"src buffer address can't be in Z_R0\");\n+  assert(dstBuff->encoding()     != 0, \"dst buffer address can't be in Z_R0\");\n+  assert(ctrBuff->encoding()     != 0, \"ctr buffer address can't be in Z_R0\");\n+  assert(ctrBuff->encoding() % 2 == 0, \"ctr buffer addr must be an even register\");\n+  assert(dstBuff->encoding() % 2 == 0, \"dst buffer addr must be an even register\");\n+  assert(srcBuff->encoding() % 2 == 0, \"src buffer addr\/len must be an even\/odd register pair\");\n+\n+  Label retry;\n+  bind(retry);\n+  Assembler::z_kmctr(dstBuff, ctrBuff, srcBuff);\n+  Assembler::z_brc(Assembler::bcondOverflow \/* CC==3 (iterate) *\/, retry);\n+}\n+\n","filename":"src\/hotspot\/cpu\/s390\/macroAssembler_s390.cpp","additions":17,"deletions":1,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -401,7 +401,0 @@\n-  \/\/ if the last instruction is a call (typically to do a throw which\n-  \/\/ is coming at the end after block reordering) the return address\n-  \/\/ must still point into the code area in order to avoid assertion\n-  \/\/ failures when searching for the corresponding bci => add a nop\n-  \/\/ (was bug 5\/14\/1999 - gri)\n-  __ nop();\n-\n@@ -448,1 +441,1 @@\n-  NOT_LP64(__ get_thread(rsi));\n+  NOT_LP64(__ get_thread(thread));\n@@ -502,7 +495,0 @@\n-  \/\/ if the last instruction is a call (typically to do a throw which\n-  \/\/ is coming at the end after block reordering) the return address\n-  \/\/ must still point into the code area in order to avoid assertion\n-  \/\/ failures when searching for the corresponding bci => add a nop\n-  \/\/ (was bug 5\/14\/1999 - gri)\n-  __ nop();\n-\n@@ -2881,0 +2867,1 @@\n+  __ post_call_nop();\n@@ -2889,0 +2876,1 @@\n+  __ post_call_nop();\n@@ -3517,0 +3505,4 @@\n+    if (op->info() != NULL) {\n+      add_debug_info_for_null_check_here(op->info());\n+      __ null_check(obj);\n+    }\n@@ -3903,0 +3895,1 @@\n+  __ post_call_nop();\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":8,"deletions":15,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -98,0 +98,3 @@\n+\n+  inc_held_monitor_count();\n+\n@@ -101,1 +104,0 @@\n-\n@@ -129,0 +131,2 @@\n+\n+  dec_held_monitor_count();\n@@ -137,1 +141,1 @@\n-    eden_allocate(noreg, obj, var_size_in_bytes, con_size_in_bytes, t1, slow_case);\n+    jmp(slow_case);\n@@ -311,1 +315,2 @@\n-  bs->nmethod_entry_barrier(this);\n+  \/\/ C1 code is not hot enough to micro optimize the nmethod entry barrier with an out-of-line stub\n+  bs->nmethod_entry_barrier(this, NULL \/* slow_path *\/, NULL \/* continuation *\/);\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":8,"deletions":3,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -1033,55 +1033,0 @@\n-        \/\/ If TLAB is disabled, see if there is support for inlining contiguous\n-        \/\/ allocations.\n-        \/\/ Otherwise, just go to the slow path.\n-        if ((id == fast_new_instance_id || id == fast_new_instance_init_check_id) && !UseTLAB\n-            && Universe::heap()->supports_inline_contig_alloc()) {\n-          Label slow_path;\n-          Register obj_size = rcx;\n-          Register t1       = rbx;\n-          Register t2       = rsi;\n-          assert_different_registers(klass, obj, obj_size, t1, t2);\n-\n-          __ push(rdi);\n-          __ push(rbx);\n-\n-          if (id == fast_new_instance_init_check_id) {\n-            \/\/ make sure the klass is initialized\n-            __ cmpb(Address(klass, InstanceKlass::init_state_offset()), InstanceKlass::fully_initialized);\n-            __ jcc(Assembler::notEqual, slow_path);\n-          }\n-\n-#ifdef ASSERT\n-          \/\/ assert object can be fast path allocated\n-          {\n-            Label ok, not_ok;\n-            __ movl(obj_size, Address(klass, Klass::layout_helper_offset()));\n-            __ cmpl(obj_size, 0);  \/\/ make sure it's an instance (LH > 0)\n-            __ jcc(Assembler::lessEqual, not_ok);\n-            __ testl(obj_size, Klass::_lh_instance_slow_path_bit);\n-            __ jcc(Assembler::zero, ok);\n-            __ bind(not_ok);\n-            __ stop(\"assert(can be fast path allocated)\");\n-            __ should_not_reach_here();\n-            __ bind(ok);\n-          }\n-#endif \/\/ ASSERT\n-\n-          const Register thread = NOT_LP64(rdi) LP64_ONLY(r15_thread);\n-          NOT_LP64(__ get_thread(thread));\n-\n-          \/\/ get the instance size (size is positive so movl is fine for 64bit)\n-          __ movl(obj_size, Address(klass, Klass::layout_helper_offset()));\n-\n-          __ eden_allocate(thread, obj, obj_size, 0, t1, slow_path);\n-\n-          __ initialize_object(obj, klass, obj_size, 0, t1, t2, \/* is_tlab_allocated *\/ false);\n-          __ verify_oop(obj);\n-          __ pop(rbx);\n-          __ pop(rdi);\n-          __ ret(0);\n-\n-          __ bind(slow_path);\n-          __ pop(rbx);\n-          __ pop(rdi);\n-        }\n-\n@@ -1162,41 +1107,0 @@\n-        \/\/ If TLAB is disabled, see if there is support for inlining contiguous\n-        \/\/ allocations.\n-        \/\/ Otherwise, just go to the slow path.\n-        if (!UseTLAB && Universe::heap()->supports_inline_contig_alloc()) {\n-          Register arr_size = rsi;\n-          Register t1       = rcx;  \/\/ must be rcx for use as shift count\n-          Register t2       = rdi;\n-          Label slow_path;\n-\n-          \/\/ get the allocation size: round_up(hdr + length << (layout_helper & 0x1F))\n-          \/\/ since size is positive movl does right thing on 64bit\n-          __ movl(t1, Address(klass, Klass::layout_helper_offset()));\n-          \/\/ since size is positive movl does right thing on 64bit\n-          __ movl(arr_size, length);\n-          assert(t1 == rcx, \"fixed register usage\");\n-          __ shlptr(arr_size \/* by t1=rcx, mod 32 *\/);\n-          __ shrptr(t1, Klass::_lh_header_size_shift);\n-          __ andptr(t1, Klass::_lh_header_size_mask);\n-          __ addptr(arr_size, t1);\n-          __ addptr(arr_size, MinObjAlignmentInBytesMask); \/\/ align up\n-          __ andptr(arr_size, ~MinObjAlignmentInBytesMask);\n-\n-          \/\/ Using t2 for non 64-bit.\n-          const Register thread = NOT_LP64(t2) LP64_ONLY(r15_thread);\n-          NOT_LP64(__ get_thread(thread));\n-          __ eden_allocate(thread, obj, arr_size, 0, t1, slow_path);  \/\/ preserves arr_size\n-\n-          __ initialize_header(obj, klass, length, t1, t2);\n-          __ movb(t1, Address(klass, in_bytes(Klass::layout_helper_offset()) + (Klass::_lh_header_size_shift \/ BitsPerByte)));\n-          assert(Klass::_lh_header_size_shift % BitsPerByte == 0, \"bytewise\");\n-          assert(Klass::_lh_header_size_mask <= 0xFF, \"bytewise\");\n-          __ andptr(t1, Klass::_lh_header_size_mask);\n-          __ subptr(arr_size, t1);  \/\/ body length\n-          __ addptr(t1, obj);       \/\/ body start\n-          __ initialize_body(t1, arr_size, 0, t2);\n-          __ verify_oop(obj);\n-          __ ret(0);\n-\n-          __ bind(slow_path);\n-        }\n-\n","filename":"src\/hotspot\/cpu\/x86\/c1_Runtime1_x86.cpp","additions":0,"deletions":96,"binary":false,"changes":96,"status":"modified"},{"patch":"@@ -36,0 +36,2 @@\n+#define SUPPORT_MONITOR_COUNT\n+\n","filename":"src\/hotspot\/cpu\/x86\/globalDefinitions_x86.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -41,1 +42,0 @@\n-#include \"runtime\/thread.inline.hpp\"\n@@ -1171,0 +1171,1 @@\n+  pop_cont_fastpath();\n@@ -1204,1 +1205,1 @@\n-    Label done;\n+    Label count_locking, done, slow_case;\n@@ -1216,2 +1217,0 @@\n-    Label slow_case;\n-\n@@ -1242,1 +1241,1 @@\n-    jcc(Assembler::zero, done);\n+    jcc(Assembler::zero, count_locking);\n@@ -1278,1 +1277,5 @@\n-    jcc(Assembler::zero, done);\n+    jcc(Assembler::notZero, slow_case);\n+\n+    bind(count_locking);\n+    inc_held_monitor_count();\n+    jmp(done);\n@@ -1311,1 +1314,1 @@\n-    Label done;\n+    Label count_locking, done, slow_case;\n@@ -1337,1 +1340,1 @@\n-    jcc(Assembler::zero, done);\n+    jcc(Assembler::zero, count_locking);\n@@ -1344,1 +1347,1 @@\n-    jcc(Assembler::zero, done);\n+    jcc(Assembler::notZero, slow_case);\n@@ -1346,0 +1349,3 @@\n+    bind(count_locking);\n+    dec_held_monitor_count();\n+    jmp(done);\n@@ -1347,0 +1353,1 @@\n+    bind(slow_case);\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":16,"deletions":9,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -45,0 +45,1 @@\n+#include \"runtime\/continuation.hpp\"\n@@ -47,0 +48,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -54,2 +56,0 @@\n-#include \"runtime\/thread.hpp\"\n-#include \"utilities\/align.hpp\"\n@@ -509,3 +509,1 @@\n-  {\n-    call(RuntimeAddress(entry_point));\n-  }\n+  call(RuntimeAddress(entry_point));\n@@ -516,3 +514,1 @@\n-  {\n-    call(RuntimeAddress(entry_point));\n-  }\n+  call(RuntimeAddress(entry_point));\n@@ -918,1 +914,1 @@\n-void MacroAssembler::long_move(VMRegPair src, VMRegPair dst) {\n+void MacroAssembler::long_move(VMRegPair src, VMRegPair dst, Register tmp, int in_stk_bias, int out_stk_bias) {\n@@ -929,2 +925,3 @@\n-      assert(dst.is_single_reg(), \"not a stack pair\");\n-      movq(Address(rsp, reg2offset_out(dst.first())), src.first()->as_Register());\n+      assert(dst.is_single_reg(), \"not a stack pair: (%s, %s), (%s, %s)\",\n+             src.first()->name(), src.second()->name(), dst.first()->name(), dst.second()->name());\n+      movq(Address(rsp, reg2offset_out(dst.first()) + out_stk_bias), src.first()->as_Register());\n@@ -934,1 +931,1 @@\n-    movq(dst.first()->as_Register(), Address(rbp, reg2offset_out(src.first())));\n+    movq(dst.first()->as_Register(), Address(rbp, reg2offset_in(src.first()) + in_stk_bias));\n@@ -937,2 +934,2 @@\n-    movq(rax, Address(rbp, reg2offset_in(src.first())));\n-    movq(Address(rsp, reg2offset_out(dst.first())), rax);\n+    movq(tmp, Address(rbp, reg2offset_in(src.first()) + in_stk_bias));\n+    movq(Address(rsp, reg2offset_out(dst.first()) + out_stk_bias), tmp);\n@@ -943,1 +940,1 @@\n-void MacroAssembler::double_move(VMRegPair src, VMRegPair dst) {\n+void MacroAssembler::double_move(VMRegPair src, VMRegPair dst, Register tmp, int in_stk_bias, int out_stk_bias) {\n@@ -956,1 +953,1 @@\n-      movdbl(Address(rsp, reg2offset_out(dst.first())), src.first()->as_XMMRegister());\n+      movdbl(Address(rsp, reg2offset_out(dst.first()) + out_stk_bias), src.first()->as_XMMRegister());\n@@ -960,1 +957,1 @@\n-    movdbl(dst.first()->as_XMMRegister(), Address(rbp, reg2offset_out(src.first())));\n+    movdbl(dst.first()->as_XMMRegister(), Address(rbp, reg2offset_in(src.first()) + in_stk_bias));\n@@ -963,2 +960,2 @@\n-    movq(rax, Address(rbp, reg2offset_in(src.first())));\n-    movq(Address(rsp, reg2offset_out(dst.first())), rax);\n+    movq(tmp, Address(rbp, reg2offset_in(src.first()) + in_stk_bias));\n+    movq(Address(rsp, reg2offset_out(dst.first()) + out_stk_bias), tmp);\n@@ -970,1 +967,1 @@\n-void MacroAssembler::float_move(VMRegPair src, VMRegPair dst) {\n+void MacroAssembler::float_move(VMRegPair src, VMRegPair dst, Register tmp, int in_stk_bias, int out_stk_bias) {\n@@ -978,2 +975,2 @@\n-      movl(rax, Address(rbp, reg2offset_in(src.first())));\n-      movptr(Address(rsp, reg2offset_out(dst.first())), rax);\n+      movl(tmp, Address(rbp, reg2offset_in(src.first()) + in_stk_bias));\n+      movptr(Address(rsp, reg2offset_out(dst.first()) + out_stk_bias), tmp);\n@@ -983,1 +980,1 @@\n-      movflt(dst.first()->as_XMMRegister(), Address(rbp, reg2offset_in(src.first())));\n+      movflt(dst.first()->as_XMMRegister(), Address(rbp, reg2offset_in(src.first()) + in_stk_bias));\n@@ -988,1 +985,1 @@\n-    movflt(Address(rsp, reg2offset_out(dst.first())), src.first()->as_XMMRegister());\n+    movflt(Address(rsp, reg2offset_out(dst.first()) + out_stk_bias), src.first()->as_XMMRegister());\n@@ -1002,1 +999,1 @@\n-void MacroAssembler::move32_64(VMRegPair src, VMRegPair dst) {\n+void MacroAssembler::move32_64(VMRegPair src, VMRegPair dst, Register tmp, int in_stk_bias, int out_stk_bias) {\n@@ -1006,2 +1003,2 @@\n-      movslq(rax, Address(rbp, reg2offset_in(src.first())));\n-      movq(Address(rsp, reg2offset_out(dst.first())), rax);\n+      movslq(tmp, Address(rbp, reg2offset_in(src.first()) + in_stk_bias));\n+      movq(Address(rsp, reg2offset_out(dst.first()) + out_stk_bias), tmp);\n@@ -1010,1 +1007,1 @@\n-      movslq(dst.first()->as_Register(), Address(rbp, reg2offset_in(src.first())));\n+      movslq(dst.first()->as_Register(), Address(rbp, reg2offset_in(src.first()) + in_stk_bias));\n@@ -1016,1 +1013,1 @@\n-    movq(Address(rsp, reg2offset_out(dst.first())), src.first()->as_Register());\n+    movq(Address(rsp, reg2offset_out(dst.first()) + out_stk_bias), src.first()->as_Register());\n@@ -1076,1 +1073,1 @@\n-    \/\/ Oop is in an a register we must store it to the space we reserve\n+    \/\/ Oop is in a register we must store it to the space we reserve\n@@ -1184,0 +1181,20 @@\n+void MacroAssembler::push_f(XMMRegister r) {\n+  subptr(rsp, wordSize);\n+  movflt(Address(rsp, 0), r);\n+}\n+\n+void MacroAssembler::pop_f(XMMRegister r) {\n+  movflt(r, Address(rsp, 0));\n+  addptr(rsp, wordSize);\n+}\n+\n+void MacroAssembler::push_d(XMMRegister r) {\n+  subptr(rsp, 2 * wordSize);\n+  movdbl(Address(rsp, 0), r);\n+}\n+\n+void MacroAssembler::pop_d(XMMRegister r) {\n+  movdbl(r, Address(rsp, 0));\n+  addptr(rsp, 2 * Interpreter::stackElementSize);\n+}\n+\n@@ -1316,0 +1333,7 @@\n+void MacroAssembler::emit_static_call_stub() {\n+  \/\/ Static stub relocation also tags the Method* in the code-stream.\n+  mov_metadata(rbx, (Metadata*) NULL);  \/\/ Method is zapped till fixup time.\n+  \/\/ This is recognized as unresolved by relocs\/nativeinst\/ic code.\n+  jump(RuntimeAddress(pc()));\n+}\n+\n@@ -1641,0 +1665,14 @@\n+void MacroAssembler::call_VM_leaf(address entry_point, Register arg_0, Register arg_1, Register arg_2, Register arg_3) {\n+  LP64_ONLY(assert(arg_0 != c_rarg3, \"smashed arg\"));\n+  LP64_ONLY(assert(arg_1 != c_rarg3, \"smashed arg\"));\n+  LP64_ONLY(assert(arg_2 != c_rarg3, \"smashed arg\"));\n+  pass_arg3(this, arg_3);\n+  LP64_ONLY(assert(arg_0 != c_rarg2, \"smashed arg\"));\n+  LP64_ONLY(assert(arg_1 != c_rarg2, \"smashed arg\"));\n+  pass_arg2(this, arg_2);\n+  LP64_ONLY(assert(arg_0 != c_rarg1, \"smashed arg\"));\n+  pass_arg1(this, arg_1);\n+  pass_arg0(this, arg_0);\n+  call_VM_leaf(entry_point, 3);\n+}\n+\n@@ -1921,1 +1959,1 @@\n-  assert (shift_value > 0, \"illegal shift value\");\n+  assert(shift_value > 0, \"illegal shift value\");\n@@ -1960,0 +1998,13 @@\n+void MacroAssembler::post_call_nop() {\n+  if (!Continuations::enabled()) {\n+    return;\n+  }\n+  InstructionMark im(this);\n+  relocate(post_call_nop_Relocation::spec());\n+  emit_int8((int8_t)0x0f);\n+  emit_int8((int8_t)0x1f);\n+  emit_int8((int8_t)0x84);\n+  emit_int8((int8_t)0x00);\n+  emit_int32(0x00);\n+}\n+\n@@ -1965,5 +2016,5 @@\n-    emit_int8(0x26); \/\/ es:\n-    emit_int8(0x2e); \/\/ cs:\n-    emit_int8(0x64); \/\/ fs:\n-    emit_int8(0x65); \/\/ gs:\n-    emit_int8((unsigned char)0x90);\n+    emit_int8((int8_t)0x26); \/\/ es:\n+    emit_int8((int8_t)0x2e); \/\/ cs:\n+    emit_int8((int8_t)0x64); \/\/ fs:\n+    emit_int8((int8_t)0x65); \/\/ gs:\n+    emit_int8((int8_t)0x90);\n@@ -2538,2 +2589,3 @@\n-  assert(vector_len <= AVX_256bit, \"AVX2 vector length\");\n-  if (vector_len == AVX_256bit) {\n+  if (vector_len == AVX_512bit) {\n+    evmovdquq(dst, src, AVX_512bit, scratch_reg);\n+  } else if (vector_len == AVX_256bit) {\n@@ -2612,5 +2664,1 @@\n-    if (mask == k0) {\n-      Assembler::evmovdqub(dst, as_Address(src), merge, vector_len);\n-    } else {\n-      Assembler::evmovdqub(dst, mask, as_Address(src), merge, vector_len);\n-    }\n+    Assembler::evmovdqub(dst, mask, as_Address(src), merge, vector_len);\n@@ -2619,5 +2667,1 @@\n-    if (mask == k0) {\n-      Assembler::evmovdqub(dst, Address(scratch_reg, 0), merge, vector_len);\n-    } else {\n-      Assembler::evmovdqub(dst, mask, Address(scratch_reg, 0), merge, vector_len);\n-    }\n+    Assembler::evmovdqub(dst, mask, Address(scratch_reg, 0), merge, vector_len);\n@@ -2801,0 +2845,103 @@\n+void MacroAssembler::push_cont_fastpath() {\n+  if (!Continuations::enabled()) return;\n+\n+#ifndef _LP64\n+  Register rthread = rax;\n+  Register rrealsp = rbx;\n+  push(rthread);\n+  push(rrealsp);\n+\n+  get_thread(rthread);\n+\n+  \/\/ The code below wants the original RSP.\n+  \/\/ Move it back after the pushes above.\n+  movptr(rrealsp, rsp);\n+  addptr(rrealsp, 2*wordSize);\n+#else\n+  Register rthread = r15_thread;\n+  Register rrealsp = rsp;\n+#endif\n+\n+  Label done;\n+  cmpptr(rrealsp, Address(rthread, JavaThread::cont_fastpath_offset()));\n+  jccb(Assembler::belowEqual, done);\n+  movptr(Address(rthread, JavaThread::cont_fastpath_offset()), rrealsp);\n+  bind(done);\n+\n+#ifndef _LP64\n+  pop(rrealsp);\n+  pop(rthread);\n+#endif\n+}\n+\n+void MacroAssembler::pop_cont_fastpath() {\n+  if (!Continuations::enabled()) return;\n+\n+#ifndef _LP64\n+  Register rthread = rax;\n+  Register rrealsp = rbx;\n+  push(rthread);\n+  push(rrealsp);\n+\n+  get_thread(rthread);\n+\n+  \/\/ The code below wants the original RSP.\n+  \/\/ Move it back after the pushes above.\n+  movptr(rrealsp, rsp);\n+  addptr(rrealsp, 2*wordSize);\n+#else\n+  Register rthread = r15_thread;\n+  Register rrealsp = rsp;\n+#endif\n+\n+  Label done;\n+  cmpptr(rrealsp, Address(rthread, JavaThread::cont_fastpath_offset()));\n+  jccb(Assembler::below, done);\n+  movptr(Address(rthread, JavaThread::cont_fastpath_offset()), 0);\n+  bind(done);\n+\n+#ifndef _LP64\n+  pop(rrealsp);\n+  pop(rthread);\n+#endif\n+}\n+\n+void MacroAssembler::inc_held_monitor_count() {\n+#ifndef _LP64\n+  Register thread = rax;\n+  push(thread);\n+  get_thread(thread);\n+  incrementl(Address(thread, JavaThread::held_monitor_count_offset()));\n+  pop(thread);\n+#else \/\/ LP64\n+  incrementq(Address(r15_thread, JavaThread::held_monitor_count_offset()));\n+#endif\n+}\n+\n+void MacroAssembler::dec_held_monitor_count() {\n+#ifndef _LP64\n+  Register thread = rax;\n+  push(thread);\n+  get_thread(thread);\n+  decrementl(Address(thread, JavaThread::held_monitor_count_offset()));\n+  pop(thread);\n+#else \/\/ LP64\n+  decrementq(Address(r15_thread, JavaThread::held_monitor_count_offset()));\n+#endif\n+}\n+\n+#ifdef ASSERT\n+void MacroAssembler::stop_if_in_cont(Register cont, const char* name) {\n+#ifdef _LP64\n+  Label no_cont;\n+  movptr(cont, Address(r15_thread, JavaThread::cont_entry_offset()));\n+  testl(cont, cont);\n+  jcc(Assembler::zero, no_cont);\n+  stop(name);\n+  bind(no_cont);\n+#else\n+  Unimplemented();\n+#endif\n+}\n+#endif\n+\n@@ -3151,0 +3298,9 @@\n+void MacroAssembler::vpbroadcastq(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch) {\n+  if (reachable(src)) {\n+    Assembler::vpbroadcastq(dst, as_Address(src), vector_len);\n+  } else {\n+    lea(rscratch, src);\n+    Assembler::vpbroadcastq(dst, Address(rscratch, 0), vector_len);\n+  }\n+}\n+\n@@ -3769,10 +3925,0 @@\n-\/\/ Defines obj, preserves var_size_in_bytes\n-void MacroAssembler::eden_allocate(Register thread, Register obj,\n-                                   Register var_size_in_bytes,\n-                                   int con_size_in_bytes,\n-                                   Register t1,\n-                                   Label& slow_case) {\n-  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-  bs->eden_allocate(this, thread, obj, var_size_in_bytes, con_size_in_bytes, t1, slow_case);\n-}\n-\n@@ -4087,1 +4233,1 @@\n-  if (super_klass != rax || UseCompressedOops) {\n+  if (super_klass != rax) {\n@@ -5268,89 +5414,0 @@\n-\/\/ C2 compiled method's prolog code.\n-void MacroAssembler::verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub) {\n-\n-  \/\/ WARNING: Initial instruction MUST be 5 bytes or longer so that\n-  \/\/ NativeJump::patch_verified_entry will be able to patch out the entry\n-  \/\/ code safely. The push to verify stack depth is ok at 5 bytes,\n-  \/\/ the frame allocation can be either 3 or 6 bytes. So if we don't do\n-  \/\/ stack bang then we must use the 6 byte frame allocation even if\n-  \/\/ we have no frame. :-(\n-  assert(stack_bang_size >= framesize || stack_bang_size <= 0, \"stack bang size incorrect\");\n-\n-  assert((framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n-  \/\/ Remove word for return addr\n-  framesize -= wordSize;\n-  stack_bang_size -= wordSize;\n-\n-  \/\/ Calls to C2R adapters often do not accept exceptional returns.\n-  \/\/ We require that their callers must bang for them.  But be careful, because\n-  \/\/ some VM calls (such as call site linkage) can use several kilobytes of\n-  \/\/ stack.  But the stack safety zone should account for that.\n-  \/\/ See bugs 4446381, 4468289, 4497237.\n-  if (stack_bang_size > 0) {\n-    generate_stack_overflow_check(stack_bang_size);\n-\n-    \/\/ We always push rbp, so that on return to interpreter rbp, will be\n-    \/\/ restored correctly and we can correct the stack.\n-    push(rbp);\n-    \/\/ Save caller's stack pointer into RBP if the frame pointer is preserved.\n-    if (PreserveFramePointer) {\n-      mov(rbp, rsp);\n-    }\n-    \/\/ Remove word for ebp\n-    framesize -= wordSize;\n-\n-    \/\/ Create frame\n-    if (framesize) {\n-      subptr(rsp, framesize);\n-    }\n-  } else {\n-    \/\/ Create frame (force generation of a 4 byte immediate value)\n-    subptr_imm32(rsp, framesize);\n-\n-    \/\/ Save RBP register now.\n-    framesize -= wordSize;\n-    movptr(Address(rsp, framesize), rbp);\n-    \/\/ Save caller's stack pointer into RBP if the frame pointer is preserved.\n-    if (PreserveFramePointer) {\n-      movptr(rbp, rsp);\n-      if (framesize > 0) {\n-        addptr(rbp, framesize);\n-      }\n-    }\n-  }\n-\n-  if (VerifyStackAtCalls) { \/\/ Majik cookie to verify stack depth\n-    framesize -= wordSize;\n-    movptr(Address(rsp, framesize), (int32_t)0xbadb100d);\n-  }\n-\n-#ifndef _LP64\n-  \/\/ If method sets FPU control word do it now\n-  if (fp_mode_24b) {\n-    fldcw(ExternalAddress(StubRoutines::x86::addr_fpu_cntrl_wrd_24()));\n-  }\n-  if (UseSSE >= 2 && VerifyFPU) {\n-    verify_FPU(0, \"FPU stack must be clean on entry\");\n-  }\n-#endif\n-\n-#ifdef ASSERT\n-  if (VerifyStackAtCalls) {\n-    Label L;\n-    push(rax);\n-    mov(rax, rsp);\n-    andptr(rax, StackAlignmentInBytes-1);\n-    cmpptr(rax, StackAlignmentInBytes-wordSize);\n-    pop(rax);\n-    jcc(Assembler::equal, L);\n-    STOP(\"Stack is not properly aligned!\");\n-    bind(L);\n-  }\n-#endif\n-\n-  if (!is_stub) {\n-    BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    bs->nmethod_entry_barrier(this);\n-  }\n-}\n-\n@@ -5432,0 +5489,2 @@\n+  const int fill64_per_loop = 4;\n+  const int max_unrolled_fill64 = 8;\n@@ -5435,1 +5494,17 @@\n-  for (int i = 0; i < vector64_count; i++) {\n+  int start64 = 0;\n+  if (vector64_count > max_unrolled_fill64) {\n+    Label LOOP;\n+    Register index = rtmp;\n+\n+    start64 = vector64_count - (vector64_count % fill64_per_loop);\n+\n+    movl(index, 0);\n+    BIND(LOOP);\n+    for (int i = 0; i < fill64_per_loop; i++) {\n+      fill64(Address(base, index, Address::times_1, i * 64), xtmp, use64byteVector);\n+    }\n+    addl(index, fill64_per_loop * 64);\n+    cmpl(index, start64 * 64);\n+    jccb(Assembler::less, LOOP);\n+  }\n+  for (int i = start64; i < vector64_count; i++) {\n@@ -5447,1 +5522,1 @@\n-        evmovdqu(T_LONG, k0, Address(base, disp), xtmp, Assembler::AVX_128bit);\n+        evmovdqu(T_LONG, k0, Address(base, disp), xtmp, false, Assembler::AVX_128bit);\n@@ -5452,1 +5527,1 @@\n-        evmovdqu(T_LONG, mask, Address(base, disp), xtmp, Assembler::AVX_256bit);\n+        evmovdqu(T_LONG, mask, Address(base, disp), xtmp, true, Assembler::AVX_256bit);\n@@ -5455,1 +5530,1 @@\n-        evmovdqu(T_LONG, k0, Address(base, disp), xtmp, Assembler::AVX_256bit);\n+        evmovdqu(T_LONG, k0, Address(base, disp), xtmp, false, Assembler::AVX_256bit);\n@@ -5461,1 +5536,1 @@\n-          evmovdqu(T_LONG, mask, Address(base, disp), xtmp, Assembler::AVX_512bit);\n+          evmovdqu(T_LONG, mask, Address(base, disp), xtmp, true, Assembler::AVX_512bit);\n@@ -5463,1 +5538,1 @@\n-          evmovdqu(T_LONG, k0, Address(base, disp), xtmp, Assembler::AVX_256bit);\n+          evmovdqu(T_LONG, k0, Address(base, disp), xtmp, false, Assembler::AVX_256bit);\n@@ -5471,1 +5546,1 @@\n-          evmovdqu(T_LONG, mask, Address(base, disp), xtmp, Assembler::AVX_512bit);\n+          evmovdqu(T_LONG, mask, Address(base, disp), xtmp, true, Assembler::AVX_512bit);\n@@ -5473,2 +5548,2 @@\n-          evmovdqu(T_LONG, k0, Address(base, disp), xtmp, Assembler::AVX_256bit);\n-          evmovdqu(T_LONG, k0, Address(base, disp + 32), xtmp, Assembler::AVX_128bit);\n+          evmovdqu(T_LONG, k0, Address(base, disp), xtmp, false, Assembler::AVX_256bit);\n+          evmovdqu(T_LONG, k0, Address(base, disp + 32), xtmp, false, Assembler::AVX_128bit);\n@@ -5481,1 +5556,1 @@\n-          evmovdqu(T_LONG, mask, Address(base, disp), xtmp, Assembler::AVX_512bit);\n+          evmovdqu(T_LONG, mask, Address(base, disp), xtmp, true, Assembler::AVX_512bit);\n@@ -5483,1 +5558,1 @@\n-          evmovdqu(T_LONG, k0, Address(base, disp), xtmp, Assembler::AVX_256bit);\n+          evmovdqu(T_LONG, k0, Address(base, disp), xtmp, false, Assembler::AVX_256bit);\n@@ -5486,1 +5561,1 @@\n-          evmovdqu(T_LONG, mask, Address(base, disp + 32), xtmp, Assembler::AVX_256bit);\n+          evmovdqu(T_LONG, mask, Address(base, disp + 32), xtmp, true, Assembler::AVX_256bit);\n@@ -6458,1 +6533,1 @@\n-    evmovdqub(rymm0, Address(obja, result), false, Assembler::AVX_512bit);\n+    evmovdqub(rymm0, Address(obja, result), Assembler::AVX_512bit);\n@@ -8299,1 +8374,1 @@\n-    evmovdquw(tmp1Reg, Address(src, len, Address::times_2), \/*merge*\/ false, Assembler::AVX_512bit);\n+    evmovdquw(tmp1Reg, Address(src, len, Address::times_2), Assembler::AVX_512bit);\n@@ -8469,1 +8544,1 @@\n-    evmovdquw(Address(dst, len, Address::times_2), tmp1, \/*merge*\/ false, Assembler::AVX_512bit);\n+    evmovdquw(Address(dst, len, Address::times_2), tmp1, Assembler::AVX_512bit);\n@@ -8571,1 +8646,1 @@\n-void MacroAssembler::evmovdqu(BasicType type, KRegister kmask, XMMRegister dst, Address src, int vector_len) {\n+void MacroAssembler::evmovdqu(BasicType type, KRegister kmask, XMMRegister dst, Address src, bool merge, int vector_len) {\n@@ -8575,1 +8650,1 @@\n-      evmovdqub(dst, kmask, src, false, vector_len);\n+      evmovdqub(dst, kmask, src, merge, vector_len);\n@@ -8579,1 +8654,1 @@\n-      evmovdquw(dst, kmask, src, false, vector_len);\n+      evmovdquw(dst, kmask, src, merge, vector_len);\n@@ -8583,1 +8658,1 @@\n-      evmovdqul(dst, kmask, src, false, vector_len);\n+      evmovdqul(dst, kmask, src, merge, vector_len);\n@@ -8587,1 +8662,1 @@\n-      evmovdquq(dst, kmask, src, false, vector_len);\n+      evmovdquq(dst, kmask, src, merge, vector_len);\n@@ -8595,1 +8670,1 @@\n-void MacroAssembler::evmovdqu(BasicType type, KRegister kmask, Address dst, XMMRegister src, int vector_len) {\n+void MacroAssembler::evmovdqu(BasicType type, KRegister kmask, Address dst, XMMRegister src, bool merge, int vector_len) {\n@@ -8599,1 +8674,1 @@\n-      evmovdqub(dst, kmask, src, true, vector_len);\n+      evmovdqub(dst, kmask, src, merge, vector_len);\n@@ -8603,1 +8678,1 @@\n-      evmovdquw(dst, kmask, src, true, vector_len);\n+      evmovdquw(dst, kmask, src, merge, vector_len);\n@@ -8607,1 +8682,1 @@\n-      evmovdqul(dst, kmask, src, true, vector_len);\n+      evmovdqul(dst, kmask, src, merge, vector_len);\n@@ -8611,1 +8686,1 @@\n-      evmovdquq(dst, kmask, src, true, vector_len);\n+      evmovdquq(dst, kmask, src, merge, vector_len);\n@@ -9000,1 +9075,1 @@\n-  evmovdqu(bt, mask, dst, xmm, vec_enc);\n+  evmovdqu(bt, mask, dst, xmm, true, vec_enc);\n@@ -9029,1 +9104,1 @@\n-void MacroAssembler::fill32(Register dst, int disp, XMMRegister xmm) {\n+void MacroAssembler::fill32(Address dst, XMMRegister xmm) {\n@@ -9031,1 +9106,1 @@\n-  vmovdqu(Address(dst, disp), xmm);\n+  vmovdqu(dst, xmm);\n@@ -9034,1 +9109,5 @@\n-void MacroAssembler::fill64(Register dst, int disp, XMMRegister xmm, bool use64byteVector) {\n+void MacroAssembler::fill32(Register dst, int disp, XMMRegister xmm) {\n+  fill32(Address(dst, disp), xmm);\n+}\n+\n+void MacroAssembler::fill64(Address dst, XMMRegister xmm, bool use64byteVector) {\n@@ -9036,3 +9115,2 @@\n-  BasicType type[] = {T_BYTE,  T_SHORT,  T_INT,   T_LONG};\n-    fill32(dst, disp, xmm);\n-    fill32(dst, disp + 32, xmm);\n+    fill32(dst, xmm);\n+    fill32(dst.plus_disp(32), xmm);\n@@ -9041,1 +9119,1 @@\n-    evmovdquq(Address(dst, disp), xmm, Assembler::AVX_512bit);\n+    evmovdquq(dst, xmm, Assembler::AVX_512bit);\n@@ -9045,0 +9123,4 @@\n+void MacroAssembler::fill64(Register dst, int disp, XMMRegister xmm, bool use64byteVector) {\n+  fill64(Address(dst, disp), xmm, use64byteVector);\n+}\n+\n@@ -9125,1 +9207,1 @@\n-      evmovdqu(T_BYTE, k2, Address(to, 0), xtmp, Assembler::AVX_256bit);\n+      evmovdqu(T_BYTE, k2, Address(to, 0), xtmp, true, Assembler::AVX_256bit);\n@@ -9195,1 +9277,1 @@\n-      evmovdqu(T_BYTE, k2, Address(to, 0), xtmp, Assembler::AVX_512bit);\n+      evmovdqu(T_BYTE, k2, Address(to, 0), xtmp, true, Assembler::AVX_512bit);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":265,"deletions":183,"binary":false,"changes":448,"status":"modified"},{"patch":"@@ -226,0 +226,1 @@\n+  void post_call_nop();\n@@ -240,4 +241,5 @@\n-  void move32_64(VMRegPair src, VMRegPair dst);\n-  void long_move(VMRegPair src, VMRegPair dst);\n-  void float_move(VMRegPair src, VMRegPair dst);\n-  void double_move(VMRegPair src, VMRegPair dst);\n+  \/\/ bias in bytes\n+  void move32_64(VMRegPair src, VMRegPair dst, Register tmp = rax, int in_stk_bias = 0, int out_stk_bias = 0);\n+  void long_move(VMRegPair src, VMRegPair dst, Register tmp = rax, int in_stk_bias = 0, int out_stk_bias = 0);\n+  void float_move(VMRegPair src, VMRegPair dst, Register tmp = rax, int in_stk_bias = 0, int out_stk_bias = 0);\n+  void double_move(VMRegPair src, VMRegPair dst, Register tmp = rax, int in_stk_bias = 0, int out_stk_bias = 0);\n@@ -320,0 +322,3 @@\n+  void call_VM_leaf(address entry_point,\n+                    Register arg_1, Register arg_2, Register arg_3, Register arg_4);\n+\n@@ -547,0 +552,8 @@\n+  void push_cont_fastpath();\n+  void pop_cont_fastpath();\n+\n+  void inc_held_monitor_count();\n+  void dec_held_monitor_count();\n+\n+  DEBUG_ONLY(void stop_if_in_cont(Register cont_reg, const char* name);)\n+\n@@ -580,8 +593,0 @@\n-  void eden_allocate(\n-    Register thread,                   \/\/ Current thread\n-    Register obj,                      \/\/ result: pointer to object after successful allocation\n-    Register var_size_in_bytes,        \/\/ object size in bytes if unknown at compile time; invalid otherwise\n-    int      con_size_in_bytes,        \/\/ object size in bytes if   known at compile time\n-    Register t1,                       \/\/ temp register\n-    Label&   slow_case                 \/\/ continuation point if fast allocation fails\n-  );\n@@ -727,1 +732,1 @@\n-  Condition negate_condition(Condition cond);\n+  static Condition negate_condition(Condition cond);\n@@ -881,0 +886,2 @@\n+  void emit_static_call_stub();\n+\n@@ -896,0 +903,5 @@\n+  void push_f(XMMRegister r);\n+  void pop_f(XMMRegister r);\n+  void push_d(XMMRegister r);\n+  void pop_d(XMMRegister r);\n+\n@@ -1175,6 +1187,10 @@\n-  void evmovdqu(BasicType type, KRegister kmask, Address dst, XMMRegister src, int vector_len);\n-  void evmovdqu(BasicType type, KRegister kmask, XMMRegister dst, Address src, int vector_len);\n-\n-  void evmovdqub(Address dst, XMMRegister src, bool merge, int vector_len) { Assembler::evmovdqub(dst, src, merge, vector_len); }\n-  void evmovdqub(XMMRegister dst, Address src, bool merge, int vector_len) { Assembler::evmovdqub(dst, src, merge, vector_len); }\n-  void evmovdqub(XMMRegister dst, XMMRegister src, bool merge, int vector_len) { Assembler::evmovdqub(dst, src, merge, vector_len); }\n+  void evmovdqu(BasicType type, KRegister kmask, Address dst, XMMRegister src,  bool merge, int vector_len);\n+  void evmovdqu(BasicType type, KRegister kmask, XMMRegister dst, Address src, bool merge, int vector_len);\n+\n+  void evmovdqub(XMMRegister dst, XMMRegister src, int vector_len) { Assembler::evmovdqub(dst, src, vector_len); }\n+  void evmovdqub(XMMRegister dst, Address src, int vector_len) { Assembler::evmovdqub(dst, src, vector_len); }\n+  void evmovdqub(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n+    if (dst->encoding() != src->encoding() || mask != k0)  {\n+      Assembler::evmovdqub(dst, mask, src, merge, vector_len);\n+    }\n+  }\n@@ -1185,3 +1201,7 @@\n-  void evmovdquw(Address dst, XMMRegister src, bool merge, int vector_len) { Assembler::evmovdquw(dst, src, merge, vector_len); }\n-  void evmovdquw(Address dst, KRegister mask, XMMRegister src, bool merge, int vector_len) { Assembler::evmovdquw(dst, mask, src, merge, vector_len); }\n-  void evmovdquw(XMMRegister dst, Address src, bool merge, int vector_len) { Assembler::evmovdquw(dst, src, merge, vector_len); }\n+  void evmovdquw(XMMRegister dst, Address src, int vector_len) { Assembler::evmovdquw(dst, src, vector_len); }\n+  void evmovdquw(Address dst, XMMRegister src, int vector_len) { Assembler::evmovdquw(dst, src, vector_len); }\n+  void evmovdquw(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n+    if (dst->encoding() != src->encoding() || mask != k0) {\n+      Assembler::evmovdquw(dst, mask, src, merge, vector_len);\n+    }\n+  }\n@@ -1189,0 +1209,1 @@\n+  void evmovdquw(Address dst, KRegister mask, XMMRegister src, bool merge, int vector_len) { Assembler::evmovdquw(dst, mask, src, merge, vector_len); }\n@@ -1191,0 +1212,5 @@\n+  void evmovdqul(XMMRegister dst, XMMRegister src, int vector_len) {\n+     if (dst->encoding() != src->encoding()) {\n+       Assembler::evmovdqul(dst, src, vector_len);\n+     }\n+  }\n@@ -1193,3 +1219,5 @@\n-  void evmovdqul(XMMRegister dst, XMMRegister src, int vector_len) {\n-     if (dst->encoding() == src->encoding()) return;\n-     Assembler::evmovdqul(dst, src, vector_len);\n+\n+  void evmovdqul(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n+    if (dst->encoding() != src->encoding() || mask != k0)  {\n+      Assembler::evmovdqul(dst, mask, src, merge, vector_len);\n+    }\n@@ -1197,5 +1225,1 @@\n-  void evmovdqul(Address dst, KRegister mask, XMMRegister src, bool merge, int vector_len) { Assembler::evmovdqul(dst, mask, src, merge, vector_len); }\n-  void evmovdqul(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n-    if (dst->encoding() == src->encoding() && mask == k0) return;\n-    Assembler::evmovdqul(dst, mask, src, merge, vector_len);\n-   }\n+  void evmovdqul(Address dst, KRegister mask, XMMRegister src, bool merge, int vector_len) { Assembler::evmovdqul(dst, mask, src, merge, vector_len); }\n@@ -1205,0 +1229,5 @@\n+  void evmovdquq(XMMRegister dst, XMMRegister src, int vector_len) {\n+    if (dst->encoding() != src->encoding()) {\n+      Assembler::evmovdquq(dst, src, vector_len);\n+    }\n+  }\n@@ -1208,6 +1237,1 @@\n-  void evmovdquq(XMMRegister dst, XMMRegister src, int vector_len) {\n-    if (dst->encoding() == src->encoding()) return;\n-    Assembler::evmovdquq(dst, src, vector_len);\n-  }\n-  void evmovdquq(Address dst, KRegister mask, XMMRegister src, bool merge, int vector_len) { Assembler::evmovdquq(dst, mask, src, merge, vector_len); }\n-  void evmovdquq(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len) { Assembler::evmovdquq(dst, mask, src, merge, vector_len); }\n+\n@@ -1215,2 +1239,3 @@\n-    if (dst->encoding() == src->encoding() && mask == k0) return;\n-    Assembler::evmovdquq(dst, mask, src, merge, vector_len);\n+    if (dst->encoding() != src->encoding() || mask != k0) {\n+      Assembler::evmovdquq(dst, mask, src, merge, vector_len);\n+    }\n@@ -1218,0 +1243,2 @@\n+  void evmovdquq(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len) { Assembler::evmovdquq(dst, mask, src, merge, vector_len); }\n+  void evmovdquq(Address dst, KRegister mask, XMMRegister src, bool merge, int vector_len) { Assembler::evmovdquq(dst, mask, src, merge, vector_len); }\n@@ -1343,0 +1370,5 @@\n+  void vpbroadcastq(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch = rscratch1);\n+  void vpbroadcastq(XMMRegister dst, XMMRegister src, int vector_len) { Assembler::vpbroadcastq(dst, src, vector_len); }\n+  void vpbroadcastq(XMMRegister dst, Address src, int vector_len) { Assembler::vpbroadcastq(dst, src, vector_len); }\n+\n+\n@@ -1870,3 +1902,0 @@\n-  \/\/ C2 compiled method's prolog code.\n-  void verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub);\n-\n@@ -2013,0 +2042,2 @@\n+  void fill32(Address dst, XMMRegister xmm);\n+\n@@ -2015,0 +2046,2 @@\n+  void fill64(Address dst, XMMRegister xmm, bool use64byteVector = false);\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":74,"deletions":41,"binary":false,"changes":115,"status":"modified"},{"patch":"@@ -206,0 +206,15 @@\n+void MethodHandles::jump_to_native_invoker(MacroAssembler* _masm, Register nep_reg, Register temp_target) {\n+  BLOCK_COMMENT(\"jump_to_native_invoker {\");\n+  assert_different_registers(nep_reg, temp_target);\n+  assert(nep_reg != noreg, \"required register\");\n+\n+  \/\/ Load the invoker, as NEP -> .invoker\n+  __ verify_oop(nep_reg);\n+  __ access_load_at(T_ADDRESS, IN_HEAP, temp_target,\n+                    Address(nep_reg, NONZERO(jdk_internal_foreign_abi_NativeEntryPoint::downcall_stub_address_offset_in_bytes())),\n+                    noreg, noreg);\n+\n+  __ jmp(temp_target);\n+  BLOCK_COMMENT(\"} jump_to_native_invoker\");\n+}\n+\n@@ -317,1 +332,1 @@\n-    assert(receiver_reg == (iid == vmIntrinsics::_linkToStatic ? noreg : j_rarg0), \"only valid assignment\");\n+    assert(receiver_reg == (iid == vmIntrinsics::_linkToStatic || iid == vmIntrinsics::_linkToNative ? noreg : j_rarg0), \"only valid assignment\");\n@@ -327,1 +342,1 @@\n-    assert(receiver_reg == (iid == vmIntrinsics::_linkToStatic ? noreg : rcx), \"only valid assignment\");\n+    assert(receiver_reg == (iid == vmIntrinsics::_linkToStatic || iid == vmIntrinsics::_linkToNative ? noreg : rcx), \"only valid assignment\");\n@@ -339,4 +354,1 @@\n-  if (iid == vmIntrinsics::_invokeBasic || iid == vmIntrinsics::_linkToNative) {\n-    if (iid == vmIntrinsics::_linkToNative) {\n-      assert(for_compiler_entry, \"only compiler entry is supported\");\n-    }\n+  if (iid == vmIntrinsics::_invokeBasic) {\n@@ -345,1 +357,3 @@\n-\n+  } else if (iid == vmIntrinsics::_linkToNative) {\n+    assert(for_compiler_entry, \"only compiler entry is supported\");\n+    jump_to_native_invoker(_masm, member_reg, temp1);\n","filename":"src\/hotspot\/cpu\/x86\/methodHandles_x86.cpp","additions":21,"deletions":7,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"prims\/jvmtiExport.hpp\"\n@@ -43,0 +44,2 @@\n+#include \"runtime\/continuation.hpp\"\n+#include \"runtime\/continuationEntry.inline.hpp\"\n@@ -45,0 +48,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -48,1 +52,0 @@\n-#include \"runtime\/thread.inline.hpp\"\n@@ -58,0 +61,3 @@\n+#if INCLUDE_JFR\n+#include \"jfr\/support\/jfrIntrinsics.hpp\"\n+#endif\n@@ -76,0 +82,4 @@\n+OopMap* continuation_enter_setup(MacroAssembler* masm, int& stack_slots);\n+void fill_continuation_entry(MacroAssembler* masm);\n+void continuation_enter_cleanup(MacroAssembler* masm);\n+\n@@ -386,0 +396,2 @@\n+    __ pop_cont_fastpath();\n+\n@@ -798,0 +810,15 @@\n+  address generate_count_leading_zeros_lut(const char *stub_name) {\n+    __ align64();\n+    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+    address start = __ pc();\n+    __ emit_data64(0x0101010102020304, relocInfo::none);\n+    __ emit_data64(0x0000000000000000, relocInfo::none);\n+    __ emit_data64(0x0101010102020304, relocInfo::none);\n+    __ emit_data64(0x0000000000000000, relocInfo::none);\n+    __ emit_data64(0x0101010102020304, relocInfo::none);\n+    __ emit_data64(0x0000000000000000, relocInfo::none);\n+    __ emit_data64(0x0101010102020304, relocInfo::none);\n+    __ emit_data64(0x0000000000000000, relocInfo::none);\n+    return start;\n+  }\n+\n@@ -828,0 +855,60 @@\n+  address generate_vector_reverse_bit_lut(const char *stub_name) {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+    address start = __ pc();\n+    __ emit_data64(0x0E060A020C040800, relocInfo::none);\n+    __ emit_data64(0x0F070B030D050901, relocInfo::none);\n+    __ emit_data64(0x0E060A020C040800, relocInfo::none);\n+    __ emit_data64(0x0F070B030D050901, relocInfo::none);\n+    __ emit_data64(0x0E060A020C040800, relocInfo::none);\n+    __ emit_data64(0x0F070B030D050901, relocInfo::none);\n+    __ emit_data64(0x0E060A020C040800, relocInfo::none);\n+    __ emit_data64(0x0F070B030D050901, relocInfo::none);\n+    return start;\n+  }\n+\n+  address generate_vector_reverse_byte_perm_mask_long(const char *stub_name) {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+    address start = __ pc();\n+    __ emit_data64(0x0001020304050607, relocInfo::none);\n+    __ emit_data64(0x08090A0B0C0D0E0F, relocInfo::none);\n+    __ emit_data64(0x0001020304050607, relocInfo::none);\n+    __ emit_data64(0x08090A0B0C0D0E0F, relocInfo::none);\n+    __ emit_data64(0x0001020304050607, relocInfo::none);\n+    __ emit_data64(0x08090A0B0C0D0E0F, relocInfo::none);\n+    __ emit_data64(0x0001020304050607, relocInfo::none);\n+    __ emit_data64(0x08090A0B0C0D0E0F, relocInfo::none);\n+    return start;\n+  }\n+\n+  address generate_vector_reverse_byte_perm_mask_int(const char *stub_name) {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+    address start = __ pc();\n+    __ emit_data64(0x0405060700010203, relocInfo::none);\n+    __ emit_data64(0x0C0D0E0F08090A0B, relocInfo::none);\n+    __ emit_data64(0x0405060700010203, relocInfo::none);\n+    __ emit_data64(0x0C0D0E0F08090A0B, relocInfo::none);\n+    __ emit_data64(0x0405060700010203, relocInfo::none);\n+    __ emit_data64(0x0C0D0E0F08090A0B, relocInfo::none);\n+    __ emit_data64(0x0405060700010203, relocInfo::none);\n+    __ emit_data64(0x0C0D0E0F08090A0B, relocInfo::none);\n+    return start;\n+  }\n+\n+  address generate_vector_reverse_byte_perm_mask_short(const char *stub_name) {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+    address start = __ pc();\n+    __ emit_data64(0x0607040502030001, relocInfo::none);\n+    __ emit_data64(0x0E0F0C0D0A0B0809, relocInfo::none);\n+    __ emit_data64(0x0607040502030001, relocInfo::none);\n+    __ emit_data64(0x0E0F0C0D0A0B0809, relocInfo::none);\n+    __ emit_data64(0x0607040502030001, relocInfo::none);\n+    __ emit_data64(0x0E0F0C0D0A0B0809, relocInfo::none);\n+    __ emit_data64(0x0607040502030001, relocInfo::none);\n+    __ emit_data64(0x0E0F0C0D0A0B0809, relocInfo::none);\n+    return start;\n+  }\n+\n@@ -7457,0 +7544,254 @@\n+  RuntimeStub* generate_cont_doYield() {\n+    if (!Continuations::enabled()) return nullptr;\n+\n+    enum layout {\n+      rbp_off,\n+      rbpH_off,\n+      return_off,\n+      return_off2,\n+      framesize \/\/ inclusive of return address\n+    };\n+\n+    CodeBuffer code(\"cont_doYield\", 512, 64);\n+    MacroAssembler* _masm = new MacroAssembler(&code);\n+\n+    address start = __ pc();\n+    __ enter();\n+    address the_pc = __ pc();\n+\n+    int frame_complete = the_pc - start;\n+\n+    \/\/ This nop must be exactly at the PC we push into the frame info.\n+    \/\/ We use this nop for fast CodeBlob lookup, associate the OopMap\n+    \/\/ with it right away.\n+    __ post_call_nop();\n+    OopMapSet* oop_maps = new OopMapSet();\n+    OopMap* map = new OopMap(framesize, 1);\n+    oop_maps->add_gc_map(frame_complete, map);\n+\n+    __ set_last_Java_frame(rsp, rbp, the_pc);\n+    __ movptr(c_rarg0, r15_thread);\n+    __ movptr(c_rarg1, rsp);\n+    __ call_VM_leaf(Continuation::freeze_entry(), 2);\n+    __ reset_last_Java_frame(true);\n+\n+    Label L_pinned;\n+\n+    __ testptr(rax, rax);\n+    __ jcc(Assembler::notZero, L_pinned);\n+\n+    __ movptr(rsp, Address(r15_thread, JavaThread::cont_entry_offset()));\n+    continuation_enter_cleanup(_masm);\n+    __ pop(rbp);\n+    __ ret(0);\n+\n+    __ bind(L_pinned);\n+\n+    \/\/ Pinned, return to caller\n+    __ leave();\n+    __ ret(0);\n+\n+    RuntimeStub* stub =\n+      RuntimeStub::new_runtime_stub(code.name(),\n+                                    &code,\n+                                    frame_complete,\n+                                    (framesize >> (LogBytesPerWord - LogBytesPerInt)),\n+                                    oop_maps,\n+                                    false);\n+    return stub;\n+  }\n+\n+  address generate_cont_thaw(const char* label, Continuation::thaw_kind kind) {\n+    if (!Continuations::enabled()) return nullptr;\n+\n+    bool return_barrier = Continuation::is_thaw_return_barrier(kind);\n+    bool return_barrier_exception = Continuation::is_thaw_return_barrier_exception(kind);\n+\n+    StubCodeMark mark(this, \"StubRoutines\", label);\n+    address start = __ pc();\n+\n+    \/\/ TODO: Handle Valhalla return types. May require generating different return barriers.\n+\n+    if (!return_barrier) {\n+      \/\/ Pop return address. If we don't do this, we get a drift,\n+      \/\/ where the bottom-most frozen frame continuously grows.\n+      __ pop(c_rarg3);\n+    } else {\n+      __ movptr(rsp, Address(r15_thread, JavaThread::cont_entry_offset()));\n+    }\n+\n+#ifdef ASSERT\n+    {\n+      Label L_good_sp;\n+      __ cmpptr(rsp, Address(r15_thread, JavaThread::cont_entry_offset()));\n+      __ jcc(Assembler::equal, L_good_sp);\n+      __ stop(\"Incorrect rsp at thaw entry\");\n+      __ BIND(L_good_sp);\n+    }\n+#endif\n+\n+    if (return_barrier) {\n+      \/\/ Preserve possible return value from a method returning to the return barrier.\n+      __ push(rax);\n+      __ push_d(xmm0);\n+    }\n+\n+    __ movptr(c_rarg0, r15_thread);\n+    __ movptr(c_rarg1, (return_barrier ? 1 : 0));\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, Continuation::prepare_thaw), 2);\n+    __ movptr(rbx, rax);\n+\n+    if (return_barrier) {\n+      \/\/ Restore return value from a method returning to the return barrier.\n+      \/\/ No safepoint in the call to thaw, so even an oop return value should be OK.\n+      __ pop_d(xmm0);\n+      __ pop(rax);\n+    }\n+\n+#ifdef ASSERT\n+    {\n+      Label L_good_sp;\n+      __ cmpptr(rsp, Address(r15_thread, JavaThread::cont_entry_offset()));\n+      __ jcc(Assembler::equal, L_good_sp);\n+      __ stop(\"Incorrect rsp after prepare thaw\");\n+      __ BIND(L_good_sp);\n+    }\n+#endif\n+\n+    \/\/ rbx contains the size of the frames to thaw, 0 if overflow or no more frames\n+    Label L_thaw_success;\n+    __ testptr(rbx, rbx);\n+    __ jccb(Assembler::notZero, L_thaw_success);\n+    __ jump(ExternalAddress(StubRoutines::throw_StackOverflowError_entry()));\n+    __ bind(L_thaw_success);\n+\n+    \/\/ Make room for the thawed frames and align the stack.\n+    __ subptr(rsp, rbx);\n+    __ andptr(rsp, -StackAlignmentInBytes);\n+\n+    if (return_barrier) {\n+      \/\/ Preserve possible return value from a method returning to the return barrier. (Again.)\n+      __ push(rax);\n+      __ push_d(xmm0);\n+    }\n+\n+    \/\/ If we want, we can templatize thaw by kind, and have three different entries.\n+    __ movptr(c_rarg0, r15_thread);\n+    __ movptr(c_rarg1, kind);\n+    __ call_VM_leaf(Continuation::thaw_entry(), 2);\n+    __ movptr(rbx, rax);\n+\n+    if (return_barrier) {\n+      \/\/ Restore return value from a method returning to the return barrier. (Again.)\n+      \/\/ No safepoint in the call to thaw, so even an oop return value should be OK.\n+      __ pop_d(xmm0);\n+      __ pop(rax);\n+    } else {\n+      \/\/ Return 0 (success) from doYield.\n+      __ xorptr(rax, rax);\n+    }\n+\n+    \/\/ After thawing, rbx is the SP of the yielding frame.\n+    \/\/ Move there, and then to saved RBP slot.\n+    __ movptr(rsp, rbx);\n+    __ subptr(rsp, 2*wordSize);\n+\n+    if (return_barrier_exception) {\n+      __ movptr(c_rarg0, r15_thread);\n+      __ movptr(c_rarg1, Address(rsp, wordSize)); \/\/ return address\n+\n+      \/\/ rax still holds the original exception oop, save it before the call\n+      __ push(rax);\n+\n+      __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::exception_handler_for_return_address), 2);\n+      __ movptr(rbx, rax);\n+\n+      \/\/ Continue at exception handler:\n+      \/\/   rax: exception oop\n+      \/\/   rbx: exception handler\n+      \/\/   rdx: exception pc\n+      __ pop(rax);\n+      __ verify_oop(rax);\n+      __ pop(rbp); \/\/ pop out RBP here too\n+      __ pop(rdx);\n+      __ jmp(rbx);\n+    } else {\n+      \/\/ We are \"returning\" into the topmost thawed frame; see Thaw::push_return_frame\n+      __ pop(rbp);\n+      __ ret(0);\n+    }\n+\n+    return start;\n+  }\n+\n+  address generate_cont_thaw() {\n+    return generate_cont_thaw(\"Cont thaw\", Continuation::thaw_top);\n+  }\n+\n+  \/\/ TODO: will probably need multiple return barriers depending on return type\n+\n+  address generate_cont_returnBarrier() {\n+    return generate_cont_thaw(\"Cont thaw return barrier\", Continuation::thaw_return_barrier);\n+  }\n+\n+  address generate_cont_returnBarrier_exception() {\n+    return generate_cont_thaw(\"Cont thaw return barrier exception\", Continuation::thaw_return_barrier_exception);\n+  }\n+\n+#if INCLUDE_JFR\n+\n+  \/\/ For c2: c_rarg0 is junk, call to runtime to write a checkpoint.\n+  \/\/ It returns a jobject handle to the event writer.\n+  \/\/ The handle is dereferenced and the return value is the event writer oop.\n+  RuntimeStub* generate_jfr_write_checkpoint() {\n+    enum layout {\n+      rbp_off,\n+      rbpH_off,\n+      return_off,\n+      return_off2,\n+      framesize \/\/ inclusive of return address\n+    };\n+\n+    CodeBuffer code(\"jfr_write_checkpoint\", 512, 64);\n+    MacroAssembler* _masm = new MacroAssembler(&code);\n+\n+    address start = __ pc();\n+    __ enter();\n+    address the_pc = __ pc();\n+\n+    int frame_complete = the_pc - start;\n+\n+    __ set_last_Java_frame(rsp, rbp, the_pc);\n+    __ movptr(c_rarg0, r15_thread);\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, JfrIntrinsicSupport::write_checkpoint), 1);\n+    __ reset_last_Java_frame(true);\n+\n+    \/\/ rax is jobject handle result, unpack and process it through a barrier.\n+    Label L_null_jobject;\n+    __ testptr(rax, rax);\n+    __ jcc(Assembler::zero, L_null_jobject);\n+\n+    BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+    bs->load_at(_masm, ACCESS_READ | IN_NATIVE, T_OBJECT, rax, Address(rax, 0), c_rarg0, r15_thread);\n+\n+    __ bind(L_null_jobject);\n+\n+    __ leave();\n+    __ ret(0);\n+\n+    OopMapSet* oop_maps = new OopMapSet();\n+    OopMap* map = new OopMap(framesize, 1);\n+    oop_maps->add_gc_map(frame_complete, map);\n+\n+    RuntimeStub* stub =\n+      RuntimeStub::new_runtime_stub(code.name(),\n+                                    &code,\n+                                    frame_complete,\n+                                    (framesize >> (LogBytesPerWord - LogBytesPerInt)),\n+                                    oop_maps,\n+                                    false);\n+    return stub;\n+  }\n+\n+#endif \/\/ INCLUDE_JFR\n+\n@@ -7681,0 +8022,13 @@\n+  void generate_phase1() {\n+    \/\/ Continuation stubs:\n+    StubRoutines::_cont_thaw          = generate_cont_thaw();\n+    StubRoutines::_cont_returnBarrier = generate_cont_returnBarrier();\n+    StubRoutines::_cont_returnBarrierExc = generate_cont_returnBarrier_exception();\n+    StubRoutines::_cont_doYield_stub = generate_cont_doYield();\n+    StubRoutines::_cont_doYield      = StubRoutines::_cont_doYield_stub == nullptr ? nullptr\n+                                        : StubRoutines::_cont_doYield_stub->entry_point();\n+\n+    JFR_ONLY(StubRoutines::_jfr_write_checkpoint_stub = generate_jfr_write_checkpoint();)\n+    JFR_ONLY(StubRoutines::_jfr_write_checkpoint = StubRoutines::_jfr_write_checkpoint_stub->entry_point();)\n+  }\n+\n@@ -7726,0 +8080,5 @@\n+    StubRoutines::x86::_vector_count_leading_zeros_lut = generate_count_leading_zeros_lut(\"count_leading_zeros_lut\");\n+    StubRoutines::x86::_vector_reverse_bit_lut = generate_vector_reverse_bit_lut(\"reverse_bit_lut\");\n+    StubRoutines::x86::_vector_reverse_byte_perm_mask_long = generate_vector_reverse_byte_perm_mask_long(\"perm_mask_long\");\n+    StubRoutines::x86::_vector_reverse_byte_perm_mask_int = generate_vector_reverse_byte_perm_mask_int(\"perm_mask_int\");\n+    StubRoutines::x86::_vector_reverse_byte_perm_mask_short = generate_vector_reverse_byte_perm_mask_short(\"perm_mask_short\");\n@@ -7727,1 +8086,1 @@\n-    if (UsePopCountInstruction && VM_Version::supports_avx2() && !VM_Version::supports_avx512_vpopcntdq()) {\n+    if (VM_Version::supports_avx2() && !VM_Version::supports_avx512_vpopcntdq()) {\n@@ -7944,4 +8303,2 @@\n-  StubGenerator(CodeBuffer* code, bool all) : StubCodeGenerator(code) {\n-    if (all) {\n-      generate_all();\n-    } else {\n+  StubGenerator(CodeBuffer* code, int phase) : StubCodeGenerator(code) {\n+    if (phase == 0) {\n@@ -7949,0 +8306,4 @@\n+    } else if (phase == 1) {\n+      generate_phase1(); \/\/ stubs that must be available for the interpreter\n+    } else {\n+      generate_all();\n@@ -7954,1 +8315,1 @@\n-void StubGenerator_generate(CodeBuffer* code, bool all) {\n+void StubGenerator_generate(CodeBuffer* code, int phase) {\n@@ -7958,1 +8319,1 @@\n-  StubGenerator g(code, all);\n+  StubGenerator g(code, phase);\n@@ -7960,0 +8321,98 @@\n+\n+#undef __\n+#define __ masm->\n+\n+\/\/---------------------------- continuation_enter_setup ---------------------------\n+\/\/\n+\/\/ Arguments:\n+\/\/   None.\n+\/\/\n+\/\/ Results:\n+\/\/   rsp: pointer to blank ContinuationEntry\n+\/\/\n+\/\/ Kills:\n+\/\/   rax\n+\/\/\n+OopMap* continuation_enter_setup(MacroAssembler* masm, int& stack_slots) {\n+  assert(ContinuationEntry::size() % VMRegImpl::stack_slot_size == 0, \"\");\n+  assert(in_bytes(ContinuationEntry::cont_offset())  % VMRegImpl::stack_slot_size == 0, \"\");\n+  assert(in_bytes(ContinuationEntry::chunk_offset()) % VMRegImpl::stack_slot_size == 0, \"\");\n+\n+  stack_slots += checked_cast<int>(ContinuationEntry::size()) \/ wordSize;\n+  __ subptr(rsp, checked_cast<int32_t>(ContinuationEntry::size()));\n+\n+  int frame_size = (checked_cast<int>(ContinuationEntry::size()) + wordSize) \/ VMRegImpl::stack_slot_size;\n+  OopMap* map = new OopMap(frame_size, 0);\n+  ContinuationEntry::setup_oopmap(map);\n+\n+  __ movptr(rax, Address(r15_thread, JavaThread::cont_entry_offset()));\n+  __ movptr(Address(rsp, ContinuationEntry::parent_offset()), rax);\n+  __ movptr(Address(r15_thread, JavaThread::cont_entry_offset()), rsp);\n+\n+  return map;\n+}\n+\n+\/\/---------------------------- fill_continuation_entry ---------------------------\n+\/\/\n+\/\/ Arguments:\n+\/\/   rsp: pointer to blank Continuation entry\n+\/\/   reg_cont_obj: pointer to the continuation\n+\/\/   reg_flags: flags\n+\/\/\n+\/\/ Results:\n+\/\/   rsp: pointer to filled out ContinuationEntry\n+\/\/\n+\/\/ Kills:\n+\/\/   rax\n+\/\/\n+void fill_continuation_entry(MacroAssembler* masm, Register reg_cont_obj, Register reg_flags) {\n+  assert_different_registers(rax, reg_cont_obj, reg_flags);\n+\n+  DEBUG_ONLY(__ movl(Address(rsp, ContinuationEntry::cookie_offset()), ContinuationEntry::cookie_value());)\n+\n+  __ movptr(Address(rsp, ContinuationEntry::cont_offset()), reg_cont_obj);\n+  __ movl  (Address(rsp, ContinuationEntry::flags_offset()), reg_flags);\n+  __ movptr(Address(rsp, ContinuationEntry::chunk_offset()), 0);\n+  __ movl(Address(rsp, ContinuationEntry::argsize_offset()), 0);\n+  __ movl(Address(rsp, ContinuationEntry::pin_count_offset()), 0);\n+\n+  __ movptr(rax, Address(r15_thread, JavaThread::cont_fastpath_offset()));\n+  __ movptr(Address(rsp, ContinuationEntry::parent_cont_fastpath_offset()), rax);\n+  __ movq(rax, Address(r15_thread, JavaThread::held_monitor_count_offset()));\n+  __ movq(Address(rsp, ContinuationEntry::parent_held_monitor_count_offset()), rax);\n+\n+  __ movptr(Address(r15_thread, JavaThread::cont_fastpath_offset()), 0);\n+  __ movq(Address(r15_thread, JavaThread::held_monitor_count_offset()), 0);\n+}\n+\n+\/\/---------------------------- continuation_enter_cleanup ---------------------------\n+\/\/\n+\/\/ Arguments:\n+\/\/   rsp: pointer to the ContinuationEntry\n+\/\/\n+\/\/ Results:\n+\/\/   rsp: pointer to the spilled rbp in the entry frame\n+\/\/\n+\/\/ Kills:\n+\/\/   rbx\n+\/\/\n+void continuation_enter_cleanup(MacroAssembler* masm) {\n+#ifdef ASSERT\n+  Label L_good_sp;\n+  __ cmpptr(rsp, Address(r15_thread, JavaThread::cont_entry_offset()));\n+  __ jcc(Assembler::equal, L_good_sp);\n+  __ stop(\"Incorrect rsp at continuation_enter_cleanup\");\n+  __ bind(L_good_sp);\n+#endif\n+\n+  __ movptr(rbx, Address(rsp, ContinuationEntry::parent_cont_fastpath_offset()));\n+  __ movptr(Address(r15_thread, JavaThread::cont_fastpath_offset()), rbx);\n+  __ movq(rbx, Address(rsp, ContinuationEntry::parent_held_monitor_count_offset()));\n+  __ movq(Address(r15_thread, JavaThread::held_monitor_count_offset()), rbx);\n+\n+  __ movptr(rbx, Address(rsp, ContinuationEntry::parent_offset()));\n+  __ movptr(Address(r15_thread, JavaThread::cont_entry_offset()), rbx);\n+  __ addptr(rsp, (int32_t)ContinuationEntry::size());\n+}\n+\n+#undef __\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":467,"deletions":8,"binary":false,"changes":475,"status":"modified"},{"patch":"@@ -2595,0 +2595,1 @@\n+    __ push_cont_fastpath();\n@@ -2597,0 +2598,1 @@\n+    __ pop_cont_fastpath();\n@@ -3920,1 +3922,0 @@\n-  Label initialize_object;  \/\/ including clearing the fields\n@@ -3950,5 +3951,0 @@\n-  \/\/  Else If inline contiguous allocations are enabled:\n-  \/\/    Try to allocate in eden.\n-  \/\/    If fails due to heap end, go to slow path.\n-  \/\/\n-  \/\/  If TLAB is enabled OR inline contiguous is enabled:\n@@ -3960,8 +3956,0 @@\n-  const bool allow_shared_alloc =\n-    Universe::heap()->supports_inline_contig_alloc();\n-\n-#ifndef _LP64\n-  if (UseTLAB || allow_shared_alloc) {\n-    __ get_thread(thread);\n-  }\n-#endif \/\/ _LP64\n@@ -3971,0 +3959,1 @@\n+    NOT_LP64(__ get_thread(thread);)\n@@ -3975,12 +3964,0 @@\n-    } else {\n-      \/\/ initialize both the header and fields\n-      __ jmp(initialize_object);\n-  } else {\n-    \/\/ Allocation in the shared Eden, if allowed.\n-    \/\/\n-    \/\/ rdx: instance size in bytes\n-    __ eden_allocate(thread, rax, rdx, 0, rbx, slow_case);\n-  }\n-  \/\/ If UseTLAB or allow_shared_alloc are true, the object is created above and\n-  \/\/ there is an initialize need. Otherwise, skip and go to the slow path.\n-  if (UseTLAB || allow_shared_alloc) {\n@@ -3991,1 +3968,0 @@\n-    __ bind(initialize_object);\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.cpp","additions":3,"deletions":27,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -341,2 +341,0 @@\n-extern RegMask _STACK_OR_LONG_NO_RAX_RDX_REG_mask;\n-extern RegMask _STACK_OR_INT_NO_RAX_RDX_REG_mask;\n@@ -347,6 +345,0 @@\n-inline const RegMask& STACK_OR_LONG_NO_RAX_RDX_REG_mask() {\n-  return _STACK_OR_LONG_NO_RAX_RDX_REG_mask;\n-}\n-inline const RegMask& STACK_OR_INT_NO_RAX_RDX_REG_mask() {\n-  return _STACK_OR_INT_NO_RAX_RDX_REG_mask;\n-}\n@@ -379,2 +371,0 @@\n-RegMask _STACK_OR_LONG_NO_RAX_RDX_REG_mask;\n-RegMask _STACK_OR_INT_NO_RAX_RDX_REG_mask;\n@@ -442,3 +432,0 @@\n-  _STACK_OR_LONG_NO_RAX_RDX_REG_mask = _LONG_NO_RAX_RDX_REG_mask;\n-  _STACK_OR_LONG_NO_RAX_RDX_REG_mask.OR(STACK_OR_STACK_SLOTS_mask());\n-\n@@ -467,3 +454,0 @@\n-  _STACK_OR_INT_NO_RAX_RDX_REG_mask = _INT_NO_RAX_RDX_REG_mask;\n-  _STACK_OR_INT_NO_RAX_RDX_REG_mask.OR(STACK_OR_STACK_SLOTS_mask());\n-\n@@ -507,6 +491,0 @@\n-\n-int MachCallNativeNode::ret_addr_offset() {\n-  int offset = 13; \/\/ movq r10,#addr; callq (r10)\n-  offset += clear_avx_size();\n-  return offset;\n-}\n@@ -919,1 +897,1 @@\n-  MacroAssembler _masm(&cbuf);\n+  C2_MacroAssembler _masm(&cbuf);\n@@ -1926,0 +1904,117 @@\n+  enc_class cdql_enc(no_rax_rdx_RegI div)\n+  %{\n+    \/\/ Full implementation of Java idiv and irem; checks for\n+    \/\/ special case as described in JVM spec., p.243 & p.271.\n+    \/\/\n+    \/\/         normal case                           special case\n+    \/\/\n+    \/\/ input : rax: dividend                         min_int\n+    \/\/         reg: divisor                          -1\n+    \/\/\n+    \/\/ output: rax: quotient  (= rax idiv reg)       min_int\n+    \/\/         rdx: remainder (= rax irem reg)       0\n+    \/\/\n+    \/\/  Code sequnce:\n+    \/\/\n+    \/\/    0:   3d 00 00 00 80          cmp    $0x80000000,%eax\n+    \/\/    5:   75 07\/08                jne    e <normal>\n+    \/\/    7:   33 d2                   xor    %edx,%edx\n+    \/\/  [div >= 8 -> offset + 1]\n+    \/\/  [REX_B]\n+    \/\/    9:   83 f9 ff                cmp    $0xffffffffffffffff,$div\n+    \/\/    c:   74 03\/04                je     11 <done>\n+    \/\/ 000000000000000e <normal>:\n+    \/\/    e:   99                      cltd\n+    \/\/  [div >= 8 -> offset + 1]\n+    \/\/  [REX_B]\n+    \/\/    f:   f7 f9                   idiv   $div\n+    \/\/ 0000000000000011 <done>:\n+    MacroAssembler _masm(&cbuf);\n+    Label normal;\n+    Label done;\n+\n+    \/\/ cmp    $0x80000000,%eax\n+    __ cmpl(as_Register(RAX_enc), 0x80000000);\n+\n+    \/\/ jne    e <normal>\n+    __ jccb(Assembler::notEqual, normal);\n+\n+    \/\/ xor    %edx,%edx\n+    __ xorl(as_Register(RDX_enc), as_Register(RDX_enc));\n+\n+    \/\/ cmp    $0xffffffffffffffff,%ecx\n+    __ cmpl($div$$Register, -1);\n+\n+    \/\/ je     11 <done>\n+    __ jccb(Assembler::equal, done);\n+\n+    \/\/ <normal>\n+    \/\/ cltd\n+    __ bind(normal);\n+    __ cdql();\n+\n+    \/\/ idivl\n+    \/\/ <done>\n+    __ idivl($div$$Register);\n+    __ bind(done);\n+  %}\n+\n+  enc_class cdqq_enc(no_rax_rdx_RegL div)\n+  %{\n+    \/\/ Full implementation of Java ldiv and lrem; checks for\n+    \/\/ special case as described in JVM spec., p.243 & p.271.\n+    \/\/\n+    \/\/         normal case                           special case\n+    \/\/\n+    \/\/ input : rax: dividend                         min_long\n+    \/\/         reg: divisor                          -1\n+    \/\/\n+    \/\/ output: rax: quotient  (= rax idiv reg)       min_long\n+    \/\/         rdx: remainder (= rax irem reg)       0\n+    \/\/\n+    \/\/  Code sequnce:\n+    \/\/\n+    \/\/    0:   48 ba 00 00 00 00 00    mov    $0x8000000000000000,%rdx\n+    \/\/    7:   00 00 80\n+    \/\/    a:   48 39 d0                cmp    %rdx,%rax\n+    \/\/    d:   75 08                   jne    17 <normal>\n+    \/\/    f:   33 d2                   xor    %edx,%edx\n+    \/\/   11:   48 83 f9 ff             cmp    $0xffffffffffffffff,$div\n+    \/\/   15:   74 05                   je     1c <done>\n+    \/\/ 0000000000000017 <normal>:\n+    \/\/   17:   48 99                   cqto\n+    \/\/   19:   48 f7 f9                idiv   $div\n+    \/\/ 000000000000001c <done>:\n+    MacroAssembler _masm(&cbuf);\n+    Label normal;\n+    Label done;\n+\n+    \/\/ mov    $0x8000000000000000,%rdx\n+    __ mov64(as_Register(RDX_enc), 0x8000000000000000);\n+\n+    \/\/ cmp    %rdx,%rax\n+    __ cmpq(as_Register(RAX_enc), as_Register(RDX_enc));\n+\n+    \/\/ jne    17 <normal>\n+    __ jccb(Assembler::notEqual, normal);\n+\n+    \/\/ xor    %edx,%edx\n+    __ xorl(as_Register(RDX_enc), as_Register(RDX_enc));\n+\n+    \/\/ cmp    $0xffffffffffffffff,$div\n+    __ cmpq($div$$Register, -1);\n+\n+    \/\/ je     1e <done>\n+    __ jccb(Assembler::equal, done);\n+\n+    \/\/ <normal>\n+    \/\/ cqto\n+    __ bind(normal);\n+    __ cdqq();\n+\n+    \/\/ idivq (note: must be emitted by the user of this rule)\n+    \/\/ <done>\n+    __ idivq($div$$Register);\n+    __ bind(done);\n+  %}\n+\n@@ -2054,13 +2149,1 @@\n-  %}\n-\n-  enc_class Java_To_Interpreter(method meth)\n-  %{\n-    \/\/ CALL Java_To_Interpreter\n-    \/\/ This is the instruction starting address for relocation info.\n-    cbuf.set_insts_mark();\n-    $$$emit8$primary;\n-    \/\/ CALL directly to the runtime\n-    emit_d32_reloc(cbuf,\n-                   (int) ($meth$$method - ((intptr_t) cbuf.insts_end()) - 4),\n-                   runtime_call_Relocation::spec(),\n-                   RELOC_DISP32);\n+    __ post_call_nop();\n@@ -2074,0 +2157,1 @@\n+    MacroAssembler _masm(&cbuf);\n@@ -2087,5 +2171,11 @@\n-      \/\/ Emit stubs for static call.\n-      address stub = CompiledStaticCall::emit_to_interp_stub(cbuf, mark);\n-      if (stub == NULL) {\n-        ciEnv::current()->record_failure(\"CodeCache is full\");\n-        return;\n+      if (CodeBuffer::supports_shared_stubs() && _method->can_be_statically_bound()) {\n+        \/\/ Calls of the same statically bound method can share\n+        \/\/ a stub to the interpreter.\n+        cbuf.shared_stub_to_interp_for(_method, cbuf.insts()->mark_off());\n+      } else {\n+        \/\/ Emit stubs for static call.\n+        address stub = CompiledStaticCall::emit_to_interp_stub(cbuf, mark);\n+        if (stub == NULL) {\n+          ciEnv::current()->record_failure(\"CodeCache is full\");\n+          return;\n+        }\n@@ -2095,0 +2185,2 @@\n+    _masm.clear_inst_mark();\n+    __ post_call_nop();\n@@ -2100,20 +2192,1 @@\n-  %}\n-\n-  enc_class Java_Compiled_Call(method meth)\n-  %{\n-    \/\/ JAVA COMPILED CALL\n-    int disp = in_bytes(Method:: from_compiled_offset());\n-\n-    \/\/ XXX XXX offset is 128 is 1.5 NON-PRODUCT !!!\n-    \/\/ assert(-0x80 <= disp && disp < 0x80, \"compiled_code_offset isn't small\");\n-\n-    \/\/ callq *disp(%rax)\n-    cbuf.set_insts_mark();\n-    $$$emit8$primary;\n-    if (disp < 0x80) {\n-      emit_rm(cbuf, 0x01, $secondary, RAX_enc); \/\/ R\/M byte\n-      emit_d8(cbuf, disp); \/\/ Displacement\n-    } else {\n-      emit_rm(cbuf, 0x02, $secondary, RAX_enc); \/\/ R\/M byte\n-      emit_d32(cbuf, disp); \/\/ Displacement\n-    }\n+    __ post_call_nop();\n@@ -4204,1 +4277,1 @@\n-    greater_equal(0x3, \"nb\");\n+    greater_equal(0x3, \"ae\");\n@@ -4206,1 +4279,1 @@\n-    greater(0x7, \"nbe\");\n+    greater(0x7, \"a\");\n@@ -4213,1 +4286,3 @@\n-\/\/ Floating comparisons that don't require any fixup for the unordered case\n+\/\/ Floating comparisons that don't require any fixup for the unordered case,\n+\/\/ If both inputs of the comparison are the same, ZF is always set so we\n+\/\/ don't need to use cmpOpUCF2 for eq\/ne\n@@ -4219,1 +4294,2 @@\n-            n->as_Bool()->_test._test == BoolTest::gt);\n+            n->as_Bool()->_test._test == BoolTest::gt ||\n+            n->in(1)->in(1) == n->in(1)->in(2));\n@@ -4222,2 +4298,2 @@\n-    equal(0x4, \"e\");\n-    not_equal(0x5, \"ne\");\n+    equal(0xb, \"np\");\n+    not_equal(0xa, \"p\");\n@@ -4225,1 +4301,1 @@\n-    greater_equal(0x3, \"nb\");\n+    greater_equal(0x3, \"ae\");\n@@ -4227,1 +4303,1 @@\n-    greater(0x7, \"nbe\");\n+    greater(0x7, \"a\");\n@@ -4237,2 +4313,3 @@\n-  predicate(n->as_Bool()->_test._test == BoolTest::ne ||\n-            n->as_Bool()->_test._test == BoolTest::eq);\n+  predicate((n->as_Bool()->_test._test == BoolTest::ne ||\n+             n->as_Bool()->_test._test == BoolTest::eq) &&\n+            n->in(1)->in(1) != n->in(1)->in(2));\n@@ -4244,1 +4321,1 @@\n-    greater_equal(0x3, \"nb\");\n+    greater_equal(0x3, \"ae\");\n@@ -4246,1 +4323,1 @@\n-    greater(0x7, \"nbe\");\n+    greater(0x7, \"a\");\n@@ -6475,0 +6552,12 @@\n+instruct countLeadingZerosI_mem(rRegI dst, memory src, rFlagsReg cr) %{\n+  predicate(UseCountLeadingZerosInstruction);\n+  match(Set dst (CountLeadingZerosI (LoadI src)));\n+  effect(KILL cr);\n+  ins_cost(175);\n+  format %{ \"lzcntl  $dst, $src\\t# count leading zeros (int)\" %}\n+  ins_encode %{\n+    __ lzcntl($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n@@ -6512,0 +6601,12 @@\n+instruct countLeadingZerosL_mem(rRegI dst, memory src, rFlagsReg cr) %{\n+  predicate(UseCountLeadingZerosInstruction);\n+  match(Set dst (CountLeadingZerosL (LoadL src)));\n+  effect(KILL cr);\n+  ins_cost(175);\n+  format %{ \"lzcntq  $dst, $src\\t# count leading zeros (long)\" %}\n+  ins_encode %{\n+    __ lzcntq($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n@@ -6549,0 +6650,12 @@\n+instruct countTrailingZerosI_mem(rRegI dst, memory src, rFlagsReg cr) %{\n+  predicate(UseCountTrailingZerosInstruction);\n+  match(Set dst (CountTrailingZerosI (LoadI src)));\n+  effect(KILL cr);\n+  ins_cost(175);\n+  format %{ \"tzcntl    $dst, $src\\t# count trailing zeros (int)\" %}\n+  ins_encode %{\n+    __ tzcntl($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n@@ -6581,0 +6694,12 @@\n+instruct countTrailingZerosL_mem(rRegI dst, memory src, rFlagsReg cr) %{\n+  predicate(UseCountTrailingZerosInstruction);\n+  match(Set dst (CountTrailingZerosL (LoadL src)));\n+  effect(KILL cr);\n+  ins_cost(175);\n+  format %{ \"tzcntq    $dst, $src\\t# count trailing zeros (long)\" %}\n+  ins_encode %{\n+    __ tzcntq($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n@@ -6944,0 +7069,14 @@\n+instruct cmovI_imm_01(rRegI dst, immI_1 src, rFlagsReg cr, cmpOp cop)\n+%{\n+  predicate(n->in(2)->in(2)->is_Con() && n->in(2)->in(2)->get_int() == 0);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary src dst)));\n+\n+  ins_cost(100); \/\/ XXX\n+  format %{ \"setbn$cop $dst\\t# signed, int\" %}\n+  ins_encode %{\n+    Assembler::Condition cond = (Assembler::Condition)($cop$$cmpcode);\n+    __ setb(MacroAssembler::negate_condition(cond), $dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n@@ -6956,0 +7095,14 @@\n+instruct cmovI_imm_01U(rRegI dst, immI_1 src, rFlagsRegU cr, cmpOpU cop)\n+%{\n+  predicate(n->in(2)->in(2)->is_Con() && n->in(2)->in(2)->get_int() == 0);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary src dst)));\n+\n+  ins_cost(100); \/\/ XXX\n+  format %{ \"setbn$cop $dst\\t# unsigned, int\" %}\n+  ins_encode %{\n+    Assembler::Condition cond = (Assembler::Condition)($cop$$cmpcode);\n+    __ setb(MacroAssembler::negate_condition(cond), $dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n@@ -6967,0 +7120,14 @@\n+instruct cmovI_imm_01UCF(rRegI dst, immI_1 src, rFlagsRegUCF cr, cmpOpUCF cop)\n+%{\n+  predicate(n->in(2)->in(2)->is_Con() && n->in(2)->in(2)->get_int() == 0);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary src dst)));\n+\n+  ins_cost(100); \/\/ XXX\n+  format %{ \"setbn$cop $dst\\t# unsigned, int\" %}\n+  ins_encode %{\n+    Assembler::Condition cond = (Assembler::Condition)($cop$$cmpcode);\n+    __ setb(MacroAssembler::negate_condition(cond), $dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n@@ -6975,0 +7142,30 @@\n+instruct cmovI_regUCF2_ne(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegI dst, rRegI src) %{\n+  predicate(n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::ne);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovpl  $dst, $src\\n\\t\"\n+            \"cmovnel $dst, $src\" %}\n+  ins_encode %{\n+    __ cmovl(Assembler::parity, $dst$$Register, $src$$Register);\n+    __ cmovl(Assembler::notEqual, $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+\/\/ Since (x == y) == !(x != y), we can flip the sense of the test by flipping the\n+\/\/ inputs of the CMove\n+instruct cmovI_regUCF2_eq(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegI dst, rRegI src) %{\n+  predicate(n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::eq);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary src dst)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovpl  $dst, $src\\n\\t\"\n+            \"cmovnel $dst, $src\" %}\n+  ins_encode %{\n+    __ cmovl(Assembler::parity, $dst$$Register, $src$$Register);\n+    __ cmovl(Assembler::notEqual, $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n@@ -7042,0 +7239,30 @@\n+instruct cmovN_regUCF2_ne(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegN dst, rRegN src) %{\n+  predicate(n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::ne);\n+  match(Set dst (CMoveN (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovpl  $dst, $src\\n\\t\"\n+            \"cmovnel $dst, $src\" %}\n+  ins_encode %{\n+    __ cmovl(Assembler::parity, $dst$$Register, $src$$Register);\n+    __ cmovl(Assembler::notEqual, $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+\/\/ Since (x == y) == !(x != y), we can flip the sense of the test by flipping the\n+\/\/ inputs of the CMove\n+instruct cmovN_regUCF2_eq(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegN dst, rRegN src) %{\n+  predicate(n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::eq);\n+  match(Set dst (CMoveN (Binary cop cr) (Binary src dst)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovpl  $dst, $src\\n\\t\"\n+            \"cmovnel $dst, $src\" %}\n+  ins_encode %{\n+    __ cmovl(Assembler::parity, $dst$$Register, $src$$Register);\n+    __ cmovl(Assembler::notEqual, $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n@@ -7076,0 +7303,30 @@\n+instruct cmovP_regUCF2_ne(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegP dst, rRegP src) %{\n+  predicate(n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::ne);\n+  match(Set dst (CMoveP (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovpq  $dst, $src\\n\\t\"\n+            \"cmovneq $dst, $src\" %}\n+  ins_encode %{\n+    __ cmovq(Assembler::parity, $dst$$Register, $src$$Register);\n+    __ cmovq(Assembler::notEqual, $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+\/\/ Since (x == y) == !(x != y), we can flip the sense of the test by flipping the\n+\/\/ inputs of the CMove\n+instruct cmovP_regUCF2_eq(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegP dst, rRegP src) %{\n+  predicate(n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::eq);\n+  match(Set dst (CMoveP (Binary cop cr) (Binary src dst)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovpq  $dst, $src\\n\\t\"\n+            \"cmovneq $dst, $src\" %}\n+  ins_encode %{\n+    __ cmovq(Assembler::parity, $dst$$Register, $src$$Register);\n+    __ cmovq(Assembler::notEqual, $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n@@ -7103,0 +7360,14 @@\n+instruct cmovL_imm_01(rRegL dst, immI_1 src, rFlagsReg cr, cmpOp cop)\n+%{\n+  predicate(n->in(2)->in(2)->is_Con() && n->in(2)->in(2)->get_long() == 0);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary src dst)));\n+\n+  ins_cost(100); \/\/ XXX\n+  format %{ \"setbn$cop $dst\\t# signed, long\" %}\n+  ins_encode %{\n+    Assembler::Condition cond = (Assembler::Condition)($cop$$cmpcode);\n+    __ setb(MacroAssembler::negate_condition(cond), $dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n@@ -7127,0 +7398,14 @@\n+instruct cmovL_imm_01U(rRegL dst, immI_1 src, rFlagsRegU cr, cmpOpU cop)\n+%{\n+  predicate(n->in(2)->in(2)->is_Con() && n->in(2)->in(2)->get_long() == 0);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary src dst)));\n+\n+  ins_cost(100); \/\/ XXX\n+  format %{ \"setbn$cop $dst\\t# unsigned, long\" %}\n+  ins_encode %{\n+    Assembler::Condition cond = (Assembler::Condition)($cop$$cmpcode);\n+    __ setb(MacroAssembler::negate_condition(cond), $dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n@@ -7139,0 +7424,14 @@\n+instruct cmovL_imm_01UCF(rRegL dst, immI_1 src, rFlagsRegUCF cr, cmpOpUCF cop)\n+%{\n+  predicate(n->in(2)->in(2)->is_Con() && n->in(2)->in(2)->get_long() == 0);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary src dst)));\n+\n+  ins_cost(100); \/\/ XXX\n+  format %{ \"setbn$cop $dst\\t# unsigned, long\" %}\n+  ins_encode %{\n+    Assembler::Condition cond = (Assembler::Condition)($cop$$cmpcode);\n+    __ setb(MacroAssembler::negate_condition(cond), $dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n@@ -7147,0 +7446,30 @@\n+instruct cmovL_regUCF2_ne(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegL dst, rRegL src) %{\n+  predicate(n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::ne);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovpq  $dst, $src\\n\\t\"\n+            \"cmovneq $dst, $src\" %}\n+  ins_encode %{\n+    __ cmovq(Assembler::parity, $dst$$Register, $src$$Register);\n+    __ cmovq(Assembler::notEqual, $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+\/\/ Since (x == y) == !(x != y), we can flip the sense of the test by flipping the\n+\/\/ inputs of the CMove\n+instruct cmovL_regUCF2_eq(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegL dst, rRegL src) %{\n+  predicate(n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::eq);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary src dst)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovpq  $dst, $src\\n\\t\"\n+            \"cmovneq $dst, $src\" %}\n+  ins_encode %{\n+    __ cmovq(Assembler::parity, $dst$$Register, $src$$Register);\n+    __ cmovq(Assembler::notEqual, $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n@@ -8556,17 +8885,1 @@\n-  match(Set rax (NoOvfDivI rax div));\n-  effect(KILL rdx, KILL cr);\n-\n-  ins_cost(500);\n-  format %{ \"cdql\\n\\t\"\n-            \"idivl   $div\" %}\n-  ins_encode %{\n-    __ cdql();\n-    __ idivl($div$$Register);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct divI_mem(rax_RegI rax, rdx_RegI rdx, memory div,\n-                   rFlagsReg cr)\n-%{\n-  match(Set rax (NoOvfDivI rax (LoadI div)));\n+  match(Set rax (DivI rax div));\n@@ -8575,8 +8888,11 @@\n-  ins_cost(575);\n-  format %{ \"cdql\\n\\t\"\n-            \"idivl   $div\" %}\n-  ins_encode %{\n-    __ cdql();\n-    __ idivl($div$$Address);\n-  %}\n-  ins_pipe(pipe_slow);\n+  ins_cost(30*100+10*100); \/\/ XXX\n+  format %{ \"cmpl    rax, 0x80000000\\t# idiv\\n\\t\"\n+            \"jne,s   normal\\n\\t\"\n+            \"xorl    rdx, rdx\\n\\t\"\n+            \"cmpl    $div, -1\\n\\t\"\n+            \"je,s    done\\n\"\n+    \"normal: cdql\\n\\t\"\n+            \"idivl   $div\\n\"\n+    \"done:\"        %}\n+  ins_encode(cdql_enc(div));\n+  ins_pipe(ialu_reg_reg_alu0);\n@@ -8588,17 +8904,1 @@\n-  match(Set rax (NoOvfDivL rax div));\n-  effect(KILL rdx, KILL cr);\n-\n-  ins_cost(500);\n-  format %{ \"cdqq\\n\\t\"\n-            \"idivq   $div\" %}\n-  ins_encode %{\n-    __ cdqq();\n-    __ idivq($div$$Register);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct divL_mem(rax_RegL rax, rdx_RegL rdx, memory div,\n-                   rFlagsReg cr)\n-%{\n-  match(Set rax (NoOvfDivL rax (LoadL div)));\n+  match(Set rax (DivL rax div));\n@@ -8607,8 +8907,12 @@\n-  ins_cost(575);\n-  format %{ \"cdqq\\n\\t\"\n-            \"idivq   $div\" %}\n-  ins_encode %{\n-    __ cdqq();\n-    __ idivq($div$$Address);\n-  %}\n-  ins_pipe(pipe_slow);\n+  ins_cost(30*100+10*100); \/\/ XXX\n+  format %{ \"movq    rdx, 0x8000000000000000\\t# ldiv\\n\\t\"\n+            \"cmpq    rax, rdx\\n\\t\"\n+            \"jne,s   normal\\n\\t\"\n+            \"xorl    rdx, rdx\\n\\t\"\n+            \"cmpq    $div, -1\\n\\t\"\n+            \"je,s    done\\n\"\n+    \"normal: cdqq\\n\\t\"\n+            \"idivq   $div\\n\"\n+    \"done:\"        %}\n+  ins_encode(cdqq_enc(div));\n+  ins_pipe(ialu_reg_reg_alu0);\n@@ -8647,1 +8951,1 @@\n-  match(NoOvfDivModI rax div);\n+  match(DivModI rax div);\n@@ -8650,7 +8954,10 @@\n-  ins_cost(500);\n-  format %{ \"cdql\\n\\t\"\n-            \"idivl   $div\" %}\n-  ins_encode %{\n-    __ cdql();\n-    __ idivl($div$$Register);\n-  %}\n+  ins_cost(30*100+10*100); \/\/ XXX\n+  format %{ \"cmpl    rax, 0x80000000\\t# idiv\\n\\t\"\n+            \"jne,s   normal\\n\\t\"\n+            \"xorl    rdx, rdx\\n\\t\"\n+            \"cmpl    $div, -1\\n\\t\"\n+            \"je,s    done\\n\"\n+    \"normal: cdql\\n\\t\"\n+            \"idivl   $div\\n\"\n+    \"done:\"        %}\n+  ins_encode(cdql_enc(div));\n@@ -8664,1 +8971,1 @@\n-  match(NoOvfDivModL rax div);\n+  match(DivModL rax div);\n@@ -8667,7 +8974,11 @@\n-  ins_cost(500);\n-  format %{ \"cdqq\\n\\t\"\n-            \"idivq   $div\" %}\n-  ins_encode %{\n-    __ cdqq();\n-    __ idivq($div$$Register);\n-  %}\n+  ins_cost(30*100+10*100); \/\/ XXX\n+  format %{ \"movq    rdx, 0x8000000000000000\\t# ldiv\\n\\t\"\n+            \"cmpq    rax, rdx\\n\\t\"\n+            \"jne,s   normal\\n\\t\"\n+            \"xorl    rdx, rdx\\n\\t\"\n+            \"cmpq    $div, -1\\n\\t\"\n+            \"je,s    done\\n\"\n+    \"normal: cdqq\\n\\t\"\n+            \"idivq   $div\\n\"\n+    \"done:\"        %}\n+  ins_encode(cdqq_enc(div));\n@@ -8711,2 +9022,6 @@\n-instruct modI_rReg(rdx_RegI rdx, rax_RegI rax, no_rax_rdx_RegI div,\n-                   rFlagsReg cr)\n+\n+\/\/----------- DivL-By-Constant-Expansions--------------------------------------\n+\/\/ DivI cases are handled by the compiler\n+\n+\/\/ Magic constant, reciprocal of 10\n+instruct loadConL_0x6666666666666667(rRegL dst)\n@@ -8714,2 +9029,1 @@\n-  match(Set rdx (NoOvfModI rax div));\n-  effect(KILL rax, KILL cr);\n+  effect(DEF dst);\n@@ -8717,8 +9031,3 @@\n-  ins_cost(500);\n-  format %{ \"cdql\\n\\t\"\n-            \"idivl   $div\" %}\n-  ins_encode %{\n-    __ cdql();\n-    __ idivl($div$$Register);\n-  %}\n-  ins_pipe(pipe_slow);\n+  format %{ \"movq    $dst, #0x666666666666667\\t# Used in div-by-10\" %}\n+  ins_encode(load_immL(dst, 0x6666666666666667));\n+  ins_pipe(ialu_reg);\n@@ -8727,2 +9036,1 @@\n-instruct modI_mem(rdx_RegI rdx, rax_RegI rax, memory div,\n-                   rFlagsReg cr)\n+instruct mul_hi(rdx_RegL dst, no_rax_RegL src, rax_RegL rax, rFlagsReg cr)\n@@ -8730,2 +9038,1 @@\n-  match(Set rdx (NoOvfModI rax (LoadI div)));\n-  effect(KILL rax, KILL cr);\n+  effect(DEF dst, USE src, USE_KILL rax, KILL cr);\n@@ -8733,3 +9040,1 @@\n-  ins_cost(575);\n-  format %{ \"cdql\\n\\t\"\n-            \"idivl   $div\" %}\n+  format %{ \"imulq   rdx:rax, rax, $src\\t# Used in div-by-10\" %}\n@@ -8737,2 +9042,1 @@\n-    __ cdql();\n-    __ idivl($div$$Address);\n+    __ imulq($src$$Register);\n@@ -8740,1 +9044,1 @@\n-  ins_pipe(pipe_slow);\n+  ins_pipe(ialu_reg_reg_alu0);\n@@ -8743,2 +9047,1 @@\n-instruct modL_rReg(rdx_RegL rdx, rax_RegL rax, no_rax_rdx_RegL div,\n-                   rFlagsReg cr)\n+instruct sarL_rReg_63(rRegL dst, rFlagsReg cr)\n@@ -8746,2 +9049,1 @@\n-  match(Set rdx (NoOvfModL rax div));\n-  effect(KILL rax, KILL cr);\n+  effect(USE_DEF dst, KILL cr);\n@@ -8749,3 +9051,1 @@\n-  ins_cost(500);\n-  format %{ \"cdqq\\n\\t\"\n-            \"idivq   $div\" %}\n+  format %{ \"sarq    $dst, #63\\t# Used in div-by-10\" %}\n@@ -8753,2 +9053,1 @@\n-    __ cdqq();\n-    __ idivq($div$$Register);\n+    __ sarq($dst$$Register, 63);\n@@ -8756,1 +9055,1 @@\n-  ins_pipe(pipe_slow);\n+  ins_pipe(ialu_reg);\n@@ -8759,2 +9058,1 @@\n-instruct modL_mem(rdx_RegL rdx, rax_RegL rax, memory div,\n-                   rFlagsReg cr)\n+instruct sarL_rReg_2(rRegL dst, rFlagsReg cr)\n@@ -8762,2 +9060,1 @@\n-  match(Set rdx (NoOvfModL rax (LoadL div)));\n-  effect(KILL rax, KILL cr);\n+  effect(USE_DEF dst, KILL cr);\n@@ -8765,3 +9062,1 @@\n-  ins_cost(575);\n-  format %{ \"cdqq\\n\\t\"\n-            \"idivq   $div\" %}\n+  format %{ \"sarq    $dst, #2\\t# Used in div-by-10\" %}\n@@ -8769,2 +9064,1 @@\n-    __ cdqq();\n-    __ idivq($div$$Address);\n+    __ sarq($dst$$Register, 2);\n@@ -8772,1 +9066,58 @@\n-  ins_pipe(pipe_slow);\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct divL_10(rdx_RegL dst, no_rax_RegL src, immL10 div)\n+%{\n+  match(Set dst (DivL src div));\n+\n+  ins_cost((5+8)*100);\n+  expand %{\n+    rax_RegL rax;                     \/\/ Killed temp\n+    rFlagsReg cr;                     \/\/ Killed\n+    loadConL_0x6666666666666667(rax); \/\/ movq  rax, 0x6666666666666667\n+    mul_hi(dst, src, rax, cr);        \/\/ mulq  rdx:rax <= rax * $src\n+    sarL_rReg_63(src, cr);            \/\/ sarq  src, 63\n+    sarL_rReg_2(dst, cr);             \/\/ sarq  rdx, 2\n+    subL_rReg(dst, src, cr);          \/\/ subl  rdx, src\n+  %}\n+%}\n+\n+\/\/-----------------------------------------------------------------------------\n+\n+instruct modI_rReg(rdx_RegI rdx, rax_RegI rax, no_rax_rdx_RegI div,\n+                   rFlagsReg cr)\n+%{\n+  match(Set rdx (ModI rax div));\n+  effect(KILL rax, KILL cr);\n+\n+  ins_cost(300); \/\/ XXX\n+  format %{ \"cmpl    rax, 0x80000000\\t# irem\\n\\t\"\n+            \"jne,s   normal\\n\\t\"\n+            \"xorl    rdx, rdx\\n\\t\"\n+            \"cmpl    $div, -1\\n\\t\"\n+            \"je,s    done\\n\"\n+    \"normal: cdql\\n\\t\"\n+            \"idivl   $div\\n\"\n+    \"done:\"        %}\n+  ins_encode(cdql_enc(div));\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct modL_rReg(rdx_RegL rdx, rax_RegL rax, no_rax_rdx_RegL div,\n+                   rFlagsReg cr)\n+%{\n+  match(Set rdx (ModL rax div));\n+  effect(KILL rax, KILL cr);\n+\n+  ins_cost(300); \/\/ XXX\n+  format %{ \"movq    rdx, 0x8000000000000000\\t# lrem\\n\\t\"\n+            \"cmpq    rax, rdx\\n\\t\"\n+            \"jne,s   normal\\n\\t\"\n+            \"xorl    rdx, rdx\\n\\t\"\n+            \"cmpq    $div, -1\\n\\t\"\n+            \"je,s    done\\n\"\n+    \"normal: cdqq\\n\\t\"\n+            \"idivq   $div\\n\"\n+    \"done:\"        %}\n+  ins_encode(cdqq_enc(div));\n+  ins_pipe(ialu_reg_reg_alu0);\n@@ -8857,0 +9208,1 @@\n+  predicate(!VM_Version::supports_bmi2());\n@@ -8870,0 +9222,1 @@\n+  predicate(!VM_Version::supports_bmi2());\n@@ -8880,0 +9233,24 @@\n+instruct salI_rReg_rReg(rRegI dst, rRegI src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (LShiftI src shift));\n+\n+  format %{ \"shlxl   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ shlxl($dst$$Register, $src$$Register, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct salI_mem_rReg(rRegI dst, memory src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (LShiftI (LoadI src) shift));\n+  ins_cost(175);\n+  format %{ \"shlxl   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ shlxl($dst$$Register, $src$$Address, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n@@ -8935,0 +9312,1 @@\n+  predicate(!VM_Version::supports_bmi2());\n@@ -8947,0 +9325,1 @@\n+  predicate(!VM_Version::supports_bmi2());\n@@ -8957,0 +9336,24 @@\n+instruct sarI_rReg_rReg(rRegI dst, rRegI src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (RShiftI src shift));\n+\n+  format %{ \"sarxl   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ sarxl($dst$$Register, $src$$Register, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct sarI_mem_rReg(rRegI dst, memory src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (RShiftI (LoadI src) shift));\n+  ins_cost(175);\n+  format %{ \"sarxl   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ sarxl($dst$$Register, $src$$Address, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n@@ -9012,0 +9415,1 @@\n+  predicate(!VM_Version::supports_bmi2());\n@@ -9025,0 +9429,1 @@\n+  predicate(!VM_Version::supports_bmi2());\n@@ -9035,0 +9440,24 @@\n+instruct shrI_rReg_rReg(rRegI dst, rRegI src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (URShiftI src shift));\n+\n+  format %{ \"shrxl   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ shrxl($dst$$Register, $src$$Register, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct shrI_mem_rReg(rRegI dst, memory src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (URShiftI (LoadI src) shift));\n+  ins_cost(175);\n+  format %{ \"shrxl   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ shrxl($dst$$Register, $src$$Address, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n@@ -9091,0 +9520,1 @@\n+  predicate(!VM_Version::supports_bmi2());\n@@ -9104,0 +9534,1 @@\n+  predicate(!VM_Version::supports_bmi2());\n@@ -9114,0 +9545,24 @@\n+instruct salL_rReg_rReg(rRegL dst, rRegL src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (LShiftL src shift));\n+\n+  format %{ \"shlxq   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ shlxq($dst$$Register, $src$$Register, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct salL_mem_rReg(rRegL dst, memory src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (LShiftL (LoadL src) shift));\n+  ins_cost(175);\n+  format %{ \"shlxq   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ shlxq($dst$$Register, $src$$Address, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n@@ -9141,1 +9596,1 @@\n-instruct sarL_rReg_imm(rRegL dst, immI8 shift, rFlagsReg cr)\n+instruct sarL_rReg_imm(rRegL dst, immI shift, rFlagsReg cr)\n@@ -9154,1 +9609,1 @@\n-instruct sarL_mem_imm(memory dst, immI8 shift, rFlagsReg cr)\n+instruct sarL_mem_imm(memory dst, immI shift, rFlagsReg cr)\n@@ -9169,0 +9624,1 @@\n+  predicate(!VM_Version::supports_bmi2());\n@@ -9182,0 +9638,1 @@\n+  predicate(!VM_Version::supports_bmi2());\n@@ -9192,0 +9649,24 @@\n+instruct sarL_rReg_rReg(rRegL dst, rRegL src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (RShiftL src shift));\n+\n+  format %{ \"sarxq   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ sarxq($dst$$Register, $src$$Register, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct sarL_mem_rReg(rRegL dst, memory src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (RShiftL (LoadL src) shift));\n+  ins_cost(175);\n+  format %{ \"sarxq   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ sarxq($dst$$Register, $src$$Address, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n@@ -9247,0 +9728,1 @@\n+  predicate(!VM_Version::supports_bmi2());\n@@ -9260,0 +9742,1 @@\n+  predicate(!VM_Version::supports_bmi2());\n@@ -9270,0 +9753,24 @@\n+instruct shrL_rReg_rReg(rRegL dst, rRegL src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (URShiftL src shift));\n+\n+  format %{ \"shrxq   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ shrxq($dst$$Register, $src$$Register, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct shrL_mem_rReg(rRegL dst, memory src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (URShiftL (LoadL src) shift));\n+  ins_cost(175);\n+  format %{ \"shrxq   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ shrxq($dst$$Register, $src$$Address, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n@@ -9299,1 +9806,1 @@\n-instruct rolI_imm(rRegI dst, immI8 shift, rFlagsReg cr)\n+instruct rolI_immI8_legacy(rRegI dst, immI8 shift, rFlagsReg cr)\n@@ -9301,1 +9808,1 @@\n-  predicate(n->bottom_type()->basic_type() == T_INT);\n+  predicate(!VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_INT);\n@@ -9311,0 +9818,25 @@\n+instruct rolI_immI8(rRegI dst, rRegI src, immI8 shift)\n+%{\n+  predicate(VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_INT);\n+  match(Set dst (RotateLeft src shift));\n+  format %{ \"rolxl   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    int shift = 32 - ($shift$$constant & 31);\n+    __ rorxl($dst$$Register, $src$$Register, shift);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct rolI_mem_immI8(rRegI dst, memory src, immI8 shift)\n+%{\n+  predicate(VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_INT);\n+  match(Set dst (RotateLeft (LoadI src) shift));\n+  ins_cost(175);\n+  format %{ \"rolxl   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    int shift = 32 - ($shift$$constant & 31);\n+    __ rorxl($dst$$Register, $src$$Address, shift);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n@@ -9338,1 +9870,1 @@\n-instruct rorI_immI8(rRegI dst, immI8 shift)\n+instruct rorI_immI8(rRegI dst, rRegI src, immI8 shift)\n@@ -9341,2 +9873,2 @@\n-  match(Set dst (RotateRight dst shift));\n-  format %{ \"rorxd     $dst, $shift\" %}\n+  match(Set dst (RotateRight src shift));\n+  format %{ \"rorxl   $dst, $src, $shift\" %}\n@@ -9344,1 +9876,1 @@\n-    __ rorxd($dst$$Register, $dst$$Register, $shift$$constant);\n+    __ rorxl($dst$$Register, $src$$Register, $shift$$constant);\n@@ -9349,0 +9881,12 @@\n+instruct rorI_mem_immI8(rRegI dst, memory src, immI8 shift)\n+%{\n+  predicate(VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_INT);\n+  match(Set dst (RotateRight (LoadI src) shift));\n+  ins_cost(175);\n+  format %{ \"rorxl   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ rorxl($dst$$Register, $src$$Address, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n@@ -9362,2 +9906,1 @@\n-\n-instruct rolL_immI8(rRegL dst, immI8 shift, rFlagsReg cr)\n+instruct rolL_immI8_legacy(rRegL dst, immI8 shift, rFlagsReg cr)\n@@ -9366,1 +9909,1 @@\n-  predicate(n->bottom_type()->basic_type() == T_LONG);\n+  predicate(!VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_LONG);\n@@ -9376,0 +9919,25 @@\n+instruct rolL_immI8(rRegL dst, rRegL src, immI8 shift)\n+%{\n+  predicate(VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_LONG);\n+  match(Set dst (RotateLeft src shift));\n+  format %{ \"rolxq   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    int shift = 64 - ($shift$$constant & 63);\n+    __ rorxq($dst$$Register, $src$$Register, shift);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct rolL_mem_immI8(rRegL dst, memory src, immI8 shift)\n+%{\n+  predicate(VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_LONG);\n+  match(Set dst (RotateLeft (LoadL src) shift));\n+  ins_cost(175);\n+  format %{ \"rolxq   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    int shift = 64 - ($shift$$constant & 63);\n+    __ rorxq($dst$$Register, $src$$Address, shift);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n@@ -9389,1 +9957,0 @@\n-\n@@ -9403,2 +9970,1 @@\n-\n-instruct rorL_immI8(rRegL dst, immI8 shift)\n+instruct rorL_immI8(rRegL dst, rRegL src, immI8 shift)\n@@ -9408,2 +9974,2 @@\n-  match(Set dst (RotateRight dst shift));\n-  format %{ \"rorxq    $dst, $shift\" %}\n+  match(Set dst (RotateRight src shift));\n+  format %{ \"rorxq   $dst, $src, $shift\" %}\n@@ -9411,1 +9977,1 @@\n-    __ rorxq($dst$$Register, $dst$$Register, $shift$$constant);\n+    __ rorxq($dst$$Register, $src$$Register, $shift$$constant);\n@@ -9416,0 +9982,12 @@\n+instruct rorL_mem_immI8(rRegL dst, memory src, immI8 shift)\n+%{\n+  predicate(VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_LONG);\n+  match(Set dst (RotateRight (LoadL src) shift));\n+  ins_cost(175);\n+  format %{ \"rorxq   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ rorxq($dst$$Register, $src$$Address, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n@@ -9429,0 +10007,42 @@\n+\/\/----------------------------- CompressBits\/ExpandBits ------------------------\n+\n+instruct compressBitsL_reg(rRegL dst, rRegL src, rRegL mask) %{\n+  predicate(n->bottom_type()->isa_long());\n+  match(Set dst (CompressBits src mask));\n+  format %{ \"pextq  $dst, $src, $mask\\t! parallel bit extract\" %}\n+  ins_encode %{\n+    __ pextq($dst$$Register, $src$$Register, $mask$$Register);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct expandBitsL_reg(rRegL dst, rRegL src, rRegL mask) %{\n+  predicate(n->bottom_type()->isa_long());\n+  match(Set dst (ExpandBits src mask));\n+  format %{ \"pdepq  $dst, $src, $mask\\t! parallel bit deposit\" %}\n+  ins_encode %{\n+    __ pdepq($dst$$Register, $src$$Register, $mask$$Register);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct compressBitsL_mem(rRegL dst, rRegL src, memory mask) %{\n+  predicate(n->bottom_type()->isa_long());\n+  match(Set dst (CompressBits src (LoadL mask)));\n+  format %{ \"pextq  $dst, $src, $mask\\t! parallel bit extract\" %}\n+  ins_encode %{\n+    __ pextq($dst$$Register, $src$$Register, $mask$$Address);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct expandBitsL_mem(rRegL dst, rRegL src, memory mask) %{\n+  predicate(n->bottom_type()->isa_long());\n+  match(Set dst (ExpandBits src (LoadL mask)));\n+  format %{ \"pdepq  $dst, $src, $mask\\t! parallel bit deposit\" %}\n+  ins_encode %{\n+    __ pdepq($dst$$Register, $src$$Register, $mask$$Address);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n@@ -10395,0 +11015,1 @@\n+\/\/ Really expensive, avoid\n@@ -10399,1 +11020,1 @@\n-  ins_cost(145);\n+  ins_cost(500);\n@@ -10424,18 +11045,0 @@\n-instruct cmpF_cc_mem(rFlagsRegU cr, regF src1, memory src2)\n-%{\n-  match(Set cr (CmpF src1 (LoadF src2)));\n-\n-  ins_cost(145);\n-  format %{ \"ucomiss $src1, $src2\\n\\t\"\n-            \"jnp,s   exit\\n\\t\"\n-            \"pushfq\\t# saw NaN, set CF\\n\\t\"\n-            \"andq    [rsp], #0xffffff2b\\n\\t\"\n-            \"popfq\\n\"\n-    \"exit:\" %}\n-  ins_encode %{\n-    __ ucomiss($src1$$XMMRegister, $src2$$Address);\n-    emit_cmpfp_fixup(_masm);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n@@ -10453,17 +11056,0 @@\n-instruct cmpF_cc_imm(rFlagsRegU cr, regF src, immF con) %{\n-  match(Set cr (CmpF src con));\n-\n-  ins_cost(145);\n-  format %{ \"ucomiss $src, [$constantaddress]\\t# load from constant table: float=$con\\n\\t\"\n-            \"jnp,s   exit\\n\\t\"\n-            \"pushfq\\t# saw NaN, set CF\\n\\t\"\n-            \"andq    [rsp], #0xffffff2b\\n\\t\"\n-            \"popfq\\n\"\n-    \"exit:\" %}\n-  ins_encode %{\n-    __ ucomiss($src$$XMMRegister, $constantaddress($con));\n-    emit_cmpfp_fixup(_masm);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n@@ -10480,0 +11066,1 @@\n+\/\/ Really expensive, avoid\n@@ -10484,1 +11071,1 @@\n-  ins_cost(145);\n+  ins_cost(500);\n@@ -10509,18 +11096,0 @@\n-instruct cmpD_cc_mem(rFlagsRegU cr, regD src1, memory src2)\n-%{\n-  match(Set cr (CmpD src1 (LoadD src2)));\n-\n-  ins_cost(145);\n-  format %{ \"ucomisd $src1, $src2\\n\\t\"\n-            \"jnp,s   exit\\n\\t\"\n-            \"pushfq\\t# saw NaN, set CF\\n\\t\"\n-            \"andq    [rsp], #0xffffff2b\\n\\t\"\n-            \"popfq\\n\"\n-    \"exit:\" %}\n-  ins_encode %{\n-    __ ucomisd($src1$$XMMRegister, $src2$$Address);\n-    emit_cmpfp_fixup(_masm);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n@@ -10538,17 +11107,0 @@\n-instruct cmpD_cc_imm(rFlagsRegU cr, regD src, immD con) %{\n-  match(Set cr (CmpD src con));\n-\n-  ins_cost(145);\n-  format %{ \"ucomisd $src, [$constantaddress]\\t# load from constant table: double=$con\\n\\t\"\n-            \"jnp,s   exit\\n\\t\"\n-            \"pushfq\\t# saw NaN, set CF\\n\\t\"\n-            \"andq    [rsp], #0xffffff2b\\n\\t\"\n-            \"popfq\\n\"\n-    \"exit:\" %}\n-  ins_encode %{\n-    __ ucomisd($src$$XMMRegister, $constantaddress($con));\n-    emit_cmpfp_fixup(_masm);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n@@ -12485,0 +13037,26 @@\n+\/\/ Manifest a CmpU result in an integer register.  Very painful.\n+\/\/ This is the test to avoid.\n+instruct cmpU3_reg_reg(rRegI dst, rRegI src1, rRegI src2, rFlagsReg flags)\n+%{\n+  match(Set dst (CmpU3 src1 src2));\n+  effect(KILL flags);\n+\n+  ins_cost(275); \/\/ XXX\n+  format %{ \"cmpl    $src1, $src2\\t# CmpL3\\n\\t\"\n+            \"movl    $dst, -1\\n\\t\"\n+            \"jb,u    done\\n\\t\"\n+            \"setne   $dst\\n\\t\"\n+            \"movzbl  $dst, $dst\\n\\t\"\n+    \"done:\" %}\n+  ins_encode %{\n+    Label done;\n+    __ cmpl($src1$$Register, $src2$$Register);\n+    __ movl($dst$$Register, -1);\n+    __ jccb(Assembler::below, done);\n+    __ setne($dst$$Register);\n+    __ movzbl($dst$$Register, $dst$$Register);\n+    __ bind(done);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -12511,0 +13089,26 @@\n+\/\/ Manifest a CmpUL result in an integer register.  Very painful.\n+\/\/ This is the test to avoid.\n+instruct cmpUL3_reg_reg(rRegI dst, rRegL src1, rRegL src2, rFlagsReg flags)\n+%{\n+  match(Set dst (CmpUL3 src1 src2));\n+  effect(KILL flags);\n+\n+  ins_cost(275); \/\/ XXX\n+  format %{ \"cmpq    $src1, $src2\\t# CmpL3\\n\\t\"\n+            \"movl    $dst, -1\\n\\t\"\n+            \"jb,u    done\\n\\t\"\n+            \"setne   $dst\\n\\t\"\n+            \"movzbl  $dst, $dst\\n\\t\"\n+    \"done:\" %}\n+  ins_encode %{\n+    Label done;\n+    __ cmpq($src1$$Register, $src2$$Register);\n+    __ movl($dst$$Register, -1);\n+    __ jccb(Assembler::below, done);\n+    __ setne($dst$$Register);\n+    __ movzbl($dst$$Register, $dst$$Register);\n+    __ bind(done);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -13142,12 +13746,0 @@\n-\/\/\n-instruct CallNativeDirect(method meth)\n-%{\n-  match(CallNative);\n-  effect(USE meth);\n-\n-  ins_cost(300);\n-  format %{ \"call_native \" %}\n-  ins_encode(clear_avx, Java_To_Runtime(meth));\n-  ins_pipe(pipe_slow);\n-%}\n-\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":874,"deletions":282,"binary":false,"changes":1156,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -60,0 +60,1 @@\n+  virtual int nr_immediate_oops_patched() const  { return 0; }\n@@ -415,0 +416,7 @@\n+  virtual int nr_immediate_oops_patched() const  {\n+    if (_id == load_mirror_id || _id == load_appendix_id) {\n+      return 1;\n+    }\n+    return 0;\n+  }\n+\n","filename":"src\/hotspot\/share\/c1\/c1_CodeStubs.hpp","additions":9,"deletions":1,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -201,5 +201,5 @@\n-  \/\/     data       opr-type opr-kind\n-  \/\/ +--------------+-------+-------+\n-  \/\/ [max...........|7 6 5 4|3 2 1 0]\n-  \/\/                               ^\n-  \/\/                         is_pointer bit\n+  \/\/          data        other-non-data opr-type opr-kind\n+  \/\/ +-------------------+--------------+-------+-----+\n+  \/\/ [max...............................|6 5 4 3|2 1 0]\n+  \/\/                                                 ^\n+  \/\/                                           is_pointer bit\n@@ -208,1 +208,1 @@\n-  \/\/ we need  4 bits to represent types\n+  \/\/ we need 4 bits to represent types\n@@ -239,1 +239,1 @@\n-    , non_data_bits  = pointer_bits + kind_bits + type_bits + size_bits + destroys_bits + virtual_bits\n+    , non_data_bits  = kind_bits + type_bits + size_bits + destroys_bits + virtual_bits\n@@ -245,1 +245,1 @@\n-  enum OprShift {\n+  enum OprShift : uintptr_t {\n@@ -278,1 +278,1 @@\n-  uintptr_t data() const                         { return value() >> data_shift; }\n+  uint32_t data() const                          { return (uint32_t)value() >> data_shift; }\n@@ -302,1 +302,3 @@\n-    vreg_max = (1 << data_bits) - 1\n+    data_max = (1 << data_bits) - 1,      \/\/ max unsigned value for data bit field\n+    vreg_limit =  10000,                  \/\/ choose a reasonable limit,\n+    vreg_max = MIN2(vreg_limit, data_max) \/\/ and make sure if fits in the bit field\n@@ -758,1 +760,0 @@\n-    assert(index <= (max_jint >> LIR_Opr::data_shift), \"index is too big\");\n@@ -837,1 +838,1 @@\n-    assert(index <= (max_jint >> LIR_Opr::data_shift), \"index is too big\");\n+    assert(index == (int)res->data(), \"conversion check\");\n@@ -2443,1 +2444,1 @@\n-    maxNumberOfOperands = 20,\n+    maxNumberOfOperands = 21,\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.hpp","additions":14,"deletions":13,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -1190,1 +1190,1 @@\n-\/\/ Examble: ref.get()\n+\/\/ Example: ref.get()\n@@ -1201,1 +1201,1 @@\n-  \/\/ need to perform the null check on the reference objecy\n+  \/\/ need to perform the null check on the reference object\n@@ -1302,2 +1302,13 @@\n-  LabelObj* L_not_prim = new LabelObj();\n-  LabelObj* L_done = new LabelObj();\n+  \/\/ While reading off the universal constant mirror is less efficient than doing\n+  \/\/ another branch and returning the constant answer, this branchless code runs into\n+  \/\/ much less risk of confusion for C1 register allocator. The choice of the universe\n+  \/\/ object here is correct as long as it returns the same modifiers we would expect\n+  \/\/ from the primitive class itself. See spec for Class.getModifiers that provides\n+  \/\/ the typed array klasses with similar modifiers as their component types.\n+\n+  Klass* univ_klass_obj = Universe::byteArrayKlassObj();\n+  assert(univ_klass_obj->modifier_flags() == (JVM_ACC_ABSTRACT | JVM_ACC_FINAL | JVM_ACC_PUBLIC), \"Sanity\");\n+  LIR_Opr prim_klass = LIR_OprFact::metadataConst(univ_klass_obj);\n+\n+  LIR_Opr recv_klass = new_register(T_METADATA);\n+  __ move(new LIR_Address(receiver.result(), java_lang_Class::klass_offset(), T_ADDRESS), recv_klass, info);\n@@ -1305,0 +1316,1 @@\n+  \/\/ Check if this is a Java mirror of primitive type, and select the appropriate klass.\n@@ -1306,6 +1318,2 @@\n-  \/\/ Checking if it's a java mirror of primitive type\n-  __ move(new LIR_Address(receiver.result(), java_lang_Class::klass_offset(), T_ADDRESS), klass, info);\n-  __ cmp(lir_cond_notEqual, klass, LIR_OprFact::metadataConst(0));\n-  __ branch(lir_cond_notEqual, L_not_prim->label());\n-  __ move(LIR_OprFact::intConst(JVM_ACC_ABSTRACT | JVM_ACC_FINAL | JVM_ACC_PUBLIC), result);\n-  __ branch(lir_cond_always, L_done->label());\n+  __ cmp(lir_cond_equal, recv_klass, LIR_OprFact::metadataConst(0));\n+  __ cmove(lir_cond_equal, prim_klass, recv_klass, klass, T_ADDRESS);\n@@ -1313,1 +1321,1 @@\n-  __ branch_destination(L_not_prim->label());\n+  \/\/ Get the answer.\n@@ -1315,12 +1323,0 @@\n-  __ branch_destination(L_done->label());\n-}\n-\n-\/\/ Example: Thread.currentThread()\n-void LIRGenerator::do_currentThread(Intrinsic* x) {\n-  assert(x->number_of_arguments() == 0, \"wrong type\");\n-  LIR_Opr temp = new_register(T_ADDRESS);\n-  LIR_Opr reg = rlock_result(x);\n-  __ move(new LIR_Address(getThreadPointer(), in_bytes(JavaThread::threadObj_offset()), T_ADDRESS), temp);\n-  \/\/ threadObj = ((OopHandle)_threadObj)->resolve();\n-  access_load(IN_NATIVE, T_OBJECT,\n-              LIR_OprFact::address(new LIR_Address(temp, T_OBJECT)), reg);\n@@ -1349,1 +1345,0 @@\n-  __ convert(Bytecodes::_i2l, layout, result_reg);\n@@ -1352,2 +1347,4 @@\n-  jlong mask = ~(jlong) right_n_bits(LogBytesPerLong);\n-  __ logical_and(result_reg, LIR_OprFact::longConst(mask), result_reg);\n+\n+  LIR_Opr mask = load_immediate(~(jint) right_n_bits(LogBytesPerLong), T_INT);\n+  __ logical_and(layout, mask, layout);\n+  __ convert(Bytecodes::_i2l, layout, result_reg);\n@@ -1366,2 +1363,2 @@\n-  LIR_Opr hss = LIR_OprFact::intConst(Klass::_lh_header_size_shift);\n-  LIR_Opr hsm = LIR_OprFact::intConst(Klass::_lh_header_size_mask);\n+  LIR_Opr hss = load_immediate(Klass::_lh_header_size_shift, T_INT);\n+  LIR_Opr hsm = load_immediate(Klass::_lh_header_size_mask, T_INT);\n@@ -1378,1 +1375,1 @@\n-  LIR_Opr l2esm = LIR_OprFact::intConst(Klass::_lh_log2_element_size_mask);\n+  LIR_Opr l2esm = load_immediate(Klass::_lh_log2_element_size_mask, T_INT);\n@@ -1418,1 +1415,2 @@\n-    __ logical_and(length, LIR_OprFact::longConst(~round_mask), length);\n+    LIR_Opr round_mask_opr = load_immediate(~(jlong)round_mask, T_LONG);\n+    __ logical_and(length, round_mask_opr, length);\n@@ -1424,1 +1422,2 @@\n-    __ logical_and(length_int, LIR_OprFact::intConst(~round_mask), length_int);\n+    LIR_Opr round_mask_opr = load_immediate(~round_mask, T_INT);\n+    __ logical_and(length_int, round_mask_opr, length_int);\n@@ -1432,0 +1431,22 @@\n+void LIRGenerator::do_extentLocalCache(Intrinsic* x) {\n+  do_JavaThreadField(x, JavaThread::extentLocalCache_offset());\n+}\n+\n+\/\/ Example: Thread.currentCarrierThread()\n+void LIRGenerator::do_currentCarrierThread(Intrinsic* x) {\n+  do_JavaThreadField(x, JavaThread::threadObj_offset());\n+}\n+\n+void LIRGenerator::do_vthread(Intrinsic* x) {\n+  do_JavaThreadField(x, JavaThread::vthread_offset());\n+}\n+\n+void LIRGenerator::do_JavaThreadField(Intrinsic* x, ByteSize offset) {\n+  assert(x->number_of_arguments() == 0, \"wrong type\");\n+  LIR_Opr temp = new_register(T_ADDRESS);\n+  LIR_Opr reg = rlock_result(x);\n+  __ move(new LIR_Address(getThreadPointer(), in_bytes(offset), T_ADDRESS), temp);\n+  access_load(IN_NATIVE, T_OBJECT,\n+              LIR_OprFact::address(new LIR_Address(temp, T_OBJECT)), reg);\n+}\n+\n@@ -2888,25 +2909,0 @@\n-#ifdef JFR_HAVE_INTRINSICS\n-\n-void LIRGenerator::do_getEventWriter(Intrinsic* x) {\n-  LabelObj* L_end = new LabelObj();\n-\n-  \/\/ FIXME T_ADDRESS should actually be T_METADATA but it can't because the\n-  \/\/ meaning of these two is mixed up (see JDK-8026837).\n-  LIR_Address* jobj_addr = new LIR_Address(getThreadPointer(),\n-                                           in_bytes(THREAD_LOCAL_WRITER_OFFSET_JFR),\n-                                           T_ADDRESS);\n-  LIR_Opr result = rlock_result(x);\n-  __ move(LIR_OprFact::oopConst(NULL), result);\n-  LIR_Opr jobj = new_register(T_METADATA);\n-  __ move_wide(jobj_addr, jobj);\n-  __ cmp(lir_cond_equal, jobj, LIR_OprFact::metadataConst(0));\n-  __ branch(lir_cond_equal, L_end->label());\n-\n-  access_load(IN_NATIVE, T_OBJECT, LIR_OprFact::address(new LIR_Address(jobj, T_OBJECT)), result);\n-\n-  __ branch_destination(L_end->label());\n-}\n-\n-#endif\n-\n-\n@@ -2938,4 +2934,1 @@\n-  case vmIntrinsics::_getEventWriter:\n-    do_getEventWriter(x);\n-    break;\n-    do_RuntimeCall(CAST_FROM_FN_PTR(address, JFR_TIME_FUNCTION), x);\n+    do_RuntimeCall(CAST_FROM_FN_PTR(address, JfrTime::time_function()), x);\n@@ -2959,1 +2952,3 @@\n-  case vmIntrinsics::_currentThread:  do_currentThread(x); break;\n+  case vmIntrinsics::_currentCarrierThread: do_currentCarrierThread(x); break;\n+  case vmIntrinsics::_currentThread:  do_vthread(x);       break;\n+  case vmIntrinsics::_extentLocalCache: do_extentLocalCache(x); break;\n@@ -3028,0 +3023,4 @@\n+  case vmIntrinsics::_Continuation_doYield:\n+    do_continuation_doYield(x);\n+    break;\n+\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.cpp","additions":60,"deletions":61,"binary":false,"changes":121,"status":"modified"},{"patch":"@@ -169,1 +169,4 @@\n-  RegisterMap reg_map(current, false);\n+  RegisterMap reg_map(current,\n+                      RegisterMap::UpdateMap::skip,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n@@ -179,1 +182,4 @@\n-    RegisterMap reg_map(current, false);\n+    RegisterMap reg_map(current,\n+                        RegisterMap::UpdateMap::skip,\n+                        RegisterMap::ProcessFrames::include,\n+                        RegisterMap::WalkContinuation::skip);\n@@ -327,1 +333,1 @@\n-  FUNCTION_CASE(entry, JFR_TIME_FUNCTION);\n+  FUNCTION_CASE(entry, JfrTime::time_function());\n@@ -339,0 +345,1 @@\n+  FUNCTION_CASE(entry, StubRoutines::cont_doYield());\n@@ -443,1 +450,4 @@\n-  RegisterMap map(current, false);\n+  RegisterMap map(current,\n+                  RegisterMap::UpdateMap::skip,\n+                  RegisterMap::ProcessFrames::include,\n+                  RegisterMap::WalkContinuation::skip);\n@@ -482,1 +492,4 @@\n-      RegisterMap map(current, false);\n+      RegisterMap map(current,\n+                      RegisterMap::UpdateMap::skip,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n@@ -528,1 +541,4 @@\n-    RegisterMap map(current, false);\n+    RegisterMap map(current,\n+                    RegisterMap::UpdateMap::skip,\n+                    RegisterMap::ProcessFrames::include,\n+                    RegisterMap::WalkContinuation::skip);\n@@ -567,1 +583,4 @@\n-    RegisterMap reg_map(current);\n+    RegisterMap reg_map(current,\n+                        RegisterMap::UpdateMap::include,\n+                        RegisterMap::ProcessFrames::include,\n+                        RegisterMap::WalkContinuation::skip);\n@@ -763,1 +782,4 @@\n-  RegisterMap reg_map(current, false);\n+  RegisterMap reg_map(current,\n+                      RegisterMap::UpdateMap::skip,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n@@ -912,1 +934,4 @@\n-  RegisterMap reg_map(current, false);\n+  RegisterMap reg_map(current,\n+                      RegisterMap::UpdateMap::skip,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n@@ -1277,1 +1302,4 @@\n-  RegisterMap reg_map(current, false);\n+  RegisterMap reg_map(current,\n+                      RegisterMap::UpdateMap::skip,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n@@ -1404,1 +1432,4 @@\n-  RegisterMap reg_map(current, false);\n+  RegisterMap reg_map(current,\n+                      RegisterMap::UpdateMap::skip,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n@@ -1418,1 +1449,1 @@\n-    Method::build_interpreter_method_data(m, THREAD);\n+    Method::build_profiling_method_data(m, THREAD);\n","filename":"src\/hotspot\/share\/c1\/c1_Runtime1.cpp","additions":43,"deletions":12,"binary":false,"changes":55,"status":"modified"},{"patch":"@@ -50,0 +50,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -51,1 +52,0 @@\n-#include \"runtime\/thread.hpp\"\n@@ -336,1 +336,1 @@\n-    vm_direct_exit(0);\n+    os::_exit(0);\n@@ -386,1 +386,1 @@\n-    vm_direct_exit(0);\n+    os::_exit(0);\n@@ -740,0 +740,1 @@\n+    const char* generated = \"\";\n@@ -791,0 +792,3 @@\n+      if (ik->is_generated_shared_class()) {\n+        generated = \" ** generated\";\n+      }\n@@ -797,1 +801,3 @@\n-      log_debug(cds, class)(\"klasses[%5d] = \" PTR_FORMAT \" %-5s %s%s%s\", i, p2i(to_requested(k)), type, k->external_name(), hidden, unlinked);\n+      log_debug(cds, class)(\"klasses[%5d] = \" PTR_FORMAT \" %-5s %s%s%s%s\", i,\n+                            p2i(to_requested(k)), type, k->external_name(),\n+                            hidden, unlinked, generated);\n@@ -1038,1 +1044,1 @@\n-  \/\/ runtime, this region will be mapped to runtime_base.  runtime_base is 0 if this\n+  \/\/ runtime, this region will be mapped to requested_base. requested_base is 0 if this\n@@ -1041,1 +1047,4 @@\n-  static void log_region(const char* name, address base, address top, address runtime_base) {\n+  \/\/\n+  \/\/ Note: across -Xshare:dump runs, base may be different, but requested_base should\n+  \/\/ be the same as the archive contents should be deterministic.\n+  static void log_region(const char* name, address base, address top, address requested_base) {\n@@ -1043,2 +1052,2 @@\n-    base = runtime_base;\n-    top = runtime_base + size;\n+    base = requested_base;\n+    top = requested_base + size;\n@@ -1049,0 +1058,1 @@\n+#if INCLUDE_CDS_JAVA_HEAP\n@@ -1051,1 +1061,0 @@\n-#if INCLUDE_CDS_JAVA_HEAP\n@@ -1055,1 +1064,1 @@\n-      log_region(which, start, end, start);\n+      log_region(which, start, end, to_requested(start));\n@@ -1064,1 +1073,1 @@\n-                             p2i(start), original_oop->klass()->external_name());\n+                             p2i(to_requested(start)), original_oop->klass()->external_name());\n@@ -1069,2 +1078,2 @@\n-          log_info(cds, map)(PTR_FORMAT \": @@ Object HeapShared:roots (ObjArray)\",\n-                             p2i(start));\n+          log_info(cds, map)(PTR_FORMAT \": @@ Object HeapShared::roots (ObjArray)\",\n+                             p2i(to_requested(start)));\n@@ -1077,1 +1086,1 @@\n-        log_data(start, oop_end, start, \/*is_heap=*\/true);\n+        log_data(start, oop_end, to_requested(start), \/*is_heap=*\/true);\n@@ -1082,2 +1091,2 @@\n-                           p2i(start), size_t(end - start));\n-        log_data(start, end, start, \/*is_heap=*\/true);\n+                           p2i(to_requested(start)), size_t(end - start));\n+        log_data(start, end, to_requested(start), \/*is_heap=*\/true);\n@@ -1086,1 +1095,4 @@\n-#endif\n+  static address to_requested(address p) {\n+    return HeapShared::to_requested_address(p);\n+  }\n+#endif\n@@ -1090,2 +1102,2 @@\n-  \/\/ will be mapped to runtime_base at run-time.\n-  static void log_data(address base, address top, address runtime_base, bool is_heap = false) {\n+  \/\/ will be mapped to requested_base at run-time.\n+  static void log_data(address base, address top, address requested_base, bool is_heap = false) {\n@@ -1102,1 +1114,1 @@\n-      os::print_hex_dump(&lsh, base, top, unitsize, 32, runtime_base);\n+      os::print_hex_dump(&lsh, base, top, unitsize, 32, requested_base);\n@@ -1136,0 +1148,1 @@\n+#if INCLUDE_CDS_JAVA_HEAP\n@@ -1142,0 +1155,1 @@\n+#endif\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.cpp","additions":34,"deletions":20,"binary":false,"changes":54,"status":"modified"},{"patch":"@@ -282,2 +282,7 @@\n-    _dump_region->append_intptr_t(\n-      UseCompressedOops ? (intptr_t)CompressedOops::encode_not_null(*o) : (intptr_t)((void*)(*o)));\n+    intptr_t p;\n+    if (UseCompressedOops) {\n+      p = (intptr_t)CompressedOops::encode_not_null(*o);\n+    } else {\n+      p = cast_from_oop<intptr_t>(HeapShared::to_requested_address(*o));\n+    }\n+    _dump_region->append_intptr_t(p);\n","filename":"src\/hotspot\/share\/cds\/archiveUtils.cpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -183,2 +183,3 @@\n-  unsigned hash = (unsigned)p->identity_hash();\n-  return hash;\n+  \/\/ Do not call p->identity_hash() as that will update the\n+  \/\/ object header.\n+  return primitive_hash(cast_from_oop<intptr_t>(p));\n@@ -308,0 +309,2 @@\n+  assert(!obj->is_stackChunk(), \"do not archive stack chunks\");\n+\n@@ -359,3 +362,3 @@\n-    vm_direct_exit(-1,\n-      err_msg(\"Out of memory. Please run with a larger Java heap, current MaxHeapSize = \"\n-              SIZE_FORMAT \"M\", MaxHeapSize\/M));\n+    log_error(cds)(\"Out of memory. Please run with a larger Java heap, current MaxHeapSize = \"\n+        SIZE_FORMAT \"M\", MaxHeapSize\/M);\n+    os::_exit(-1);\n@@ -1175,1 +1178,1 @@\n-    vm_direct_exit(1);\n+    os::_exit(1);\n@@ -1185,1 +1188,1 @@\n-    vm_direct_exit(1);\n+    os::_exit(1);\n@@ -1221,1 +1224,1 @@\n-        vm_direct_exit(1);\n+        os::_exit(1);\n@@ -1590,0 +1593,1 @@\n+    assert(UseCompressedOops, \"sanity\");\n@@ -1593,0 +1597,2 @@\n+      \/\/ Note: HeapShared::to_requested_address() is not necessary because\n+      \/\/ the heap always starts at a deterministic address with UseCompressedOops==true.\n@@ -1600,0 +1606,1 @@\n+    assert(!UseCompressedOops, \"sanity\");\n@@ -1604,0 +1611,4 @@\n+      if (DumpSharedSpaces) {\n+        \/\/ Make heap content deterministic.\n+        *p = HeapShared::to_requested_address(*p);\n+      }\n@@ -1612,0 +1623,28 @@\n+\n+address HeapShared::to_requested_address(address dumptime_addr) {\n+  assert(DumpSharedSpaces, \"static dump time only\");\n+  if (dumptime_addr == NULL || UseCompressedOops) {\n+    return dumptime_addr;\n+  }\n+\n+  \/\/ With UseCompressedOops==false, actual_base is selected by the OS so\n+  \/\/ it's different across -Xshare:dump runs.\n+  address actual_base = (address)G1CollectedHeap::heap()->reserved().start();\n+  address actual_end  = (address)G1CollectedHeap::heap()->reserved().end();\n+  assert(actual_base <= dumptime_addr && dumptime_addr <= actual_end, \"must be an address in the heap\");\n+\n+  \/\/ We always write the objects as if the heap started at this address. This\n+  \/\/ makes the heap content deterministic.\n+  \/\/\n+  \/\/ Note that at runtime, the heap address is also selected by the OS, so\n+  \/\/ the archive heap will not be mapped at 0x10000000. Instead, we will call\n+  \/\/ HeapShared::patch_embedded_pointers() to relocate the heap contents\n+  \/\/ accordingly.\n+  const address REQUESTED_BASE = (address)0x10000000;\n+  intx delta = REQUESTED_BASE - actual_base;\n+\n+  address requested_addr = dumptime_addr + delta;\n+  assert(REQUESTED_BASE != 0 && requested_addr != NULL, \"sanity\");\n+  return requested_addr;\n+}\n+\n@@ -1632,1 +1671,1 @@\n-  log_info(cds, heap)(\"calculate_oopmap: objects = %6d, embedded oops = %7d, nulls = %7d\",\n+  log_info(cds, heap)(\"calculate_oopmap: objects = %6d, oop fields = %7d (nulls = %7d)\",\n","filename":"src\/hotspot\/share\/cds\/heapShared.cpp","additions":48,"deletions":9,"binary":false,"changes":57,"status":"modified"},{"patch":"@@ -77,0 +77,3 @@\n+SystemDictionaryShared::ArchiveInfo SystemDictionaryShared::_static_archive;\n+SystemDictionaryShared::ArchiveInfo SystemDictionaryShared::_dynamic_archive;\n+\n@@ -81,9 +84,0 @@\n-\/\/ SystemDictionaries in the base layer static archive\n-RunTimeSharedDictionary SystemDictionaryShared::_builtin_dictionary;\n-RunTimeSharedDictionary SystemDictionaryShared::_unregistered_dictionary;\n-\/\/ SystemDictionaries in the top layer dynamic archive\n-RunTimeSharedDictionary SystemDictionaryShared::_dynamic_builtin_dictionary;\n-RunTimeSharedDictionary SystemDictionaryShared::_dynamic_unregistered_dictionary;\n-\n-LambdaProxyClassDictionary SystemDictionaryShared::_lambda_proxy_class_dictionary;\n-LambdaProxyClassDictionary SystemDictionaryShared::_dynamic_lambda_proxy_class_dictionary;\n@@ -131,1 +125,3 @@\n-  const RunTimeClassInfo* record = find_record(&_unregistered_dictionary, &_dynamic_unregistered_dictionary, class_name);\n+  const RunTimeClassInfo* record = find_record(&_static_archive._unregistered_dictionary,\n+                                               &_dynamic_archive._unregistered_dictionary,\n+                                               class_name);\n@@ -621,1 +617,1 @@\n-  bool do_entry(InstanceKlass* k, DumpTimeClassInfo& info) {\n+  void do_entry(InstanceKlass* k, DumpTimeClassInfo& info) {\n@@ -625,1 +621,0 @@\n-    return true;  \/\/ keep on iterating\n@@ -657,8 +652,0 @@\n-class ExcludeDumpTimeSharedClasses : StackObj {\n-public:\n-  bool do_entry(InstanceKlass* k, DumpTimeClassInfo& info) {\n-    SystemDictionaryShared::check_for_exclusion(k, &info);\n-    return true; \/\/ keep on iterating\n-  }\n-};\n-\n@@ -671,1 +658,1 @@\n-    \/\/ all of its subclasses will also be excluded by ExcludeDumpTimeSharedClasses\n+    \/\/ all of its subclasses will also be excluded.\n@@ -674,1 +661,1 @@\n-    _dumptime_table->iterate(&dup_checker);\n+    _dumptime_table->iterate_all_live_classes(&dup_checker);\n@@ -678,2 +665,4 @@\n-  ExcludeDumpTimeSharedClasses excl;\n-  _dumptime_table->iterate(&excl);\n+  auto check_for_exclusion = [&] (InstanceKlass* k, DumpTimeClassInfo& info) {\n+    SystemDictionaryShared::check_for_exclusion(k, &info);\n+  };\n+  _dumptime_table->iterate_all_live_classes(check_for_exclusion);\n@@ -729,4 +718,2 @@\n-class IterateDumpTimeSharedClassTable : StackObj {\n-  MetaspaceClosure *_it;\n-public:\n-  IterateDumpTimeSharedClassTable(MetaspaceClosure* it) : _it(it) {}\n+void SystemDictionaryShared::dumptime_classes_do(class MetaspaceClosure* it) {\n+  assert_lock_strong(DumpTimeTable_lock);\n@@ -734,2 +721,1 @@\n-  bool do_entry(InstanceKlass* k, DumpTimeClassInfo& info) {\n-    assert_lock_strong(DumpTimeTable_lock);\n+  auto do_klass = [&] (InstanceKlass* k, DumpTimeClassInfo& info) {\n@@ -737,1 +723,1 @@\n-      info.metaspace_pointers_do(_it);\n+      info.metaspace_pointers_do(it);\n@@ -739,3 +725,2 @@\n-    return true; \/\/ keep on iterating\n-  }\n-};\n+  };\n+  _dumptime_table->iterate_all_live_classes(do_klass);\n@@ -743,21 +728,7 @@\n-class IterateDumpTimeLambdaProxyClassDictionary : StackObj {\n-  MetaspaceClosure *_it;\n-public:\n-  IterateDumpTimeLambdaProxyClassDictionary(MetaspaceClosure* it) : _it(it) {}\n-\n-  bool do_entry(LambdaProxyClassKey& key, DumpTimeLambdaProxyClassInfo& info) {\n-    assert_lock_strong(DumpTimeTable_lock);\n-    if (key.caller_ik()->is_loader_alive()) {\n-      info.metaspace_pointers_do(_it);\n-      key.metaspace_pointers_do(_it);\n-    }\n-    return true; \/\/ keep on iterating\n-  }\n-};\n-\n-void SystemDictionaryShared::dumptime_classes_do(class MetaspaceClosure* it) {\n-  assert_lock_strong(DumpTimeTable_lock);\n-  IterateDumpTimeSharedClassTable iter(it);\n-  _dumptime_table->iterate(&iter);\n-    IterateDumpTimeLambdaProxyClassDictionary iter_lambda(it);\n-    _dumptime_lambda_proxy_class_dictionary->iterate(&iter_lambda);\n+    auto do_lambda = [&] (LambdaProxyClassKey& key, DumpTimeLambdaProxyClassInfo& info) {\n+      if (key.caller_ik()->is_loader_alive()) {\n+        info.metaspace_pointers_do(it);\n+        key.metaspace_pointers_do(it);\n+      }\n+    };\n+    _dumptime_lambda_proxy_class_dictionary->iterate_all(do_lambda);\n@@ -872,1 +843,1 @@\n-  const RunTimeLambdaProxyClassInfo* info = _lambda_proxy_class_dictionary.lookup(&key, key.hash(), 0);\n+  const RunTimeLambdaProxyClassInfo* info = _static_archive.lookup_lambda_proxy_class(&key);\n@@ -874,2 +845,1 @@\n-    \/\/ Try lookup from the dynamic lambda proxy class dictionary.\n-    info = _dynamic_lambda_proxy_class_dictionary.lookup(&key, key.hash(), 0);\n+    info = _dynamic_archive.lookup_lambda_proxy_class(&key);\n@@ -1176,1 +1146,1 @@\n-  bool do_entry(InstanceKlass* k, DumpTimeClassInfo& info) {\n+  void do_entry(InstanceKlass* k, DumpTimeClassInfo& info) {\n@@ -1181,1 +1151,0 @@\n-    return true; \/\/ keep on iterating\n@@ -1191,1 +1160,1 @@\n-  _dumptime_table->iterate(&est);\n+  _dumptime_table->iterate_all_live_classes(&est);\n@@ -1285,1 +1254,1 @@\n-  bool do_entry(InstanceKlass* k, DumpTimeClassInfo& info) {\n+  void do_entry(InstanceKlass* k, DumpTimeClassInfo& info) {\n@@ -1309,1 +1278,0 @@\n-    return true; \/\/ keep on iterating\n@@ -1329,1 +1297,1 @@\n-  _dumptime_table->iterate(&copy);\n+  _dumptime_table->iterate_all_live_classes(&copy);\n@@ -1334,6 +1302,5 @@\n-  if (is_static_archive) {\n-    write_dictionary(&_builtin_dictionary, true);\n-    write_dictionary(&_unregistered_dictionary, false);\n-  } else {\n-    write_dictionary(&_dynamic_builtin_dictionary, true);\n-    write_dictionary(&_dynamic_unregistered_dictionary, false);\n+  ArchiveInfo* archive = get_archive(is_static_archive);\n+\n+  if (_dumptime_table != NULL) {\n+    write_dictionary(&archive->_builtin_dictionary, true);\n+    write_dictionary(&archive->_unregistered_dictionary, false);\n@@ -1342,1 +1309,1 @@\n-    write_lambda_proxy_class_dictionary(&_lambda_proxy_class_dictionary);\n+    write_lambda_proxy_class_dictionary(&archive->_lambda_proxy_class_dictionary);\n@@ -1355,16 +1322,5 @@\n-  FileMapInfo *dynamic_mapinfo = FileMapInfo::dynamic_info();\n-  if (is_static_archive) {\n-    _builtin_dictionary.serialize_header(soc);\n-    _unregistered_dictionary.serialize_header(soc);\n-    if (dynamic_mapinfo == NULL || DynamicDumpSharedSpaces || (dynamic_mapinfo != NULL && UseSharedSpaces)) {\n-      _lambda_proxy_class_dictionary.serialize_header(soc);\n-    }\n-  } else {\n-    _dynamic_builtin_dictionary.serialize_header(soc);\n-    _dynamic_unregistered_dictionary.serialize_header(soc);\n-    if (DynamicDumpSharedSpaces) {\n-      _lambda_proxy_class_dictionary.serialize_header(soc);\n-    } else {\n-      _dynamic_lambda_proxy_class_dictionary.serialize_header(soc);\n-    }\n-  }\n+  ArchiveInfo* archive = get_archive(is_static_archive);\n+\n+  archive->_builtin_dictionary.serialize_header(soc);\n+  archive->_unregistered_dictionary.serialize_header(soc);\n+  archive->_lambda_proxy_class_dictionary.serialize_header(soc);\n@@ -1389,1 +1345,2 @@\n-    \/\/ Those regenerated holder classes are in dynamic archive\n+    \/\/ Use the regenerated holder classes in the dynamic archive as they\n+    \/\/ have more methods than those in the base archive.\n@@ -1415,1 +1372,3 @@\n-  const RunTimeClassInfo* record = find_record(&_builtin_dictionary, &_dynamic_builtin_dictionary, name);\n+  const RunTimeClassInfo* record = find_record(&_static_archive._builtin_dictionary,\n+                                               &_dynamic_archive._builtin_dictionary,\n+                                               name);\n@@ -1423,2 +1382,1 @@\n-    \/\/ assert(check_alignment(record->_klass), \"Address not aligned\");\n-    \/\/ We did not save the classfile data of the regenerated LambdaForm invoker classes,\n+    \/\/ We did not save the classfile data of the generated LambdaForm invoker classes,\n@@ -1426,1 +1384,1 @@\n-    if (record->_klass->is_regenerated() && JvmtiExport::should_post_class_file_load_hook()) {\n+    if (record->_klass->is_generated_shared_class() && JvmtiExport::should_post_class_file_load_hook()) {\n@@ -1492,5 +1450,2 @@\n-void SystemDictionaryShared::print_on(const char* prefix,\n-                                      RunTimeSharedDictionary* builtin_dictionary,\n-                                      RunTimeSharedDictionary* unregistered_dictionary,\n-                                      LambdaProxyClassDictionary* lambda_dictionary,\n-                                      outputStream* st) {\n+void SystemDictionaryShared::ArchiveInfo::print_on(const char* prefix,\n+                                                   outputStream* st) {\n@@ -1500,1 +1455,1 @@\n-  builtin_dictionary->iterate(&p);\n+  _builtin_dictionary.iterate(&p);\n@@ -1502,2 +1457,2 @@\n-  unregistered_dictionary->iterate(&p);\n-  if (!lambda_dictionary->empty()) {\n+  _unregistered_dictionary.iterate(&p);\n+  if (!_lambda_proxy_class_dictionary.empty()) {\n@@ -1506,1 +1461,1 @@\n-    lambda_dictionary->iterate(&ldp);\n+    _lambda_proxy_class_dictionary.iterate(&ldp);\n@@ -1510,0 +1465,8 @@\n+void SystemDictionaryShared::ArchiveInfo::print_table_statistics(const char* prefix,\n+                                                                 outputStream* st) {\n+  st->print_cr(\"%sArchve Statistics\", prefix);\n+  _builtin_dictionary.print_table_statistics(st, \"Builtin Shared Dictionary\");\n+  _unregistered_dictionary.print_table_statistics(st, \"Unregistered Shared Dictionary\");\n+  _lambda_proxy_class_dictionary.print_table_statistics(st, \"Lambda Shared Dictionary\");\n+}\n+\n@@ -1513,1 +1476,1 @@\n-      print_on(\"\", &_builtin_dictionary, &_unregistered_dictionary, &_lambda_proxy_class_dictionary, st);\n+      _static_archive.print_on(\"\", st);\n@@ -1516,2 +1479,1 @@\n-        print_on(\"\", &_dynamic_builtin_dictionary, &_dynamic_unregistered_dictionary,\n-               &_dynamic_lambda_proxy_class_dictionary, st);\n+        _dynamic_archive.print_on(\"Dynamic \", st);\n@@ -1524,7 +1486,2 @@\n-  if (UseSharedSpaces) {\n-    print_on(\"\", &_builtin_dictionary, &_unregistered_dictionary, &_lambda_proxy_class_dictionary, st);\n-    if (DynamicArchive::is_mapped()) {\n-      print_on(\"\", &_dynamic_builtin_dictionary, &_dynamic_unregistered_dictionary,\n-               &_dynamic_lambda_proxy_class_dictionary, st);\n-    }\n-  }\n+  print_shared_archive(st, true);\n+  print_shared_archive(st, false);\n@@ -1535,3 +1492,1 @@\n-    _builtin_dictionary.print_table_statistics(st, \"Builtin Shared Dictionary\");\n-    _unregistered_dictionary.print_table_statistics(st, \"Unregistered Shared Dictionary\");\n-    _lambda_proxy_class_dictionary.print_table_statistics(st, \"Lambda Shared Dictionary\");\n+    _static_archive.print_table_statistics(\"Static \", st);\n@@ -1539,3 +1494,1 @@\n-      _dynamic_builtin_dictionary.print_table_statistics(st, \"Dynamic Builtin Shared Dictionary\");\n-      _dynamic_unregistered_dictionary.print_table_statistics(st, \"Unregistered Shared Dictionary\");\n-      _dynamic_lambda_proxy_class_dictionary.print_table_statistics(st, \"Dynamic Lambda Shared Dictionary\");\n+      _dynamic_archive.print_table_statistics(\"Dynamic \", st);\n@@ -1567,1 +1520,1 @@\n-  bool do_entry(InstanceKlass* k, DumpTimeClassInfo& info) {\n+  void do_entry(InstanceKlass* k, DumpTimeClassInfo& info) {\n@@ -1572,1 +1525,0 @@\n-    return true; \/\/ keep on iterating\n@@ -1605,1 +1557,1 @@\n-    _dumptime_table->iterate(&copy_classes);\n+    _dumptime_table->iterate_all_live_classes(&copy_classes);\n@@ -1634,8 +1586,7 @@\n-    if (SystemDictionaryShared::check_for_exclusion(caller_ik, NULL)) {\n-      \/\/ If the caller class is excluded, unregister all the associated lambda proxy classes\n-      \/\/ so that they will not be included in the CDS archive.\n-      for (int i = info._proxy_klasses->length() - 1; i >= 0; i--) {\n-        SystemDictionaryShared::reset_registered_lambda_proxy_class(info._proxy_klasses->at(i));\n-        info._proxy_klasses->remove_at(i);\n-      }\n-    }\n+    InstanceKlass* nest_host = caller_ik->nest_host_not_null();\n+\n+    \/\/ If the caller class and\/or nest_host are excluded, the associated lambda proxy\n+    \/\/ must also be excluded.\n+    bool always_exclude = SystemDictionaryShared::check_for_exclusion(caller_ik, NULL) ||\n+                          SystemDictionaryShared::check_for_exclusion(nest_host, NULL);\n+\n@@ -1644,1 +1595,1 @@\n-      if (SystemDictionaryShared::check_for_exclusion(ik, NULL)) {\n+      if (always_exclude || SystemDictionaryShared::check_for_exclusion(ik, NULL)) {\n@@ -1718,3 +1669,4 @@\n-  update_archived_mirror_native_pointers_for(&_builtin_dictionary);\n-  update_archived_mirror_native_pointers_for(&_unregistered_dictionary);\n-  update_archived_mirror_native_pointers_for(&_lambda_proxy_class_dictionary);\n+  \/\/ mirrors are not archived for the classes in the dynamic archive\n+  update_archived_mirror_native_pointers_for(&_static_archive._builtin_dictionary);\n+  update_archived_mirror_native_pointers_for(&_static_archive._unregistered_dictionary);\n+  update_archived_mirror_native_pointers_for(&_static_archive._lambda_proxy_class_dictionary);\n@@ -1726,0 +1678,1 @@\n+  ArchivedMirrorPatcher::update_array_klasses(Universe::fillerArrayKlassObj());\n","filename":"src\/hotspot\/share\/classfile\/systemDictionaryShared.cpp","additions":85,"deletions":132,"binary":false,"changes":217,"status":"modified"},{"patch":"@@ -290,2 +290,0 @@\n-  _verifier->check_bitmaps(\"Humongous Region Allocation\", first_hr);\n-\n@@ -441,1 +439,1 @@\n-                                                              : GCCause::_g1_inc_collection_pause;\n+                                                               : GCCause::_g1_inc_collection_pause;\n@@ -990,1 +988,1 @@\n-void G1CollectedHeap::abort_concurrent_cycle() {\n+bool G1CollectedHeap::abort_concurrent_cycle() {\n@@ -1007,1 +1005,1 @@\n-  concurrent_mark()->concurrent_cycle_abort();\n+  return concurrent_mark()->concurrent_cycle_abort();\n@@ -1032,1 +1030,1 @@\n-  _verifier->check_bitmaps(\"Full GC Start\");\n+  _verifier->verify_bitmap_clear(true \/* above_tams_only *\/);\n@@ -1081,3 +1079,1 @@\n-\n-  \/\/ This call implicitly verifies that the next bitmap is clear after Full GC.\n-  _verifier->check_bitmaps(\"Full GC End\");\n+  _verifier->verify_bitmap_clear(false \/* above_tams_only *\/);\n@@ -1341,1 +1337,1 @@\n-  log_debug(gc, ergo, heap)(\"Shrink the heap. requested shrinking amount: \" SIZE_FORMAT \"B aligned shrinking amount: \" SIZE_FORMAT \"B attempted shrinking amount: \" SIZE_FORMAT \"B\",\n+  log_debug(gc, ergo, heap)(\"Shrink the heap. requested shrinking amount: \" SIZE_FORMAT \"B aligned shrinking amount: \" SIZE_FORMAT \"B actual amount shrunk: \" SIZE_FORMAT \"B\",\n@@ -1347,1 +1343,1 @@\n-    log_debug(gc, ergo, heap)(\"Did not expand the heap (heap shrinking operation failed)\");\n+    log_debug(gc, ergo, heap)(\"Did not shrink the heap (heap shrinking operation failed)\");\n@@ -1457,1 +1453,0 @@\n-  _humongous_reclaim_candidates(),\n@@ -1498,0 +1493,3 @@\n+  \/\/ Override the default _stack_chunk_max_size so that no humongous stack chunks are created\n+  _stack_chunk_max_size = _humongous_object_threshold_in_words;\n+\n@@ -1632,1 +1630,1 @@\n-  \/\/ Create storage for the BOT, card table, card counts table (hot card cache) and the bitmaps.\n+  \/\/ Create storage for the BOT, card table, card counts table (hot card cache) and the bitmap.\n@@ -1649,4 +1647,2 @@\n-  G1RegionToSpaceMapper* prev_bitmap_storage =\n-    create_aux_memory_mapper(\"Prev Bitmap\", bitmap_size, G1CMBitMap::heap_map_factor());\n-  G1RegionToSpaceMapper* next_bitmap_storage =\n-    create_aux_memory_mapper(\"Next Bitmap\", bitmap_size, G1CMBitMap::heap_map_factor());\n+  G1RegionToSpaceMapper* bitmap_storage =\n+    create_aux_memory_mapper(\"Mark Bitmap\", bitmap_size, G1CMBitMap::heap_map_factor());\n@@ -1654,1 +1650,1 @@\n-  _hrm.initialize(heap_storage, prev_bitmap_storage, next_bitmap_storage, bot_storage, cardtable_storage, card_counts_storage);\n+  _hrm.initialize(heap_storage, bitmap_storage, bot_storage, cardtable_storage, card_counts_storage);\n@@ -1688,1 +1684,0 @@\n-    _humongous_reclaim_candidates.initialize(reserved(), granularity);\n@@ -1701,1 +1696,1 @@\n-  _cm = new G1ConcurrentMark(this, prev_bitmap_storage, next_bitmap_storage);\n+  _cm = new G1ConcurrentMark(this, bitmap_storage);\n@@ -1900,0 +1895,1 @@\n+    case GCCause::_codecache_GC_threshold:  return true;\n@@ -2351,0 +2347,6 @@\n+  \/\/ The CollectedHeap API requires us to not fail for any given address within\n+  \/\/ the heap. HeapRegion::block_start() has been optimized to not accept addresses\n+  \/\/ outside of the allocated area.\n+  if (addr >= hr->top()) {\n+    return nullptr;\n+  }\n@@ -2356,1 +2358,1 @@\n-  return hr->block_is_obj(addr);\n+  return hr->block_is_obj(addr, hr->parsable_bottom_acquire());\n@@ -2411,3 +2413,3 @@\n-  case VerifyOption_G1UsePrevMarking: return is_obj_dead(obj, hr);\n-  case VerifyOption_G1UseFullMarking: return is_obj_dead_full(obj, hr);\n-  default:                            ShouldNotReachHere();\n+    case VerifyOption::G1UseConcMarking: return is_obj_dead(obj, hr);\n+    case VerifyOption::G1UseFullMarking: return is_obj_dead_full(obj, hr);\n+    default:                             ShouldNotReachHere();\n@@ -2421,3 +2423,3 @@\n-  case VerifyOption_G1UsePrevMarking: return is_obj_dead(obj);\n-  case VerifyOption_G1UseFullMarking: return is_obj_dead_full(obj);\n-  default:                            ShouldNotReachHere();\n+    case VerifyOption::G1UseConcMarking: return is_obj_dead(obj);\n+    case VerifyOption::G1UseFullMarking: return is_obj_dead_full(obj);\n+    default:                             ShouldNotReachHere();\n@@ -2471,1 +2473,2 @@\n-               \"TAMS=top-at-mark-start (previous, next)\");\n+               \"TAMS=top-at-mark-start, \"\n+               \"PB=parsable bottom\");\n@@ -2684,3 +2687,1 @@\n-  return G1EagerReclaimHumongousObjectsWithStaleRefs ?\n-         rem_set->occupancy_less_or_equal_than(G1EagerReclaimRemSetThreshold) :\n-         G1EagerReclaimHumongousObjects && rem_set->is_empty();\n+  return rem_set->occupancy_less_or_equal_than(G1EagerReclaimRemSetThreshold);\n@@ -2757,1 +2758,0 @@\n-  _verifier->check_bitmaps(\"GC Start\");\n@@ -2773,1 +2773,0 @@\n-  _verifier->check_bitmaps(\"GC End\");\n@@ -2806,1 +2805,3 @@\n-  _g1h->rem_set()->print_periodic_summary_info(\"Before GC RS summary\", _g1h->total_collections());\n+  _g1h->rem_set()->print_periodic_summary_info(\"Before GC RS summary\",\n+                                               _g1h->total_collections(),\n+                                               true \/* show_thread_times *\/);\n@@ -2815,1 +2816,3 @@\n-  _g1h->rem_set()->print_periodic_summary_info(\"After GC RS summary\", _g1h->total_collections() - 1);\n+  _g1h->rem_set()->print_periodic_summary_info(\"After GC RS summary\",\n+                                               _g1h->total_collections() - 1,\n+                                               false \/* show_thread_times *\/);\n@@ -2882,0 +2885,1 @@\n+    verifier()->verify_bitmap_clear(true \/* above_tams_only *\/);\n@@ -2913,1 +2917,1 @@\n-      _cm->mark_in_next_bitmap(0 \/* worker_id *\/, pll_head);\n+      _cm->mark_in_bitmap(0 \/* worker_id *\/, pll_head);\n@@ -2918,12 +2922,0 @@\n-static bool do_humongous_object_logging() {\n-  return log_is_enabled(Debug, gc, humongous);\n-}\n-\n-bool G1CollectedHeap::should_do_eager_reclaim() const {\n-  \/\/ As eager reclaim logging also gives information about humongous objects in\n-  \/\/ the heap in general, always do the eager reclaim pass even without known\n-  \/\/ candidates.\n-  return (G1EagerReclaimHumongousObjects &&\n-          (has_humongous_reclaim_candidates() || do_humongous_object_logging()));\n-}\n-\n@@ -2956,3 +2948,2 @@\n-void G1CollectedHeap::clear_prev_bitmap_for_region(HeapRegion* hr) {\n-  MemRegion mr(hr->bottom(), hr->end());\n-  concurrent_mark()->clear_range_in_prev_bitmap(mr);\n+void G1CollectedHeap::clear_bitmap_for_region(HeapRegion* hr) {\n+  concurrent_mark()->clear_bitmap_for_region(hr);\n@@ -2966,4 +2957,0 @@\n-  if (G1VerifyBitmaps) {\n-    clear_prev_bitmap_for_region(hr);\n-  }\n-\n@@ -3217,1 +3204,0 @@\n-      _verifier->check_bitmaps(\"Mutator Region Allocation\", new_alloc_region);\n@@ -3274,1 +3260,0 @@\n-      _verifier->check_bitmaps(\"Survivor Region Allocation\", new_alloc_region);\n@@ -3278,1 +3263,0 @@\n-      _verifier->check_bitmaps(\"Old Region Allocation\", new_alloc_region);\n@@ -3301,1 +3285,1 @@\n-    _cm->root_regions()->add(alloc_region->next_top_at_mark_start(), alloc_region->top());\n+    _cm->root_regions()->add(alloc_region->top_at_mark_start(), alloc_region->top());\n@@ -3320,1 +3304,1 @@\n-void G1CollectedHeap::mark_evac_failure_object(const oop obj, uint worker_id) const {\n+void G1CollectedHeap::mark_evac_failure_object(const oop obj) const {\n@@ -3322,3 +3306,3 @@\n-  \/\/ that we'll update the prev marking info so that they are\n-  \/\/ all under PTAMS and explicitly marked.\n-  _cm->par_mark_in_prev_bitmap(obj);\n+  \/\/ that we'll update the marking info so that they are\n+  \/\/ all below TAMS and explicitly marked.\n+  _cm->raw_mark_in_bitmap(obj);\n@@ -3328,1 +3312,0 @@\n-\n@@ -3401,1 +3384,1 @@\n-    \/\/ The \"used\" of the the collection set have already been subtracted\n+    \/\/ The \"used\" of the collection set have already been subtracted\n@@ -3458,0 +3441,13 @@\n+\n+void G1CollectedHeap::start_codecache_marking_cycle_if_inactive() {\n+  if (!Continuations::is_gc_marking_cycle_active()) {\n+    \/\/ This is the normal case when we do not call collect when a\n+    \/\/ concurrent mark is ongoing. We then start a new code marking\n+    \/\/ cycle. If, on the other hand, a concurrent mark is ongoing, we\n+    \/\/ will be conservative and use the last code marking cycle. Code\n+    \/\/ caches marked between the two concurrent marks will live a bit\n+    \/\/ longer than needed.\n+    Continuations::on_gc_marking_cycle_start();\n+    Continuations::arm_all_nmethods();\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":61,"deletions":65,"binary":false,"changes":126,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -58,0 +58,1 @@\n+#include \"runtime\/mutexLocker.hpp\"\n@@ -234,20 +235,0 @@\n-  \/\/ Records whether the region at the given index is (still) a\n-  \/\/ candidate for eager reclaim.  Only valid for humongous start\n-  \/\/ regions; other regions have unspecified values.  Humongous start\n-  \/\/ regions are initialized at start of collection pause, with\n-  \/\/ candidates removed from the set as they are found reachable from\n-  \/\/ roots or the young generation.\n-  class HumongousReclaimCandidates : public G1BiasedMappedArray<bool> {\n-  protected:\n-    bool default_value() const override { return false; }\n-  public:\n-    void clear() { G1BiasedMappedArray<bool>::clear(); }\n-    void set_candidate(uint region, bool value) {\n-      set_by_index(region, value);\n-    }\n-    bool is_candidate(uint region) {\n-      return get_by_index(region);\n-    }\n-  };\n-\n-  HumongousReclaimCandidates _humongous_reclaim_candidates;\n@@ -261,2 +242,0 @@\n-  bool should_do_eager_reclaim() const;\n-\n@@ -518,1 +497,1 @@\n-  void abort_concurrent_cycle();\n+  bool abort_concurrent_cycle();\n@@ -599,3 +578,0 @@\n-  \/\/ Modify the reclaim candidate set and test for presence.\n-  \/\/ These are only valid for starts_humongous regions.\n-  inline void set_humongous_reclaim_candidate(uint region, bool value);\n@@ -633,1 +609,1 @@\n-  void clear_prev_bitmap_for_region(HeapRegion* hr);\n+  void clear_bitmap_for_region(HeapRegion* hr);\n@@ -947,0 +923,2 @@\n+  static void start_codecache_marking_cycle_if_inactive();\n+\n@@ -1052,3 +1030,3 @@\n-  inline bool is_in_cset(const HeapRegion *hr);\n-  inline bool is_in_cset(oop obj);\n-  inline bool is_in_cset(HeapWord* addr);\n+  inline bool is_in_cset(const HeapRegion *hr) const;\n+  inline bool is_in_cset(oop obj) const;\n+  inline bool is_in_cset(HeapWord* addr) const;\n@@ -1190,0 +1168,1 @@\n+  inline bool requires_barriers(stackChunkOop obj) const override;\n@@ -1241,1 +1220,1 @@\n-  bool is_marked_next(oop obj) const;\n+  bool is_marked(oop obj) const;\n@@ -1250,3 +1229,1 @@\n-\n-  \/\/ Added if it is NULL it isn't dead.\n-\n+  \/\/ If obj is NULL it is not dead.\n@@ -1259,1 +1236,1 @@\n-  void mark_evac_failure_object(const oop obj, uint worker_id) const;\n+  void mark_evac_failure_object(oop obj) const;\n@@ -1304,10 +1281,0 @@\n-\n-  \/\/ vo == UsePrevMarking -> use \"prev\" marking information,\n-  \/\/ vo == UseFullMarking -> use \"next\" marking bitmap but no TAMS\n-  \/\/\n-  \/\/ NOTE: Only the \"prev\" marking information is guaranteed to be\n-  \/\/ consistent most of the time, so most calls to this should use\n-  \/\/ vo == UsePrevMarking.\n-  \/\/ Currently there is only one place where this is called with\n-  \/\/ vo == UseFullMarking, which is to verify the marking during a\n-  \/\/ full GC.\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.hpp","additions":13,"deletions":46,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -48,0 +48,1 @@\n+#include \"runtime\/continuation.hpp\"\n@@ -70,1 +71,1 @@\n-  return _heap->concurrent_mark()->next_mark_bitmap();\n+  return _heap->concurrent_mark()->mark_bitmap();\n@@ -123,1 +124,1 @@\n-    _is_alive(this, heap->concurrent_mark()->next_mark_bitmap()),\n+    _is_alive(this, heap->concurrent_mark()->mark_bitmap()),\n@@ -174,1 +175,2 @@\n-  _heap->abort_concurrent_cycle();\n+  \/\/ Verification needs the bitmap, so we should clear the bitmap only later.\n+  bool in_concurrent_cycle = _heap->abort_concurrent_cycle();\n@@ -176,0 +178,4 @@\n+  if (in_concurrent_cycle) {\n+    GCTraceTime(Debug, gc) debug(\"Clear Bitmap\");\n+    _heap->concurrent_mark()->clear_bitmap(_heap->workers());\n+  }\n@@ -191,0 +197,2 @@\n+  G1CollectedHeap::start_codecache_marking_cycle_if_inactive();\n+\n@@ -202,0 +210,3 @@\n+\n+  Continuations::on_gc_marking_cycle_finish();\n+  Continuations::arm_all_nmethods();\n@@ -212,2 +223,1 @@\n-  _heap->concurrent_mark()->swap_mark_bitmaps();\n-  _heap->concurrent_mark()->clear_next_bitmap(_heap->workers());\n+  _heap->concurrent_mark()->clear_bitmap(_heap->workers());\n@@ -422,1 +432,1 @@\n-  _heap->verify(VerifyOption_G1UseFullMarking);\n+  _heap->verify(VerifyOption::G1UseFullMarking);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullCollector.cpp","additions":16,"deletions":6,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -72,1 +72,1 @@\n-void G1FullGCCompactTask::G1CompactRegionClosure::clear_in_prev_bitmap(oop obj) {\n+void G1FullGCCompactTask::G1CompactRegionClosure::clear_in_bitmap(oop obj) {\n@@ -79,6 +79,7 @@\n-  if (!obj->is_forwarded()) {\n-    \/\/ Object not moving, but clear the mark to allow reuse of the bitmap.\n-    clear_in_prev_bitmap(obj);\n-    return size;\n-  }\n-  HeapWord* destination = cast_from_oop<HeapWord*>(_forwarding->forwardee(obj));\n+  if (obj->is_forwarded()) {\n+    HeapWord* destination = cast_from_oop<HeapWord*>(_forwarding->forwardee(obj));\n+\n+    \/\/ copy object and reinit its mark\n+    HeapWord* obj_addr = cast_from_oop<HeapWord*>(obj);\n+    assert(obj_addr != destination, \"everything in this pass should be moving\");\n+    Copy::aligned_conjoint_words(obj_addr, destination, size);\n@@ -86,6 +87,4 @@\n-  \/\/ copy object and reinit its mark\n-  HeapWord* obj_addr = cast_from_oop<HeapWord*>(obj);\n-  assert(obj_addr != destination, \"everything in this pass should be moving\");\n-  Copy::aligned_conjoint_words(obj_addr, destination, size);\n-  cast_to_oop(destination)->init_mark();\n-  assert(cast_to_oop(destination)->klass() != NULL, \"should have a class\");\n+    \/\/ There is no need to transform stack chunks - marking already did that.\n+    cast_to_oop(destination)->init_mark();\n+    assert(cast_to_oop(destination)->klass() != NULL, \"should have a class\");\n+  }\n@@ -95,1 +94,1 @@\n-  clear_in_prev_bitmap(obj);\n+  clear_in_bitmap(obj);\n@@ -106,1 +105,1 @@\n-    \/\/ for bitmap verification and to be able to use the prev_bitmap\n+    \/\/ for bitmap verification and to be able to use the bitmap\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactTask.cpp","additions":15,"deletions":16,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -55,1 +55,1 @@\n-    void clear_in_prev_bitmap(oop object);\n+    void clear_in_bitmap(oop object);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactTask.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -80,0 +80,2 @@\n+  virtual void do_method(Method* m);\n+  virtual void do_nmethod(nmethod* nm);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCOopClosures.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -67,0 +67,8 @@\n+inline void G1MarkAndPushClosure::do_method(Method* m) {\n+  m->record_gc_epoch();\n+}\n+\n+inline void G1MarkAndPushClosure::do_nmethod(nmethod* nm) {\n+  nm->follow_nmethod(this);\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCOopClosures.inline.hpp","additions":9,"deletions":1,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -135,6 +135,2 @@\n-    if (hr->is_young()) {\n-      \/\/ G1 updates the BOT for old region contents incrementally, but young regions\n-      \/\/ lack BOT information for performance reasons.\n-      \/\/ Recreate BOT information of high live ratio young regions here to keep expected\n-      \/\/ performance during scanning their card tables in the collection pauses later.\n-      hr->update_bot();\n+    if (hr->needs_scrubbing_during_full_gc()) {\n+      scrub_skip_compacting_region(hr, hr->is_young());\n@@ -165,0 +161,28 @@\n+\n+void G1FullGCPrepareTask::G1ResetMetadataClosure::scrub_skip_compacting_region(HeapRegion* hr, bool update_bot_for_live) {\n+  assert(hr->needs_scrubbing_during_full_gc(), \"must be\");\n+\n+  HeapWord* limit = hr->top();\n+  HeapWord* current_obj = hr->bottom();\n+  G1CMBitMap* bitmap = _collector->mark_bitmap();\n+\n+  while (current_obj < limit) {\n+    if (bitmap->is_marked(current_obj)) {\n+      oop current = cast_to_oop(current_obj);\n+      size_t size = current->size();\n+      if (update_bot_for_live) {\n+        hr->update_bot_for_block(current_obj, current_obj + size);\n+      }\n+      current_obj += size;\n+      continue;\n+    }\n+    \/\/ Found dead object, which is potentially unloaded, scrub to next\n+    \/\/ marked object.\n+    HeapWord* scrub_start = current_obj;\n+    HeapWord* scrub_end = bitmap->get_next_marked_addr(scrub_start, limit);\n+    assert(scrub_start != scrub_end, \"must advance\");\n+    hr->fill_range_with_dead_objects(scrub_start, scrub_end);\n+\n+    current_obj = scrub_end;\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCPrepareTask.cpp","additions":30,"deletions":6,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -99,0 +99,4 @@\n+    \/\/ Scrub all runs of dead objects within the given region by putting filler\n+    \/\/ objects and updating the corresponding BOT. If update_bot_for_live is true,\n+    \/\/ also update the BOT for live objects.\n+    void scrub_skip_compacting_region(HeapRegion* hr, bool update_bot_for_live);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCPrepareTask.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -111,1 +111,1 @@\n-  _cm->mark_in_next_bitmap(_worker_id, obj);\n+  _cm->mark_in_bitmap(_worker_id, obj);\n@@ -213,1 +213,1 @@\n-  _cm->mark_in_next_bitmap(_worker_id, obj);\n+  _cm->mark_in_bitmap(_worker_id, obj);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1OopClosures.inline.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"gc\/shared\/continuationGCSupport.inline.hpp\"\n@@ -536,0 +537,2 @@\n+    ContinuationGCSupport::transform_stack_chunk(obj);\n+\n@@ -631,3 +634,3 @@\n-    \/\/ are relabeled as such. We mark the failing objects in the prev bitmap and\n-    \/\/ later use it to handle all failed objects.\n-    _g1h->mark_evac_failure_object(old, _worker_id);\n+    \/\/ are relabeled as such. We mark the failing objects in the marking bitmap\n+    \/\/ and later use it to handle all failed objects.\n+    _g1h->mark_evac_failure_object(old);\n@@ -640,0 +643,3 @@\n+\n+    ContinuationGCSupport::transform_stack_chunk(old);\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.cpp","additions":9,"deletions":3,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2006, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2006, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -36,0 +36,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -37,1 +38,0 @@\n-#include \"runtime\/thread.inline.hpp\"\n@@ -731,1 +731,1 @@\n-\/\/ Mark the the holes in chunks below the top() as invalid.\n+\/\/ Mark the holes in chunks below the top() as invalid.\n","filename":"src\/hotspot\/share\/gc\/parallel\/mutableNUMASpace.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/shared\/continuationGCSupport.inline.hpp\"\n@@ -338,0 +339,2 @@\n+    ContinuationGCSupport::transform_stack_chunk(obj);\n+\n","filename":"src\/hotspot\/share\/gc\/parallel\/psPromotionManager.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"gc\/shared\/continuationGCSupport.inline.hpp\"\n@@ -44,0 +45,1 @@\n+#include \"utilities\/copy.hpp\"\n@@ -253,0 +255,6 @@\n+  if (!new_obj->mark().is_marked()) {\n+    \/\/ Parallel GC claims with a release - so other threads might access this object\n+    \/\/ after claiming and they should see the \"completed\" object.\n+    ContinuationGCSupport::transform_stack_chunk(new_obj);\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/parallel\/psPromotionManager.inline.hpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -35,0 +35,1 @@\n+#include \"gc\/shared\/continuationGCSupport.inline.hpp\"\n@@ -57,0 +58,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -58,1 +60,1 @@\n-#include \"runtime\/thread.inline.hpp\"\n+#include \"runtime\/threads.hpp\"\n@@ -459,3 +461,0 @@\n-HeapWord* volatile* DefNewGeneration::top_addr() const { return eden()->top_addr(); }\n-HeapWord** DefNewGeneration::end_addr() const { return eden()->end_addr(); }\n-\n@@ -710,0 +709,3 @@\n+\n+  ContinuationGCSupport::transform_stack_chunk(old);\n+\n@@ -751,0 +753,2 @@\n+    ContinuationGCSupport::transform_stack_chunk(obj);\n+\n","filename":"src\/hotspot\/share\/gc\/serial\/defNewGeneration.cpp","additions":9,"deletions":5,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -56,0 +56,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -57,1 +58,0 @@\n-#include \"runtime\/thread.inline.hpp\"\n","filename":"src\/hotspot\/share\/gc\/serial\/genMarkSweep.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -41,0 +41,2 @@\n+class Method;\n+class nmethod;\n@@ -183,0 +185,2 @@\n+  virtual void do_method(Method* m);\n+  virtual void do_nmethod(nmethod* nm);\n","filename":"src\/hotspot\/share\/gc\/serial\/markSweep.hpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -33,0 +33,2 @@\n+#include \"code\/nmethod.hpp\"\n+#include \"gc\/shared\/continuationGCSupport.inline.hpp\"\n@@ -38,0 +40,1 @@\n+#include \"oops\/method.hpp\"\n@@ -54,0 +57,2 @@\n+  ContinuationGCSupport::transform_stack_chunk(obj);\n+\n@@ -85,0 +90,2 @@\n+inline void MarkAndPushClosure::do_method(Method* m)         { m->record_gc_epoch(); }\n+inline void MarkAndPushClosure::do_nmethod(nmethod* nm)      { nm->follow_nmethod(this); }\n","filename":"src\/hotspot\/share\/gc\/serial\/markSweep.inline.hpp","additions":8,"deletions":1,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -52,0 +52,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -53,1 +54,0 @@\n-#include \"runtime\/thread.inline.hpp\"\n@@ -63,0 +63,2 @@\n+size_t CollectedHeap::_lab_alignment_reserve = ~(size_t)0;\n+Klass* CollectedHeap::_filler_object_klass = NULL;\n@@ -64,0 +66,1 @@\n+size_t CollectedHeap::_stack_chunk_max_size = 0;\n@@ -239,0 +242,8 @@\n+  \/\/ If the minimum object size is greater than MinObjAlignment, we can\n+  \/\/ end up with a shard at the end of the buffer that's smaller than\n+  \/\/ the smallest object.  We can't allow that because the buffer must\n+  \/\/ look like it's full of objects when we retire it, so we make\n+  \/\/ sure we have enough space for a filler int array object.\n+  size_t min_size = min_dummy_object_size();\n+  _lab_alignment_reserve = min_size > (size_t)MinObjAlignment ? align_object_size(min_size) : 0;\n+\n@@ -279,0 +290,1 @@\n+    case GCCause::_codecache_GC_threshold:\n@@ -449,1 +461,1 @@\n-  ObjArrayAllocator allocator(Universe::intArrayKlassObj(), words, (int)len, \/* do_zero *\/ false);\n+  ObjArrayAllocator allocator(Universe::fillerArrayKlassObj(), words, (int)len, \/* do_zero *\/ false);\n@@ -469,1 +481,1 @@\n-    ObjAllocator allocator(vmClasses::Object_klass(), words);\n+    ObjAllocator allocator(CollectedHeap::filler_object_klass(), words);\n@@ -505,5 +517,0 @@\n-size_t CollectedHeap::tlab_alloc_reserve() const {\n-  size_t min_size = min_dummy_object_size();\n-  return min_size > (size_t)MinObjAlignment ? align_object_size(min_size) : 0;\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.cpp","additions":15,"deletions":8,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"oops\/stackChunkOop.hpp\"\n@@ -107,0 +108,4 @@\n+  \/\/ First, set it to java_lang_Object.\n+  \/\/ Then, set it to FillerObject after the FillerObject_klass loading is complete.\n+  static Klass* _filler_object_klass;\n+\n@@ -113,0 +118,2 @@\n+  \/\/ (Minimum) Alignment reserve for TLABs and PLABs.\n+  static size_t _lab_alignment_reserve;\n@@ -116,0 +123,2 @@\n+  static size_t _stack_chunk_max_size; \/\/ 0 for no limit\n+\n@@ -206,0 +215,12 @@\n+  static inline size_t stack_chunk_max_size() {\n+    return _stack_chunk_max_size;\n+  }\n+\n+  static inline Klass* filler_object_klass() {\n+    return _filler_object_klass;\n+  }\n+\n+  static inline void set_filler_object_klass(Klass* k) {\n+    _filler_object_klass = k;\n+  }\n+\n@@ -296,21 +317,3 @@\n-  size_t tlab_alloc_reserve() const;\n-\n-  \/\/ Some heaps may offer a contiguous region for shared non-blocking\n-  \/\/ allocation, via inlined code (by exporting the address of the top and\n-  \/\/ end fields defining the extent of the contiguous allocation region.)\n-\n-  \/\/ This function returns \"true\" iff the heap supports this kind of\n-  \/\/ allocation.  (Default is \"no\".)\n-  virtual bool supports_inline_contig_alloc() const {\n-    return false;\n-  }\n-  \/\/ These functions return the addresses of the fields that define the\n-  \/\/ boundaries of the contiguous allocation area.  (These fields should be\n-  \/\/ physically near to one another.)\n-  virtual HeapWord* volatile* top_addr() const {\n-    guarantee(false, \"inline contiguous allocation not supported\");\n-    return NULL;\n-  }\n-  virtual HeapWord** end_addr() const {\n-    guarantee(false, \"inline contiguous allocation not supported\");\n-    return NULL;\n+  static size_t lab_alignment_reserve() {\n+    assert(_lab_alignment_reserve != ~(size_t)0, \"uninitialized\");\n+    return _lab_alignment_reserve;\n@@ -375,0 +378,9 @@\n+  \/\/ Return true, if accesses to the object would require barriers.\n+  \/\/ This is used by continuations to copy chunks of a thread stack into StackChunk object or out of a StackChunk\n+  \/\/ object back into the thread stack. These chunks may contain references to objects. It is crucial that\n+  \/\/ the GC does not attempt to traverse the object while we modify it, because its structure (oopmap) is changed\n+  \/\/ when stack chunks are stored into it.\n+  \/\/ StackChunk objects may be reused, the GC must not assume that a StackChunk object is always a freshly\n+  \/\/ allocated object.\n+  virtual bool requires_barriers(stackChunkOop obj) const = 0;\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":33,"deletions":21,"binary":false,"changes":54,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"gc\/shared\/continuationGCSupport.inline.hpp\"\n@@ -66,0 +67,1 @@\n+#include \"runtime\/continuation.hpp\"\n@@ -69,0 +71,1 @@\n+#include \"runtime\/threads.hpp\"\n@@ -305,2 +308,0 @@\n-    assert(young->supports_inline_contig_alloc(),\n-      \"Otherwise, must do alloc within heap lock\");\n@@ -611,0 +612,3 @@\n+    Continuations::on_gc_marking_cycle_start();\n+    Continuations::arm_all_nmethods();\n+\n@@ -618,0 +622,3 @@\n+    Continuations::on_gc_marking_cycle_finish();\n+    Continuations::arm_all_nmethods();\n+\n@@ -795,1 +802,4 @@\n-  MarkingCodeBlobClosure mark_code_closure(root_closure, is_adjust_phase);\n+  \/\/ Called from either the marking phase or the adjust phase.\n+  const bool is_marking_phase = !is_adjust_phase;\n+\n+  MarkingCodeBlobClosure mark_code_closure(root_closure, is_adjust_phase, is_marking_phase);\n@@ -810,12 +820,0 @@\n-bool GenCollectedHeap::supports_inline_contig_alloc() const {\n-  return _young_gen->supports_inline_contig_alloc();\n-}\n-\n-HeapWord* volatile* GenCollectedHeap::top_addr() const {\n-  return _young_gen->top_addr();\n-}\n-\n-HeapWord** GenCollectedHeap::end_addr() const {\n-  return _young_gen->end_addr();\n-}\n-\n@@ -893,1 +891,1 @@\n-bool GenCollectedHeap::is_in_young(oop p) {\n+bool GenCollectedHeap::is_in_young(oop p) const {\n@@ -900,0 +898,4 @@\n+bool GenCollectedHeap::requires_barriers(stackChunkOop obj) const {\n+  return !is_in_young(obj);\n+}\n+\n@@ -1179,2 +1181,0 @@\n-  size_t actual_gap = pointer_delta((HeapWord*) (max_uintx-3), *(end_addr()));\n-  guarantee(!CompilerConfig::is_c2_or_jvmci_compiler_enabled() || actual_gap > (size_t)FastAllocateSizeLimit, \"inline allocation wraps\");\n@@ -1220,15 +1220,0 @@\n-\n-oop GenCollectedHeap::handle_failed_promotion(Generation* old_gen,\n-                                              oop obj,\n-                                              size_t obj_size) {\n-  guarantee(old_gen == _old_gen, \"We only get here with an old generation\");\n-  assert(obj_size == obj->size(), \"bad obj_size passed in\");\n-  HeapWord* result = NULL;\n-\n-  result = old_gen->expand_and_allocate(obj_size, false);\n-\n-  if (result != NULL) {\n-    Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(obj), result, obj_size);\n-  }\n-  return cast_to_oop(result);\n-}\n","filename":"src\/hotspot\/share\/gc\/shared\/genCollectedHeap.cpp","additions":18,"deletions":33,"binary":false,"changes":51,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -187,6 +187,0 @@\n-  \/\/ We may support a shared contiguous allocation area, if the youngest\n-  \/\/ generation does.\n-  bool supports_inline_contig_alloc() const;\n-  HeapWord* volatile* top_addr() const;\n-  HeapWord** end_addr() const;\n-\n@@ -210,2 +204,4 @@\n-  \/\/ Assumes the the young gen address range is less than that of the old gen.\n-  bool is_in_young(oop p);\n+  \/\/ Assumes the young gen address range is less than that of the old gen.\n+  bool is_in_young(oop p) const;\n+\n+  virtual bool requires_barriers(stackChunkOop obj) const;\n","filename":"src\/hotspot\/share\/gc\/shared\/genCollectedHeap.hpp","additions":5,"deletions":9,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -38,1 +38,1 @@\n-#include \"runtime\/thread.inline.hpp\"\n+#include \"runtime\/javaThread.hpp\"\n","filename":"src\/hotspot\/share\/gc\/shared\/jvmFlagConstraintsGC.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+#include \"classfile\/vmClasses.hpp\"\n@@ -36,1 +37,1 @@\n-#include \"runtime\/sharedRuntime.hpp\"\n+#include \"runtime\/continuationJavaClasses.inline.hpp\"\n@@ -38,1 +39,2 @@\n-#include \"runtime\/thread.inline.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"runtime\/javaThread.hpp\"\n@@ -272,1 +274,1 @@\n-  HeapWord* mem = _thread->tlab().allocate(_word_size);\n+  HeapWord* mem = allocate_inside_tlab_fast();\n@@ -281,0 +283,4 @@\n+HeapWord* MemAllocator::allocate_inside_tlab_fast() const {\n+  return _thread->tlab().allocate(_word_size);\n+}\n+\n@@ -375,0 +381,15 @@\n+oop MemAllocator::try_allocate_in_existing_tlab() {\n+  oop obj = NULL;\n+  {\n+    HeapWord* mem = allocate_inside_tlab_fast();\n+    if (mem != NULL) {\n+      obj = initialize(mem);\n+    } else {\n+      \/\/ The unhandled oop detector will poison local variable obj,\n+      \/\/ so reset it to NULL if mem is NULL.\n+      obj = NULL;\n+    }\n+  }\n+  return obj;\n+}\n+\n@@ -431,0 +452,17 @@\n+\n+\/\/ Does the minimal amount of initialization needed for a TLAB allocation.\n+\/\/ We don't need to do a full initialization, as such an allocation need not be immediately walkable.\n+oop StackChunkAllocator::initialize(HeapWord* mem) const {\n+  assert(_stack_size > 0, \"\");\n+  assert(_stack_size <= max_jint, \"\");\n+  assert(_word_size > _stack_size, \"\");\n+\n+  \/\/ zero out fields (but not the stack)\n+  const size_t hs = oopDesc::header_size();\n+  Copy::fill_to_aligned_words(mem + hs, vmClasses::StackChunk_klass()->size_helper() - hs);\n+\n+  jdk_internal_vm_StackChunk::set_size(mem, (int)_stack_size);\n+  jdk_internal_vm_StackChunk::set_sp(mem, (int)_stack_size);\n+\n+  return finish(mem);\n+}\n","filename":"src\/hotspot\/share\/gc\/shared\/memAllocator.cpp","additions":42,"deletions":4,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -563,2 +563,6 @@\n-      cast_to_oop(compaction_top)->init_mark();\n-      assert(cast_to_oop(compaction_top)->klass() != NULL, \"should have a class\");\n+      oop new_obj = cast_to_oop(compaction_top);\n+\n+      ContinuationGCSupport::transform_stack_chunk(new_obj);\n+\n+      new_obj->init_mark();\n+      assert(new_obj->klass() != NULL, \"should have a class\");\n","filename":"src\/hotspot\/share\/gc\/shared\/space.cpp","additions":7,"deletions":3,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -54,0 +54,1 @@\n+class FilteringClosure;\n@@ -450,1 +451,1 @@\n-  \/\/ Used to save the an address in a space for later use during mangling.\n+  \/\/ Used to save the address in a space for later use during mangling.\n","filename":"src\/hotspot\/share\/gc\/shared\/space.hpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n-#include \"runtime\/thread.hpp\"\n+#include \"runtime\/javaThread.hpp\"\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahForwarding.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"gc\/shared\/continuationGCSupport.hpp\"\n@@ -58,0 +59,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -59,1 +61,0 @@\n-#include \"runtime\/thread.hpp\"\n@@ -750,0 +751,2 @@\n+  void do_method(Method* m) {}\n+  void do_nmethod(nmethod* nm) {}\n@@ -852,0 +855,2 @@\n+\n+      ContinuationGCSupport::relativize_stack_chunk(new_obj);\n@@ -964,3 +969,2 @@\n-      Copy::aligned_conjoint_words(heap->get_region(old_start)->bottom(),\n-                                   heap->get_region(new_start)->bottom(),\n-                                   words_size);\n+      Copy::aligned_conjoint_words(r->bottom(), heap->get_region(new_start)->bottom(), words_size);\n+      ContinuationGCSupport::relativize_stack_chunk(cast_to_oop<HeapWord*>(r->bottom()));\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":8,"deletions":4,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2013, 2021, Red Hat, Inc. All rights reserved.\n+ * Copyright (c) 2013, 2022, Red Hat, Inc. All rights reserved.\n@@ -43,0 +43,1 @@\n+#include \"gc\/shenandoah\/shenandoahMarkingContext.inline.hpp\"\n@@ -2315,0 +2316,18 @@\n+\n+bool ShenandoahHeap::requires_barriers(stackChunkOop obj) const {\n+  if (is_idle()) return false;\n+\n+  \/\/ Objects allocated after marking start are implicitly alive, don't need any barriers during\n+  \/\/ marking phase.\n+  if (is_concurrent_mark_in_progress() &&\n+     !marking_context()->allocated_after_mark_start(obj)) {\n+    return true;\n+  }\n+\n+  \/\/ Can not guarantee obj is deeply good.\n+  if (has_forwarded_objects()) {\n+    return true;\n+  }\n+\n+  return false;\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":20,"deletions":1,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -477,0 +477,2 @@\n+  bool requires_barriers(stackChunkOop obj) const;\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"gc\/shared\/continuationGCSupport.inline.hpp\"\n@@ -49,0 +50,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -50,1 +52,0 @@\n-#include \"runtime\/thread.hpp\"\n@@ -338,0 +339,7 @@\n+  if (!copy_val->mark().is_marked()) {\n+    \/\/ If we copied a mark-word that indicates 'forwarded' state, then\n+    \/\/ another thread beat us, and this new copy will never be published.\n+    \/\/ ContinuationGCSupport would get a corrupt Klass* in that case,\n+    \/\/ so don't even attempt it.\n+    ContinuationGCSupport::relativize_stack_chunk(copy_val);\n+  }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":9,"deletions":1,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+#include \"runtime\/threads.hpp\"\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -629,0 +629,1 @@\n+        bool inc_monitor_count = true;\n@@ -634,0 +635,1 @@\n+            inc_monitor_count = false;\n@@ -637,0 +639,3 @@\n+        if (inc_monitor_count) {\n+          THREAD->inc_held_monitor_count();\n+        }\n@@ -723,0 +728,1 @@\n+      bool inc_monitor_count = true;\n@@ -728,0 +734,1 @@\n+          inc_monitor_count = false;\n@@ -731,0 +738,3 @@\n+      if (inc_monitor_count) {\n+        THREAD->inc_held_monitor_count();\n+      }\n@@ -1631,0 +1641,1 @@\n+          bool inc_monitor_count = true;\n@@ -1636,0 +1647,1 @@\n+              inc_monitor_count = false;\n@@ -1639,0 +1651,3 @@\n+          if (inc_monitor_count) {\n+            THREAD->inc_held_monitor_count();\n+          }\n@@ -1660,0 +1675,1 @@\n+            bool dec_monitor_count = true;\n@@ -1666,0 +1682,1 @@\n+                dec_monitor_count = false;\n@@ -1669,0 +1686,3 @@\n+            if (dec_monitor_count) {\n+              THREAD->dec_held_monitor_count();\n+            }\n@@ -3086,0 +3106,1 @@\n+          bool dec_monitor_count = true;\n@@ -3091,0 +3112,1 @@\n+              dec_monitor_count = false;\n@@ -3094,0 +3116,3 @@\n+          if (dec_monitor_count) {\n+            THREAD->dec_held_monitor_count();\n+          }\n@@ -3152,0 +3177,1 @@\n+            bool dec_monitor_count = true;\n@@ -3157,0 +3183,1 @@\n+                dec_monitor_count = false;\n@@ -3164,0 +3191,3 @@\n+            if (dec_monitor_count) {\n+              THREAD->dec_held_monitor_count();\n+            }\n@@ -3181,1 +3211,1 @@\n-    \/\/ NOTE Further! It turns out the the JVMTI spec in fact expects to see\n+    \/\/ NOTE Further! It turns out the JVMTI spec in fact expects to see\n","filename":"src\/hotspot\/share\/interpreter\/zero\/bytecodeInterpreter.cpp","additions":31,"deletions":1,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -231,2 +231,6 @@\n-C2V_VMENTRY_NULL(jbyteArray, getBytecode, (JNIEnv* env, jobject, jobject jvmci_method))\n-  methodHandle method(THREAD, JVMCIENV->asMethod(jvmci_method));\n+\/\/ Macros for argument pairs representing a wrapper object and its wrapped VM pointer\n+#define ARGUMENT_PAIR(name) jobject name ## _obj, jlong name ## _pointer\n+#define UNPACK_PAIR(type, name) ((type*) name ## _pointer)\n+\n+C2V_VMENTRY_NULL(jbyteArray, getBytecode, (JNIEnv* env, jobject, ARGUMENT_PAIR(method)))\n+  methodHandle method(THREAD, UNPACK_PAIR(Method, method));\n@@ -309,2 +313,2 @@\n-C2V_VMENTRY_0(jint, getExceptionTableLength, (JNIEnv* env, jobject, jobject jvmci_method))\n-  Method* method = JVMCIENV->asMethod(jvmci_method);\n+C2V_VMENTRY_0(jint, getExceptionTableLength, (JNIEnv* env, jobject, ARGUMENT_PAIR(method)))\n+  Method* method = UNPACK_PAIR(Method, method);\n@@ -314,2 +318,2 @@\n-C2V_VMENTRY_0(jlong, getExceptionTableStart, (JNIEnv* env, jobject, jobject jvmci_method))\n-  Method* method = JVMCIENV->asMethod(jvmci_method);\n+C2V_VMENTRY_0(jlong, getExceptionTableStart, (JNIEnv* env, jobject, ARGUMENT_PAIR(method)))\n+  Method* method = UNPACK_PAIR(Method, method);\n@@ -347,1 +351,1 @@\n-  } else if (JVMCIENV->isa_HotSpotObjectConstantImpl(base_object)) {\n+  } else {\n@@ -354,2 +358,0 @@\n-  } else if (JVMCIENV->isa_HotSpotResolvedJavaMethodImpl(base_object)) {\n-    method = JVMCIENV->asMethod(base_object);\n@@ -365,1 +367,1 @@\n-C2V_VMENTRY_NULL(jobject, getConstantPool, (JNIEnv* env, jobject, jobject object_handle))\n+C2V_VMENTRY_NULL(jobject, getConstantPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(klass_or_method), jboolean is_klass))\n@@ -367,2 +369,1 @@\n-  JVMCIObject object = JVMCIENV->wrap(object_handle);\n-  if (object.is_null()) {\n+  if (UNPACK_PAIR(address, klass_or_method) == 0) {\n@@ -371,4 +372,2 @@\n-  if (JVMCIENV->isa_HotSpotResolvedJavaMethodImpl(object)) {\n-    cp = JVMCIENV->asMethod(object)->constMethod()->constants();\n-  } else if (JVMCIENV->isa_HotSpotResolvedObjectTypeImpl(object)) {\n-    cp = InstanceKlass::cast(JVMCIENV->asKlass(object))->constants();\n+  if (!is_klass) {\n+    cp = (UNPACK_PAIR(Method, klass_or_method))->constMethod()->constants();\n@@ -376,2 +375,1 @@\n-    JVMCI_THROW_MSG_NULL(IllegalArgumentException,\n-                err_msg(\"Unexpected type: %s\", JVMCIENV->klass_name(object)));\n+    cp = InstanceKlass::cast(UNPACK_PAIR(Klass, klass_or_method))->constants();\n@@ -379,1 +377,0 @@\n-  assert(cp != NULL, \"npe\");\n@@ -390,1 +387,0 @@\n-    \/\/ klass = JVMCIENV->unhandle(base_object)->klass();\n@@ -428,3 +424,3 @@\n-C2V_VMENTRY_NULL(jobject, findUniqueConcreteMethod, (JNIEnv* env, jobject, jobject jvmci_type, jobject jvmci_method))\n-  methodHandle method (THREAD, JVMCIENV->asMethod(jvmci_method));\n-  InstanceKlass* holder = InstanceKlass::cast(JVMCIENV->asKlass(jvmci_type));\n+C2V_VMENTRY_NULL(jobject, findUniqueConcreteMethod, (JNIEnv* env, jobject, ARGUMENT_PAIR(klass), ARGUMENT_PAIR(method)))\n+  methodHandle method (THREAD, UNPACK_PAIR(Method, method));\n+  InstanceKlass* holder = InstanceKlass::cast(UNPACK_PAIR(Klass, klass));\n@@ -447,2 +443,2 @@\n-C2V_VMENTRY_NULL(jobject, getImplementor, (JNIEnv* env, jobject, jobject jvmci_type))\n-  Klass* klass = JVMCIENV->asKlass(jvmci_type);\n+C2V_VMENTRY_NULL(jobject, getImplementor, (JNIEnv* env, jobject, ARGUMENT_PAIR(klass)))\n+  Klass* klass = UNPACK_PAIR(Klass, klass);\n@@ -464,2 +460,2 @@\n-C2V_VMENTRY_0(jboolean, methodIsIgnoredBySecurityStackWalk,(JNIEnv* env, jobject, jobject jvmci_method))\n-  Method* method = JVMCIENV->asMethod(jvmci_method);\n+C2V_VMENTRY_0(jboolean, methodIsIgnoredBySecurityStackWalk,(JNIEnv* env, jobject, ARGUMENT_PAIR(method)))\n+  Method* method = UNPACK_PAIR(Method, method);\n@@ -469,2 +465,2 @@\n-C2V_VMENTRY_0(jboolean, isCompilable,(JNIEnv* env, jobject, jobject jvmci_method))\n-  Method* method = JVMCIENV->asMethod(jvmci_method);\n+C2V_VMENTRY_0(jboolean, isCompilable,(JNIEnv* env, jobject, ARGUMENT_PAIR(method)))\n+  Method* method = UNPACK_PAIR(Method, method);\n@@ -478,2 +474,2 @@\n-C2V_VMENTRY_0(jboolean, hasNeverInlineDirective,(JNIEnv* env, jobject, jobject jvmci_method))\n-  methodHandle method (THREAD, JVMCIENV->asMethod(jvmci_method));\n+C2V_VMENTRY_0(jboolean, hasNeverInlineDirective,(JNIEnv* env, jobject, ARGUMENT_PAIR(method)))\n+  methodHandle method (THREAD, UNPACK_PAIR(Method, method));\n@@ -483,2 +479,2 @@\n-C2V_VMENTRY_0(jboolean, shouldInlineMethod,(JNIEnv* env, jobject, jobject jvmci_method))\n-  methodHandle method (THREAD, JVMCIENV->asMethod(jvmci_method));\n+C2V_VMENTRY_0(jboolean, shouldInlineMethod,(JNIEnv* env, jobject, ARGUMENT_PAIR(method)))\n+  methodHandle method (THREAD, UNPACK_PAIR(Method, method));\n@@ -488,1 +484,1 @@\n-C2V_VMENTRY_NULL(jobject, lookupType, (JNIEnv* env, jobject, jstring jname, jclass accessing_class, jboolean resolve))\n+C2V_VMENTRY_NULL(jobject, lookupType, (JNIEnv* env, jobject, jstring jname, ARGUMENT_PAIR(accessing_klass), jboolean resolve))\n@@ -498,1 +494,1 @@\n-  Klass* accessing_klass = NULL;\n+  Klass* accessing_klass = UNPACK_PAIR(Klass, accessing_klass);\n@@ -501,2 +497,1 @@\n-  if (accessing_class != NULL) {\n-    accessing_klass = JVMCIENV->asKlass(accessing_class);\n+  if (accessing_klass != nullptr) {\n@@ -513,1 +508,1 @@\n-    if (resolved_klass == NULL) {\n+    if (resolved_klass == nullptr) {\n@@ -548,6 +543,1 @@\n-C2V_VMENTRY_NULL(jobject, getArrayType, (JNIEnv* env, jobject, jobject jvmci_type))\n-  if (jvmci_type == NULL) {\n-    JVMCI_THROW_0(NullPointerException);\n-  }\n-\n-  JVMCIObject jvmci_type_object = JVMCIENV->wrap(jvmci_type);\n+C2V_VMENTRY_NULL(jobject, getArrayType, (JNIEnv* env, jobject, jchar type_char, ARGUMENT_PAIR(klass)))\n@@ -555,2 +545,3 @@\n-  if (JVMCIENV->isa_HotSpotResolvedPrimitiveType(jvmci_type_object)) {\n-    BasicType type = JVMCIENV->kindToBasicType(JVMCIENV->get_HotSpotResolvedPrimitiveType_kind(jvmci_type_object), JVMCI_CHECK_0);\n+  Klass* klass = UNPACK_PAIR(Klass, klass);\n+  if (klass == nullptr) {\n+    BasicType type = JVMCIENV->typeCharToBasicType(type_char, JVMCI_CHECK_0);\n@@ -558,1 +549,1 @@\n-      return NULL;\n+      return nullptr;\n@@ -561,1 +552,1 @@\n-    if (array_klass == NULL) {\n+    if (array_klass == nullptr) {\n@@ -565,4 +556,0 @@\n-    Klass* klass = JVMCIENV->asKlass(jvmci_type);\n-    if (klass == NULL) {\n-      JVMCI_THROW_0(NullPointerException);\n-    }\n@@ -577,2 +564,2 @@\n-  if (mirror == NULL) {\n-    return NULL;\n+  if (mirror == nullptr) {\n+    return nullptr;\n@@ -582,1 +569,1 @@\n-  if (klass == NULL) {\n+  if (klass == nullptr) {\n@@ -589,2 +576,2 @@\n-C2V_VMENTRY_NULL(jobject, resolvePossiblyCachedConstantInPool, (JNIEnv* env, jobject, jobject jvmci_constant_pool, jint index))\n-  constantPoolHandle cp(THREAD, JVMCIENV->asConstantPool(jvmci_constant_pool));\n+C2V_VMENTRY_NULL(jobject, resolvePossiblyCachedConstantInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint index))\n+  constantPoolHandle cp(THREAD, UNPACK_PAIR(ConstantPool, cp));\n@@ -606,1 +593,1 @@\n-      JVMCIObject kind;\n+      jchar type_char;\n@@ -610,8 +597,8 @@\n-        case T_LONG:    kind = JVMCIENV->get_JavaKind_Long();    raw_value = value.j; break;\n-        case T_DOUBLE:  kind = JVMCIENV->get_JavaKind_Double();  raw_value = value.j; break;\n-        case T_FLOAT:   kind = JVMCIENV->get_JavaKind_Float();   raw_value = value.i; break;\n-        case T_INT:     kind = JVMCIENV->get_JavaKind_Int();     raw_value = value.i; break;\n-        case T_SHORT:   kind = JVMCIENV->get_JavaKind_Short();   raw_value = value.s; break;\n-        case T_BYTE:    kind = JVMCIENV->get_JavaKind_Byte();    raw_value = value.b; break;\n-        case T_CHAR:    kind = JVMCIENV->get_JavaKind_Char();    raw_value = value.c; break;\n-        case T_BOOLEAN: kind = JVMCIENV->get_JavaKind_Boolean(); raw_value = value.z; break;\n+        case T_LONG:    type_char = 'J'; raw_value = value.j; break;\n+        case T_DOUBLE:  type_char = 'D'; raw_value = value.j; break;\n+        case T_FLOAT:   type_char = 'F'; raw_value = value.i; break;\n+        case T_INT:     type_char = 'I'; raw_value = value.i; break;\n+        case T_SHORT:   type_char = 'S'; raw_value = value.s; break;\n+        case T_BYTE:    type_char = 'B'; raw_value = value.b; break;\n+        case T_CHAR:    type_char = 'C'; raw_value = value.c; break;\n+        case T_BOOLEAN: type_char = 'Z'; raw_value = value.z; break;\n@@ -621,1 +608,1 @@\n-      JVMCIObject result = JVMCIENV->call_JavaConstant_forPrimitive(kind, raw_value, JVMCI_CHECK_NULL);\n+      JVMCIObject result = JVMCIENV->call_JavaConstant_forPrimitive(type_char, raw_value, JVMCI_CHECK_NULL);\n@@ -628,2 +615,2 @@\n-C2V_VMENTRY_NULL(jobjectArray, resolveBootstrapMethod, (JNIEnv* env, jobject, jobject jvmci_constant_pool, jint index))\n-  constantPoolHandle cp(THREAD, JVMCIENV->asConstantPool(jvmci_constant_pool));\n+C2V_VMENTRY_NULL(jobjectArray, resolveBootstrapMethod, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint index))\n+  constantPoolHandle cp(THREAD, UNPACK_PAIR(ConstantPool, cp));\n@@ -699,2 +686,2 @@\n-C2V_VMENTRY_0(jint, lookupNameAndTypeRefIndexInPool, (JNIEnv* env, jobject, jobject jvmci_constant_pool, jint index))\n-  constantPoolHandle cp(THREAD, JVMCIENV->asConstantPool(jvmci_constant_pool));\n+C2V_VMENTRY_0(jint, lookupNameAndTypeRefIndexInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint index))\n+  constantPoolHandle cp(THREAD, UNPACK_PAIR(ConstantPool, cp));\n@@ -704,2 +691,2 @@\n-C2V_VMENTRY_NULL(jobject, lookupNameInPool, (JNIEnv* env, jobject, jobject jvmci_constant_pool, jint which))\n-  constantPoolHandle cp(THREAD, JVMCIENV->asConstantPool(jvmci_constant_pool));\n+C2V_VMENTRY_NULL(jobject, lookupNameInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint which))\n+  constantPoolHandle cp(THREAD, UNPACK_PAIR(ConstantPool, cp));\n@@ -710,2 +697,2 @@\n-C2V_VMENTRY_NULL(jobject, lookupSignatureInPool, (JNIEnv* env, jobject, jobject jvmci_constant_pool, jint which))\n-  constantPoolHandle cp(THREAD, JVMCIENV->asConstantPool(jvmci_constant_pool));\n+C2V_VMENTRY_NULL(jobject, lookupSignatureInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint which))\n+  constantPoolHandle cp(THREAD, UNPACK_PAIR(ConstantPool, cp));\n@@ -716,2 +703,2 @@\n-C2V_VMENTRY_0(jint, lookupKlassRefIndexInPool, (JNIEnv* env, jobject, jobject jvmci_constant_pool, jint index))\n-  constantPoolHandle cp(THREAD, JVMCIENV->asConstantPool(jvmci_constant_pool));\n+C2V_VMENTRY_0(jint, lookupKlassRefIndexInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint index))\n+  constantPoolHandle cp(THREAD, UNPACK_PAIR(ConstantPool, cp));\n@@ -721,2 +708,2 @@\n-C2V_VMENTRY_NULL(jobject, resolveTypeInPool, (JNIEnv* env, jobject, jobject jvmci_constant_pool, jint index))\n-  constantPoolHandle cp(THREAD, JVMCIENV->asConstantPool(jvmci_constant_pool));\n+C2V_VMENTRY_NULL(jobject, resolveTypeInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint index))\n+  constantPoolHandle cp(THREAD, UNPACK_PAIR(ConstantPool, cp));\n@@ -736,2 +723,2 @@\n-C2V_VMENTRY_NULL(jobject, lookupKlassInPool, (JNIEnv* env, jobject, jobject jvmci_constant_pool, jint index, jbyte opcode))\n-  constantPoolHandle cp(THREAD, JVMCIENV->asConstantPool(jvmci_constant_pool));\n+C2V_VMENTRY_NULL(jobject, lookupKlassInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint index))\n+  constantPoolHandle cp(THREAD, UNPACK_PAIR(ConstantPool, cp));\n@@ -741,1 +728,1 @@\n-  Symbol* symbol = NULL;\n+  Symbol* symbol = nullptr;\n@@ -764,2 +751,2 @@\n-C2V_VMENTRY_NULL(jobject, lookupAppendixInPool, (JNIEnv* env, jobject, jobject jvmci_constant_pool, jint index))\n-  constantPoolHandle cp(THREAD, JVMCIENV->asConstantPool(jvmci_constant_pool));\n+C2V_VMENTRY_NULL(jobject, lookupAppendixInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint index))\n+  constantPoolHandle cp(THREAD, UNPACK_PAIR(ConstantPool, cp));\n@@ -770,2 +757,2 @@\n-C2V_VMENTRY_NULL(jobject, lookupMethodInPool, (JNIEnv* env, jobject, jobject jvmci_constant_pool, jint index, jbyte opcode))\n-  constantPoolHandle cp(THREAD, JVMCIENV->asConstantPool(jvmci_constant_pool));\n+C2V_VMENTRY_NULL(jobject, lookupMethodInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint index, jbyte opcode))\n+  constantPoolHandle cp(THREAD, UNPACK_PAIR(ConstantPool, cp));\n@@ -779,2 +766,2 @@\n-C2V_VMENTRY_0(jint, constantPoolRemapInstructionOperandFromCache, (JNIEnv* env, jobject, jobject jvmci_constant_pool, jint index))\n-  constantPoolHandle cp(THREAD, JVMCIENV->asConstantPool(jvmci_constant_pool));\n+C2V_VMENTRY_0(jint, constantPoolRemapInstructionOperandFromCache, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint index))\n+  constantPoolHandle cp(THREAD, UNPACK_PAIR(ConstantPool, cp));\n@@ -784,2 +771,2 @@\n-C2V_VMENTRY_NULL(jobject, resolveFieldInPool, (JNIEnv* env, jobject, jobject jvmci_constant_pool, jint index, jobject jvmci_method, jbyte opcode, jintArray info_handle))\n-  constantPoolHandle cp(THREAD, JVMCIENV->asConstantPool(jvmci_constant_pool));\n+C2V_VMENTRY_NULL(jobject, resolveFieldInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint index, ARGUMENT_PAIR(method), jbyte opcode, jintArray info_handle))\n+  constantPoolHandle cp(THREAD, UNPACK_PAIR(ConstantPool, cp));\n@@ -788,1 +775,1 @@\n-  methodHandle mh(THREAD, (jvmci_method != NULL) ? JVMCIENV->asMethod(jvmci_method) : NULL);\n+  methodHandle mh(THREAD, UNPACK_PAIR(Method, method));\n@@ -803,3 +790,3 @@\n-C2V_VMENTRY_0(jint, getVtableIndexForInterfaceMethod, (JNIEnv* env, jobject, jobject jvmci_type, jobject jvmci_method))\n-  Klass* klass = JVMCIENV->asKlass(jvmci_type);\n-  methodHandle method(THREAD, JVMCIENV->asMethod(jvmci_method));\n+C2V_VMENTRY_0(jint, getVtableIndexForInterfaceMethod, (JNIEnv* env, jobject, ARGUMENT_PAIR(klass), ARGUMENT_PAIR(method)))\n+  Klass* klass = UNPACK_PAIR(Klass, klass);\n+  methodHandle method(THREAD, UNPACK_PAIR(Method, method));\n@@ -825,4 +812,4 @@\n-C2V_VMENTRY_NULL(jobject, resolveMethod, (JNIEnv* env, jobject, jobject receiver_jvmci_type, jobject jvmci_method, jobject caller_jvmci_type))\n-  Klass* recv_klass = JVMCIENV->asKlass(receiver_jvmci_type);\n-  Klass* caller_klass = JVMCIENV->asKlass(caller_jvmci_type);\n-  methodHandle method(THREAD, JVMCIENV->asMethod(jvmci_method));\n+C2V_VMENTRY_NULL(jobject, resolveMethod, (JNIEnv* env, jobject, ARGUMENT_PAIR(receiver), ARGUMENT_PAIR(method), ARGUMENT_PAIR(caller)))\n+  Klass* recv_klass = UNPACK_PAIR(Klass, receiver);\n+  Klass* caller_klass = UNPACK_PAIR(Klass, caller);\n+  methodHandle method(THREAD, UNPACK_PAIR(Method, method));\n@@ -836,1 +823,1 @@\n-      return NULL;\n+      return nullptr;\n@@ -851,1 +838,1 @@\n-  Method* m = NULL;\n+  Method* m = nullptr;\n@@ -863,1 +850,1 @@\n-  if (m == NULL) {\n+  if (m == nullptr) {\n@@ -865,1 +852,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -872,3 +859,3 @@\n-C2V_VMENTRY_0(jboolean, hasFinalizableSubclass,(JNIEnv* env, jobject, jobject jvmci_type))\n-  Klass* klass = JVMCIENV->asKlass(jvmci_type);\n-  assert(klass != NULL, \"method must not be called for primitive types\");\n+C2V_VMENTRY_0(jboolean, hasFinalizableSubclass,(JNIEnv* env, jobject, ARGUMENT_PAIR(klass)))\n+  Klass* klass = UNPACK_PAIR(Klass, klass);\n+  assert(klass != nullptr, \"method must not be called for primitive types\");\n@@ -879,1 +866,1 @@\n-  return Dependencies::find_finalizable_subclass(iklass) != NULL;\n+  return Dependencies::find_finalizable_subclass(iklass) != nullptr;\n@@ -882,2 +869,2 @@\n-C2V_VMENTRY_NULL(jobject, getClassInitializer, (JNIEnv* env, jobject, jobject jvmci_type))\n-  Klass* klass = JVMCIENV->asKlass(jvmci_type);\n+C2V_VMENTRY_NULL(jobject, getClassInitializer, (JNIEnv* env, jobject, ARGUMENT_PAIR(klass)))\n+  Klass* klass = UNPACK_PAIR(Klass, klass);\n@@ -885,1 +872,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -903,2 +890,2 @@\n-C2V_VMENTRY(void, setNotInlinableOrCompilable,(JNIEnv* env, jobject,  jobject jvmci_method))\n-  methodHandle method(THREAD, JVMCIENV->asMethod(jvmci_method));\n+C2V_VMENTRY(void, setNotInlinableOrCompilable,(JNIEnv* env, jobject, ARGUMENT_PAIR(method)))\n+  methodHandle method(THREAD, UNPACK_PAIR(Method, method));\n@@ -910,2 +897,22 @@\n-C2V_VMENTRY_0(jint, installCode, (JNIEnv *env, jobject, jobject target, jobject compiled_code,\n-            jobject installed_code, jlong failed_speculations_address, jbyteArray speculations_obj))\n+C2V_VMENTRY_0(jint, getInstallCodeFlags, (JNIEnv *env, jobject))\n+  int flags = 0;\n+#ifndef PRODUCT\n+  flags |= 0x0001; \/\/ VM will install block comments\n+  flags |= 0x0004; \/\/ Enable HotSpotJVMCIRuntime.Option.CodeSerializationTypeInfo if not explicitly set\n+#endif\n+  if (JvmtiExport::can_hotswap_or_post_breakpoint()) {\n+    \/\/ VM needs to track method dependencies\n+    flags |= 0x0002;\n+  }\n+  return flags;\n+C2V_END\n+\n+C2V_VMENTRY_0(jint, installCode0, (JNIEnv *env, jobject,\n+    jlong compiled_code_buffer,\n+    jlong serialization_ns,\n+    bool with_type_info,\n+    jobject compiled_code,\n+    jobjectArray object_pool,\n+    jobject installed_code,\n+    jlong failed_speculations_address,\n+    jbyteArray speculations_obj))\n@@ -915,1 +922,2 @@\n-  JVMCIObject target_handle = JVMCIENV->wrap(target);\n+  objArrayHandle object_pool_handle(thread, JVMCIENV->is_hotspot() ? (objArrayOop) JNIHandles::resolve(object_pool) : nullptr);\n+\n@@ -926,2 +934,4 @@\n-\n-  TraceTime install_time(\"installCode\", JVMCICompiler::codeInstallTimer(!thread->is_Compiler_thread()));\n+  JVMCICompiler::CodeInstallStats* stats = compiler->code_install_stats(!thread->is_Compiler_thread());\n+  elapsedTimer *timer = stats->timer();\n+  timer->add_nanoseconds(serialization_ns);\n+  TraceTime install_time(\"installCode\", timer);\n@@ -932,1 +942,2 @@\n-      target_handle,\n+      compiled_code_buffer,\n+      with_type_info,\n@@ -934,0 +945,1 @@\n+      object_pool_handle,\n@@ -956,0 +968,1 @@\n+    stats->on_install(cb);\n@@ -1017,1 +1030,1 @@\n-C2V_VMENTRY_NULL(jobject, getStackTraceElement, (JNIEnv* env, jobject, jobject jvmci_method, int bci))\n+C2V_VMENTRY_NULL(jobject, getStackTraceElement, (JNIEnv* env, jobject, ARGUMENT_PAIR(method), int bci))\n@@ -1020,1 +1033,1 @@\n-  methodHandle method(THREAD, JVMCIENV->asMethod(jvmci_method));\n+  methodHandle method(THREAD, UNPACK_PAIR(Method, method));\n@@ -1075,2 +1088,2 @@\n-C2V_VMENTRY_NULL(jlongArray, getLineNumberTable, (JNIEnv* env, jobject, jobject jvmci_method))\n-  Method* method = JVMCIENV->asMethod(jvmci_method);\n+C2V_VMENTRY_NULL(jlongArray, getLineNumberTable, (JNIEnv* env, jobject, ARGUMENT_PAIR(method)))\n+  Method* method = UNPACK_PAIR(Method, method);\n@@ -1102,2 +1115,2 @@\n-C2V_VMENTRY_0(jlong, getLocalVariableTableStart, (JNIEnv* env, jobject, jobject jvmci_method))\n-  Method* method = JVMCIENV->asMethod(jvmci_method);\n+C2V_VMENTRY_0(jlong, getLocalVariableTableStart, (JNIEnv* env, jobject, ARGUMENT_PAIR(method)))\n+  Method* method = UNPACK_PAIR(Method, method);\n@@ -1110,2 +1123,2 @@\n-C2V_VMENTRY_0(jint, getLocalVariableTableLength, (JNIEnv* env, jobject, jobject jvmci_method))\n-  Method* method = JVMCIENV->asMethod(jvmci_method);\n+C2V_VMENTRY_0(jint, getLocalVariableTableLength, (JNIEnv* env, jobject, ARGUMENT_PAIR(method)))\n+  Method* method = UNPACK_PAIR(Method, method);\n@@ -1115,2 +1128,2 @@\n-C2V_VMENTRY(void, reprofile, (JNIEnv* env, jobject, jobject jvmci_method))\n-  methodHandle method(THREAD, JVMCIENV->asMethod(jvmci_method));\n+C2V_VMENTRY(void, reprofile, (JNIEnv* env, jobject, ARGUMENT_PAIR(method)))\n+  methodHandle method(THREAD, UNPACK_PAIR(Method, method));\n@@ -1163,1 +1176,1 @@\n-C2V_VMENTRY_0(jint, allocateCompileId, (JNIEnv* env, jobject, jobject jvmci_method, int entry_bci))\n+C2V_VMENTRY_0(jint, allocateCompileId, (JNIEnv* env, jobject, ARGUMENT_PAIR(method), int entry_bci))\n@@ -1165,1 +1178,2 @@\n-  if (jvmci_method == NULL) {\n+  methodHandle method(THREAD, UNPACK_PAIR(Method, method));\n+  if (method.is_null()) {\n@@ -1168,1 +1182,0 @@\n-  methodHandle method(THREAD, JVMCIENV->asMethod(jvmci_method));\n@@ -1176,2 +1189,2 @@\n-C2V_VMENTRY_0(jboolean, isMature, (JNIEnv* env, jobject, jlong metaspace_method_data))\n-  MethodData* mdo = JVMCIENV->asMethodData(metaspace_method_data);\n+C2V_VMENTRY_0(jboolean, isMature, (JNIEnv* env, jobject, jlong method_data_pointer))\n+  MethodData* mdo = (MethodData*) method_data_pointer;\n@@ -1181,2 +1194,2 @@\n-C2V_VMENTRY_0(jboolean, hasCompiledCodeForOSR, (JNIEnv* env, jobject, jobject jvmci_method, int entry_bci, int comp_level))\n-  Method* method = JVMCIENV->asMethod(jvmci_method);\n+C2V_VMENTRY_0(jboolean, hasCompiledCodeForOSR, (JNIEnv* env, jobject, ARGUMENT_PAIR(method), int entry_bci, int comp_level))\n+  Method* method = UNPACK_PAIR(Method, method);\n@@ -1191,0 +1204,6 @@\n+C2V_VMENTRY_NULL(jobject, getSignatureName, (JNIEnv* env, jobject, jlong klass_pointer))\n+  Klass* klass = UNPACK_PAIR(Klass, klass);\n+  JVMCIObject signature = JVMCIENV->create_string(klass->signature_name(), JVMCI_CHECK_NULL);\n+  return JVMCIENV->get_jobject(signature);\n+C2V_END\n+\n@@ -1436,2 +1455,2 @@\n-C2V_VMENTRY(void, resolveInvokeDynamicInPool, (JNIEnv* env, jobject, jobject jvmci_constant_pool, jint index))\n-  constantPoolHandle cp(THREAD, JVMCIENV->asConstantPool(jvmci_constant_pool));\n+C2V_VMENTRY(void, resolveInvokeDynamicInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint index))\n+  constantPoolHandle cp(THREAD, UNPACK_PAIR(ConstantPool, cp));\n@@ -1444,2 +1463,2 @@\n-C2V_VMENTRY(void, resolveInvokeHandleInPool, (JNIEnv* env, jobject, jobject jvmci_constant_pool, jint index))\n-  constantPoolHandle cp(THREAD, JVMCIENV->asConstantPool(jvmci_constant_pool));\n+C2V_VMENTRY(void, resolveInvokeHandleInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint index))\n+  constantPoolHandle cp(THREAD, UNPACK_PAIR(ConstantPool, cp));\n@@ -1456,2 +1475,2 @@\n-C2V_VMENTRY_0(jint, isResolvedInvokeHandleInPool, (JNIEnv* env, jobject, jobject jvmci_constant_pool, jint index))\n-  constantPoolHandle cp(THREAD, JVMCIENV->asConstantPool(jvmci_constant_pool));\n+C2V_VMENTRY_0(jint, isResolvedInvokeHandleInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint index))\n+  constantPoolHandle cp(THREAD, UNPACK_PAIR(ConstantPool, cp));\n@@ -1587,1 +1606,1 @@\n-    GrowableArray<ScopeValue*>* scopeLocals = cvf->scope()->locals();\n+    GrowableArray<ScopeValue*>* extentLocals = cvf->scope()->locals();\n@@ -1592,1 +1611,1 @@\n-        if (var->type() == T_OBJECT && scopeLocals->at(i2)->is_object()) {\n+        if (var->type() == T_OBJECT && extentLocals->at(i2)->is_object()) {\n@@ -1655,2 +1674,2 @@\n-C2V_VMENTRY_0(jint, methodDataProfileDataSize, (JNIEnv* env, jobject, jlong metaspace_method_data, jint position))\n-  MethodData* mdo = JVMCIENV->asMethodData(metaspace_method_data);\n+C2V_VMENTRY_0(jint, methodDataProfileDataSize, (JNIEnv* env, jobject, jlong method_data_pointer, jint position))\n+  MethodData* mdo = (MethodData*) method_data_pointer;\n@@ -1673,2 +1692,3 @@\n-C2V_VMENTRY_NULL(jobject, getInterfaces, (JNIEnv* env, jobject, jobject jvmci_type))\n-  if (jvmci_type == NULL) {\n+C2V_VMENTRY_NULL(jobject, getInterfaces, (JNIEnv* env, jobject, ARGUMENT_PAIR(klass)))\n+  Klass* klass = UNPACK_PAIR(Klass, klass);\n+  if (klass == nullptr) {\n@@ -1678,4 +1698,0 @@\n-  Klass* klass = JVMCIENV->asKlass(jvmci_type);\n-  if (klass == NULL) {\n-    JVMCI_THROW_0(NullPointerException);\n-  }\n@@ -1700,2 +1716,3 @@\n-C2V_VMENTRY_NULL(jobject, getComponentType, (JNIEnv* env, jobject, jobject jvmci_type))\n-  if (jvmci_type == NULL) {\n+C2V_VMENTRY_NULL(jobject, getComponentType, (JNIEnv* env, jobject, ARGUMENT_PAIR(klass)))\n+  Klass* klass = UNPACK_PAIR(Klass, klass);\n+  if (klass == nullptr) {\n@@ -1705,5 +1722,2 @@\n-  Klass* klass = JVMCIENV->asKlass(jvmci_type);\n-  oop mirror = klass->java_mirror();\n-  if (java_lang_Class::is_primitive(mirror) ||\n-      !java_lang_Class::as_Klass(mirror)->is_array_klass()) {\n-    return NULL;\n+  if (!klass->is_array_klass()) {\n+    return nullptr;\n@@ -1711,1 +1725,1 @@\n-\n+  oop mirror = klass->java_mirror();\n@@ -1713,2 +1727,3 @@\n-  if (component_mirror == NULL) {\n-    return NULL;\n+  if (component_mirror == nullptr) {\n+    JVMCI_THROW_MSG_0(NullPointerException,\n+                    err_msg(\"Component mirror for array class %s is null\", klass->external_name()))\n@@ -1716,0 +1731,1 @@\n+\n@@ -1718,2 +1734,1 @@\n-    JVMCIKlassHandle klass_handle(THREAD);\n-    klass_handle = component_klass;\n+    JVMCIKlassHandle klass_handle(THREAD, component_klass);\n@@ -1728,2 +1743,3 @@\n-C2V_VMENTRY(void, ensureInitialized, (JNIEnv* env, jobject, jobject jvmci_type))\n-  if (jvmci_type == NULL) {\n+C2V_VMENTRY(void, ensureInitialized, (JNIEnv* env, jobject, ARGUMENT_PAIR(klass)))\n+  Klass* klass = UNPACK_PAIR(Klass, klass);\n+  if (klass == nullptr) {\n@@ -1732,3 +1748,1 @@\n-\n-  Klass* klass = JVMCIENV->asKlass(jvmci_type);\n-  if (klass != NULL && klass->should_be_initialized()) {\n+  if (klass->should_be_initialized()) {\n@@ -1740,2 +1754,3 @@\n-C2V_VMENTRY(void, ensureLinked, (JNIEnv* env, jobject, jobject jvmci_type))\n-  if (jvmci_type == NULL) {\n+C2V_VMENTRY(void, ensureLinked, (JNIEnv* env, jobject, ARGUMENT_PAIR(klass)))\n+  Klass* klass = UNPACK_PAIR(Klass, klass);\n+  if (klass == nullptr) {\n@@ -1744,3 +1759,1 @@\n-\n-  Klass* klass = JVMCIENV->asKlass(jvmci_type);\n-  if (klass != NULL && klass->is_instance_klass()) {\n+  if (klass->is_instance_klass()) {\n@@ -1874,2 +1887,3 @@\n-C2V_VMENTRY_NULL(jobjectArray, getDeclaredConstructors, (JNIEnv* env, jobject, jobject holder))\n-  if (holder == NULL) {\n+C2V_VMENTRY_NULL(jobjectArray, getDeclaredConstructors, (JNIEnv* env, jobject, ARGUMENT_PAIR(klass)))\n+  Klass* klass = UNPACK_PAIR(Klass, klass);\n+  if (klass == nullptr) {\n@@ -1878,1 +1892,0 @@\n-  Klass* klass = JVMCIENV->asKlass(holder);\n@@ -1904,2 +1917,3 @@\n-C2V_VMENTRY_NULL(jobjectArray, getDeclaredMethods, (JNIEnv* env, jobject, jobject holder))\n-  if (holder == NULL) {\n+C2V_VMENTRY_NULL(jobjectArray, getDeclaredMethods, (JNIEnv* env, jobject, ARGUMENT_PAIR(klass)))\n+  Klass* klass = UNPACK_PAIR(Klass, klass);\n+  if (klass == nullptr) {\n@@ -1908,1 +1922,0 @@\n-  Klass* klass = JVMCIENV->asKlass(holder);\n@@ -1934,39 +1947,1 @@\n-C2V_VMENTRY_NULL(jobject, readFieldValue, (JNIEnv* env, jobject, jobject object, jobject expected_type, jlong displacement, jobject kind_object))\n-  if (object == NULL || kind_object == NULL) {\n-    JVMCI_THROW_0(NullPointerException);\n-  }\n-\n-  JVMCIObject kind = JVMCIENV->wrap(kind_object);\n-  BasicType basic_type = JVMCIENV->kindToBasicType(kind, JVMCI_CHECK_NULL);\n-\n-  InstanceKlass* holder = NULL;\n-  if (expected_type != NULL) {\n-    holder = InstanceKlass::cast(JVMCIENV->asKlass(JVMCIENV->wrap(expected_type)));\n-  }\n-\n-  bool is_static = false;\n-  Handle obj;\n-  JVMCIObject base = JVMCIENV->wrap(object);\n-  if (JVMCIENV->isa_HotSpotObjectConstantImpl(base)) {\n-    obj = JVMCIENV->asConstant(base, JVMCI_CHECK_NULL);\n-    \/\/ asConstant will throw an NPE if a constant contains NULL\n-\n-    if (holder != NULL && !obj->is_a(holder)) {\n-      \/\/ Not a subtype of field holder\n-      return NULL;\n-    }\n-    is_static = false;\n-    if (holder == NULL && java_lang_Class::is_instance(obj()) && displacement >= InstanceMirrorKlass::offset_of_static_fields()) {\n-      is_static = true;\n-    }\n-  } else if (JVMCIENV->isa_HotSpotResolvedObjectTypeImpl(base)) {\n-    is_static = true;\n-    Klass* klass = JVMCIENV->asKlass(base);\n-    if (holder != NULL && holder != klass) {\n-      return NULL;\n-    }\n-    obj = Handle(THREAD, klass->java_mirror());\n-  } else {\n-    \/\/ The Java code is expected to guard against this path\n-    ShouldNotReachHere();\n-  }\n+static jobject read_field_value(Handle obj, long displacement, jchar type_char, bool is_static, Thread* THREAD, JVMCIEnv* JVMCIENV) {\n@@ -1974,0 +1949,1 @@\n+  BasicType basic_type = JVMCIENV->typeCharToBasicType(type_char, JVMCI_CHECK_NULL);\n@@ -2040,1 +2016,1 @@\n-          (java_lang_Class::as_Klass(obj()) == NULL || !java_lang_Class::as_Klass(obj())->is_array_klass())) {\n+          (java_lang_Class::as_Klass(obj()) == nullptr || !java_lang_Class::as_Klass(obj())->is_array_klass())) {\n@@ -2048,1 +2024,1 @@\n-      if (value == NULL) {\n+      if (value == nullptr) {\n@@ -2051,1 +2027,1 @@\n-        if (value != NULL && !oopDesc::is_oop(value)) {\n+        if (value != nullptr && !oopDesc::is_oop(value)) {\n@@ -2068,1 +2044,1 @@\n-  JVMCIObject result = JVMCIENV->call_JavaConstant_forPrimitive(kind, value, JVMCI_CHECK_NULL);\n+  JVMCIObject result = JVMCIENV->call_JavaConstant_forPrimitive(type_char, value, JVMCI_CHECK_NULL);\n@@ -2070,0 +2046,26 @@\n+}\n+\n+C2V_VMENTRY_NULL(jobject, readStaticFieldValue, (JNIEnv* env, jobject, ARGUMENT_PAIR(klass), long displacement, jchar type_char))\n+  Klass* klass = UNPACK_PAIR(Klass, klass);\n+  Handle obj(THREAD, klass->java_mirror());\n+  return read_field_value(obj, displacement, type_char, true, THREAD, JVMCIENV);\n+C2V_END\n+\n+C2V_VMENTRY_NULL(jobject, readFieldValue, (JNIEnv* env, jobject, jobject object, ARGUMENT_PAIR(expected_type), long displacement, jchar type_char))\n+  if (object == nullptr) {\n+    JVMCI_THROW_0(NullPointerException);\n+  }\n+\n+  \/\/ asConstant will throw an NPE if a constant contains NULL\n+  Handle obj = JVMCIENV->asConstant(JVMCIENV->wrap(object), JVMCI_CHECK_NULL);\n+\n+  Klass* expected_klass = UNPACK_PAIR(Klass, expected_type);\n+  if (expected_klass != nullptr) {\n+    InstanceKlass* expected_iklass = InstanceKlass::cast(expected_klass);\n+    if (!obj->is_a(expected_iklass)) {\n+      \/\/ Not of the expected type\n+      return nullptr;\n+    }\n+  }\n+  bool is_static = expected_klass == nullptr && java_lang_Class::is_instance(obj()) && displacement >= InstanceMirrorKlass::offset_of_static_fields();\n+  return read_field_value(obj, displacement, type_char, is_static, THREAD, JVMCIENV);\n@@ -2072,2 +2074,3 @@\n-C2V_VMENTRY_0(jboolean, isInstance, (JNIEnv* env, jobject, jobject holder, jobject object))\n-  if (object == NULL || holder == NULL) {\n+C2V_VMENTRY_0(jboolean, isInstance, (JNIEnv* env, jobject, ARGUMENT_PAIR(klass), jobject object))\n+  Klass* klass = UNPACK_PAIR(Klass, klass);\n+  if (object == NULL || klass == nullptr) {\n@@ -2077,1 +2080,0 @@\n-  Klass* klass = JVMCIENV->asKlass(JVMCIENV->wrap(holder));\n@@ -2081,2 +2083,4 @@\n-C2V_VMENTRY_0(jboolean, isAssignableFrom, (JNIEnv* env, jobject, jobject holder, jobject otherHolder))\n-  if (holder == NULL || otherHolder == NULL) {\n+C2V_VMENTRY_0(jboolean, isAssignableFrom, (JNIEnv* env, jobject, ARGUMENT_PAIR(klass), ARGUMENT_PAIR(subklass)))\n+  Klass* klass = UNPACK_PAIR(Klass, klass);\n+  Klass* subklass = UNPACK_PAIR(Klass, subklass);\n+  if (klass == nullptr || subklass == nullptr) {\n@@ -2085,3 +2089,1 @@\n-  Klass* klass = JVMCIENV->asKlass(JVMCIENV->wrap(holder));\n-  Klass* otherKlass = JVMCIENV->asKlass(JVMCIENV->wrap(otherHolder));\n-  return otherKlass->is_subtype_of(klass);\n+  return subklass->is_subtype_of(klass);\n@@ -2090,2 +2092,3 @@\n-C2V_VMENTRY_0(jboolean, isTrustedForIntrinsics, (JNIEnv* env, jobject, jobject holder))\n-  if (holder == NULL) {\n+C2V_VMENTRY_0(jboolean, isTrustedForIntrinsics, (JNIEnv* env, jobject, ARGUMENT_PAIR(klass)))\n+  Klass* klass = UNPACK_PAIR(Klass, klass);\n+  if (klass == nullptr) {\n@@ -2094,1 +2097,1 @@\n-  InstanceKlass* ik = InstanceKlass::cast(JVMCIENV->asKlass(JVMCIENV->wrap(holder)));\n+  InstanceKlass* ik = InstanceKlass::cast(klass);\n@@ -2139,2 +2142,3 @@\n-C2V_VMENTRY_NULL(jobject, getJavaMirror, (JNIEnv* env, jobject, jobject object))\n-  if (object == NULL) {\n+C2V_VMENTRY_NULL(jobject, getJavaMirror, (JNIEnv* env, jobject, ARGUMENT_PAIR(klass)))\n+  Klass* klass = UNPACK_PAIR(Klass, klass);\n+  if (klass == nullptr) {\n@@ -2143,10 +2147,1 @@\n-  JVMCIObject base_object = JVMCIENV->wrap(object);\n-  Handle mirror;\n-  if (JVMCIENV->isa_HotSpotResolvedObjectTypeImpl(base_object)) {\n-    mirror = Handle(THREAD, JVMCIENV->asKlass(base_object)->java_mirror());\n-  } else if (JVMCIENV->isa_HotSpotResolvedPrimitiveType(base_object)) {\n-    mirror = JVMCIENV->asConstant(JVMCIENV->get_HotSpotResolvedPrimitiveType_mirror(base_object), JVMCI_CHECK_NULL);\n-  } else {\n-    JVMCI_THROW_MSG_NULL(IllegalArgumentException,\n-                         err_msg(\"Unexpected type: %s\", JVMCIENV->klass_name(base_object)));\n- }\n+  Handle mirror(THREAD, klass->java_mirror());\n@@ -2210,5 +2205,2 @@\n-C2V_VMENTRY_0(jint, arrayBaseOffset, (JNIEnv* env, jobject, jobject kind))\n-  if (kind == NULL) {\n-    JVMCI_THROW_0(NullPointerException);\n-  }\n-  BasicType type = JVMCIENV->kindToBasicType(JVMCIENV->wrap(kind), JVMCI_CHECK_0);\n+C2V_VMENTRY_0(jint, arrayBaseOffset, (JNIEnv* env, jobject, jchar type_char))\n+  BasicType type = JVMCIENV->typeCharToBasicType(type_char, JVMCI_CHECK_0);\n@@ -2218,5 +2210,2 @@\n-C2V_VMENTRY_0(jint, arrayIndexScale, (JNIEnv* env, jobject, jobject kind))\n-  if (kind == NULL) {\n-    JVMCI_THROW_0(NullPointerException);\n-  }\n-  BasicType type = JVMCIENV->kindToBasicType(JVMCIENV->wrap(kind), JVMCI_CHECK_0);\n+C2V_VMENTRY_0(jint, arrayIndexScale, (JNIEnv* env, jobject, jchar type_char))\n+  BasicType type = JVMCIENV->typeCharToBasicType(type_char, JVMCI_CHECK_0);\n@@ -2428,0 +2417,3 @@\n+    if (runtime->GetEnv(thread, (void**) &peerJNIEnv, JNI_VERSION_1_2) == JNI_OK) {\n+      return false;\n+    }\n@@ -2617,1 +2609,1 @@\n-C2V_VMENTRY_NULL(jobject, asReflectionExecutable, (JNIEnv* env, jobject, jobject jvmci_method))\n+C2V_VMENTRY_NULL(jobject, asReflectionExecutable, (JNIEnv* env, jobject, ARGUMENT_PAIR(method)))\n@@ -2619,1 +2611,1 @@\n-  methodHandle m(THREAD, JVMCIENV->asMethod(jvmci_method));\n+  methodHandle m(THREAD, UNPACK_PAIR(Method, method));\n@@ -2633,1 +2625,1 @@\n-C2V_VMENTRY_NULL(jobject, asReflectionField, (JNIEnv* env, jobject, jobject jvmci_type, jint index))\n+C2V_VMENTRY_NULL(jobject, asReflectionField, (JNIEnv* env, jobject, ARGUMENT_PAIR(klass), jint index))\n@@ -2635,1 +2627,1 @@\n-  Klass* klass = JVMCIENV->asKlass(jvmci_type);\n+  Klass* klass = UNPACK_PAIR(Klass, klass);\n@@ -2683,2 +2675,2 @@\n-C2V_VMENTRY_0(jlong, getFailedSpeculationsAddress, (JNIEnv* env, jobject, jobject jvmci_method))\n-  methodHandle method(THREAD, JVMCIENV->asMethod(jvmci_method));\n+C2V_VMENTRY_0(jlong, getFailedSpeculationsAddress, (JNIEnv* env, jobject, ARGUMENT_PAIR(method)))\n+  methodHandle method(THREAD, UNPACK_PAIR(Method, method));\n@@ -2739,1 +2731,1 @@\n-C2V_VMENTRY(void, notifyCompilerInliningEvent, (JNIEnv* env, jobject, jint compileId, jobject caller, jobject callee, jboolean succeeded, jstring jmessage, jint bci))\n+C2V_VMENTRY(void, notifyCompilerInliningEvent, (JNIEnv* env, jobject, jint compileId, ARGUMENT_PAIR(caller), ARGUMENT_PAIR(callee), jboolean succeeded, jstring jmessage, jint bci))\n@@ -2742,2 +2734,2 @@\n-    Method* caller_method = JVMCIENV->asMethod(caller);\n-    Method* callee_method = JVMCIENV->asMethod(callee);\n+    Method* caller = UNPACK_PAIR(Method, caller);\n+    Method* callee = UNPACK_PAIR(Method, callee);\n@@ -2745,1 +2737,1 @@\n-    CompilerEvent::InlineEvent::post(event, compileId, caller_method, callee_method, succeeded, JVMCIENV->as_utf8_string(message), bci);\n+    CompilerEvent::InlineEvent::post(event, compileId, caller, callee, succeeded, JVMCIENV->as_utf8_string(message), bci);\n@@ -2799,1 +2791,0 @@\n-#define HANDLECONSTANT          \"Ljdk\/vm\/ci\/hotspot\/IndirectHotSpotObjectConstantImpl;\"\n@@ -2803,1 +2794,0 @@\n-#define TARGET_DESCRIPTION      \"Ljdk\/vm\/ci\/code\/TargetDescription;\"\n@@ -2808,3 +2798,0 @@\n-#define HS_RESOLVED_METHOD      \"Ljdk\/vm\/ci\/hotspot\/HotSpotResolvedJavaMethodImpl;\"\n-#define HS_RESOLVED_KLASS       \"Ljdk\/vm\/ci\/hotspot\/HotSpotResolvedObjectTypeImpl;\"\n-#define HS_RESOLVED_FIELD       \"Ljdk\/vm\/ci\/hotspot\/HotSpotResolvedJavaField;\"\n@@ -2814,1 +2801,0 @@\n-#define HS_CONSTANT_POOL        \"Ljdk\/vm\/ci\/hotspot\/HotSpotConstantPool;\"\n@@ -2819,1 +2805,0 @@\n-#define METASPACE_OBJECT        \"Ljdk\/vm\/ci\/hotspot\/MetaspaceObject;\"\n@@ -2822,1 +2807,8 @@\n-#define METASPACE_METHOD_DATA   \"J\"\n+\n+\/\/ Types wrapping VM pointers. The ...2 macro is for a pair: (wrapper, pointer)\n+#define HS_METHOD               \"Ljdk\/vm\/ci\/hotspot\/HotSpotResolvedJavaMethodImpl;\"\n+#define HS_METHOD2              \"Ljdk\/vm\/ci\/hotspot\/HotSpotResolvedJavaMethodImpl;J\"\n+#define HS_KLASS                \"Ljdk\/vm\/ci\/hotspot\/HotSpotResolvedObjectTypeImpl;\"\n+#define HS_KLASS2               \"Ljdk\/vm\/ci\/hotspot\/HotSpotResolvedObjectTypeImpl;J\"\n+#define HS_CONSTANT_POOL        \"Ljdk\/vm\/ci\/hotspot\/HotSpotConstantPool;\"\n+#define HS_CONSTANT_POOL2       \"Ljdk\/vm\/ci\/hotspot\/HotSpotConstantPool;J\"\n@@ -2825,13 +2817,13 @@\n-  {CC \"getBytecode\",                                  CC \"(\" HS_RESOLVED_METHOD \")[B\",                                                      FN_PTR(getBytecode)},\n-  {CC \"getExceptionTableStart\",                       CC \"(\" HS_RESOLVED_METHOD \")J\",                                                       FN_PTR(getExceptionTableStart)},\n-  {CC \"getExceptionTableLength\",                      CC \"(\" HS_RESOLVED_METHOD \")I\",                                                       FN_PTR(getExceptionTableLength)},\n-  {CC \"findUniqueConcreteMethod\",                     CC \"(\" HS_RESOLVED_KLASS HS_RESOLVED_METHOD \")\" HS_RESOLVED_METHOD,                   FN_PTR(findUniqueConcreteMethod)},\n-  {CC \"getImplementor\",                               CC \"(\" HS_RESOLVED_KLASS \")\" HS_RESOLVED_KLASS,                                       FN_PTR(getImplementor)},\n-  {CC \"getStackTraceElement\",                         CC \"(\" HS_RESOLVED_METHOD \"I)\" STACK_TRACE_ELEMENT,                                   FN_PTR(getStackTraceElement)},\n-  {CC \"methodIsIgnoredBySecurityStackWalk\",           CC \"(\" HS_RESOLVED_METHOD \")Z\",                                                       FN_PTR(methodIsIgnoredBySecurityStackWalk)},\n-  {CC \"setNotInlinableOrCompilable\",                  CC \"(\" HS_RESOLVED_METHOD \")V\",                                                       FN_PTR(setNotInlinableOrCompilable)},\n-  {CC \"isCompilable\",                                 CC \"(\" HS_RESOLVED_METHOD \")Z\",                                                       FN_PTR(isCompilable)},\n-  {CC \"hasNeverInlineDirective\",                      CC \"(\" HS_RESOLVED_METHOD \")Z\",                                                       FN_PTR(hasNeverInlineDirective)},\n-  {CC \"shouldInlineMethod\",                           CC \"(\" HS_RESOLVED_METHOD \")Z\",                                                       FN_PTR(shouldInlineMethod)},\n-  {CC \"lookupType\",                                   CC \"(\" STRING HS_RESOLVED_KLASS \"Z)\" HS_RESOLVED_TYPE,                                FN_PTR(lookupType)},\n-  {CC \"getArrayType\",                                 CC \"(\" HS_RESOLVED_TYPE \")\" HS_RESOLVED_KLASS,                                        FN_PTR(getArrayType)},\n+  {CC \"getBytecode\",                                  CC \"(\" HS_METHOD2 \")[B\",                                                              FN_PTR(getBytecode)},\n+  {CC \"getExceptionTableStart\",                       CC \"(\" HS_METHOD2 \")J\",                                                               FN_PTR(getExceptionTableStart)},\n+  {CC \"getExceptionTableLength\",                      CC \"(\" HS_METHOD2 \")I\",                                                               FN_PTR(getExceptionTableLength)},\n+  {CC \"findUniqueConcreteMethod\",                     CC \"(\" HS_KLASS2 HS_METHOD2 \")\" HS_METHOD,                                            FN_PTR(findUniqueConcreteMethod)},\n+  {CC \"getImplementor\",                               CC \"(\" HS_KLASS2 \")\" HS_KLASS,                                                        FN_PTR(getImplementor)},\n+  {CC \"getStackTraceElement\",                         CC \"(\" HS_METHOD2 \"I)\" STACK_TRACE_ELEMENT,                                           FN_PTR(getStackTraceElement)},\n+  {CC \"methodIsIgnoredBySecurityStackWalk\",           CC \"(\" HS_METHOD2 \")Z\",                                                               FN_PTR(methodIsIgnoredBySecurityStackWalk)},\n+  {CC \"setNotInlinableOrCompilable\",                  CC \"(\" HS_METHOD2 \")V\",                                                               FN_PTR(setNotInlinableOrCompilable)},\n+  {CC \"isCompilable\",                                 CC \"(\" HS_METHOD2 \")Z\",                                                               FN_PTR(isCompilable)},\n+  {CC \"hasNeverInlineDirective\",                      CC \"(\" HS_METHOD2 \")Z\",                                                               FN_PTR(hasNeverInlineDirective)},\n+  {CC \"shouldInlineMethod\",                           CC \"(\" HS_METHOD2 \")Z\",                                                               FN_PTR(shouldInlineMethod)},\n+  {CC \"lookupType\",                                   CC \"(\" STRING HS_KLASS2 \"Z)\" HS_RESOLVED_TYPE,                                        FN_PTR(lookupType)},\n+  {CC \"getArrayType\",                                 CC \"(C\" HS_KLASS2 \")\" HS_KLASS,                                                       FN_PTR(getArrayType)},\n@@ -2839,16 +2831,16 @@\n-  {CC \"lookupNameInPool\",                             CC \"(\" HS_CONSTANT_POOL \"I)\" STRING,                                                  FN_PTR(lookupNameInPool)},\n-  {CC \"lookupNameAndTypeRefIndexInPool\",              CC \"(\" HS_CONSTANT_POOL \"I)I\",                                                        FN_PTR(lookupNameAndTypeRefIndexInPool)},\n-  {CC \"lookupSignatureInPool\",                        CC \"(\" HS_CONSTANT_POOL \"I)\" STRING,                                                  FN_PTR(lookupSignatureInPool)},\n-  {CC \"lookupKlassRefIndexInPool\",                    CC \"(\" HS_CONSTANT_POOL \"I)I\",                                                        FN_PTR(lookupKlassRefIndexInPool)},\n-  {CC \"lookupKlassInPool\",                            CC \"(\" HS_CONSTANT_POOL \"I)Ljava\/lang\/Object;\",                                       FN_PTR(lookupKlassInPool)},\n-  {CC \"lookupAppendixInPool\",                         CC \"(\" HS_CONSTANT_POOL \"I)\" OBJECTCONSTANT,                                          FN_PTR(lookupAppendixInPool)},\n-  {CC \"lookupMethodInPool\",                           CC \"(\" HS_CONSTANT_POOL \"IB)\" HS_RESOLVED_METHOD,                                     FN_PTR(lookupMethodInPool)},\n-  {CC \"constantPoolRemapInstructionOperandFromCache\", CC \"(\" HS_CONSTANT_POOL \"I)I\",                                                        FN_PTR(constantPoolRemapInstructionOperandFromCache)},\n-  {CC \"resolveBootstrapMethod\",                       CC \"(\" HS_CONSTANT_POOL \"I)[\" OBJECT,                                                 FN_PTR(resolveBootstrapMethod)},\n-  {CC \"resolvePossiblyCachedConstantInPool\",          CC \"(\" HS_CONSTANT_POOL \"I)\" JAVACONSTANT,                                            FN_PTR(resolvePossiblyCachedConstantInPool)},\n-  {CC \"resolveTypeInPool\",                            CC \"(\" HS_CONSTANT_POOL \"I)\" HS_RESOLVED_KLASS,                                       FN_PTR(resolveTypeInPool)},\n-  {CC \"resolveFieldInPool\",                           CC \"(\" HS_CONSTANT_POOL \"I\" HS_RESOLVED_METHOD \"B[I)\" HS_RESOLVED_KLASS,              FN_PTR(resolveFieldInPool)},\n-  {CC \"resolveInvokeDynamicInPool\",                   CC \"(\" HS_CONSTANT_POOL \"I)V\",                                                        FN_PTR(resolveInvokeDynamicInPool)},\n-  {CC \"resolveInvokeHandleInPool\",                    CC \"(\" HS_CONSTANT_POOL \"I)V\",                                                        FN_PTR(resolveInvokeHandleInPool)},\n-  {CC \"isResolvedInvokeHandleInPool\",                 CC \"(\" HS_CONSTANT_POOL \"I)I\",                                                        FN_PTR(isResolvedInvokeHandleInPool)},\n-  {CC \"resolveMethod\",                                CC \"(\" HS_RESOLVED_KLASS HS_RESOLVED_METHOD HS_RESOLVED_KLASS \")\" HS_RESOLVED_METHOD, FN_PTR(resolveMethod)},\n+  {CC \"lookupNameInPool\",                             CC \"(\" HS_CONSTANT_POOL2 \"I)\" STRING,                                                 FN_PTR(lookupNameInPool)},\n+  {CC \"lookupNameAndTypeRefIndexInPool\",              CC \"(\" HS_CONSTANT_POOL2 \"I)I\",                                                       FN_PTR(lookupNameAndTypeRefIndexInPool)},\n+  {CC \"lookupSignatureInPool\",                        CC \"(\" HS_CONSTANT_POOL2 \"I)\" STRING,                                                 FN_PTR(lookupSignatureInPool)},\n+  {CC \"lookupKlassRefIndexInPool\",                    CC \"(\" HS_CONSTANT_POOL2 \"I)I\",                                                       FN_PTR(lookupKlassRefIndexInPool)},\n+  {CC \"lookupKlassInPool\",                            CC \"(\" HS_CONSTANT_POOL2 \"I)Ljava\/lang\/Object;\",                                      FN_PTR(lookupKlassInPool)},\n+  {CC \"lookupAppendixInPool\",                         CC \"(\" HS_CONSTANT_POOL2 \"I)\" OBJECTCONSTANT,                                         FN_PTR(lookupAppendixInPool)},\n+  {CC \"lookupMethodInPool\",                           CC \"(\" HS_CONSTANT_POOL2 \"IB)\" HS_METHOD,                                             FN_PTR(lookupMethodInPool)},\n+  {CC \"constantPoolRemapInstructionOperandFromCache\", CC \"(\" HS_CONSTANT_POOL2 \"I)I\",                                                       FN_PTR(constantPoolRemapInstructionOperandFromCache)},\n+  {CC \"resolveBootstrapMethod\",                       CC \"(\" HS_CONSTANT_POOL2 \"I)[\" OBJECT,                                                FN_PTR(resolveBootstrapMethod)},\n+  {CC \"resolvePossiblyCachedConstantInPool\",          CC \"(\" HS_CONSTANT_POOL2 \"I)\" JAVACONSTANT,                                           FN_PTR(resolvePossiblyCachedConstantInPool)},\n+  {CC \"resolveTypeInPool\",                            CC \"(\" HS_CONSTANT_POOL2 \"I)\" HS_KLASS,                                               FN_PTR(resolveTypeInPool)},\n+  {CC \"resolveFieldInPool\",                           CC \"(\" HS_CONSTANT_POOL2 \"I\" HS_METHOD2 \"B[I)\" HS_KLASS,                              FN_PTR(resolveFieldInPool)},\n+  {CC \"resolveInvokeDynamicInPool\",                   CC \"(\" HS_CONSTANT_POOL2 \"I)V\",                                                       FN_PTR(resolveInvokeDynamicInPool)},\n+  {CC \"resolveInvokeHandleInPool\",                    CC \"(\" HS_CONSTANT_POOL2 \"I)V\",                                                       FN_PTR(resolveInvokeHandleInPool)},\n+  {CC \"isResolvedInvokeHandleInPool\",                 CC \"(\" HS_CONSTANT_POOL2 \"I)I\",                                                       FN_PTR(isResolvedInvokeHandleInPool)},\n+  {CC \"resolveMethod\",                                CC \"(\" HS_KLASS2 HS_METHOD2 HS_KLASS2 \")\" HS_METHOD,                                  FN_PTR(resolveMethod)},\n@@ -2856,3 +2848,3 @@\n-  {CC \"getVtableIndexForInterfaceMethod\",             CC \"(\" HS_RESOLVED_KLASS HS_RESOLVED_METHOD \")I\",                                     FN_PTR(getVtableIndexForInterfaceMethod)},\n-  {CC \"getClassInitializer\",                          CC \"(\" HS_RESOLVED_KLASS \")\" HS_RESOLVED_METHOD,                                      FN_PTR(getClassInitializer)},\n-  {CC \"hasFinalizableSubclass\",                       CC \"(\" HS_RESOLVED_KLASS \")Z\",                                                        FN_PTR(hasFinalizableSubclass)},\n+  {CC \"getVtableIndexForInterfaceMethod\",             CC \"(\" HS_KLASS2 HS_METHOD2 \")I\",                                                     FN_PTR(getVtableIndexForInterfaceMethod)},\n+  {CC \"getClassInitializer\",                          CC \"(\" HS_KLASS2 \")\" HS_METHOD,                                                       FN_PTR(getClassInitializer)},\n+  {CC \"hasFinalizableSubclass\",                       CC \"(\" HS_KLASS2 \")Z\",                                                                FN_PTR(hasFinalizableSubclass)},\n@@ -2860,4 +2852,4 @@\n-  {CC \"asResolvedJavaMethod\",                         CC \"(\" EXECUTABLE \")\" HS_RESOLVED_METHOD,                                             FN_PTR(asResolvedJavaMethod)},\n-  {CC \"getResolvedJavaMethod\",                        CC \"(\" OBJECTCONSTANT \"J)\" HS_RESOLVED_METHOD,                                        FN_PTR(getResolvedJavaMethod)},\n-  {CC \"getConstantPool\",                              CC \"(\" METASPACE_OBJECT \")\" HS_CONSTANT_POOL,                                         FN_PTR(getConstantPool)},\n-  {CC \"getResolvedJavaType0\",                         CC \"(Ljava\/lang\/Object;JZ)\" HS_RESOLVED_KLASS,                                        FN_PTR(getResolvedJavaType0)},\n+  {CC \"asResolvedJavaMethod\",                         CC \"(\" EXECUTABLE \")\" HS_METHOD,                                                      FN_PTR(asResolvedJavaMethod)},\n+  {CC \"getResolvedJavaMethod\",                        CC \"(\" OBJECTCONSTANT \"J)\" HS_METHOD,                                                 FN_PTR(getResolvedJavaMethod)},\n+  {CC \"getConstantPool\",                              CC \"(\" OBJECT \"JZ)\" HS_CONSTANT_POOL,                                                 FN_PTR(getConstantPool)},\n+  {CC \"getResolvedJavaType0\",                         CC \"(Ljava\/lang\/Object;JZ)\" HS_KLASS,                                                 FN_PTR(getResolvedJavaType0)},\n@@ -2865,1 +2857,2 @@\n-  {CC \"installCode\",                                  CC \"(\" TARGET_DESCRIPTION HS_COMPILED_CODE INSTALLED_CODE \"J[B)I\",                    FN_PTR(installCode)},\n+  {CC \"installCode0\",                                 CC \"(JJZ\" HS_COMPILED_CODE \"[\" OBJECT INSTALLED_CODE \"J[B)I\",                         FN_PTR(installCode0)},\n+  {CC \"getInstallCodeFlags\",                          CC \"()I\",                                                                             FN_PTR(getInstallCodeFlags)},\n@@ -2869,4 +2862,4 @@\n-  {CC \"getLineNumberTable\",                           CC \"(\" HS_RESOLVED_METHOD \")[J\",                                                      FN_PTR(getLineNumberTable)},\n-  {CC \"getLocalVariableTableStart\",                   CC \"(\" HS_RESOLVED_METHOD \")J\",                                                       FN_PTR(getLocalVariableTableStart)},\n-  {CC \"getLocalVariableTableLength\",                  CC \"(\" HS_RESOLVED_METHOD \")I\",                                                       FN_PTR(getLocalVariableTableLength)},\n-  {CC \"reprofile\",                                    CC \"(\" HS_RESOLVED_METHOD \")V\",                                                       FN_PTR(reprofile)},\n+  {CC \"getLineNumberTable\",                           CC \"(\" HS_METHOD2 \")[J\",                                                              FN_PTR(getLineNumberTable)},\n+  {CC \"getLocalVariableTableStart\",                   CC \"(\" HS_METHOD2 \")J\",                                                               FN_PTR(getLocalVariableTableStart)},\n+  {CC \"getLocalVariableTableLength\",                  CC \"(\" HS_METHOD2 \")I\",                                                               FN_PTR(getLocalVariableTableLength)},\n+  {CC \"reprofile\",                                    CC \"(\" HS_METHOD2 \")V\",                                                               FN_PTR(reprofile)},\n@@ -2877,3 +2870,3 @@\n-  {CC \"allocateCompileId\",                            CC \"(\" HS_RESOLVED_METHOD \"I)I\",                                                      FN_PTR(allocateCompileId)},\n-  {CC \"isMature\",                                     CC \"(\" METASPACE_METHOD_DATA \")Z\",                                                    FN_PTR(isMature)},\n-  {CC \"hasCompiledCodeForOSR\",                        CC \"(\" HS_RESOLVED_METHOD \"II)Z\",                                                     FN_PTR(hasCompiledCodeForOSR)},\n+  {CC \"allocateCompileId\",                            CC \"(\" HS_METHOD2 \"I)I\",                                                              FN_PTR(allocateCompileId)},\n+  {CC \"isMature\",                                     CC \"(J)Z\",                                                                            FN_PTR(isMature)},\n+  {CC \"hasCompiledCodeForOSR\",                        CC \"(\" HS_METHOD2 \"II)Z\",                                                             FN_PTR(hasCompiledCodeForOSR)},\n@@ -2881,0 +2874,1 @@\n+  {CC \"getSignatureName\",                             CC \"(J)\" STRING,                                                                      FN_PTR(getSignatureName)},\n@@ -2890,4 +2884,4 @@\n-  {CC \"getInterfaces\",                                CC \"(\" HS_RESOLVED_KLASS \")[\" HS_RESOLVED_KLASS,                                      FN_PTR(getInterfaces)},\n-  {CC \"getComponentType\",                             CC \"(\" HS_RESOLVED_KLASS \")\" HS_RESOLVED_TYPE,                                        FN_PTR(getComponentType)},\n-  {CC \"ensureInitialized\",                            CC \"(\" HS_RESOLVED_KLASS \")V\",                                                        FN_PTR(ensureInitialized)},\n-  {CC \"ensureLinked\",                                 CC \"(\" HS_RESOLVED_KLASS \")V\",                                                        FN_PTR(ensureLinked)},\n+  {CC \"getInterfaces\",                                CC \"(\" HS_KLASS2 \")[\" HS_KLASS,                                                       FN_PTR(getInterfaces)},\n+  {CC \"getComponentType\",                             CC \"(\" HS_KLASS2 \")\" HS_RESOLVED_TYPE,                                                FN_PTR(getComponentType)},\n+  {CC \"ensureInitialized\",                            CC \"(\" HS_KLASS2 \")V\",                                                                FN_PTR(ensureInitialized)},\n+  {CC \"ensureLinked\",                                 CC \"(\" HS_KLASS2 \")V\",                                                                FN_PTR(ensureLinked)},\n@@ -2898,7 +2892,7 @@\n-  {CC \"getDeclaredConstructors\",                      CC \"(\" HS_RESOLVED_KLASS \")[\" RESOLVED_METHOD,                                        FN_PTR(getDeclaredConstructors)},\n-  {CC \"getDeclaredMethods\",                           CC \"(\" HS_RESOLVED_KLASS \")[\" RESOLVED_METHOD,                                        FN_PTR(getDeclaredMethods)},\n-  {CC \"readFieldValue\",                               CC \"(\" HS_RESOLVED_KLASS HS_RESOLVED_KLASS \"JLjdk\/vm\/ci\/meta\/JavaKind;)\" JAVACONSTANT, FN_PTR(readFieldValue)},\n-  {CC \"readFieldValue\",                               CC \"(\" OBJECTCONSTANT HS_RESOLVED_KLASS \"JLjdk\/vm\/ci\/meta\/JavaKind;)\" JAVACONSTANT,   FN_PTR(readFieldValue)},\n-  {CC \"isInstance\",                                   CC \"(\" HS_RESOLVED_KLASS OBJECTCONSTANT \")Z\",                                         FN_PTR(isInstance)},\n-  {CC \"isAssignableFrom\",                             CC \"(\" HS_RESOLVED_KLASS HS_RESOLVED_KLASS \")Z\",                                      FN_PTR(isAssignableFrom)},\n-  {CC \"isTrustedForIntrinsics\",                       CC \"(\" HS_RESOLVED_KLASS \")Z\",                                                        FN_PTR(isTrustedForIntrinsics)},\n+  {CC \"getDeclaredConstructors\",                      CC \"(\" HS_KLASS2 \")[\" RESOLVED_METHOD,                                                FN_PTR(getDeclaredConstructors)},\n+  {CC \"getDeclaredMethods\",                           CC \"(\" HS_KLASS2 \")[\" RESOLVED_METHOD,                                                FN_PTR(getDeclaredMethods)},\n+  {CC \"readStaticFieldValue\",                         CC \"(\" HS_KLASS2 \"JC)\" JAVACONSTANT,                                                  FN_PTR(readStaticFieldValue)},\n+  {CC \"readFieldValue\",                               CC \"(\" OBJECTCONSTANT HS_KLASS2 \"JC)\" JAVACONSTANT,                                   FN_PTR(readFieldValue)},\n+  {CC \"isInstance\",                                   CC \"(\" HS_KLASS2 OBJECTCONSTANT \")Z\",                                                 FN_PTR(isInstance)},\n+  {CC \"isAssignableFrom\",                             CC \"(\" HS_KLASS2 HS_KLASS2 \")Z\",                                                      FN_PTR(isAssignableFrom)},\n+  {CC \"isTrustedForIntrinsics\",                       CC \"(\" HS_KLASS2 \")Z\",                                                                FN_PTR(isTrustedForIntrinsics)},\n@@ -2908,1 +2902,1 @@\n-  {CC \"getJavaMirror\",                                CC \"(\" HS_RESOLVED_TYPE \")\" OBJECTCONSTANT,                                           FN_PTR(getJavaMirror)},\n+  {CC \"getJavaMirror\",                                CC \"(\" HS_KLASS2 \")\" OBJECTCONSTANT,                                                  FN_PTR(getJavaMirror)},\n@@ -2911,2 +2905,2 @@\n-  {CC \"arrayBaseOffset\",                              CC \"(Ljdk\/vm\/ci\/meta\/JavaKind;)I\",                                                    FN_PTR(arrayBaseOffset)},\n-  {CC \"arrayIndexScale\",                              CC \"(Ljdk\/vm\/ci\/meta\/JavaKind;)I\",                                                    FN_PTR(arrayIndexScale)},\n+  {CC \"arrayBaseOffset\",                              CC \"(C)I\",                                                                            FN_PTR(arrayBaseOffset)},\n+  {CC \"arrayIndexScale\",                              CC \"(C)I\",                                                                            FN_PTR(arrayIndexScale)},\n@@ -2923,2 +2917,2 @@\n-  {CC \"asReflectionExecutable\",                       CC \"(\" HS_RESOLVED_METHOD \")\" REFLECTION_EXECUTABLE,                                  FN_PTR(asReflectionExecutable)},\n-  {CC \"asReflectionField\",                            CC \"(\" HS_RESOLVED_KLASS \"I)\" REFLECTION_FIELD,                                       FN_PTR(asReflectionField)},\n+  {CC \"asReflectionExecutable\",                       CC \"(\" HS_METHOD2 \")\" REFLECTION_EXECUTABLE,                                          FN_PTR(asReflectionExecutable)},\n+  {CC \"asReflectionField\",                            CC \"(\" HS_KLASS2 \"I)\" REFLECTION_FIELD,                                               FN_PTR(asReflectionField)},\n@@ -2926,1 +2920,1 @@\n-  {CC \"getFailedSpeculationsAddress\",                 CC \"(\" HS_RESOLVED_METHOD \")J\",                                                       FN_PTR(getFailedSpeculationsAddress)},\n+  {CC \"getFailedSpeculationsAddress\",                 CC \"(\" HS_METHOD2 \")J\",                                                               FN_PTR(getFailedSpeculationsAddress)},\n@@ -2937,1 +2931,1 @@\n-  {CC \"notifyCompilerInliningEvent\",                  CC \"(I\" HS_RESOLVED_METHOD HS_RESOLVED_METHOD \"ZLjava\/lang\/String;I)V\",               FN_PTR(notifyCompilerInliningEvent)},\n+  {CC \"notifyCompilerInliningEvent\",                  CC \"(I\" HS_METHOD2 HS_METHOD2 \"ZLjava\/lang\/String;I)V\",                               FN_PTR(notifyCompilerInliningEvent)},\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":328,"deletions":334,"binary":false,"changes":662,"status":"modified"},{"patch":"@@ -159,1 +159,1 @@\n-  nonstatic_field(InstanceKlass,               _init_state,                                   u1)                                    \\\n+  nonstatic_field(InstanceKlass,               _init_state,                                   InstanceKlass::ClassState)             \\\n@@ -174,0 +174,1 @@\n+  nonstatic_field(JavaThread,                  _vthread,                                      OopHandle)                             \\\n@@ -193,0 +194,1 @@\n+  nonstatic_field(JavaThread,                  _held_monitor_count,                           int64_t)                               \\\n@@ -337,0 +339,2 @@\n+  static_field(StubRoutines,                _cont_doYield,                                    address)                               \\\n+  static_field(StubRoutines,                _cont_thaw,                                       address)                               \\\n@@ -491,0 +495,64 @@\n+  declare_constant(CodeInstaller::ILLEGAL)                                \\\n+  declare_constant(CodeInstaller::REGISTER_PRIMITIVE)                     \\\n+  declare_constant(CodeInstaller::REGISTER_OOP)                           \\\n+  declare_constant(CodeInstaller::REGISTER_NARROW_OOP)                    \\\n+  declare_constant(CodeInstaller::STACK_SLOT_PRIMITIVE)                   \\\n+  declare_constant(CodeInstaller::STACK_SLOT_OOP)                         \\\n+  declare_constant(CodeInstaller::STACK_SLOT_NARROW_OOP)                  \\\n+  declare_constant(CodeInstaller::VIRTUAL_OBJECT_ID)                      \\\n+  declare_constant(CodeInstaller::VIRTUAL_OBJECT_ID2)                     \\\n+  declare_constant(CodeInstaller::NULL_CONSTANT)                          \\\n+  declare_constant(CodeInstaller::RAW_CONSTANT)                           \\\n+  declare_constant(CodeInstaller::PRIMITIVE_0)                            \\\n+  declare_constant(CodeInstaller::PRIMITIVE4)                             \\\n+  declare_constant(CodeInstaller::PRIMITIVE8)                             \\\n+  declare_constant(CodeInstaller::JOBJECT)                                \\\n+  declare_constant(CodeInstaller::OBJECT_ID)                              \\\n+  declare_constant(CodeInstaller::OBJECT_ID2)                             \\\n+                                                                          \\\n+  declare_constant(CodeInstaller::NO_FINALIZABLE_SUBCLASS)                \\\n+  declare_constant(CodeInstaller::CONCRETE_SUBTYPE)                       \\\n+  declare_constant(CodeInstaller::LEAF_TYPE)                              \\\n+  declare_constant(CodeInstaller::CONCRETE_METHOD)                        \\\n+  declare_constant(CodeInstaller::CALLSITE_TARGET_VALUE)                  \\\n+                                                                          \\\n+  declare_constant(CodeInstaller::PATCH_OBJECT_ID)                        \\\n+  declare_constant(CodeInstaller::PATCH_OBJECT_ID2)                       \\\n+  declare_constant(CodeInstaller::PATCH_NARROW_OBJECT_ID)                 \\\n+  declare_constant(CodeInstaller::PATCH_NARROW_OBJECT_ID2)                \\\n+  declare_constant(CodeInstaller::PATCH_JOBJECT)                          \\\n+  declare_constant(CodeInstaller::PATCH_NARROW_JOBJECT)                   \\\n+  declare_constant(CodeInstaller::PATCH_KLASS)                            \\\n+  declare_constant(CodeInstaller::PATCH_NARROW_KLASS)                     \\\n+  declare_constant(CodeInstaller::PATCH_METHOD)                           \\\n+  declare_constant(CodeInstaller::PATCH_DATA_SECTION_REFERENCE)           \\\n+                                                                          \\\n+  declare_constant(CodeInstaller::SITE_CALL)                              \\\n+  declare_constant(CodeInstaller::SITE_FOREIGN_CALL)                      \\\n+  declare_constant(CodeInstaller::SITE_FOREIGN_CALL_NO_DEBUG_INFO)        \\\n+  declare_constant(CodeInstaller::SITE_SAFEPOINT)                         \\\n+  declare_constant(CodeInstaller::SITE_INFOPOINT)                         \\\n+  declare_constant(CodeInstaller::SITE_IMPLICIT_EXCEPTION)                \\\n+  declare_constant(CodeInstaller::SITE_IMPLICIT_EXCEPTION_DISPATCH)       \\\n+  declare_constant(CodeInstaller::SITE_MARK)                              \\\n+  declare_constant(CodeInstaller::SITE_DATA_PATCH)                        \\\n+  declare_constant(CodeInstaller::SITE_EXCEPTION_HANDLER)                 \\\n+                                                                          \\\n+  declare_constant(CodeInstaller::DI_HAS_REFERENCE_MAP)                   \\\n+  declare_constant(CodeInstaller::DI_HAS_CALLEE_SAVE_INFO)                \\\n+  declare_constant(CodeInstaller::DI_HAS_FRAMES)                          \\\n+                                                                          \\\n+  declare_constant(CodeInstaller::DIF_HAS_LOCALS)                         \\\n+  declare_constant(CodeInstaller::DIF_HAS_STACK)                          \\\n+  declare_constant(CodeInstaller::DIF_HAS_LOCKS)                          \\\n+  declare_constant(CodeInstaller::DIF_DURING_CALL)                        \\\n+  declare_constant(CodeInstaller::DIF_RETHROW_EXCEPTION)                  \\\n+                                                                          \\\n+  declare_constant(CodeInstaller::HCC_IS_NMETHOD)                         \\\n+  declare_constant(CodeInstaller::HCC_HAS_ASSUMPTIONS)                    \\\n+  declare_constant(CodeInstaller::HCC_HAS_METHODS)                        \\\n+  declare_constant(CodeInstaller::HCC_HAS_DEOPT_RESCUE_SLOT)              \\\n+  declare_constant(CodeInstaller::HCC_HAS_COMMENTS)                       \\\n+                                                                          \\\n+  declare_constant(CodeInstaller::NO_REGISTER)                            \\\n+                                                                          \\\n@@ -621,0 +689,1 @@\n+  declare_constant(Method::_changes_current_thread)                       \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":70,"deletions":1,"binary":false,"changes":71,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -33,0 +33,1 @@\n+#include \"runtime\/os.hpp\"\n","filename":"src\/hotspot\/share\/memory\/metaspace\/testHelpers.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -56,1 +56,1 @@\n-  InstanceRefKlass(const ClassFileParser& parser) : InstanceKlass(parser, Kind) {}\n+  InstanceRefKlass(const ClassFileParser& parser);\n","filename":"src\/hotspot\/share\/oops\/instanceRefKlass.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -37,0 +37,1 @@\n+#include \"utilities\/devirtualizer.inline.hpp\"\n","filename":"src\/hotspot\/share\/oops\/instanceRefKlass.inline.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -46,0 +46,1 @@\n+  InstanceStackChunkKlassKind,\n@@ -177,1 +178,1 @@\n-  \/\/ Flags of the current shared class.\n+  \/\/ Various attributes for shared classes. Should be zero for a non-shared class.\n@@ -179,1 +180,1 @@\n-  enum {\n+  enum CDSSharedClassFlags {\n@@ -184,1 +185,3 @@\n-    _regenerated                           = 1 << 5\n+    \/\/ This class was not loaded from a classfile in the module image\n+    \/\/ or classpath.\n+    _is_generated_shared_class             = 1 << 5\n@@ -356,2 +359,2 @@\n-  void set_regenerated() {\n-    CDS_ONLY(_shared_class_flags |= _regenerated;)\n+  void set_is_generated_shared_class() {\n+    CDS_ONLY(_shared_class_flags |= _is_generated_shared_class;)\n@@ -359,2 +362,2 @@\n-  bool is_regenerated() const {\n-    CDS_ONLY(return (_shared_class_flags & _regenerated) != 0;)\n+  bool is_generated_shared_class() const {\n+    CDS_ONLY(return (_shared_class_flags & _is_generated_shared_class) != 0;)\n@@ -622,1 +625,1 @@\n-  bool is_instance_klass()              const { return assert_same_query(_kind <= InstanceClassLoaderKlassKind, is_instance_klass_slow()); }\n+  bool is_instance_klass()              const { return assert_same_query(_kind <= InstanceStackChunkKlassKind, is_instance_klass_slow()); }\n@@ -629,0 +632,1 @@\n+  bool is_stack_chunk_instance_klass()  const { return _kind == InstanceStackChunkKlassKind; }\n","filename":"src\/hotspot\/share\/oops\/klass.hpp","additions":12,"deletions":8,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2005, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2005, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -47,0 +47,4 @@\n+\/\/ This returns false if the Klass is unloaded, or about to be unloaded because the holder of\n+\/\/ the CLD is no longer strongly reachable.\n+\/\/ The return value of this function may change from true to false after a safepoint. So the caller\n+\/\/ of this function must ensure that a safepoint doesn't happen while interpreting the return value.\n","filename":"src\/hotspot\/share\/oops\/klass.inline.hpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n-#include \"runtime\/thread.inline.hpp\"\n+#include \"runtime\/javaThread.hpp\"\n","filename":"src\/hotspot\/share\/oops\/markWord.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+  assert(!CompressedKlassPointers::is_null(narrow_klass()), \"narrow klass must not be null: \" INTPTR_FORMAT, value());\n","filename":"src\/hotspot\/share\/oops\/markWord.inline.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2010, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2010, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,1 +31,0 @@\n-#include \"memory\/iterator.hpp\"\n@@ -37,0 +36,1 @@\n+#include \"utilities\/devirtualizer.inline.hpp\"\n","filename":"src\/hotspot\/share\/oops\/objArrayKlass.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -42,0 +42,3 @@\n+  friend class Continuation;\n+  template <typename T>\n+  friend class RawOopWriter;\n","filename":"src\/hotspot\/share\/oops\/objArrayOop.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -39,2 +39,1 @@\n-#include \"runtime\/thread.inline.hpp\"\n-#include \"utilities\/copy.hpp\"\n+#include \"runtime\/javaThread.hpp\"\n@@ -44,1 +43,7 @@\n-  klass()->oop_print_on(const_cast<oopDesc*>(this), st);\n+  if (*((juint*)this) == badHeapWordVal) {\n+    st->print(\"BAD WORD\");\n+  } else if (*((juint*)this) == badMetaWordVal) {\n+    st->print(\"BAD META WORD\");\n+  } else {\n+    klass()->oop_print_on(cast_to_oop(this), st);\n+  }\n@@ -141,0 +146,1 @@\n+bool oopDesc::is_stackChunk_noinline()  const { return is_stackChunk();  }\n@@ -232,2 +238,8 @@\n-bool oopDesc::get_UseParallelGC() { return UseParallelGC; }\n-bool oopDesc::get_UseG1GC()       { return UseG1GC;       }\n+bool oopDesc::size_might_change() {\n+  \/\/ UseParallelGC and UseG1GC can change the length field\n+  \/\/ of an \"old copy\" of an object array in the young gen so it indicates\n+  \/\/ the grey portion of an already copied array. This will cause the first\n+  \/\/ disjunct below to fail if the two comparands are computed across such\n+  \/\/ a concurrent change.\n+  return Universe::heap()->is_gc_active() && is_objArray() && is_forwarded() && (UseParallelGC || UseG1GC);\n+}\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":18,"deletions":6,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -101,1 +101,1 @@\n-  \/\/ Returns the actual oop size of the object\n+  \/\/ Returns the actual oop size of the object in machine words\n@@ -111,0 +111,1 @@\n+  inline bool is_stackChunk()  const;\n@@ -118,0 +119,1 @@\n+  bool is_stackChunk_noinline()  const;\n@@ -323,3 +325,1 @@\n-  \/\/ Avoid include gc_globals.hpp in oop.inline.hpp\n-  DEBUG_ONLY(bool get_UseParallelGC();)\n-  DEBUG_ONLY(bool get_UseG1GC();)\n+  DEBUG_ONLY(bool size_might_change();)\n","filename":"src\/hotspot\/share\/oops\/oop.hpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"oops\/instanceKlass.hpp\"\n@@ -199,8 +200,1 @@\n-      \/\/ UseParallelGC and UseG1GC can change the length field\n-      \/\/ of an \"old copy\" of an object array in the young gen so it indicates\n-      \/\/ the grey portion of an already copied array. This will cause the first\n-      \/\/ disjunct below to fail if the two comparands are computed across such\n-      \/\/ a concurrent change.\n-      assert((s == klass->oop_size(this)) ||\n-             (Universe::is_gc_active() && is_objArray() && is_forwarded() && (get_UseParallelGC() || get_UseG1GC())),\n-             \"wrong array object size\");\n+      assert(s == klass->oop_size(this) || size_might_change(), \"wrong array object size\");\n@@ -218,5 +212,6 @@\n-bool oopDesc::is_instance()    const { return klass()->is_instance_klass();           }\n-bool oopDesc::is_instanceRef() const { return klass()->is_reference_instance_klass(); }\n-bool oopDesc::is_array()       const { return klass()->is_array_klass();              }\n-bool oopDesc::is_objArray()    const { return klass()->is_objArray_klass();           }\n-bool oopDesc::is_typeArray()   const { return klass()->is_typeArray_klass();          }\n+bool oopDesc::is_instance()    const { return klass()->is_instance_klass();             }\n+bool oopDesc::is_instanceRef() const { return klass()->is_reference_instance_klass();   }\n+bool oopDesc::is_stackChunk()  const { return klass()->is_stack_chunk_instance_klass(); }\n+bool oopDesc::is_array()       const { return klass()->is_array_klass();                }\n+bool oopDesc::is_objArray()    const { return klass()->is_objArray_klass();             }\n+bool oopDesc::is_typeArray()   const { return klass()->is_typeArray_klass();            }\n","filename":"src\/hotspot\/share\/oops\/oop.inline.hpp","additions":8,"deletions":13,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -45,0 +45,1 @@\n+typedef class     stackChunkOopDesc*          stackChunkOop;\n@@ -143,0 +144,1 @@\n+DEF_OOP(stackChunk);\n@@ -177,0 +179,1 @@\n+class     InstanceStackChunkKlass;\n","filename":"src\/hotspot\/share\/oops\/oopsHierarchy.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -195,2 +195,2 @@\n-void ReturnNode::dump_req(outputStream *st) const {\n-  \/\/ Dump the required inputs, enclosed in '(' and ')'\n+void ReturnNode::dump_req(outputStream *st, DumpConfig* dc) const {\n+  \/\/ Dump the required inputs, after printing \"returns\"\n@@ -199,3 +199,8 @@\n-    if (i == TypeFunc::Parms) st->print(\"returns\");\n-    if (in(i)) st->print(\"%c%d \", Compile::current()->node_arena()->contains(in(i)) ? ' ' : 'o', in(i)->_idx);\n-    else st->print(\"_ \");\n+    if (i == TypeFunc::Parms) st->print(\"returns \");\n+    Node* p = in(i);\n+    if (p != nullptr) {\n+      p->dump_idx(false, st, dc);\n+      st->print(\" \");\n+    } else {\n+      st->print(\"_ \");\n+    }\n@@ -238,2 +243,2 @@\n-void RethrowNode::dump_req(outputStream *st) const {\n-  \/\/ Dump the required inputs, enclosed in '(' and ')'\n+void RethrowNode::dump_req(outputStream *st, DumpConfig* dc) const {\n+  \/\/ Dump the required inputs, after printing \"exception\"\n@@ -242,3 +247,8 @@\n-    if (i == TypeFunc::Parms) st->print(\"exception\");\n-    if (in(i)) st->print(\"%c%d \", Compile::current()->node_arena()->contains(in(i)) ? ' ' : 'o', in(i)->_idx);\n-    else st->print(\"_ \");\n+    if (i == TypeFunc::Parms) st->print(\"exception \");\n+    Node* p = in(i);\n+    if (p != nullptr) {\n+      p->dump_idx(false, st, dc);\n+      st->print(\" \");\n+    } else {\n+      st->print(\"_ \");\n+    }\n@@ -383,1 +393,1 @@\n-      st->print(\" %s%d]=#Ptr\" INTPTR_FORMAT,msg,i,p2i(t->make_ptr()->isa_klassptr()->klass()));\n+      st->print(\" %s%d]=#Ptr\" INTPTR_FORMAT,msg,i,p2i(t->make_ptr()->isa_klassptr()->exact_klass()));\n@@ -474,1 +484,1 @@\n-      ciKlass* cik = spobj->bottom_type()->is_oopptr()->klass();\n+      ciKlass* cik = spobj->bottom_type()->is_oopptr()->exact_klass();\n@@ -692,1 +702,1 @@\n-void CallNode::dump_req(outputStream *st) const {\n+void CallNode::dump_req(outputStream *st, DumpConfig* dc) const {\n@@ -697,2 +707,7 @@\n-    if (in(i)) st->print(\"%c%d \", Compile::current()->node_arena()->contains(in(i)) ? ' ' : 'o', in(i)->_idx);\n-    else st->print(\"_ \");\n+    Node* p = in(i);\n+    if (p != nullptr) {\n+      p->dump_idx(false, st, dc);\n+      st->print(\" \");\n+    } else {\n+      st->print(\"_ \");\n+    }\n@@ -810,1 +825,1 @@\n-    ciKlass* boxing_klass = t_oop->klass();\n+    ciKlass* boxing_klass = t_oop->is_instptr()->instance_klass();\n@@ -814,1 +829,1 @@\n-      if ((proj == NULL) || (phase->type(proj)->is_instptr()->klass() != boxing_klass)) {\n+      if ((proj == NULL) || (phase->type(proj)->is_instptr()->instance_klass() != boxing_klass)) {\n@@ -829,1 +844,1 @@\n-                                 (inst_t->klass() == boxing_klass))) {\n+                                 (inst_t->instance_klass() == boxing_klass))) {\n@@ -837,1 +852,1 @@\n-                                 (inst_t->klass() == boxing_klass))) {\n+                                 (inst_t->instance_klass() == boxing_klass))) {\n@@ -1086,5 +1101,1 @@\n-      if (in(TypeFunc::Parms + callee->arg_size() - 1)->Opcode() == Op_ConP \/* NEP *\/\n-          && in(TypeFunc::Parms + 1)->Opcode() == Op_ConL \/* address *\/) {\n-        phase->C->prepend_late_inline(cg);\n-        set_generator(NULL);\n-      }\n+      \/\/ never retry\n@@ -1226,65 +1237,0 @@\n-\/\/=============================================================================\n-uint CallNativeNode::size_of() const { return sizeof(*this); }\n-bool CallNativeNode::cmp( const Node &n ) const {\n-  CallNativeNode &call = (CallNativeNode&)n;\n-  return CallNode::cmp(call) && !strcmp(_name,call._name)\n-    && _arg_regs == call._arg_regs && _ret_regs == call._ret_regs;\n-}\n-Node* CallNativeNode::match(const ProjNode *proj, const Matcher *matcher) {\n-  switch (proj->_con) {\n-    case TypeFunc::Control:\n-    case TypeFunc::I_O:\n-    case TypeFunc::Memory:\n-      return new MachProjNode(this,proj->_con,RegMask::Empty,MachProjNode::unmatched_proj);\n-    case TypeFunc::ReturnAdr:\n-    case TypeFunc::FramePtr:\n-      ShouldNotReachHere();\n-    case TypeFunc::Parms: {\n-      const Type* field_at_con = tf()->range()->field_at(proj->_con);\n-      const BasicType bt = field_at_con->basic_type();\n-      OptoReg::Name optoreg = OptoReg::as_OptoReg(_ret_regs.at(proj->_con - TypeFunc::Parms));\n-      OptoRegPair regs;\n-      if (bt == T_DOUBLE || bt == T_LONG) {\n-        regs.set2(optoreg);\n-      } else {\n-        regs.set1(optoreg);\n-      }\n-      RegMask rm = RegMask(regs.first());\n-      if(OptoReg::is_valid(regs.second()))\n-        rm.Insert(regs.second());\n-      return new MachProjNode(this, proj->_con, rm, field_at_con->ideal_reg());\n-    }\n-    case TypeFunc::Parms + 1: {\n-      assert(tf()->range()->field_at(proj->_con) == Type::HALF, \"Expected HALF\");\n-      assert(_ret_regs.at(proj->_con - TypeFunc::Parms) == VMRegImpl::Bad(), \"Unexpected register for Type::HALF\");\n-      \/\/ 2nd half of doubles and longs\n-      return new MachProjNode(this, proj->_con, RegMask::Empty, (uint) OptoReg::Bad);\n-    }\n-    default:\n-      ShouldNotReachHere();\n-  }\n-  return NULL;\n-}\n-#ifndef PRODUCT\n-void CallNativeNode::print_regs(const GrowableArray<VMReg>& regs, outputStream* st) {\n-  st->print(\"{ \");\n-  for (int i = 0; i < regs.length(); i++) {\n-    regs.at(i)->print_on(st);\n-    if (i < regs.length() - 1) {\n-      st->print(\", \");\n-    }\n-  }\n-  st->print(\" } \");\n-}\n-\n-void CallNativeNode::dump_spec(outputStream *st) const {\n-  st->print(\"# \");\n-  st->print(\"%s \", _name);\n-  st->print(\"_arg_regs: \");\n-  print_regs(_arg_regs, st);\n-  st->print(\"_ret_regs: \");\n-  print_regs(_ret_regs, st);\n-  CallNode::dump_spec(st);\n-}\n-#endif\n-\n@@ -1311,34 +1257,0 @@\n-void CallNativeNode::calling_convention( BasicType* sig_bt, VMRegPair *parm_regs, uint argcnt ) const {\n-  assert((tf()->domain()->cnt() - TypeFunc::Parms) == argcnt, \"arg counts must match!\");\n-#ifdef ASSERT\n-  for (uint i = 0; i < argcnt; i++) {\n-    assert(tf()->domain()->field_at(TypeFunc::Parms + i)->basic_type() == sig_bt[i], \"types must match!\");\n-  }\n-#endif\n-  for (uint i = 0; i < argcnt; i++) {\n-    switch (sig_bt[i]) {\n-      case T_BOOLEAN:\n-      case T_CHAR:\n-      case T_BYTE:\n-      case T_SHORT:\n-      case T_INT:\n-      case T_FLOAT:\n-        parm_regs[i].set1(_arg_regs.at(i));\n-        break;\n-      case T_LONG:\n-      case T_DOUBLE:\n-        assert((i + 1) < argcnt && sig_bt[i + 1] == T_VOID, \"expecting half\");\n-        parm_regs[i].set2(_arg_regs.at(i));\n-        break;\n-      case T_VOID: \/\/ Halves of longs and doubles\n-        assert(i != 0 && (sig_bt[i - 1] == T_LONG || sig_bt[i - 1] == T_DOUBLE), \"expecting half\");\n-        assert(_arg_regs.at(i) == VMRegImpl::Bad(), \"expecting bad reg\");\n-        parm_regs[i].set_bad();\n-        break;\n-      default:\n-        ShouldNotReachHere();\n-        break;\n-    }\n-  }\n-}\n-\n@@ -1550,0 +1462,6 @@\n+Node* SafePointNode::peek_operand(uint off) const {\n+  assert(jvms()->sp() > 0, \"must have an operand\");\n+  assert(off < jvms()->sp(), \"off is out-of-range\");\n+  return stack(jvms(), jvms()->sp() - off - 1);\n+}\n+\n@@ -1707,1 +1625,3 @@\n-      if (!allow_new_nodes) return NULL;\n+      if (!allow_new_nodes) {\n+        return NULL;\n+      }\n@@ -1711,3 +1631,4 @@\n-      assert(init != NULL, \"initialization not found\");\n-      length = new CastIINode(length, narrow_length_type);\n-      length->set_req(TypeFunc::Control, init->proj_out_or_null(TypeFunc::Control));\n+      if (init != NULL) {\n+        length = new CastIINode(length, narrow_length_type);\n+        length->set_req(TypeFunc::Control, init->proj_out_or_null(TypeFunc::Control));\n+      }\n@@ -2312,1 +2233,1 @@\n-  if (dest_t->isa_instptr() && !dest_t->klass()->equals(phase->C->env()->Object_klass())) {\n+  if (dest_t->isa_instptr() && !dest_t->is_instptr()->instance_klass()->equals(phase->C->env()->Object_klass())) {\n@@ -2320,1 +2241,1 @@\n-    if (dest_t->klass()->is_subtype_of(t_oop->klass()) || t_oop->klass()->is_subtype_of(dest_t->klass())) {\n+    if (dest_t->maybe_java_subtype_of(t_oop) || t_oop->maybe_java_subtype_of(dest_t)) {\n","filename":"src\/hotspot\/share\/opto\/callnode.cpp","additions":50,"deletions":129,"binary":false,"changes":179,"status":"modified"},{"patch":"@@ -54,0 +54,1 @@\n+#include \"runtime\/continuation.hpp\"\n@@ -162,0 +163,5 @@\n+#ifndef PRODUCT\n+  if (PrintOptoStatistics) {\n+    Atomic::inc(&PhaseMacroExpand::_GC_barriers_removed_counter);\n+  }\n+#endif\n@@ -680,1 +686,0 @@\n-  ciKlass* klass = NULL;\n@@ -686,1 +691,1 @@\n-  ciType* elem_type = NULL;\n+  const Type* field_type = NULL;\n@@ -696,1 +701,0 @@\n-    klass = res_type->klass();\n@@ -699,2 +703,1 @@\n-      assert(klass->is_instance_klass(), \"must be an instance klass.\");\n-      iklass = klass->as_instance_klass();\n+      iklass = res_type->is_instptr()->instance_klass();\n@@ -705,3 +708,2 @@\n-      assert(klass->is_array_klass() && nfields >= 0, \"must be an array klass.\");\n-      elem_type = klass->as_array_klass()->element_type();\n-      basic_elem_type = elem_type->basic_type();\n+      assert(nfields >= 0, \"must be an array klass.\");\n+      basic_elem_type = res_type->is_aryptr()->elem()->array_element_basic_type();\n@@ -710,0 +712,1 @@\n+      field_type = res_type->is_aryptr()->elem();\n@@ -739,1 +742,1 @@\n-        elem_type = field->type();\n+        ciType* elem_type = field->type();\n@@ -741,15 +744,17 @@\n-      } else {\n-        offset = array_base + j * (intptr_t)element_size;\n-      }\n-      const Type *field_type;\n-      \/\/ The next code is taken from Parse::do_get_xxx().\n-      if (is_reference_type(basic_elem_type)) {\n-        if (!elem_type->is_loaded()) {\n-          field_type = TypeInstPtr::BOTTOM;\n-        } else if (field != NULL && field->is_static_constant()) {\n-          \/\/ This can happen if the constant oop is non-perm.\n-          ciObject* con = field->constant_value().as_object();\n-          \/\/ Do not \"join\" in the previous type; it doesn't add value,\n-          \/\/ and may yield a vacuous result if the field is of interface type.\n-          field_type = TypeOopPtr::make_from_constant(con)->isa_oopptr();\n-          assert(field_type != NULL, \"field singleton type must be consistent\");\n+        \/\/ The next code is taken from Parse::do_get_xxx().\n+        if (is_reference_type(basic_elem_type)) {\n+          if (!elem_type->is_loaded()) {\n+            field_type = TypeInstPtr::BOTTOM;\n+          } else if (field != NULL && field->is_static_constant()) {\n+            ciObject* con = field->constant_value().as_object();\n+            \/\/ Do not \"join\" in the previous type; it doesn't add value,\n+            \/\/ and may yield a vacuous result if the field is of interface type.\n+            field_type = TypeOopPtr::make_from_constant(con)->isa_oopptr();\n+            assert(field_type != NULL, \"field singleton type must be consistent\");\n+          } else {\n+            field_type = TypeOopPtr::make_from_klass(elem_type->as_klass());\n+          }\n+          if (UseCompressedOops) {\n+            field_type = field_type->make_narrowoop();\n+            basic_elem_type = T_NARROWOOP;\n+          }\n@@ -758,5 +763,1 @@\n-          field_type = TypeOopPtr::make_from_klass(elem_type->as_klass());\n-        }\n-        if (UseCompressedOops) {\n-          field_type = field_type->make_narrowoop();\n-          basic_elem_type = T_NARROWOOP;\n+          field_type = Type::get_const_basic_type(basic_elem_type);\n@@ -765,1 +766,1 @@\n-        field_type = Type::get_const_basic_type(basic_elem_type);\n+        offset = array_base + j * (intptr_t)element_size;\n@@ -1026,2 +1027,2 @@\n-                      tklass->klass()->is_instance_klass()  &&\n-                      tklass->klass()->as_instance_klass()->is_box_klass();\n+                      tklass->isa_instklassptr() &&\n+                      tklass->is_instklassptr()->instance_klass()->is_box_klass();\n@@ -1056,1 +1057,1 @@\n-              log->identify(tklass->klass()));\n+              log->identify(tklass->exact_klass()));\n@@ -1097,1 +1098,1 @@\n-              log->identify(t->klass()));\n+              log->identify(t->instance_klass()));\n@@ -1119,17 +1120,0 @@\n-\/\/---------------------------set_eden_pointers-------------------------\n-void PhaseMacroExpand::set_eden_pointers(Node* &eden_top_adr, Node* &eden_end_adr) {\n-  if (UseTLAB) {                \/\/ Private allocation: load from TLS\n-    Node* thread = transform_later(new ThreadLocalNode());\n-    int tlab_top_offset = in_bytes(JavaThread::tlab_top_offset());\n-    int tlab_end_offset = in_bytes(JavaThread::tlab_end_offset());\n-    eden_top_adr = basic_plus_adr(top()\/*not oop*\/, thread, tlab_top_offset);\n-    eden_end_adr = basic_plus_adr(top()\/*not oop*\/, thread, tlab_end_offset);\n-  } else {                      \/\/ Shared allocation: load from globals\n-    CollectedHeap* ch = Universe::heap();\n-    address top_adr = (address)ch->top_addr();\n-    address end_adr = (address)ch->end_addr();\n-    eden_top_adr = makecon(TypeRawPtr::make(top_adr));\n-    eden_end_adr = basic_plus_adr(eden_top_adr, end_adr - top_adr);\n-  }\n-}\n-\n@@ -1246,1 +1230,1 @@\n-  if (!UseTLAB && !Universe::heap()->supports_inline_contig_alloc()) {\n+  if (!UseTLAB) {\n@@ -1693,3 +1677,7 @@\n-    ciKlass* k = _igvn.type(klass_node)->is_klassptr()->klass();\n-    if (k->is_array_klass())    \/\/ we know the exact header size in most cases:\n-      header_size = Klass::layout_helper_header_size(k->layout_helper());\n+    if (_igvn.type(klass_node)->isa_aryklassptr()) {   \/\/ we know the exact header size in most cases:\n+      BasicType elem = _igvn.type(klass_node)->is_klassptr()->as_instance_type()->isa_aryptr()->elem()->array_element_basic_type();\n+      if (is_reference_type(elem, true)) {\n+        elem = T_OBJECT;\n+      }\n+      header_size = Klass::layout_helper_header_size(Klass::array_layout_helper(elem));\n+    }\n@@ -1895,1 +1883,1 @@\n-  ciKlass* k = _igvn.type(klass_node)->is_klassptr()->klass();\n+  const TypeAryKlassPtr* ary_klass_t = _igvn.type(klass_node)->isa_aryklassptr();\n@@ -1898,1 +1886,1 @@\n-      k->is_type_array_klass()) {\n+      ary_klass_t && ary_klass_t->elem()->isa_klassptr() == NULL) {\n@@ -2213,1 +2201,3 @@\n-  mem_phi->init_req(1, memproj );\n+\n+  mem_phi->init_req(1, memproj);\n+\n@@ -2215,0 +2205,1 @@\n+\n@@ -2269,0 +2260,1 @@\n+\n@@ -2314,0 +2306,1 @@\n+  NOT_PRODUCT(int membar_before = count_MemBar(C);)\n@@ -2339,0 +2332,5 @@\n+#ifndef PRODUCT\n+        if (success && PrintOptoStatistics) {\n+          Atomic::inc(&PhaseMacroExpand::_monitor_objects_removed_counter);\n+        }\n+#endif\n@@ -2357,0 +2355,5 @@\n+#ifndef PRODUCT\n+        if (success && PrintOptoStatistics) {\n+          Atomic::inc(&PhaseMacroExpand::_objs_scalar_replaced_counter);\n+        }\n+#endif\n@@ -2386,0 +2389,6 @@\n+#ifndef PRODUCT\n+  if (PrintOptoStatistics) {\n+    int membar_after = count_MemBar(C);\n+    Atomic::add(&PhaseMacroExpand::_memory_barriers_removed_counter, membar_before - membar_after);\n+  }\n+#endif\n@@ -2574,0 +2583,35 @@\n+\n+#ifndef PRODUCT\n+int PhaseMacroExpand::_objs_scalar_replaced_counter = 0;\n+int PhaseMacroExpand::_monitor_objects_removed_counter = 0;\n+int PhaseMacroExpand::_GC_barriers_removed_counter = 0;\n+int PhaseMacroExpand::_memory_barriers_removed_counter = 0;\n+\n+void PhaseMacroExpand::print_statistics() {\n+  tty->print(\"Objects scalar replaced = %d, \", Atomic::load(&_objs_scalar_replaced_counter));\n+  tty->print(\"Monitor objects removed = %d, \", Atomic::load(&_monitor_objects_removed_counter));\n+  tty->print(\"GC barriers removed = %d, \", Atomic::load(&_GC_barriers_removed_counter));\n+  tty->print_cr(\"Memory barriers removed = %d\", Atomic::load(&_memory_barriers_removed_counter));\n+}\n+\n+int PhaseMacroExpand::count_MemBar(Compile *C) {\n+  if (!PrintOptoStatistics) {\n+    return 0;\n+  }\n+  Unique_Node_List ideal_nodes;\n+  int total = 0;\n+  ideal_nodes.map(C->live_nodes(), NULL);\n+  ideal_nodes.push(C->root());\n+  for (uint next = 0; next < ideal_nodes.size(); ++next) {\n+    Node* n = ideal_nodes.at(next);\n+    if (n->is_MemBar()) {\n+      total++;\n+    }\n+    for (DUIterator_Fast imax, i = n->fast_outs(imax); i < imax; i++) {\n+      Node* m = n->fast_out(i);\n+      ideal_nodes.push(m);\n+    }\n+  }\n+  return total;\n+}\n+#endif\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":102,"deletions":58,"binary":false,"changes":160,"status":"modified"},{"patch":"@@ -119,1 +119,4 @@\n-  RegisterMap map(thread, false);\n+  RegisterMap map(thread,\n+                  RegisterMap::UpdateMap::skip,\n+                  RegisterMap::ProcessFrames::include,\n+                  RegisterMap::WalkContinuation::skip);\n@@ -719,0 +722,54 @@\n+const TypeFunc* OptoRuntime::void_void_Type() {\n+   \/\/ create input type (domain)\n+   const Type **fields = TypeTuple::fields(0);\n+   const TypeTuple *domain = TypeTuple::make(TypeFunc::Parms+0, fields);\n+\n+   \/\/ create result type (range)\n+   fields = TypeTuple::fields(0);\n+   const TypeTuple *range = TypeTuple::make(TypeFunc::Parms+0, fields);\n+   return TypeFunc::make(domain, range);\n+ }\n+\n+ const TypeFunc* OptoRuntime::continuation_doYield_Type() {\n+   \/\/ create input type (domain)\n+   const Type **fields = TypeTuple::fields(0);\n+   const TypeTuple *domain = TypeTuple::make(TypeFunc::Parms+0, fields);\n+\n+   \/\/ create result type (range)\n+   fields = TypeTuple::fields(1);\n+   fields[TypeFunc::Parms+0] = TypeInt::INT;\n+   const TypeTuple *range = TypeTuple::make(TypeFunc::Parms+1, fields);\n+\n+   return TypeFunc::make(domain, range);\n+ }\n+\n+ const TypeFunc* OptoRuntime::continuation_jump_Type() {\n+  \/\/ create input type (domain)\n+  const Type **fields = TypeTuple::fields(6);\n+  fields[TypeFunc::Parms+0] = TypeLong::LONG;\n+  fields[TypeFunc::Parms+1] = Type::HALF;\n+  fields[TypeFunc::Parms+2] = TypeLong::LONG;\n+  fields[TypeFunc::Parms+3] = Type::HALF;\n+  fields[TypeFunc::Parms+4] = TypeLong::LONG;\n+  fields[TypeFunc::Parms+5] = Type::HALF;\n+  const TypeTuple *domain = TypeTuple::make(TypeFunc::Parms+6, fields);\n+\n+  \/\/ create result type (range)\n+  fields = TypeTuple::fields(0);\n+  const TypeTuple *range = TypeTuple::make(TypeFunc::Parms+0, fields);\n+  return TypeFunc::make(domain, range);\n+ }\n+\n+\n+ const TypeFunc* OptoRuntime::jfr_write_checkpoint_Type() {\n+   \/\/ create input type (domain)\n+   const Type **fields = TypeTuple::fields(0);\n+   const TypeTuple *domain = TypeTuple::make(TypeFunc::Parms, fields);\n+\n+   \/\/ create result type (range)\n+   fields = TypeTuple::fields(0);\n+   const TypeTuple *range = TypeTuple::make(TypeFunc::Parms, fields);\n+   return TypeFunc::make(domain, range);\n+ }\n+\n+\n@@ -1349,1 +1406,4 @@\n-      RegisterMap map(current, false);\n+      RegisterMap map(current,\n+                      RegisterMap::UpdateMap::skip,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n@@ -1430,1 +1490,4 @@\n-    RegisterMap map(current, false \/* update_map *\/, false \/* process_frames *\/);\n+    RegisterMap map(current,\n+                    RegisterMap::UpdateMap::skip,\n+                    RegisterMap::ProcessFrames::skip,\n+                    RegisterMap::WalkContinuation::skip);\n@@ -1517,1 +1580,4 @@\n-  RegisterMap reg_map(thread);\n+  RegisterMap reg_map(thread,\n+                      RegisterMap::UpdateMap::include,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n@@ -1529,1 +1595,4 @@\n-  RegisterMap reg_map(thread);\n+  RegisterMap reg_map(thread,\n+                      RegisterMap::UpdateMap::include,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n@@ -1554,1 +1623,1 @@\n-const TypeFunc *OptoRuntime::get_class_id_intrinsic_Type() {\n+const TypeFunc *OptoRuntime::class_id_load_barrier_Type() {\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":75,"deletions":6,"binary":false,"changes":81,"status":"modified"},{"patch":"@@ -794,2 +794,2 @@\n-    bool this_interface = this_inst->klass()->is_interface();\n-    bool    t_interface =    t_inst->klass()->is_interface();\n+    bool this_interface = this_inst->is_interface();\n+    bool    t_interface =    t_inst->is_interface();\n@@ -1465,1 +1465,1 @@\n-TypeInt::TypeInt( jint lo, jint hi, int w ) : TypeInteger(Int), _lo(lo), _hi(hi), _widen(w) {\n+TypeInt::TypeInt( jint lo, jint hi, int w ) : TypeInteger(Int, w), _lo(lo), _hi(hi) {\n@@ -1727,1 +1727,1 @@\n-TypeLong::TypeLong(jlong lo, jlong hi, int w) : TypeInteger(Long), _lo(lo), _hi(hi), _widen(w) {\n+TypeLong::TypeLong(jlong lo, jlong hi, int w) : TypeInteger(Long, w), _lo(lo), _hi(hi) {\n@@ -2279,1 +2279,1 @@\n-const Type* TypeAry::remove_speculative() const {\n+const TypeAry* TypeAry::remove_speculative() const {\n@@ -2564,1 +2564,1 @@\n-const Type *TypePtr::cast_to_ptr_type(PTR ptr) const {\n+const TypePtr* TypePtr::cast_to_ptr_type(PTR ptr) const {\n@@ -2690,0 +2690,4 @@\n+const TypePtr *TypePtr::with_offset(intptr_t offset) const {\n+  return make(AnyPtr, _ptr, offset, _speculative, _inline_depth);\n+}\n+\n@@ -2707,1 +2711,1 @@\n-const Type* TypePtr::remove_speculative() const {\n+const TypePtr* TypePtr::remove_speculative() const {\n@@ -2842,0 +2846,7 @@\n+const TypePtr* TypePtr::with_offset_speculative(intptr_t offset) const {\n+  if (_speculative == NULL) {\n+    return NULL;\n+  }\n+  return _speculative->with_offset(offset)->is_ptr();\n+}\n+\n@@ -3097,1 +3108,1 @@\n-const TypePtr *TypeRawPtr::add_offset( intptr_t offset ) const {\n+const TypePtr* TypeRawPtr::add_offset(intptr_t offset) const {\n@@ -3248,1 +3259,1 @@\n-const Type *TypeOopPtr::cast_to_exactness(bool klass_is_exact) const {\n+const TypeOopPtr* TypeOopPtr::cast_to_exactness(bool klass_is_exact) const {\n@@ -3573,0 +3584,4 @@\n+const TypeOopPtr* TypeOopPtr::with_offset(intptr_t offset) const {\n+  return make(_ptr, offset, _instance_id, with_offset_speculative(offset), _inline_depth);\n+}\n+\n@@ -3576,1 +3591,1 @@\n-const Type* TypeOopPtr::remove_speculative() const {\n+const TypeOopPtr* TypeOopPtr::remove_speculative() const {\n@@ -3656,0 +3671,4 @@\n+ciKlass* TypeInstPtr::exact_klass_helper() const {\n+  return _klass;\n+}\n+\n@@ -3659,2 +3678,1 @@\n-  : TypeOopPtr(InstPtr, ptr, k, xk, o, off, instance_id, speculative, inline_depth),\n-    _name(k->name()) {\n+  : TypeOopPtr(InstPtr, ptr, k, xk, o, off, instance_id, speculative, inline_depth) {\n@@ -3723,1 +3741,1 @@\n-const TypeInstPtr *TypeInstPtr::cast_to_ptr_type(PTR ptr) const {\n+const TypeInstPtr* TypeInstPtr::cast_to_ptr_type(PTR ptr) const {\n@@ -3727,1 +3745,1 @@\n-  return make(ptr, klass(), klass_is_exact(), const_oop(), _offset, _instance_id, _speculative, _inline_depth);\n+  return make(ptr, klass(), klass_is_exact(), ptr == Constant ? const_oop() : NULL, _offset, _instance_id, _speculative, _inline_depth);\n@@ -3732,1 +3750,1 @@\n-const Type *TypeInstPtr::cast_to_exactness(bool klass_is_exact) const {\n+const TypeInstPtr* TypeInstPtr::cast_to_exactness(bool klass_is_exact) const {\n@@ -3742,1 +3760,1 @@\n-const TypeOopPtr *TypeInstPtr::cast_to_instance_id(int instance_id) const {\n+const TypeInstPtr* TypeInstPtr::cast_to_instance_id(int instance_id) const {\n@@ -4140,0 +4158,64 @@\n+bool TypeInstPtr::is_java_subtype_of_helper(const TypeOopPtr* other, bool this_exact, bool other_exact) const {\n+  if (!is_loaded() || !other->is_loaded()) {\n+    return false;\n+  }\n+  if (!other->isa_instptr()) {\n+    return false;\n+  }\n+\n+  if (!other_exact) {\n+    return false;\n+  }\n+\n+  if (other->klass()->equals(ciEnv::current()->Object_klass())) {\n+    return true;\n+  }\n+\n+  if (!this_exact && klass()->is_interface()) {\n+    return false;\n+  }\n+\n+  return _klass->is_subtype_of(other->klass());\n+}\n+\n+bool TypeInstPtr::is_same_java_type_as(const TypeOopPtr* other) const {\n+  if (!is_loaded() || !other->is_loaded()) {\n+    return false;\n+  }\n+  if (!other->isa_instptr()) {\n+    return false;\n+  }\n+  return _klass->equals(other->_klass);\n+}\n+\n+bool TypeInstPtr::maybe_java_subtype_of_helper(const TypeOopPtr* other, bool this_exact, bool other_exact) const {\n+  if (!is_loaded() || !other->is_loaded()) {\n+    return true;\n+  }\n+\n+  if (other->isa_aryptr()) {\n+    return !this_exact && (_klass->equals(ciEnv::current()->Object_klass()) || _klass->is_interface());\n+  }\n+\n+  if ((_klass->is_interface() && !this_exact) || (other->klass()->is_interface() \/*&& !other_exact*\/)) {\n+    return true;\n+  }\n+\n+  assert(other->isa_instptr(), \"unsupported\");\n+\n+  if (this_exact && other_exact) {\n+    return is_java_subtype_of(other);\n+  }\n+\n+  if (!_klass->is_subtype_of(other->_klass) && !other->_klass->is_subtype_of(_klass)) {\n+    return false;\n+  }\n+\n+  if (this_exact) {\n+    return _klass->is_subtype_of(other->_klass);\n+  }\n+\n+  return true;\n+}\n+\n+\n@@ -4194,1 +4276,1 @@\n-const TypePtr *TypeInstPtr::add_offset(intptr_t offset) const {\n+const TypePtr* TypeInstPtr::add_offset(intptr_t offset) const {\n@@ -4199,1 +4281,6 @@\n-const Type *TypeInstPtr::remove_speculative() const {\n+const TypeInstPtr* TypeInstPtr::with_offset(intptr_t offset) const {\n+  return make(_ptr, klass(), klass_is_exact(), const_oop(), offset,\n+              _instance_id, with_offset_speculative(offset), _inline_depth);\n+}\n+\n+const TypeInstPtr* TypeInstPtr::remove_speculative() const {\n@@ -4270,1 +4357,1 @@\n-  return make(ptr, const_oop(), _ary, klass(), klass_is_exact(), _offset, _instance_id, _speculative, _inline_depth);\n+  return make(ptr, ptr == Constant ? const_oop() : NULL, _ary, klass(), klass_is_exact(), _offset, _instance_id, _speculative, _inline_depth);\n@@ -4275,1 +4362,1 @@\n-const Type *TypeAryPtr::cast_to_exactness(bool klass_is_exact) const {\n+const TypeAryPtr* TypeAryPtr::cast_to_exactness(bool klass_is_exact) const {\n@@ -4282,1 +4369,1 @@\n-const TypeOopPtr *TypeAryPtr::cast_to_instance_id(int instance_id) const {\n+const TypeAryPtr* TypeAryPtr::cast_to_instance_id(int instance_id) const {\n@@ -4398,0 +4485,67 @@\n+bool TypeAryPtr::is_java_subtype_of_helper(const TypeOopPtr* other, bool this_exact, bool other_exact) const {\n+  if (other->klass() == ciEnv::current()->Object_klass() && other_exact) {\n+    return true;\n+  }\n+\n+  if (!is_loaded() || !other->is_loaded() || other->klass() == NULL || klass() == NULL) {\n+    return false;\n+  }\n+  if (other->isa_instptr()) {\n+    return _klass->is_subtype_of(other->_klass) && other_exact;\n+  }\n+  if (klass() == NULL) {\n+    return false;\n+  }\n+  assert(other->isa_aryptr(), \"\");\n+  const TypeAryPtr* other_ary = other->isa_aryptr();\n+  if (other_ary->elem()->make_oopptr() && elem()->make_oopptr()) {\n+    return elem()->make_oopptr()->is_java_subtype_of_helper(other_ary->elem()->make_oopptr(), this_exact, other_exact);\n+  }\n+  if (!other_ary->elem()->make_oopptr() && !elem()->make_oopptr()) {\n+    return _klass->is_subtype_of(other->_klass);\n+  }\n+  return false;\n+}\n+\n+bool TypeAryPtr::is_same_java_type_as(const TypeOopPtr* other) const {\n+  if (!other->isa_aryptr() ||\n+      !is_loaded() || !other->is_loaded() || klass() == NULL || other->klass() == NULL) {\n+    return false;\n+  }\n+  const TypeAryPtr* other_ary = other->isa_aryptr();\n+  if (other_ary->elem()->make_oopptr() && elem()->make_oopptr()) {\n+    return elem()->make_oopptr()->is_same_java_type_as(other_ary->elem()->make_oopptr());\n+  }\n+  if (!other_ary->elem()->make_oopptr() && !elem()->make_oopptr()) {\n+    return _klass->equals(other->_klass);\n+  }\n+  return false;\n+}\n+\n+bool TypeAryPtr::maybe_java_subtype_of_helper(const TypeOopPtr* other, bool this_exact, bool other_exact) const {\n+  if (other->klass() == ciEnv::current()->Object_klass()) {\n+    return true;\n+  }\n+\n+  if (!is_loaded() || !other->is_loaded() || klass() == NULL || other->klass() == NULL) {\n+    return true;\n+  }\n+  if (other->isa_instptr()) {\n+    return (!other_exact && other->_klass->is_interface()) || _klass->is_subtype_of(other->_klass);\n+  }\n+  assert(other->isa_aryptr(), \"\");\n+\n+  if (this_exact && other_exact) {\n+    return is_java_subtype_of(other);\n+  }\n+\n+  const TypeAryPtr* other_ary = other->isa_aryptr();\n+  if (other_ary->elem()->make_oopptr() && elem()->make_oopptr()) {\n+    return elem()->make_oopptr()->maybe_java_subtype_of_helper(other_ary->elem()->make_oopptr(), this_exact,\n+                                                               other_exact);\n+  }\n+  if (!other_ary->elem()->make_oopptr() && !elem()->make_oopptr()) {\n+    return _klass->is_subtype_of(other->_klass);\n+  }\n+  return false;\n+}\n@@ -4693,3 +4847,8 @@\n-      int array_base = arrayOopDesc::base_offset_in_bytes(basic_elem_type);\n-      int elem_size = type2aelembytes(basic_elem_type);\n-      st->print(\"[%d]\", (_offset - array_base)\/elem_size);\n+      BasicType basic_elem_type = elem()->basic_type();\n+      if (basic_elem_type == T_ILLEGAL) {\n+        st->print(\"+any\");\n+      } else {\n+        int array_base = arrayOopDesc::base_offset_in_bytes(basic_elem_type);\n+        int elem_size = type2aelembytes(basic_elem_type);\n+        st->print(\"[%d]\", (_offset - array_base)\/elem_size);\n+      }\n@@ -4719,1 +4878,9 @@\n-const Type *TypeAryPtr::remove_speculative() const {\n+const TypeAryPtr* TypeAryPtr::with_offset(intptr_t offset) const {\n+  return make(_ptr, _const_oop, _ary, _klass, _klass_is_exact, offset, _instance_id, with_offset_speculative(offset), _inline_depth);\n+}\n+\n+const TypeAryPtr* TypeAryPtr::with_ary(const TypeAry* ary) const {\n+  return make(_ptr, _const_oop, ary, _klass, _klass_is_exact, _offset, _instance_id, _speculative, _inline_depth);\n+}\n+\n+const TypeAryPtr* TypeAryPtr::remove_speculative() const {\n@@ -4859,1 +5026,1 @@\n-const Type* TypeNarrowOop::remove_speculative() const {\n+const TypeNarrowOop* TypeNarrowOop::remove_speculative() const {\n@@ -5114,0 +5281,4 @@\n+ciKlass* TypeKlassPtr::exact_klass_helper() const {\n+  return _klass;\n+}\n+\n@@ -5181,1 +5352,3 @@\n-  return (intptr_t)klass()->constant_encoding();\n+  ciKlass* k = exact_klass();\n+\n+  return (intptr_t)k->constant_encoding();\n@@ -5252,1 +5425,1 @@\n-const TypeKlassPtr *TypeInstKlassPtr::with_offset(intptr_t offset) const {\n+const TypeInstKlassPtr* TypeInstKlassPtr::with_offset(intptr_t offset) const {\n@@ -5257,1 +5430,1 @@\n-const TypePtr* TypeInstKlassPtr::cast_to_ptr_type(PTR ptr) const {\n+const TypeInstKlassPtr* TypeInstKlassPtr::cast_to_ptr_type(PTR ptr) const {\n@@ -5283,1 +5456,1 @@\n-const TypeOopPtr* TypeInstKlassPtr::as_instance_type() const {\n+const TypeOopPtr* TypeInstKlassPtr::as_instance_type(bool klass_change) const {\n@@ -5285,1 +5458,20 @@\n-  bool    xk = klass_is_exact();\n+  bool xk = klass_is_exact();\n+  Compile* C = Compile::current();\n+  Dependencies* deps = C->dependencies();\n+  assert((deps != NULL) == (C->method() != NULL && C->method()->code_size() > 0), \"sanity\");\n+  \/\/ Element is an instance\n+  bool klass_is_exact = false;\n+  if (k->is_loaded()) {\n+    \/\/ Try to set klass_is_exact.\n+    ciInstanceKlass* ik = k->as_instance_klass();\n+    klass_is_exact = ik->is_final();\n+    if (!klass_is_exact && klass_change\n+        && deps != NULL && UseUniqueSubclasses) {\n+      ciInstanceKlass* sub = ik->unique_concrete_subklass();\n+      if (sub != NULL) {\n+        deps->assert_abstract_with_unique_concrete_subtype(ik, sub);\n+        k = ik = sub;\n+        xk = sub->is_final();\n+      }\n+    }\n+  }\n@@ -5437,0 +5629,63 @@\n+bool TypeInstKlassPtr::is_java_subtype_of_helper(const TypeKlassPtr* other, bool this_exact, bool other_exact) const {\n+  if (!is_loaded() || !other->is_loaded()) {\n+    return false;\n+  }\n+  if (!other->isa_instklassptr()) {\n+    return false;\n+  }\n+\n+  if (!other_exact) {\n+    return false;\n+  }\n+\n+  if (other->_klass->equals(ciEnv::current()->Object_klass())) {\n+    return true;\n+  }\n+\n+  if (!this_exact && klass()->is_interface()) {\n+    return false;\n+  }\n+\n+  return _klass->is_subtype_of(other->_klass);\n+}\n+\n+bool TypeInstKlassPtr::is_same_java_type_as(const TypeKlassPtr* other) const {\n+  if (!is_loaded() || !other->is_loaded()) {\n+    return false;\n+  }\n+  if (!other->isa_instklassptr()) {\n+    return false;\n+  }\n+  return _klass->equals(other->_klass);\n+}\n+\n+bool TypeInstKlassPtr::maybe_java_subtype_of_helper(const TypeKlassPtr* other, bool this_exact, bool other_exact) const {\n+  if (!is_loaded() || !other->is_loaded()) {\n+    return true;\n+  }\n+\n+  if (other->isa_aryklassptr()) {\n+    return !this_exact && (_klass->equals(ciEnv::current()->Object_klass()) || _klass->is_interface());\n+  }\n+\n+  if ((_klass->is_interface() && !this_exact) || (other->klass()->is_interface() \/*&& !other_exact*\/)) {\n+    return true;\n+  }\n+\n+  assert(other->isa_instklassptr(), \"unsupported\");\n+\n+  if (this_exact && other_exact) {\n+    return is_java_subtype_of(other);\n+  }\n+\n+  if (!_klass->is_subtype_of(other->_klass) && !other->_klass->is_subtype_of(_klass)) {\n+    return false;\n+  }\n+\n+  if (this_exact) {\n+    return _klass->is_subtype_of(other->_klass);\n+  }\n+\n+  return true;\n+}\n+\n@@ -5556,1 +5811,2 @@\n-        _offset != 0 && _offset != arrayOopDesc::length_offset_in_bytes()) {\n+        _offset != 0 && _offset != arrayOopDesc::length_offset_in_bytes() &&\n+        _offset != arrayOopDesc::klass_offset_in_bytes()) {\n@@ -5563,0 +5819,22 @@\n+ciKlass* TypeAryPtr::exact_klass_helper() const {\n+  if (_ary->_elem->make_ptr() && _ary->_elem->make_ptr()->isa_oopptr()) {\n+    ciKlass* k = _ary->_elem->make_ptr()->is_oopptr()->exact_klass_helper();\n+    if (k == NULL) {\n+      return NULL;\n+    }\n+    k = ciObjArrayKlass::make(k);\n+    return k;\n+  }\n+\n+  return klass();\n+}\n+\n+const Type* TypeAryPtr::base_element_type(int& dims) const {\n+  const Type* elem = this->elem();\n+  dims = 1;\n+  while (elem->make_ptr() && elem->make_ptr()->isa_aryptr()) {\n+    elem = elem->make_ptr()->is_aryptr()->elem();\n+    dims++;\n+  }\n+  return elem;\n+}\n@@ -5570,1 +5848,1 @@\n-const TypeKlassPtr *TypeAryKlassPtr::with_offset(intptr_t offset) const {\n+const TypeAryKlassPtr* TypeAryKlassPtr::with_offset(intptr_t offset) const {\n@@ -5575,1 +5853,1 @@\n-const TypePtr* TypeAryKlassPtr::cast_to_ptr_type(PTR ptr) const {\n+const TypeAryKlassPtr* TypeAryKlassPtr::cast_to_ptr_type(PTR ptr) const {\n@@ -5604,2 +5882,2 @@\n-\/\/ It will be exact if and only if the klass type is exact.\n-const TypeOopPtr* TypeAryKlassPtr::as_instance_type() const {\n+\/\/ It will be NotNull, and exact if and only if the klass type is exact.\n+const TypeOopPtr* TypeAryKlassPtr::as_instance_type(bool klass_change) const {\n@@ -5608,1 +5886,7 @@\n-  const Type* el = elem()->isa_klassptr() ? elem()->is_klassptr()->as_instance_type()->is_oopptr()->cast_to_exactness(false) : elem();\n+  const Type* el = NULL;\n+  if (elem()->isa_klassptr()) {\n+    el = elem()->is_klassptr()->as_instance_type(false)->cast_to_exactness(false);\n+    k = NULL;\n+  } else {\n+    el = elem();\n+  }\n@@ -5740,0 +6024,66 @@\n+bool TypeAryKlassPtr::is_java_subtype_of_helper(const TypeKlassPtr* other, bool this_exact, bool other_exact) const {\n+  if (other->klass() == ciEnv::current()->Object_klass() && other_exact) {\n+    return true;\n+  }\n+\n+  if (!is_loaded() || !other->is_loaded() || other->klass() == NULL || klass() == NULL) {\n+    return false;\n+  }\n+  if (other->isa_instklassptr()) {\n+    return _klass->is_subtype_of(other->_klass) && other_exact;\n+  }\n+  if (klass() == NULL) {\n+    return false;\n+  }\n+  assert(other->isa_aryklassptr(), \"\");\n+  const TypeAryKlassPtr* other_ary = other->isa_aryklassptr();\n+  if (other_ary->_elem->isa_klassptr() && _elem->isa_klassptr()) {\n+    return _elem->is_klassptr()->is_java_subtype_of_helper(other_ary->_elem->is_klassptr(), this_exact, other_exact);\n+  }\n+  if (!other_ary->_elem->isa_klassptr() && !_elem->isa_klassptr()) {\n+    return _klass->is_subtype_of(other->_klass);\n+  }\n+  return false;\n+}\n+\n+bool TypeAryKlassPtr::is_same_java_type_as(const TypeKlassPtr* other) const {\n+  if (!other->isa_aryklassptr() ||\n+      !is_loaded() || !other->is_loaded() || klass() == NULL || other->klass() == NULL) {\n+    return false;\n+  }\n+  const TypeAryKlassPtr* other_ary = other->isa_aryklassptr();\n+  if (other_ary->_elem->isa_klassptr() && _elem->isa_klassptr()) {\n+    return _elem->is_klassptr()->is_same_java_type_as(other_ary->_elem->is_klassptr());\n+  }\n+  if (!other_ary->_elem->isa_klassptr() && !_elem->isa_klassptr()) {\n+    return _klass->equals(other->_klass);\n+  }\n+  return false;\n+}\n+\n+bool TypeAryKlassPtr::maybe_java_subtype_of_helper(const TypeKlassPtr* other, bool this_exact, bool other_exact) const {\n+  if (other->klass() == ciEnv::current()->Object_klass()) {\n+    return true;\n+  }\n+  if (!is_loaded() || !other->is_loaded() || klass() == NULL || other->klass() == NULL) {\n+    return true;\n+  }\n+  if (other->isa_instklassptr()) {\n+    return (!other_exact && other->_klass->is_interface()) || _klass->is_subtype_of(other->_klass);\n+  }\n+  assert(other->isa_aryklassptr(), \"\");\n+\n+  if (this_exact && other_exact) {\n+    return is_java_subtype_of(other);\n+  }\n+\n+  const TypeAryKlassPtr* other_ary = other->isa_aryklassptr();\n+  if (other_ary->_elem->isa_klassptr() && _elem->isa_klassptr()) {\n+    return _elem->is_klassptr()->maybe_java_subtype_of_helper(other_ary->_elem->is_klassptr(), this_exact, other_exact);\n+  }\n+  if (!other_ary->_elem->isa_klassptr() && !_elem->isa_klassptr()) {\n+    return _klass->is_subtype_of(other->_klass);\n+  }\n+  return false;\n+}\n+\n@@ -5747,0 +6097,13 @@\n+ciKlass* TypeAryKlassPtr::exact_klass_helper() const {\n+  if (elem()->isa_klassptr()) {\n+    ciKlass* k = elem()->is_klassptr()->exact_klass_helper();\n+    if (k == NULL) {\n+      return NULL;\n+    }\n+    k = ciObjArrayKlass::make(k);\n+    return k;\n+  }\n+\n+  return klass();\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":400,"deletions":37,"binary":false,"changes":437,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -49,0 +49,1 @@\n+#include \"runtime\/javaThread.inline.hpp\"\n@@ -54,1 +55,0 @@\n-#include \"runtime\/thread.hpp\"\n@@ -532,1 +532,1 @@\n-  \/\/ offset from the base of a a klass metaobject.  Thus, the full dynamic\n+  \/\/ offset from the base of a klass metaobject.  Thus, the full dynamic\n","filename":"src\/hotspot\/share\/prims\/unsafe.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -120,1 +120,1 @@\n-PathString *Arguments::_system_boot_class_path = NULL;\n+PathString *Arguments::_boot_class_path = NULL;\n@@ -392,1 +392,1 @@\n-  \/\/ Set up _system_boot_class_path which is not a property but\n+  \/\/ Set up _boot_class_path which is not a property but\n@@ -394,2 +394,2 @@\n-  \/\/ property. It is used to store the underlying system boot class path.\n-  _system_boot_class_path = new PathString(NULL);\n+  \/\/ property. It is used to store the underlying boot class path.\n+  _boot_class_path = new PathString(NULL);\n@@ -539,7 +539,0 @@\n-#ifdef PRODUCT\n-  { \"UseHeavyMonitors\",             JDK_Version::jdk(18), JDK_Version::jdk(19), JDK_Version::jdk(20) },\n-#endif\n-  { \"ExtendedDTraceProbes\",         JDK_Version::jdk(19), JDK_Version::jdk(20), JDK_Version::jdk(21) },\n-  { \"UseContainerCpuShares\",        JDK_Version::jdk(19), JDK_Version::jdk(20), JDK_Version::jdk(21) },\n-  { \"PreferContainerQuotaForCPUCount\", JDK_Version::jdk(19), JDK_Version::jdk(20), JDK_Version::jdk(21) },\n-  { \"AliasLevel\",                   JDK_Version::jdk(19), JDK_Version::jdk(20), JDK_Version::jdk(21) },\n@@ -554,3 +547,5 @@\n-  { \"FilterSpuriousWakeups\",        JDK_Version::jdk(18), JDK_Version::jdk(19), JDK_Version::jdk(20) },\n-  { \"MinInliningThreshold\",         JDK_Version::jdk(18), JDK_Version::jdk(19), JDK_Version::jdk(20) },\n-  { \"PrefetchFieldsAhead\",          JDK_Version::undefined(), JDK_Version::jdk(19), JDK_Version::jdk(20) },\n+  { \"ExtendedDTraceProbes\",         JDK_Version::jdk(19), JDK_Version::jdk(20), JDK_Version::jdk(21) },\n+  { \"UseContainerCpuShares\",        JDK_Version::jdk(19), JDK_Version::jdk(20), JDK_Version::jdk(21) },\n+  { \"PreferContainerQuotaForCPUCount\", JDK_Version::jdk(19), JDK_Version::jdk(20), JDK_Version::jdk(21) },\n+  { \"AliasLevel\",                   JDK_Version::jdk(19), JDK_Version::jdk(20), JDK_Version::jdk(21) },\n+\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":9,"deletions":14,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -132,1 +132,1 @@\n-  product(intx, ObjectAlignmentInBytes, 8,                                  \\\n+  product(int, ObjectAlignmentInBytes, 8,                                   \\\n@@ -149,1 +149,1 @@\n-const intx ObjectAlignmentInBytes = 8;\n+const int ObjectAlignmentInBytes = 8;\n@@ -477,1 +477,1 @@\n-  develop(uintx, ErrorHandlerTest, 0,                                       \\\n+  develop(uint, ErrorHandlerTest, 0,                                        \\\n@@ -483,1 +483,1 @@\n-  develop(uintx, TestCrashInErrorHandler, 0,                                \\\n+  develop(uint, TestCrashInErrorHandler, 0,                                 \\\n@@ -689,0 +689,8 @@\n+  product(bool, PostVirtualThreadCompatibleLifecycleEvents, true, EXPERIMENTAL, \\\n+               \"Post virtual thread ThreadStart and ThreadEnd events for \"  \\\n+               \"virtual thread unaware agents\")                             \\\n+                                                                            \\\n+  product(bool, DoJVMTIVirtualThreadTransitions, true, EXPERIMENTAL,        \\\n+               \"Do JVMTI virtual thread mount\/unmount transitions \"         \\\n+               \"(disabling this flag implies no JVMTI events are posted)\")  \\\n+                                                                            \\\n@@ -791,1 +799,1 @@\n-  product(intx, DiagnoseSyncOnValueBasedClasses, 0, DIAGNOSTIC,             \\\n+  product(int, DiagnoseSyncOnValueBasedClasses, 0, DIAGNOSTIC,              \\\n@@ -968,3 +976,0 @@\n-  develop(bool, EagerInitialization, false,                                 \\\n-          \"Eagerly initialize classes if possible\")                         \\\n-                                                                            \\\n@@ -1064,1 +1069,1 @@\n-  product(bool, UseHeavyMonitors, false,                                    \\\n+  product(bool, UseHeavyMonitors, false, DIAGNOSTIC,                        \\\n@@ -1289,4 +1294,5 @@\n-  product(intx, SelfDestructTimer, 0,                                       \\\n-          \"Will cause VM to terminate after a given time (in minutes) \"     \\\n-          \"(0 means off)\")                                                  \\\n-          range(0, max_intx)                                                \\\n+  product(double, SelfDestructTimer, 0.0,                                   \\\n+          \"Will cause VM to terminate after a given time \"                  \\\n+          \"(in fractional minutes) \"                                        \\\n+          \"(0.0 means off)\")                                                \\\n+          range(0.0, (double)max_intx)                                      \\\n@@ -1606,1 +1612,1 @@\n-  product(intx, ThreadPriorityPolicy, 0,                                    \\\n+  product(int, ThreadPriorityPolicy, 0,                                     \\\n@@ -1631,1 +1637,1 @@\n-  product(intx, CompilerThreadPriority, -1,                                 \\\n+  product(int, CompilerThreadPriority, -1,                                  \\\n@@ -1636,1 +1642,1 @@\n-  product(intx, VMThreadPriority, -1,                                       \\\n+  product(int, VMThreadPriority, -1,                                        \\\n@@ -1641,1 +1647,1 @@\n-  product(intx, JavaPriority1_To_OSPriority, -1,                            \\\n+  product(int, JavaPriority1_To_OSPriority, -1,                             \\\n@@ -1645,1 +1651,1 @@\n-  product(intx, JavaPriority2_To_OSPriority, -1,                            \\\n+  product(int, JavaPriority2_To_OSPriority, -1,                             \\\n@@ -1649,1 +1655,1 @@\n-  product(intx, JavaPriority3_To_OSPriority, -1,                            \\\n+  product(int, JavaPriority3_To_OSPriority, -1,                             \\\n@@ -1653,1 +1659,1 @@\n-  product(intx, JavaPriority4_To_OSPriority, -1,                            \\\n+  product(int, JavaPriority4_To_OSPriority, -1,                             \\\n@@ -1657,1 +1663,1 @@\n-  product(intx, JavaPriority5_To_OSPriority, -1,                            \\\n+  product(int, JavaPriority5_To_OSPriority, -1,                             \\\n@@ -1661,1 +1667,1 @@\n-  product(intx, JavaPriority6_To_OSPriority, -1,                            \\\n+  product(int, JavaPriority6_To_OSPriority, -1,                             \\\n@@ -1665,1 +1671,1 @@\n-  product(intx, JavaPriority7_To_OSPriority, -1,                            \\\n+  product(int, JavaPriority7_To_OSPriority, -1,                             \\\n@@ -1669,1 +1675,1 @@\n-  product(intx, JavaPriority8_To_OSPriority, -1,                            \\\n+  product(int, JavaPriority8_To_OSPriority, -1,                             \\\n@@ -1673,1 +1679,1 @@\n-  product(intx, JavaPriority9_To_OSPriority, -1,                            \\\n+  product(int, JavaPriority9_To_OSPriority, -1,                             \\\n@@ -1677,1 +1683,1 @@\n-  product(intx, JavaPriority10_To_OSPriority,-1,                            \\\n+  product(int, JavaPriority10_To_OSPriority,-1,                             \\\n@@ -1757,1 +1763,1 @@\n-  product(intx, PerfDataMemorySize, 32*K,                                   \\\n+  product(int, PerfDataMemorySize, 32*K,                                    \\\n@@ -1762,1 +1768,1 @@\n-  product(intx, PerfMaxStringConstLength, 1024,                             \\\n+  product(int, PerfMaxStringConstLength, 1024,                              \\\n@@ -1772,1 +1778,1 @@\n-  product(intx, UnguardOnExecutionViolation, 0,                             \\\n+  product(int, UnguardOnExecutionViolation, 0,                              \\\n@@ -1820,1 +1826,1 @@\n-  product(uintx, SharedSymbolTableBucketSize, 4,                            \\\n+  product(uint, SharedSymbolTableBucketSize, 4,                             \\\n@@ -1836,0 +1842,3 @@\n+  product(bool, ShowCarrierFrames, false, DIAGNOSTIC,                       \\\n+          \"show virtual threads' carrier frames in exceptions\")             \\\n+                                                                            \\\n@@ -1943,1 +1952,1 @@\n-  product(intx, ArchiveRelocationMode, 0, DIAGNOSTIC,                       \\\n+  product(int, ArchiveRelocationMode, 0, DIAGNOSTIC,                        \\\n@@ -1994,0 +2003,20 @@\n+  product_pd(bool, VMContinuations, EXPERIMENTAL,                           \\\n+          \"Enable VM continuations support\")                                \\\n+                                                                            \\\n+  develop(bool, LoomDeoptAfterThaw, false,                                  \\\n+          \"Deopt stack after thaw\")                                         \\\n+                                                                            \\\n+  develop(bool, LoomVerifyAfterThaw, false,                                 \\\n+          \"Verify stack after thaw\")                                        \\\n+                                                                            \\\n+  develop(bool, VerifyContinuations, false,                                 \\\n+          \"Verify continuation consistency\")                                \\\n+                                                                            \\\n+  develop(bool, UseContinuationFastPath, true,                              \\\n+          \"Use fast-path frame walking in continuations\")                   \\\n+                                                                            \\\n+  product(intx, ExtentLocalCacheSize, 16,                                   \\\n+          \"Size of the cache for scoped values\")                            \\\n+           range(0, max_intx)                                               \\\n+           constraint(ExtentLocalCacheSizeConstraintFunc, AtParse)          \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":60,"deletions":31,"binary":false,"changes":91,"status":"modified"},{"patch":"@@ -44,0 +44,1 @@\n+#include \"runtime\/javaThread.inline.hpp\"\n@@ -53,1 +54,0 @@\n-#include \"runtime\/thread.inline.hpp\"\n@@ -1505,1 +1505,1 @@\n-  \/\/ by the the owner of the monitor *except* in the case where park()\n+  \/\/ by the owner of the monitor *except* in the case where park()\n@@ -1650,2 +1650,4 @@\n-  _recursions = save      \/\/ restore the old recursion count\n-                + JvmtiDeferredUpdates::get_and_reset_relock_count_after_wait(current); \/\/  increased by the deferred relock count\n+  int relock_count = JvmtiDeferredUpdates::get_and_reset_relock_count_after_wait(current);\n+  _recursions =   save          \/\/ restore the old recursion count\n+                + relock_count; \/\/  increased by the deferred relock count\n+  current->inc_held_monitor_count(relock_count); \/\/ Deopt never entered these counts.\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.cpp","additions":6,"deletions":4,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -32,2 +32,0 @@\n-#include \"runtime\/os.hpp\"\n-#include \"runtime\/park.hpp\"\n@@ -37,0 +35,1 @@\n+class ParkEvent;\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -54,0 +54,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -60,1 +61,1 @@\n-#include \"runtime\/thread.inline.hpp\"\n+#include \"runtime\/threadCrashProtection.hpp\"\n@@ -622,1 +623,1 @@\n-  assert(!os::ThreadCrashProtection::is_crash_protected(Thread::current_or_null()),\n+  assert(!ThreadCrashProtection::is_crash_protected(Thread::current_or_null()),\n@@ -661,1 +662,1 @@\n-  void* const outer_ptr = ::malloc(outer_size);\n+  ALLOW_C_FUNCTION(::malloc, void* const outer_ptr = ::malloc(outer_size);)\n@@ -711,1 +712,1 @@\n-  void* const new_outer_ptr = ::realloc(old_outer_ptr, new_outer_size);\n+  ALLOW_C_FUNCTION(::realloc, void* const new_outer_ptr = ::realloc(old_outer_ptr, new_outer_size);)\n@@ -739,1 +740,1 @@\n-  ::free(old_outer_ptr);\n+  ALLOW_C_FUNCTION(::free, ::free(old_outer_ptr);)\n@@ -1300,1 +1301,1 @@\n-    Arguments::set_sysclasspath(jimage, true);\n+    Arguments::set_boot_class_path(jimage, true);\n@@ -1310,1 +1311,1 @@\n-    Arguments::set_sysclasspath(base_classes, false);\n+    Arguments::set_boot_class_path(base_classes, false);\n@@ -1676,5 +1677,0 @@\n-void os::SuspendedThreadTask::run() {\n-  internal_do_task();\n-  _done = true;\n-}\n-\n@@ -1688,4 +1684,1 @@\n-    MemTracker::record_virtual_memory_reserve(result, bytes, CALLER_PC);\n-    if (flags != mtOther) {\n-      MemTracker::record_virtual_memory_type(result, flags);\n-    }\n+    MemTracker::record_virtual_memory_reserve(result, bytes, CALLER_PC, flags);\n@@ -1693,1 +1686,0 @@\n-\n@@ -1900,16 +1892,0 @@\n-#ifndef _WINDOWS\n-\/* try to switch state from state \"from\" to state \"to\"\n- * returns the state set after the method is complete\n- *\/\n-os::SuspendResume::State os::SuspendResume::switch_state(os::SuspendResume::State from,\n-                                                         os::SuspendResume::State to)\n-{\n-  os::SuspendResume::State result = Atomic::cmpxchg(&_state, from, to);\n-  if (result == from) {\n-    \/\/ success\n-    return to;\n-  }\n-  return result;\n-}\n-#endif\n-\n@@ -1997,0 +1973,66 @@\n+\n+\/\/ Check minimum allowable stack sizes for thread creation and to initialize\n+\/\/ the java system classes, including StackOverflowError - depends on page\n+\/\/ size.\n+\/\/ The space needed for frames during startup is platform dependent. It\n+\/\/ depends on word size, platform calling conventions, C frame layout and\n+\/\/ interpreter\/C1\/C2 design decisions. Therefore this is given in a\n+\/\/ platform (os\/cpu) dependent constant.\n+\/\/ To this, space for guard mechanisms is added, which depends on the\n+\/\/ page size which again depends on the concrete system the VM is running\n+\/\/ on. Space for libc guard pages is not included in this size.\n+jint os::set_minimum_stack_sizes() {\n+\n+  _java_thread_min_stack_allowed = _java_thread_min_stack_allowed +\n+                                   StackOverflow::stack_guard_zone_size() +\n+                                   StackOverflow::stack_shadow_zone_size();\n+\n+  _java_thread_min_stack_allowed = align_up(_java_thread_min_stack_allowed, vm_page_size());\n+  _java_thread_min_stack_allowed = MAX2(_java_thread_min_stack_allowed, _os_min_stack_allowed);\n+\n+  size_t stack_size_in_bytes = ThreadStackSize * K;\n+  if (stack_size_in_bytes != 0 &&\n+      stack_size_in_bytes < _java_thread_min_stack_allowed) {\n+    \/\/ The '-Xss' and '-XX:ThreadStackSize=N' options both set\n+    \/\/ ThreadStackSize so we go with \"Java thread stack size\" instead\n+    \/\/ of \"ThreadStackSize\" to be more friendly.\n+    tty->print_cr(\"\\nThe Java thread stack size specified is too small. \"\n+                  \"Specify at least \" SIZE_FORMAT \"k\",\n+                  _java_thread_min_stack_allowed \/ K);\n+    return JNI_ERR;\n+  }\n+\n+  \/\/ Make the stack size a multiple of the page size so that\n+  \/\/ the yellow\/red zones can be guarded.\n+  JavaThread::set_stack_size_at_create(align_up(stack_size_in_bytes, vm_page_size()));\n+\n+  \/\/ Reminder: a compiler thread is a Java thread.\n+  _compiler_thread_min_stack_allowed = _compiler_thread_min_stack_allowed +\n+                                       StackOverflow::stack_guard_zone_size() +\n+                                       StackOverflow::stack_shadow_zone_size();\n+\n+  _compiler_thread_min_stack_allowed = align_up(_compiler_thread_min_stack_allowed, vm_page_size());\n+  _compiler_thread_min_stack_allowed = MAX2(_compiler_thread_min_stack_allowed, _os_min_stack_allowed);\n+\n+  stack_size_in_bytes = CompilerThreadStackSize * K;\n+  if (stack_size_in_bytes != 0 &&\n+      stack_size_in_bytes < _compiler_thread_min_stack_allowed) {\n+    tty->print_cr(\"\\nThe CompilerThreadStackSize specified is too small. \"\n+                  \"Specify at least \" SIZE_FORMAT \"k\",\n+                  _compiler_thread_min_stack_allowed \/ K);\n+    return JNI_ERR;\n+  }\n+\n+  _vm_internal_thread_min_stack_allowed = align_up(_vm_internal_thread_min_stack_allowed, vm_page_size());\n+  _vm_internal_thread_min_stack_allowed = MAX2(_vm_internal_thread_min_stack_allowed, _os_min_stack_allowed);\n+\n+  stack_size_in_bytes = VMThreadStackSize * K;\n+  if (stack_size_in_bytes != 0 &&\n+      stack_size_in_bytes < _vm_internal_thread_min_stack_allowed) {\n+    tty->print_cr(\"\\nThe VMThreadStackSize specified is too small. \"\n+                  \"Specify at least \" SIZE_FORMAT \"k\",\n+                  _vm_internal_thread_min_stack_allowed \/ K);\n+    return JNI_ERR;\n+  }\n+  return JNI_OK;\n+}\n","filename":"src\/hotspot\/share\/runtime\/os.cpp","additions":75,"deletions":33,"binary":false,"changes":108,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"runtime\/continuation.hpp\"\n@@ -54,0 +55,1 @@\n+BufferBlob* StubRoutines::_code3                                = NULL;\n@@ -175,0 +177,8 @@\n+RuntimeStub* StubRoutines::_cont_doYield_stub = NULL;\n+address StubRoutines::_cont_doYield       = NULL;\n+address StubRoutines::_cont_thaw          = NULL;\n+address StubRoutines::_cont_returnBarrier = NULL;\n+address StubRoutines::_cont_returnBarrierExc = NULL;\n+\n+JFR_ONLY(RuntimeStub* StubRoutines::_jfr_write_checkpoint_stub = NULL;)\n+JFR_ONLY(address StubRoutines::_jfr_write_checkpoint = NULL;)\n@@ -182,1 +192,1 @@\n-extern void StubGenerator_generate(CodeBuffer* code, bool all); \/\/ only interface to generators\n+extern void StubGenerator_generate(CodeBuffer* code, int phase); \/\/ only interface to generators\n@@ -221,1 +231,1 @@\n-    StubGenerator_generate(&buffer, false);\n+    StubGenerator_generate(&buffer, 0);\n@@ -228,1 +238,0 @@\n-\n@@ -266,0 +275,16 @@\n+void StubRoutines::initializeContinuationStubs() {\n+  if (_code3 == NULL) {\n+    ResourceMark rm;\n+    TraceTime timer(\"StubRoutines generation 3\", TRACETIME_LOG(Info, startuptime));\n+    _code3 = BufferBlob::create(\"StubRoutines (3)\", code_size2);\n+    if (_code3 == NULL) {\n+      vm_exit_out_of_memory(code_size2, OOM_MALLOC_ERROR, \"CodeCache: no room for StubRoutines (3)\");\n+    }\n+    CodeBuffer buffer(_code3);\n+    StubGenerator_generate(&buffer, 1);\n+    \/\/ When new stubs added we need to make sure there is some space left\n+    \/\/ to catch situation when we should increase size again.\n+    assert(code_size2 == 0 || buffer.insts_remaining() > 200, \"increase code_size3\");\n+  }\n+}\n+\n@@ -278,1 +303,1 @@\n-    StubGenerator_generate(&buffer, true);\n+    StubGenerator_generate(&buffer, 2);\n@@ -369,0 +394,1 @@\n+void stubRoutines_initContinuationStubs() { StubRoutines::initializeContinuationStubs(); }\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.cpp","additions":30,"deletions":4,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -156,1 +156,2 @@\n-  static BufferBlob* _code2;                               \/\/ code buffer for all other routines\n+  static BufferBlob* _code2;\n+  static BufferBlob* _code3;                               \/\/ code buffer for all other routines\n@@ -253,0 +254,9 @@\n+  static RuntimeStub* _cont_doYield_stub;\n+  static address _cont_doYield;\n+  static address _cont_thaw;\n+  static address _cont_returnBarrier;\n+  static address _cont_returnBarrierExc;\n+\n+  JFR_ONLY(static RuntimeStub* _jfr_write_checkpoint_stub;)\n+  JFR_ONLY(static address _jfr_write_checkpoint;)\n+\n@@ -263,0 +273,1 @@\n+  static void    initializeContinuationStubs();            \/\/ must happen after  universe::genesis\n@@ -274,0 +285,1 @@\n+  static RuntimeBlob* code3() { return _code3; }\n@@ -424,0 +436,7 @@\n+  static RuntimeStub* cont_doYield_stub() { return _cont_doYield_stub; }\n+  static address cont_doYield()        { return _cont_doYield; }\n+  static address cont_thaw()           { return _cont_thaw; }\n+  static address cont_returnBarrier()  { return _cont_returnBarrier; }\n+  static address cont_returnBarrierExc(){return _cont_returnBarrierExc; }\n+\n+  JFR_ONLY(static address jfr_write_checkpoint() { return _jfr_write_checkpoint; })\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.hpp","additions":21,"deletions":2,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+#include \"runtime\/frame.inline.hpp\"\n@@ -41,0 +42,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -52,1 +54,1 @@\n-#include \"runtime\/thread.inline.hpp\"\n+#include \"runtime\/threads.hpp\"\n@@ -242,1 +244,1 @@\n-static os::PlatformMutex* gInflationLocks[NINFLATIONLOCKS];\n+static PlatformMutex* gInflationLocks[NINFLATIONLOCKS];\n@@ -246,1 +248,1 @@\n-    gInflationLocks[i] = new os::PlatformMutex();\n+    gInflationLocks[i] = new PlatformMutex();\n@@ -377,0 +379,1 @@\n+      current->inc_held_monitor_count();\n@@ -393,0 +396,1 @@\n+      current->inc_held_monitor_count();\n@@ -474,0 +478,2 @@\n+  current->inc_held_monitor_count();\n+\n@@ -513,0 +519,2 @@\n+  current->dec_held_monitor_count();\n+\n@@ -581,2 +589,3 @@\n-  intptr_t ret_code = monitor->complete_exit(current);\n-  return ret_code;\n+  intx recur_count = monitor->complete_exit(current);\n+  current->dec_held_monitor_count(recur_count + 1);\n+  return recur_count;\n@@ -594,0 +603,1 @@\n+      current->inc_held_monitor_count(recursions + 1);\n@@ -615,0 +625,1 @@\n+      current->inc_held_monitor_count(1, true);\n@@ -633,0 +644,1 @@\n+    current->dec_held_monitor_count(1, true);\n@@ -1661,1 +1673,2 @@\n-    (void)mid->complete_exit(_thread);\n+    intx rec = mid->complete_exit(_thread);\n+    _thread->dec_held_monitor_count(rec + 1);\n@@ -1687,0 +1700,3 @@\n+  assert(current->held_monitor_count() == 0, \"Should not be possible\");\n+  \/\/ All monitors (including entered via JNI) have been unlocked above, so we need to clear jni count.\n+  current->clear_jni_monitor_count();\n@@ -1741,1 +1757,1 @@\n-    \/\/ level at a safepoint in ObjectSynchronizer::do_safepoint_work().\n+    \/\/ level at a safepoint in SafepointSynchronize::do_cleanup_tasks.\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":23,"deletions":7,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -52,0 +52,1 @@\n+  template(CollectForCodeCacheAllocation)         \\\n@@ -73,0 +74,2 @@\n+  template(VirtualThreadGetStackTrace)            \\\n+  template(VirtualThreadGetFrameCount)            \\\n@@ -75,0 +78,2 @@\n+  template(VirtualThreadGetOrSetLocal)            \\\n+  template(VirtualThreadGetCurrentLocation)       \\\n","filename":"src\/hotspot\/share\/runtime\/vmOperation.hpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -73,0 +73,1 @@\n+#include \"oops\/instanceStackChunkKlass.hpp\"\n@@ -93,0 +94,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -103,1 +105,0 @@\n-#include \"runtime\/thread.inline.hpp\"\n@@ -239,1 +240,1 @@\n-  nonstatic_field(InstanceKlass,               _init_state,                                   u1)                                    \\\n+  nonstatic_field(InstanceKlass,               _init_state,                                   InstanceKlass::ClassState)             \\\n@@ -319,0 +320,1 @@\n+  nonstatic_field(ConstMethod,                 _num_stack_arg_slots,                          u2)                                    \\\n@@ -457,0 +459,1 @@\n+     static_field(vmClasses,                   VM_CLASS_AT(Thread_FieldHolder_klass),            InstanceKlass*)                     \\\n@@ -711,0 +714,3 @@\n+  nonstatic_field(JavaThread,                  _vthread,                                      OopHandle)                             \\\n+  nonstatic_field(JavaThread,                  _jvmti_vthread,                                OopHandle)                             \\\n+  nonstatic_field(JavaThread,                  _extentLocalCache,                              OopHandle)                             \\\n@@ -1231,0 +1237,1 @@\n+        declare_type(InstanceStackChunkKlass, InstanceKlass)              \\\n@@ -1495,1 +1502,0 @@\n-  declare_c2_type(CallNativeNode, CallNode)                               \\\n@@ -1583,2 +1589,0 @@\n-  declare_c2_type(NoOvfDivINode, DivINode)                                \\\n-  declare_c2_type(NoOvfDivLNode, DivLNode)                                \\\n@@ -1591,2 +1595,0 @@\n-  declare_c2_type(NoOvfModINode, ModINode)                                \\\n-  declare_c2_type(NoOvfModLNode, ModLNode)                                \\\n@@ -1598,2 +1600,0 @@\n-  declare_c2_type(NoOvfDivModINode, DivModINode)                          \\\n-  declare_c2_type(NoOvfDivModLNode, DivModLNode)                          \\\n@@ -1625,1 +1625,0 @@\n-  declare_c2_type(MachCallNativeNode, MachCallNode)                       \\\n@@ -1711,0 +1710,1 @@\n+  declare_c2_type(CmpU3Node, CmpUNode)                                    \\\n@@ -1716,0 +1716,1 @@\n+  declare_c2_type(CmpUL3Node, CmpULNode)                                  \\\n@@ -1778,0 +1779,3 @@\n+  declare_c2_type(CompressVNode, VectorNode)                              \\\n+  declare_c2_type(CompressMNode, VectorNode)                              \\\n+  declare_c2_type(ExpandVNode, VectorNode)                                \\\n@@ -1813,0 +1817,1 @@\n+  declare_c2_type(PopulateIndexNode, VectorNode)                          \\\n@@ -1846,0 +1851,2 @@\n+  declare_c2_type(IsInfiniteFNode, Node)                                  \\\n+  declare_c2_type(IsInfiniteDNode, Node)                                  \\\n@@ -1868,0 +1875,4 @@\n+  declare_c2_type(CountLeadingZerosVNode, VectorNode)                     \\\n+  declare_c2_type(CountTrailingZerosVNode, VectorNode)                    \\\n+  declare_c2_type(ReverseBytesVNode, VectorNode)                          \\\n+  declare_c2_type(ReverseVNode, VectorNode)                               \\\n@@ -2007,1 +2018,1 @@\n-  declare_type(OopMapValue, StackObj)                                     \\\n+  declare_toplevel_type(OopMapValue)                                      \\\n@@ -2190,0 +2201,1 @@\n+  declare_constant(Method::_changes_current_thread)                       \\\n@@ -2272,0 +2284,1 @@\n+  declare_constant(InstanceKlass::being_linked)                           \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":24,"deletions":11,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"runtime\/mutexLocker.hpp\"\n","filename":"src\/hotspot\/share\/services\/heapObjectStatistics.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -159,0 +159,29 @@\n+\n+\/\/----------------------------------------------------------------------------------------------------\n+\/\/ Forbid the use of various C library functions.\n+\/\/ Some of these have os:: replacements that should normally be used instead.\n+\/\/ Others are considered security concerns, with preferred alternatives.\n+\n+FORBID_C_FUNCTION(void exit(int), \"use os::exit\");\n+FORBID_C_FUNCTION(void _exit(int), \"use os::exit\");\n+FORBID_C_FUNCTION(char* strerror(int), \"use os::strerror\");\n+FORBID_C_FUNCTION(char* strtok(char*, const char*), \"use strtok_r\");\n+FORBID_C_FUNCTION(int vsprintf(char*, const char*, va_list), \"use os::vsnprintf\");\n+FORBID_C_FUNCTION(int vsnprintf(char*, size_t, const char*, va_list), \"use os::vsnprintf\");\n+\n+\/\/ All of the following functions return raw C-heap pointers (sometimes as an option, e.g. realpath or getwd)\n+\/\/ or, in case of free(), take raw C-heap pointers. Don't use them unless you are really sure you must.\n+FORBID_C_FUNCTION(void* malloc(size_t size), \"use os::malloc\");\n+FORBID_C_FUNCTION(void* calloc(size_t nmemb, size_t size), \"use os::malloc and zero out manually\");\n+FORBID_C_FUNCTION(void free(void *ptr), \"use os::free\");\n+FORBID_C_FUNCTION(void* realloc(void *ptr, size_t size), \"use os::realloc\");\n+FORBID_C_FUNCTION(char* strdup(const char *s), \"use os::strdup\");\n+FORBID_C_FUNCTION(char* strndup(const char *s, size_t n), \"don't use\");\n+FORBID_C_FUNCTION(int posix_memalign(void **memptr, size_t alignment, size_t size), \"don't use\");\n+FORBID_C_FUNCTION(void* aligned_alloc(size_t alignment, size_t size), \"don't use\");\n+FORBID_C_FUNCTION(char* realpath(const char* path, char* resolved_path), \"use os::Posix::realpath\");\n+FORBID_C_FUNCTION(char* get_current_dir_name(void), \"use os::get_current_directory()\");\n+FORBID_C_FUNCTION(char* getwd(char *buf), \"use os::get_current_directory()\");\n+FORBID_C_FUNCTION(wchar_t* wcsdup(const wchar_t *s), \"don't use\");\n+FORBID_C_FUNCTION(void* reallocf(void *ptr, size_t size), \"don't use\");\n+\n@@ -702,0 +731,4 @@\n+inline bool is_unsigned_subword_type(BasicType t) {\n+  return (t == T_BOOLEAN || t == T_CHAR);\n+}\n+\n@@ -706,2 +739,2 @@\n-inline bool is_reference_type(BasicType t) {\n-  return (t == T_OBJECT || t == T_ARRAY);\n+inline bool is_reference_type(BasicType t, bool include_narrow_oop = false) {\n+  return (t == T_OBJECT || t == T_ARRAY || (include_narrow_oop && t == T_NARROWOOP));\n@@ -714,0 +747,4 @@\n+inline bool is_non_subword_integral_type(BasicType t) {\n+  return t == T_INT || t == T_LONG;\n+}\n+\n@@ -721,2 +758,1 @@\n-extern const char* type2name_tab[T_CONFLICT+1];     \/\/ Map a BasicType to a jchar\n-inline const char* type2name(BasicType t) { return (uint)t < T_CONFLICT+1 ? type2name_tab[t] : NULL; }\n+extern const char* type2name_tab[T_CONFLICT+1];     \/\/ Map a BasicType to a char*\n@@ -725,0 +761,5 @@\n+inline const char* type2name(BasicType t) {\n+  assert((uint)t < T_CONFLICT + 1, \"invalid type\");\n+  return type2name_tab[t];\n+}\n+\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":45,"deletions":4,"binary":false,"changes":49,"status":"modified"},{"patch":"@@ -45,0 +45,1 @@\n+#include \"runtime\/javaThread.inline.hpp\"\n@@ -51,1 +52,1 @@\n-#include \"runtime\/thread.inline.hpp\"\n+#include \"runtime\/threads.hpp\"\n@@ -335,1 +336,4 @@\n-      RegisterMap map(JavaThread::cast(t), false); \/\/ No update\n+      RegisterMap map(JavaThread::cast(t),\n+                      RegisterMap::UpdateMap::skip,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip); \/\/ No update\n@@ -556,1 +560,1 @@\n-      st->print_cr(\"Will crash now (TestCrashInErrorHandler=\" UINTX_FORMAT \")...\",\n+      st->print_cr(\"Will crash now (TestCrashInErrorHandler=%u)...\",\n@@ -563,1 +567,1 @@\n-      st->print_cr(\"Will crash now (TestCrashInErrorHandler=\" UINTX_FORMAT \")...\",\n+      st->print_cr(\"Will crash now (TestCrashInErrorHandler=%u)...\",\n@@ -883,0 +887,8 @@\n+  STEP(\"printing registers\")\n+\n+     \/\/ printing registers\n+     if (_verbose && _context) {\n+       os::print_context(st, _context);\n+       st->cr();\n+     }\n+\n@@ -892,1 +904,1 @@\n-  STEP(\"printing registers, top of stack, instructions near pc\")\n+  STEP(\"printing top of stack, instructions near pc\")\n@@ -894,1 +906,1 @@\n-     \/\/ registers, top of stack, instructions near pc\n+     \/\/ printing top of stack, instructions near pc\n@@ -896,1 +908,1 @@\n-       os::print_context(st, _context);\n+       os::print_tos_pc(st, _context);\n@@ -1150,0 +1162,9 @@\n+#ifndef _WIN32\n+  STEP(\"printing locale settings\")\n+\n+     if (_verbose) {\n+       os::Posix::print_active_locale(st);\n+       st->cr();\n+     }\n+#endif\n+\n@@ -1331,0 +1352,6 @@\n+  \/\/ STEP(\"printing locale settings\")\n+#ifndef _WIN32\n+  os::Posix::print_active_locale(st);\n+  st->cr();\n+#endif\n+\n","filename":"src\/hotspot\/share\/utilities\/vmError.cpp","additions":34,"deletions":7,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -101,1 +101,1 @@\n-    final int instanceKlassInitStateOffset = getFieldOffset(\"InstanceKlass::_init_state\", Integer.class, \"u1\");\n+    final int instanceKlassInitStateOffset = getFieldOffset(\"InstanceKlass::_init_state\", Integer.class, \"InstanceKlass::ClassState\");\n","filename":"src\/jdk.internal.vm.ci\/share\/classes\/jdk.vm.ci.hotspot\/src\/jdk\/vm\/ci\/hotspot\/HotSpotVMConfig.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -70,6 +70,0 @@\n-compiler\/whitebox\/ClearMethodStateTest.java 8265360 macosx-all\n-compiler\/whitebox\/EnqueueMethodForCompilationTest.java 8265360 macosx-all\n-compiler\/whitebox\/MakeMethodNotCompilableTest.java 8265360 macosx-all\n-\n-compiler\/codecache\/TestStressCodeBuffers.java 8272094 generic-aarch64\n-\n@@ -82,1 +76,0 @@\n-gc\/arguments\/TestUseCompressedOopsFlagsWithUlimit.java 8285011 generic-all\n@@ -92,0 +85,1 @@\n+gc\/stress\/TestStressG1Humongous.java 8286554 windows-x64\n@@ -119,0 +113,1 @@\n+serviceability\/jvmti\/vthread\/GetSetLocalTest\/GetSetLocalTest.java 8286836 generic-all\n@@ -176,8 +171,0 @@\n-vmTestbase\/nsk\/monitoring\/MemoryPoolMBean\/isCollectionUsageThresholdExceeded\/isexceeded003\/TestDescription.java 8153598 generic-all\n-vmTestbase\/nsk\/monitoring\/MemoryPoolMBean\/isUsageThresholdExceeded\/isexceeded001\/TestDescription.java 8198668 generic-all\n-vmTestbase\/nsk\/monitoring\/MemoryPoolMBean\/isUsageThresholdExceeded\/isexceeded002\/TestDescription.java 8153598 generic-all\n-vmTestbase\/nsk\/monitoring\/MemoryPoolMBean\/isUsageThresholdExceeded\/isexceeded003\/TestDescription.java 8198668 generic-all\n-vmTestbase\/nsk\/monitoring\/MemoryPoolMBean\/isUsageThresholdExceeded\/isexceeded004\/TestDescription.java 8153598 generic-all\n-vmTestbase\/nsk\/monitoring\/MemoryPoolMBean\/isUsageThresholdExceeded\/isexceeded005\/TestDescription.java 8153598 generic-all\n-\n-vmTestbase\/nsk\/jdi\/HiddenClass\/events\/events001.java                 8257705 generic-all\n@@ -196,0 +183,1 @@\n+vmTestbase\/nsk\/jvmti\/scenarios\/capability\/CM03\/cm03t001\/TestDescription.java 8073470 linux-all\n@@ -209,2 +197,0 @@\n-\n-#############################################################################\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":3,"deletions":17,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -29,1 +29,1 @@\n-import sun.hotspot.WhiteBox;\n+import jdk.test.whitebox.WhiteBox;\n@@ -119,0 +119,1 @@\n+    public static final String LONGCOUNTEDLOOP = START + \"LongCountedLoop\\\\b\" + MID + END;\n@@ -140,0 +141,1 @@\n+    public static final String MEMBAR_STORESTORE = START + \"MemBarStoreStore\" + MID + END;\n@@ -142,0 +144,1 @@\n+    public static final String CMOVEI = START + \"CMoveI\" + MID + END;\n@@ -157,0 +160,2 @@\n+    public static final String RSHIFT_VB = START + \"RShiftVB\" + MID + END;\n+    public static final String RSHIFT_VS = START + \"RShiftVS\" + MID + END;\n@@ -164,0 +169,1 @@\n+    public static final String ADD_VI = START + \"AddVI\" + MID + END;\n@@ -169,0 +175,4 @@\n+    public static final String CMP_U = START + \"CmpU\" + MID + END;\n+    public static final String CMP_UL = START + \"CmpUL\" + MID + END;\n+    public static final String CMP_U3 = START + \"CmpU3\" + MID + END;\n+    public static final String CMP_UL3 = START + \"CmpUL3\" + MID + END;\n@@ -172,2 +182,3 @@\n-    public static final String DIV = START + \"(NoOvf)?Div(I|L|F|D)\" + MID + END;\n-    public static final String DIV_L = START + \"(NoOvf)?DivL\" + MID + END;\n+    public static final String MUL_F = START + \"MulF\" + MID + END;\n+    public static final String DIV = START + \"Div(I|L|F|D)\" + MID + END;\n+    public static final String DIV_L = START + \"DivL\" + MID + END;\n@@ -178,0 +189,2 @@\n+    public static final String CAST_II = START + \"CastII\" + MID + END;\n+    public static final String CAST_LL = START + \"CastLL\" + MID + END;\n@@ -179,0 +192,8 @@\n+    public static final String PHI = START + \"Phi\" + MID + END;\n+\n+    public static final String AND_V = START + \"AndV\" + MID + END;\n+    public static final String OR_V = START + \"OrV\" + MID + END;\n+    public static final String XOR_V = START + \"XorV\" + MID + END;\n+    public static final String AND_V_MASK = START + \"AndVMask\" + MID + END;\n+    public static final String OR_V_MASK = START + \"OrVMask\" + MID + END;\n+    public static final String XOR_V_MASK = START + \"XorVMask\" + MID + END;\n@@ -190,0 +211,8 @@\n+    public static final String VECTOR_BLEND = START + \"VectorBlend\" + MID + END;\n+    public static final String REVERSE_BYTES_V = START + \"ReverseBytesV\" + MID + END;\n+\n+    public static final String Min_V = START + \"MinV\" + MID + END;\n+    public static final String Max_V = START + \"MaxV\" + MID + END;\n+\n+    public static final String FAST_LOCK   = START + \"FastLock\" + MID + END;\n+    public static final String FAST_UNLOCK = START + \"FastUnlock\" + MID + END;\n","filename":"test\/hotspot\/jtreg\/compiler\/lib\/ir_framework\/IRNode.java","additions":32,"deletions":3,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -37,1 +37,1 @@\n-import sun.hotspot.WhiteBox;\n+import jdk.test.whitebox.WhiteBox;\n","filename":"test\/hotspot\/jtreg\/gc\/arguments\/TestUseCompressedOopsErgoTools.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -33,2 +33,2 @@\n- * @build sun.hotspot.WhiteBox\n- * @run driver jdk.test.lib.helpers.ClassFileInstaller sun.hotspot.WhiteBox\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n","filename":"test\/hotspot\/jtreg\/gc\/g1\/plab\/TestPLABPromotion.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2013, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2013, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -40,1 +40,1 @@\n-\/* @test TestMetaspacePerfCountersSerial\n+\/* @test id=Serial-64\n@@ -43,0 +43,1 @@\n+ * @requires vm.bits == \"64\"\n@@ -50,2 +51,2 @@\n- * @run main\/othervm -XX:+IgnoreUnrecognizedVMOptions -XX:-UseCompressedOops -XX:+UsePerfData -XX:+UseSerialGC gc.metaspace.TestMetaspacePerfCounters\n- * @run main\/othervm -XX:+IgnoreUnrecognizedVMOptions -XX:+UseCompressedOops -XX:+UsePerfData -XX:+UseSerialGC gc.metaspace.TestMetaspacePerfCounters\n+ * @run main\/othervm -XX:-UseCompressedOops -XX:-UseCompressedClassPointers -XX:+UsePerfData -XX:+UseSerialGC gc.metaspace.TestMetaspacePerfCounters\n+ * @run main\/othervm -XX:+UseCompressedOops -XX:+UseCompressedClassPointers -XX:+UsePerfData -XX:+UseSerialGC gc.metaspace.TestMetaspacePerfCounters\n@@ -54,1 +55,1 @@\n-\/* @test TestMetaspacePerfCountersParallel\n+\/* @test id=Parallel-64\n@@ -57,0 +58,1 @@\n+ * @requires vm.bits == \"64\"\n@@ -64,2 +66,2 @@\n- * @run main\/othervm -XX:+IgnoreUnrecognizedVMOptions -XX:-UseCompressedOops -XX:+UsePerfData -XX:+UseParallelGC gc.metaspace.TestMetaspacePerfCounters\n- * @run main\/othervm -XX:+IgnoreUnrecognizedVMOptions -XX:+UseCompressedOops -XX:+UsePerfData -XX:+UseParallelGC gc.metaspace.TestMetaspacePerfCounters\n+ * @run main\/othervm -XX:-UseCompressedOops -XX:-UseCompressedClassPointers -XX:+UsePerfData -XX:+UseParallelGC gc.metaspace.TestMetaspacePerfCounters\n+ * @run main\/othervm -XX:+UseCompressedOops -XX:+UseCompressedClassPointers -XX:+UsePerfData -XX:+UseParallelGC gc.metaspace.TestMetaspacePerfCounters\n@@ -68,1 +70,1 @@\n-\/* @test TestMetaspacePerfCountersG1\n+\/* @test id=G1-64\n@@ -71,0 +73,1 @@\n+ * @requires vm.bits == \"64\"\n@@ -78,2 +81,2 @@\n- * @run main\/othervm -XX:+IgnoreUnrecognizedVMOptions -XX:-UseCompressedOops -XX:+UsePerfData -XX:+UseG1GC gc.metaspace.TestMetaspacePerfCounters\n- * @run main\/othervm -XX:+IgnoreUnrecognizedVMOptions -XX:+UseCompressedOops -XX:+UsePerfData -XX:+UseG1GC gc.metaspace.TestMetaspacePerfCounters\n+ * @run main\/othervm -XX:-UseCompressedOops -XX:-UseCompressedClassPointers -XX:+UsePerfData -XX:+UseG1GC gc.metaspace.TestMetaspacePerfCounters\n+ * @run main\/othervm -XX:+UseCompressedOops -XX:+UseCompressedClassPointers -XX:+UsePerfData -XX:+UseG1GC gc.metaspace.TestMetaspacePerfCounters\n@@ -82,1 +85,1 @@\n-\/* @test TestMetaspacePerfCountersShenandoah\n+\/* @test id=Shenandoah-64\n@@ -85,0 +88,1 @@\n+ * @requires vm.bits == \"64\"\n@@ -92,2 +96,2 @@\n- * @run main\/othervm -XX:+IgnoreUnrecognizedVMOptions -XX:-UseCompressedOops -XX:+UsePerfData -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC gc.metaspace.TestMetaspacePerfCounters\n- * @run main\/othervm -XX:+IgnoreUnrecognizedVMOptions -XX:+UseCompressedOops -XX:+UsePerfData -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC gc.metaspace.TestMetaspacePerfCounters\n+ * @run main\/othervm -XX:-UseCompressedOops -XX:-UseCompressedClassPointers -XX:+UsePerfData -XX:+UseShenandoahGC gc.metaspace.TestMetaspacePerfCounters\n+ * @run main\/othervm -XX:+UseCompressedOops -XX:+UseCompressedClassPointers -XX:+UsePerfData -XX:+UseShenandoahGC gc.metaspace.TestMetaspacePerfCounters\n@@ -95,0 +99,87 @@\n+\n+\/* @test id=Epsilon-64\n+ * @bug 8014659\n+ * @requires vm.gc.Epsilon\n+ * @requires vm.bits == \"64\"\n+ * @library \/test\/lib \/\n+ * @summary Tests that performance counters for metaspace and compressed class\n+ *          space exists and works.\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.compiler\n+ *          java.management\/sun.management\n+ *          jdk.internal.jvmstat\/sun.jvmstat.monitor\n+ * @run main\/othervm -XX:+UnlockExperimentalVMOptions -XX:-UseCompressedOops -XX:-UseCompressedClassPointers -XX:+UsePerfData -XX:+UseEpsilonGC gc.metaspace.TestMetaspacePerfCounters\n+ * @run main\/othervm -XX:+UnlockExperimentalVMOptions -XX:+UseCompressedOops -XX:+UseCompressedClassPointers -XX:+UsePerfData -XX:+UseEpsilonGC gc.metaspace.TestMetaspacePerfCounters\n+ *\/\n+\n+\/* @test id=Serial-32\n+ * @bug 8014659\n+ * @requires vm.gc.Serial\n+ * @requires vm.bits == \"32\"\n+ * @library \/test\/lib \/\n+ * @summary Tests that performance counters for metaspace and compressed class\n+ *          space exists and works.\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.compiler\n+ *          java.management\/sun.management\n+ *          jdk.internal.jvmstat\/sun.jvmstat.monitor\n+ * @run main\/othervm -XX:+UsePerfData -XX:+UseSerialGC gc.metaspace.TestMetaspacePerfCounters\n+ *\/\n+\n+\/* @test id=Parallel-32\n+ * @bug 8014659\n+ * @requires vm.gc.Parallel\n+ * @requires vm.bits == \"32\"\n+ * @library \/test\/lib \/\n+ * @summary Tests that performance counters for metaspace and compressed class\n+ *          space exists and works.\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.compiler\n+ *          java.management\/sun.management\n+ *          jdk.internal.jvmstat\/sun.jvmstat.monitor\n+ * @run main\/othervm -XX:+UsePerfData -XX:+UseParallelGC gc.metaspace.TestMetaspacePerfCounters\n+ *\/\n+\n+\/* @test id=G1-32\n+ * @bug 8014659\n+ * @requires vm.gc.G1\n+ * @requires vm.bits == \"32\"\n+ * @library \/test\/lib \/\n+ * @summary Tests that performance counters for metaspace and compressed class\n+ *          space exists and works.\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.compiler\n+ *          java.management\/sun.management\n+ *          jdk.internal.jvmstat\/sun.jvmstat.monitor\n+ * @run main\/othervm -XX:+UsePerfData -XX:+UseG1GC gc.metaspace.TestMetaspacePerfCounters\n+ *\/\n+\n+\/* @test id=Shenandoah-32\n+ * @bug 8014659\n+ * @requires vm.gc.Shenandoah\n+ * @requires vm.bits == \"32\"\n+ * @library \/test\/lib \/\n+ * @summary Tests that performance counters for metaspace and compressed class\n+ *          space exists and works.\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.compiler\n+ *          java.management\/sun.management\n+ *          jdk.internal.jvmstat\/sun.jvmstat.monitor\n+ * @run main\/othervm -XX:+UsePerfData -XX:+UseShenandoahGC gc.metaspace.TestMetaspacePerfCounters\n+ *\/\n+\n+\n+\/* @test id=Epsilon-32\n+ * @bug 8014659\n+ * @requires vm.gc.Epsilon\n+ * @requires vm.bits == \"32\"\n+ * @library \/test\/lib \/\n+ * @summary Tests that performance counters for metaspace and compressed class\n+ *          space exists and works.\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.compiler\n+ *          java.management\/sun.management\n+ *          jdk.internal.jvmstat\/sun.jvmstat.monitor\n+ * @run main\/othervm -XX:+UsePerfData -XX:+UnlockExperimentalVMOptions -XX:+UseEpsilonGC gc.metaspace.TestMetaspacePerfCounters\n+ *\/\n+\n","filename":"test\/hotspot\/jtreg\/gc\/metaspace\/TestMetaspacePerfCounters.java","additions":104,"deletions":13,"binary":false,"changes":117,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,1 +31,1 @@\n-import sun.hotspot.WhiteBox;\n+import jdk.test.whitebox.WhiteBox;\n@@ -44,2 +44,2 @@\n- * @build sun.hotspot.WhiteBox\n- * @run driver jdk.test.lib.helpers.ClassFileInstaller sun.hotspot.WhiteBox\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n","filename":"test\/hotspot\/jtreg\/gc\/stress\/TestMultiThreadStressRSet.java","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -48,1 +48,1 @@\n-      processArgs.add(\"-XX:MaxMetaspaceSize=3m\");\n+      processArgs.add(\"-XX:MaxMetaspaceSize=2m\");\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/MaxMetaspaceSize.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -30,1 +30,1 @@\n-import sun.hotspot.WhiteBox;\n+import jdk.test.whitebox.WhiteBox;\n@@ -44,3 +44,3 @@\n- * @build sun.hotspot.WhiteBox\n- * @run driver jdk.test.lib.helpers.ClassFileInstaller sun.hotspot.WhiteBox\n- * @run main\/othervm\/timeout=240 -Xbootclasspath\/a:. -DSkipWhiteBoxInstall=true -XX:+IgnoreUnrecognizedVMOptions -XX:+UnlockDiagnosticVMOptions\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @run main\/othervm\/timeout=240 -Xbootclasspath\/a:. -XX:+IgnoreUnrecognizedVMOptions -XX:+UnlockDiagnosticVMOptions\n","filename":"test\/hotspot\/jtreg\/testlibrary_tests\/ir_framework\/tests\/TestIRMatching.java","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -30,1 +30,1 @@\n- * @build sun.hotspot.WhiteBox\n+ * @build jdk.test.whitebox.WhiteBox\n@@ -34,1 +34,1 @@\n- * @run driver jdk.test.lib.helpers.ClassFileInstaller sun.hotspot.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n@@ -58,1 +58,1 @@\n- * @build sun.hotspot.WhiteBox\n+ * @build jdk.test.whitebox.WhiteBox\n@@ -62,1 +62,1 @@\n- * @run driver jdk.test.lib.helpers.ClassFileInstaller sun.hotspot.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n@@ -86,1 +86,1 @@\n- * @build sun.hotspot.WhiteBox\n+ * @build jdk.test.whitebox.WhiteBox\n@@ -90,1 +90,1 @@\n- * @run driver jdk.test.lib.helpers.ClassFileInstaller sun.hotspot.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n@@ -114,1 +114,1 @@\n- * @build sun.hotspot.WhiteBox\n+ * @build jdk.test.whitebox.WhiteBox\n@@ -118,1 +118,1 @@\n- * @run driver jdk.test.lib.helpers.ClassFileInstaller sun.hotspot.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n@@ -146,1 +146,1 @@\n- * @build sun.hotspot.WhiteBox\n+ * @build jdk.test.whitebox.WhiteBox\n@@ -150,1 +150,1 @@\n- * @run driver jdk.test.lib.helpers.ClassFileInstaller sun.hotspot.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n@@ -178,1 +178,1 @@\n- * @build sun.hotspot.WhiteBox\n+ * @build jdk.test.whitebox.WhiteBox\n@@ -182,1 +182,1 @@\n- * @run driver jdk.test.lib.helpers.ClassFileInstaller sun.hotspot.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n@@ -210,1 +210,1 @@\n- * @build sun.hotspot.WhiteBox\n+ * @build jdk.test.whitebox.WhiteBox\n@@ -214,1 +214,1 @@\n- * @run driver jdk.test.lib.helpers.ClassFileInstaller sun.hotspot.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n@@ -242,1 +242,1 @@\n- * @build sun.hotspot.WhiteBox\n+ * @build jdk.test.whitebox.WhiteBox\n@@ -246,1 +246,1 @@\n- * @run driver jdk.test.lib.helpers.ClassFileInstaller sun.hotspot.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n@@ -275,1 +275,1 @@\n- * @build sun.hotspot.WhiteBox\n+ * @build jdk.test.whitebox.WhiteBox\n@@ -279,1 +279,1 @@\n- * @run driver jdk.test.lib.helpers.ClassFileInstaller sun.hotspot.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n@@ -300,1 +300,1 @@\n-import sun.hotspot.WhiteBox;\n+import jdk.test.whitebox.WhiteBox;\n@@ -307,1 +307,1 @@\n-    static final Long align = WhiteBox.getWhiteBox().getIntxVMFlag(\"ObjectAlignmentInBytes\");\n+    static final Long align = WhiteBox.getWhiteBox().getIntVMFlag(\"ObjectAlignmentInBytes\");\n","filename":"test\/jdk\/java\/lang\/instrument\/GetObjectSizeIntrinsicsTest.java","additions":20,"deletions":20,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -3,1 +3,1 @@\n-# Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+# Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -37,1 +37,1 @@\n-jdk\/jshell\/HighlightUITest.java                                                 8284144    generic-aarch64,macosx-x64\n+jdk\/jshell\/HighlightUITest.java                                                 8284144    generic-all\n@@ -58,1 +58,0 @@\n-tools\/javap\/T6587786.java                                                       8195589    generic-all    T6587786.java failed after JDK-8189997\n","filename":"test\/langtools\/ProblemList.txt","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"}]}