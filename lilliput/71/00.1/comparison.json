{"files":[{"patch":"@@ -1787,0 +1787,12 @@\n+  int max_monitors = C->method() != NULL ? C->max_monitors() : 0;\n+  if (UseFastLocking && max_monitors > 0) {\n+    C2CheckLockStackStub* stub = new (C->comp_arena()) C2CheckLockStackStub();\n+    C->output()->add_stub(stub);\n+    __ ldr(r9, Address(rthread, JavaThread::lock_stack_current_offset()));\n+    __ ldr(r10, Address(rthread, JavaThread::lock_stack_limit_offset()));\n+    __ add(r9, r9, max_monitors * oopSize);\n+    __ cmp(r9, r10);\n+    __ br(Assembler::GE, stub->entry());\n+    __ bind(stub->continuation());\n+  }\n+\n@@ -3767,1 +3779,1 @@\n-    Label no_count;\n+    Label count, no_count;\n@@ -3785,14 +3797,2 @@\n-      \/\/ Set tmp to be (markWord of object | UNLOCK_VALUE).\n-      __ orr(tmp, disp_hdr, markWord::unlocked_value);\n-\n-      \/\/ Initialize the box. (Must happen before we update the object mark!)\n-      __ str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-\n-      \/\/ Compare object markWord with an unlocked value (tmp) and if\n-      \/\/ equal exchange the stack address of our box with object markWord.\n-      \/\/ On failure disp_hdr contains the possibly locked markWord.\n-      __ cmpxchg(oop, tmp, box, Assembler::xword, \/*acquire*\/ true,\n-                 \/*release*\/ true, \/*weak*\/ false, disp_hdr);\n-      __ br(Assembler::EQ, cont);\n-\n-      assert(oopDesc::mark_offset_in_bytes() == 0, \"offset of _mark is not 0\");\n+      if (UseFastLocking) {\n+        __ fast_lock(oop, disp_hdr, tmp, rscratch1, no_count, false);\n@@ -3800,12 +3800,33 @@\n-      \/\/ If the compare-and-exchange succeeded, then we found an unlocked\n-      \/\/ object, will have now locked it will continue at label cont\n-\n-      \/\/ Check if the owner is self by comparing the value in the\n-      \/\/ markWord of object (disp_hdr) with the stack pointer.\n-      __ mov(rscratch1, sp);\n-      __ sub(disp_hdr, disp_hdr, rscratch1);\n-      __ mov(tmp, (address) (~(os::vm_page_size()-1) | markWord::lock_mask_in_place));\n-      \/\/ If condition is true we are cont and hence we can store 0 as the\n-      \/\/ displaced header in the box, which indicates that it is a recursive lock.\n-      __ ands(tmp\/*==0?*\/, disp_hdr, tmp);   \/\/ Sets flags for result\n-      __ str(tmp\/*==0, perhaps*\/, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+        \/\/ Indicate success at cont.\n+        __ cmp(oop, oop);\n+        __ b(count);\n+      } else {\n+        \/\/ Set tmp to be (markWord of object | UNLOCK_VALUE).\n+        __ orr(tmp, disp_hdr, markWord::unlocked_value);\n+\n+        \/\/ Initialize the box. (Must happen before we update the object mark!)\n+        __ str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+\n+        \/\/ Compare object markWord with an unlocked value (tmp) and if\n+        \/\/ equal exchange the stack address of our box with object markWord.\n+        \/\/ On failure disp_hdr contains the possibly locked markWord.\n+        __ cmpxchg(oop, tmp, box, Assembler::xword, \/*acquire*\/ true,\n+                   \/*release*\/ true, \/*weak*\/ false, disp_hdr);\n+        __ br(Assembler::EQ, cont);\n+\n+        assert(oopDesc::mark_offset_in_bytes() == 0, \"offset of _mark is not 0\");\n+\n+        \/\/ If the compare-and-exchange succeeded, then we found an unlocked\n+        \/\/ object, will have now locked it will continue at label cont\n+\n+        \/\/ Check if the owner is self by comparing the value in the\n+        \/\/ markWord of object (disp_hdr) with the stack pointer.\n+        __ mov(rscratch1, sp);\n+        __ sub(disp_hdr, disp_hdr, rscratch1);\n+        __ mov(tmp, (address) (~(os::vm_page_size()-1) | markWord::lock_mask_in_place));\n+        \/\/ If condition is true we are cont and hence we can store 0 as the\n+        \/\/ displaced header in the box, which indicates that it is a recursive lock.\n+        __ ands(tmp\/*==0?*\/, disp_hdr, tmp);   \/\/ Sets flags for result\n+        __ str(tmp\/*==0, perhaps*\/, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+        __ b(cont);\n+      }\n@@ -3814,0 +3835,1 @@\n+      __ b(cont);\n@@ -3815,1 +3837,0 @@\n-    __ b(cont);\n@@ -3828,7 +3849,8 @@\n-    \/\/ Store a non-null value into the box to avoid looking like a re-entrant\n-    \/\/ lock. The fast-path monitor unlock code checks for\n-    \/\/ markWord::monitor_value so use markWord::unused_mark which has the\n-    \/\/ relevant bit set, and also matches ObjectSynchronizer::enter.\n-    __ mov(tmp, (address)markWord::unused_mark().value());\n-    __ str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-\n+    if (!UseFastLocking) {\n+      \/\/ Store a non-null value into the box to avoid looking like a re-entrant\n+      \/\/ lock. The fast-path monitor unlock code checks for\n+      \/\/ markWord::monitor_value so use markWord::unused_mark which has the\n+      \/\/ relevant bit set, and also matches ObjectSynchronizer::enter.\n+      __ mov(tmp, (address)markWord::unused_mark().value());\n+      __ str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+    }\n@@ -3849,0 +3871,1 @@\n+    __ bind(count);\n@@ -3862,1 +3885,1 @@\n-    Label no_count;\n+    Label count, no_count;\n@@ -3866,1 +3889,1 @@\n-    if (!UseHeavyMonitors) {\n+    if (!UseHeavyMonitors && !UseFastLocking) {\n@@ -3880,3 +3903,10 @@\n-      \/\/ Check if it is still a light weight lock, this is is true if we\n-      \/\/ see the stack address of the basicLock in the markWord of the\n-      \/\/ object.\n+      if (UseFastLocking) {\n+        __ fast_unlock(oop, tmp, box, disp_hdr, no_count);\n+\n+        \/\/ Indicate success at cont.\n+        __ cmp(oop, oop);\n+        __ b(count);\n+      } else {\n+        \/\/ Check if it is still a light weight lock, this is is true if we\n+        \/\/ see the stack address of the basicLock in the markWord of the\n+        \/\/ object.\n@@ -3884,2 +3914,4 @@\n-      __ cmpxchg(oop, box, disp_hdr, Assembler::xword, \/*acquire*\/ false,\n-                 \/*release*\/ true, \/*weak*\/ false, tmp);\n+        __ cmpxchg(oop, box, disp_hdr, Assembler::xword, \/*acquire*\/ false,\n+                   \/*release*\/ true, \/*weak*\/ false, tmp);\n+        __ b(cont);\n+      }\n@@ -3888,0 +3920,1 @@\n+      __ b(cont);\n@@ -3889,1 +3922,0 @@\n-    __ b(cont);\n@@ -3897,0 +3929,11 @@\n+\n+    if (UseFastLocking) {\n+      \/\/ If the owner is anonymous, we need to fix it -- in the slow-path.\n+      __ ldr(disp_hdr, Address(tmp, ObjectMonitor::owner_offset_in_bytes()));\n+      \/\/ We cannot use tbnz here: tbnz would leave the condition flags untouched,\n+      \/\/ but we want to carry-over the NE condition to the exit at the cont label,\n+      \/\/ in order to take the slow-path.\n+      __ tst(disp_hdr, (uint64_t)(intptr_t) ANONYMOUS_OWNER);\n+      __ br(Assembler::NE, no_count);\n+    }\n+\n@@ -3923,0 +3966,1 @@\n+    __ bind(count);\n@@ -7185,1 +7229,1 @@\n-instruct loadNKlass(iRegNNoSp dst, memory4 mem)\n+instruct loadNKlass(iRegNNoSp dst, memory4 mem, rFlagsReg cr)\n@@ -7188,0 +7232,1 @@\n+  effect(TEMP_DEF dst, KILL cr);\n@@ -7192,4 +7237,16 @@\n-\n-  ins_encode(aarch64_enc_ldrw(dst, mem));\n-\n-  ins_pipe(iload_reg_mem);\n+  ins_encode %{\n+    assert($mem$$disp == oopDesc::klass_offset_in_bytes(), \"expect correct offset\");\n+    assert($mem$$index$$Register == noreg, \"expect no index\");\n+    Register dst = $dst$$Register;\n+    Register obj = $mem$$base$$Register;\n+    C2LoadNKlassStub* stub = new (Compile::current()->comp_arena()) C2LoadNKlassStub(dst);\n+    Compile::current()->output()->add_stub(stub);\n+    __ ldr(dst, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    \/\/ NOTE: We can't use tbnz here, because the target is sometimes too far away\n+    \/\/ and cannot be encoded.\n+    __ tst(dst, markWord::monitor_value);\n+    __ br(Assembler::NE, stub->entry());\n+    __ bind(stub->continuation());\n+    __ lsr(dst, dst, markWord::klass_shift);\n+  %}\n+  ins_pipe(pipe_slow);\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":105,"deletions":48,"binary":false,"changes":153,"status":"modified"},{"patch":"@@ -245,1 +245,1 @@\n-  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes());\n+  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes(), compilation()->max_monitors());\n@@ -1232,1 +1232,1 @@\n-                      arrayOopDesc::header_size(op->type()),\n+                      arrayOopDesc::base_offset_in_bytes(op->type()),\n@@ -2290,2 +2290,0 @@\n-  Address src_klass_addr = Address(src, oopDesc::klass_offset_in_bytes());\n-  Address dst_klass_addr = Address(dst, oopDesc::klass_offset_in_bytes());\n@@ -2352,9 +2350,4 @@\n-      if (UseCompressedClassPointers) {\n-        __ ldrw(tmp, src_klass_addr);\n-        __ ldrw(rscratch1, dst_klass_addr);\n-        __ cmpw(tmp, rscratch1);\n-      } else {\n-        __ ldr(tmp, src_klass_addr);\n-        __ ldr(rscratch1, dst_klass_addr);\n-        __ cmp(tmp, rscratch1);\n-      }\n+      assert(UseCompressedClassPointers, \"Lilliput\");\n+      __ load_nklass(tmp, src);\n+      __ load_nklass(rscratch1, dst);\n+      __ cmpw(tmp, rscratch1);\n@@ -2375,2 +2368,4 @@\n-      __ load_klass(src, src);\n-      __ load_klass(dst, dst);\n+      __ load_klass(tmp, src);\n+      __ mov(src, tmp);\n+      __ load_klass(tmp, dst);\n+      __ mov(dst, tmp);\n@@ -2486,0 +2481,1 @@\n+    assert(UseCompressedClassPointers, \"Lilliput\");\n@@ -2487,8 +2483,2 @@\n-\n-      if (UseCompressedClassPointers) {\n-        __ ldrw(rscratch1, dst_klass_addr);\n-        __ cmpw(tmp, rscratch1);\n-      } else {\n-        __ ldr(rscratch1, dst_klass_addr);\n-        __ cmp(tmp, rscratch1);\n-      }\n+      __ load_nklass(rscratch1, dst);\n+      __ cmpw(tmp, rscratch1);\n@@ -2496,7 +2486,2 @@\n-      if (UseCompressedClassPointers) {\n-        __ ldrw(rscratch1, src_klass_addr);\n-        __ cmpw(tmp, rscratch1);\n-      } else {\n-        __ ldr(rscratch1, src_klass_addr);\n-        __ cmp(tmp, rscratch1);\n-      }\n+      __ load_nklass(rscratch1, src);\n+      __ cmpw(tmp, rscratch1);\n@@ -2505,7 +2490,2 @@\n-      if (UseCompressedClassPointers) {\n-        __ ldrw(rscratch1, dst_klass_addr);\n-        __ cmpw(tmp, rscratch1);\n-      } else {\n-        __ ldr(rscratch1, dst_klass_addr);\n-        __ cmp(tmp, rscratch1);\n-      }\n+      __ load_nklass(rscratch1, dst);\n+      __ cmpw(tmp, rscratch1);\n@@ -2591,6 +2571,11 @@\n-  if (UseCompressedClassPointers) {\n-    __ ldrw(result, Address (obj, oopDesc::klass_offset_in_bytes()));\n-    __ decode_klass_not_null(result);\n-  } else {\n-    __ ldr(result, Address (obj, oopDesc::klass_offset_in_bytes()));\n-  }\n+  assert(UseCompressedClassPointers, \"expects UseCompressedClassPointers\");\n+\n+  \/\/ Check if we can take the (common) fast path, if obj is unlocked.\n+  __ ldr(result, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  __ tst(result, markWord::monitor_value);\n+  __ br(Assembler::NE, *op->stub()->entry());\n+  __ bind(*op->stub()->continuation());\n+\n+  \/\/ Shift and decode Klass*.\n+  __ lsr(result, result, markWord::klass_shift);\n+  __ decode_klass_not_null(result);\n@@ -2657,1 +2642,2 @@\n-      __ load_klass(recv, recv);\n+      __ load_klass(rscratch1, recv);\n+      __ mov(recv, rscratch1);\n@@ -2751,1 +2737,2 @@\n-      __ load_klass(tmp, tmp);\n+      __ load_klass(rscratch1, tmp);\n+      __ mov(tmp, rscratch1);\n@@ -2764,1 +2751,2 @@\n-          __ load_klass(tmp, tmp);\n+          __ load_klass(rscratch1, tmp);\n+          __ mov(tmp, rscratch1);\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":34,"deletions":46,"binary":false,"changes":80,"status":"modified"},{"patch":"@@ -86,31 +86,35 @@\n-  \/\/ and mark it as unlocked\n-  orr(hdr, hdr, markWord::unlocked_value);\n-  \/\/ save unlocked object header into the displaced header location on the stack\n-  str(hdr, Address(disp_hdr, 0));\n-  \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n-  \/\/ displaced header address in the object header - if it is not the same, get the\n-  \/\/ object header instead\n-  lea(rscratch2, Address(obj, hdr_offset));\n-  cmpxchgptr(hdr, disp_hdr, rscratch2, rscratch1, done, \/*fallthough*\/NULL);\n-  \/\/ if the object header was the same, we're done\n-  \/\/ if the object header was not the same, it is now in the hdr register\n-  \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n-  \/\/\n-  \/\/ 1) (hdr & aligned_mask) == 0\n-  \/\/ 2) sp <= hdr\n-  \/\/ 3) hdr <= sp + page_size\n-  \/\/\n-  \/\/ these 3 tests can be done by evaluating the following expression:\n-  \/\/\n-  \/\/ (hdr - sp) & (aligned_mask - page_size)\n-  \/\/\n-  \/\/ assuming both the stack pointer and page_size have their least\n-  \/\/ significant 2 bits cleared and page_size is a power of 2\n-  mov(rscratch1, sp);\n-  sub(hdr, hdr, rscratch1);\n-  ands(hdr, hdr, aligned_mask - os::vm_page_size());\n-  \/\/ for recursive locking, the result is zero => save it in the displaced header\n-  \/\/ location (NULL in the displaced hdr location indicates recursive locking)\n-  str(hdr, Address(disp_hdr, 0));\n-  \/\/ otherwise we don't care about the result and handle locking via runtime call\n-  cbnz(hdr, slow_case);\n+  if (UseFastLocking) {\n+    fast_lock(obj, hdr, rscratch1, rscratch2, slow_case, false);\n+  } else {\n+    \/\/ and mark it as unlocked\n+    orr(hdr, hdr, markWord::unlocked_value);\n+    \/\/ save unlocked object header into the displaced header location on the stack\n+    str(hdr, Address(disp_hdr, 0));\n+    \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n+    \/\/ displaced header address in the object header - if it is not the same, get the\n+    \/\/ object header instead\n+    lea(rscratch2, Address(obj, hdr_offset));\n+    cmpxchgptr(hdr, disp_hdr, rscratch2, rscratch1, done, \/*fallthough*\/NULL);\n+    \/\/ if the object header was the same, we're done\n+    \/\/ if the object header was not the same, it is now in the hdr register\n+    \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n+    \/\/\n+    \/\/ 1) (hdr & aligned_mask) == 0\n+    \/\/ 2) sp <= hdr\n+    \/\/ 3) hdr <= sp + page_size\n+    \/\/\n+    \/\/ these 3 tests can be done by evaluating the following expression:\n+    \/\/\n+    \/\/ (hdr - sp) & (aligned_mask - page_size)\n+    \/\/\n+    \/\/ assuming both the stack pointer and page_size have their least\n+    \/\/ significant 2 bits cleared and page_size is a power of 2\n+    mov(rscratch1, sp);\n+    sub(hdr, hdr, rscratch1);\n+    ands(hdr, hdr, aligned_mask - os::vm_page_size());\n+    \/\/ for recursive locking, the result is zero => save it in the displaced header\n+    \/\/ location (NULL in the displaced hdr location indicates recursive locking)\n+    str(hdr, Address(disp_hdr, 0));\n+    \/\/ otherwise we don't care about the result and handle locking via runtime call\n+    cbnz(hdr, slow_case);\n+  }\n@@ -130,5 +134,8 @@\n-  \/\/ load displaced header\n-  ldr(hdr, Address(disp_hdr, 0));\n-  \/\/ if the loaded hdr is NULL we had recursive locking\n-  \/\/ if we had recursive locking, we are done\n-  cbz(hdr, done);\n+  if (!UseFastLocking) {\n+    \/\/ load displaced header\n+    ldr(hdr, Address(disp_hdr, 0));\n+    \/\/ if the loaded hdr is NULL we had recursive locking\n+    \/\/ if we had recursive locking, we are done\n+    cbz(hdr, done);\n+  }\n+\n@@ -138,8 +145,4 @@\n-  \/\/ test if object header is pointing to the displaced header, and if so, restore\n-  \/\/ the displaced header in the object - if the object header is not pointing to\n-  \/\/ the displaced header, get the object header instead\n-  \/\/ if the object header was not pointing to the displaced header,\n-  \/\/ we do unlocking via runtime call\n-  if (hdr_offset) {\n-    lea(rscratch1, Address(obj, hdr_offset));\n-    cmpxchgptr(disp_hdr, hdr, rscratch1, rscratch2, done, &slow_case);\n+\n+  if (UseFastLocking) {\n+    ldr(hdr, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    fast_unlock(obj, hdr, rscratch1, rscratch2, slow_case);\n@@ -147,1 +150,13 @@\n-    cmpxchgptr(disp_hdr, hdr, obj, rscratch2, done, &slow_case);\n+    \/\/ test if object header is pointing to the displaced header, and if so, restore\n+    \/\/ the displaced header in the object - if the object header is not pointing to\n+    \/\/ the displaced header, get the object header instead\n+    \/\/ if the object header was not pointing to the displaced header,\n+    \/\/ we do unlocking via runtime call\n+    if (hdr_offset) {\n+      lea(rscratch1, Address(obj, hdr_offset));\n+      cmpxchgptr(disp_hdr, hdr, rscratch1, rscratch2, done, &slow_case);\n+    } else {\n+      cmpxchgptr(disp_hdr, hdr, obj, rscratch2, done, &slow_case);\n+    }\n+    \/\/ done\n+    bind(done);\n@@ -149,2 +164,0 @@\n-  \/\/ done\n-  bind(done);\n@@ -166,2 +179,1 @@\n-  \/\/ This assumes that all prototype bits fit in an int32_t\n-  mov(t1, (int32_t)(intptr_t)markWord::prototype().value());\n+  ldr(t1, Address(klass, Klass::prototype_header_offset()));\n@@ -170,7 +182,0 @@\n-  if (UseCompressedClassPointers) { \/\/ Take care not to kill klass\n-    encode_klass_not_null(t1, klass);\n-    strw(t1, Address(obj, oopDesc::klass_offset_in_bytes()));\n-  } else {\n-    str(klass, Address(obj, oopDesc::klass_offset_in_bytes()));\n-  }\n-\n@@ -179,2 +184,0 @@\n-  } else if (UseCompressedClassPointers) {\n-    store_klass_gap(obj, zr);\n@@ -198,0 +201,6 @@\n+  \/\/ Zero first 4 bytes, if start offset is not word aligned.\n+  if (!is_aligned(hdr_size_in_bytes, BytesPerWord)) {\n+    strw(zr, Address(obj, hdr_size_in_bytes));\n+    hdr_size_in_bytes += BytesPerInt;\n+  }\n+\n@@ -257,1 +266,1 @@\n-void C1_MacroAssembler::allocate_array(Register obj, Register len, Register t1, Register t2, int header_size, int f, Register klass, Label& slow_case) {\n+void C1_MacroAssembler::allocate_array(Register obj, Register len, Register t1, Register t2, int base_offset_in_bytes, int f, Register klass, Label& slow_case) {\n@@ -270,1 +279,1 @@\n-  mov(arr_size, (int32_t)header_size * BytesPerWord + MinObjAlignmentInBytesMask);\n+  mov(arr_size, (int32_t)base_offset_in_bytes + MinObjAlignmentInBytesMask);\n@@ -279,1 +288,1 @@\n-  initialize_body(obj, arr_size, header_size * BytesPerWord, t1, t2);\n+  initialize_body(obj, arr_size, base_offset_in_bytes, t1, t2);\n@@ -299,1 +308,1 @@\n-  assert(!MacroAssembler::needs_explicit_null_check(oopDesc::klass_offset_in_bytes()), \"must add explicit null check\");\n+  assert(!MacroAssembler::needs_explicit_null_check(oopDesc::mark_offset_in_bytes()), \"must add explicit null check\");\n@@ -305,1 +314,1 @@\n-void C1_MacroAssembler::build_frame(int framesize, int bang_size_in_bytes) {\n+void C1_MacroAssembler::build_frame(int framesize, int bang_size_in_bytes, int max_monitors) {\n@@ -312,0 +321,12 @@\n+  if (UseFastLocking && max_monitors > 0) {\n+    Label ok;\n+    ldr(r9, Address(rthread, JavaThread::lock_stack_current_offset()));\n+    ldr(r10, Address(rthread, JavaThread::lock_stack_limit_offset()));\n+    add(r9, r9, max_monitors * oopSize);\n+    cmp(r9, r10);\n+    br(Assembler::LT, ok);\n+    assert(StubRoutines::aarch64::check_lock_stack() != NULL, \"need runtime call stub\");\n+    far_call(StubRoutines::aarch64::check_lock_stack());\n+    bind(ok);\n+  }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_MacroAssembler_aarch64.cpp","additions":84,"deletions":63,"binary":false,"changes":147,"status":"modified"},{"patch":"@@ -48,0 +48,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n@@ -4065,0 +4066,21 @@\n+\/\/ Loads the obj's Klass* into dst.\n+\/\/ src and dst must be distinct registers\n+\/\/ Preserves all registers (incl src, rscratch1 and rscratch2), but clobbers condition flags\n+void MacroAssembler::load_nklass(Register dst, Register src) {\n+  assert(UseCompressedClassPointers, \"expects UseCompressedClassPointers\");\n+\n+  Label fast;\n+\n+  \/\/ Check if we can take the (common) fast path, if obj is unlocked.\n+  ldr(dst, Address(src, oopDesc::mark_offset_in_bytes()));\n+  tst(dst, markWord::monitor_value);\n+  br(Assembler::EQ, fast);\n+\n+  \/\/ Fetch displaced header\n+  ldr(dst, Address(dst, OM_OFFSET_NO_MONITOR_VALUE_TAG(header)));\n+\n+  \/\/ Fast-path: shift and decode Klass*.\n+  bind(fast);\n+  lsr(dst, dst, markWord::klass_shift);\n+}\n+\n@@ -4066,6 +4088,2 @@\n-  if (UseCompressedClassPointers) {\n-    ldrw(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n-    decode_klass_not_null(dst);\n-  } else {\n-    ldr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n-  }\n+  load_nklass(dst, src);\n+  decode_klass_not_null(dst);\n@@ -4105,14 +4123,10 @@\n-  if (UseCompressedClassPointers) {\n-    ldrw(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));\n-    if (CompressedKlassPointers::base() == NULL) {\n-      cmp(trial_klass, tmp, LSL, CompressedKlassPointers::shift());\n-      return;\n-    } else if (((uint64_t)CompressedKlassPointers::base() & 0xffffffff) == 0\n-               && CompressedKlassPointers::shift() == 0) {\n-      \/\/ Only the bottom 32 bits matter\n-      cmpw(trial_klass, tmp);\n-      return;\n-    }\n-    decode_klass_not_null(tmp);\n-  } else {\n-    ldr(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));\n+  assert(UseCompressedClassPointers, \"Lilliput\");\n+  load_nklass(tmp, oop);\n+  if (CompressedKlassPointers::base() == NULL) {\n+    cmp(trial_klass, tmp, LSL, CompressedKlassPointers::shift());\n+    return;\n+  } else if (((uint64_t)CompressedKlassPointers::base() & 0xffffffff) == 0\n+             && CompressedKlassPointers::shift() == 0) {\n+    \/\/ Only the bottom 32 bits matter\n+    cmpw(trial_klass, tmp);\n+    return;\n@@ -4120,0 +4134,1 @@\n+  decode_klass_not_null(tmp);\n@@ -4123,18 +4138,0 @@\n-void MacroAssembler::store_klass(Register dst, Register src) {\n-  \/\/ FIXME: Should this be a store release?  concurrent gcs assumes\n-  \/\/ klass length is valid if klass field is not null.\n-  if (UseCompressedClassPointers) {\n-    encode_klass_not_null(src);\n-    strw(src, Address(dst, oopDesc::klass_offset_in_bytes()));\n-  } else {\n-    str(src, Address(dst, oopDesc::klass_offset_in_bytes()));\n-  }\n-}\n-\n-void MacroAssembler::store_klass_gap(Register dst, Register src) {\n-  if (UseCompressedClassPointers) {\n-    \/\/ Store to klass gap in destination\n-    strw(src, Address(dst, oopDesc::klass_gap_offset_in_bytes()));\n-  }\n-}\n-\n@@ -4275,0 +4272,14 @@\n+\/\/ Returns a static string\n+const char* MacroAssembler::describe_klass_decode_mode(MacroAssembler::KlassDecodeMode mode) {\n+  switch (mode) {\n+  case KlassDecodeNone: return \"none\";\n+  case KlassDecodeZero: return \"zero\";\n+  case KlassDecodeXor:  return \"xor\";\n+  case KlassDecodeMovk: return \"movk\";\n+  default:\n+    ShouldNotReachHere();\n+  }\n+  return NULL;\n+}\n+\n+\/\/ Return the current narrow Klass pointer decode mode.\n@@ -4276,2 +4287,4 @@\n-  assert(UseCompressedClassPointers, \"not using compressed class pointers\");\n-  assert(Metaspace::initialized(), \"metaspace not initialized yet\");\n+  if (_klass_decode_mode == KlassDecodeNone) {\n+    \/\/ First time initialization\n+    assert(UseCompressedClassPointers, \"not using compressed class pointers\");\n+    assert(Metaspace::initialized(), \"metaspace not initialized yet\");\n@@ -4279,2 +4292,5 @@\n-  if (_klass_decode_mode != KlassDecodeNone) {\n-    return _klass_decode_mode;\n+    _klass_decode_mode = klass_decode_mode_for_base(CompressedKlassPointers::base());\n+    guarantee(_klass_decode_mode != KlassDecodeNone,\n+              PTR_FORMAT \" is not a valid encoding base on aarch64\",\n+              p2i(CompressedKlassPointers::base()));\n+    log_info(metaspace)(\"klass decode mode initialized: %s\", describe_klass_decode_mode(_klass_decode_mode));\n@@ -4282,0 +4298,2 @@\n+  return _klass_decode_mode;\n+}\n@@ -4283,2 +4301,4 @@\n-  assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift()\n-         || 0 == CompressedKlassPointers::shift(), \"decode alg wrong\");\n+\/\/ Given an arbitrary base address, return the KlassDecodeMode that would be used. Return KlassDecodeNone\n+\/\/ if base address is not valid for encoding.\n+MacroAssembler::KlassDecodeMode MacroAssembler::klass_decode_mode_for_base(address base) {\n+  assert(CompressedKlassPointers::shift() != 0, \"not lilliput?\");\n@@ -4286,2 +4306,4 @@\n-  if (CompressedKlassPointers::base() == NULL) {\n-    return (_klass_decode_mode = KlassDecodeZero);\n+  const uint64_t base_u64 = (uint64_t) base;\n+\n+  if (base_u64 == 0) {\n+    return KlassDecodeZero;\n@@ -4290,7 +4312,3 @@\n-  if (operand_valid_for_logical_immediate(\n-        \/*is32*\/false, (uint64_t)CompressedKlassPointers::base())) {\n-    const uint64_t range_mask =\n-      (1ULL << log2i(CompressedKlassPointers::range())) - 1;\n-    if (((uint64_t)CompressedKlassPointers::base() & range_mask) == 0) {\n-      return (_klass_decode_mode = KlassDecodeXor);\n-    }\n+  if (operand_valid_for_logical_immediate(false, base_u64) &&\n+      ((base_u64 & (KlassEncodingMetaspaceMax - 1)) == 0)) {\n+    return KlassDecodeXor;\n@@ -4299,4 +4317,4 @@\n-  const uint64_t shifted_base =\n-    (uint64_t)CompressedKlassPointers::base() >> CompressedKlassPointers::shift();\n-  guarantee((shifted_base & 0xffff0000ffffffff) == 0,\n-            \"compressed class base bad alignment\");\n+  const uint64_t shifted_base = base_u64 >> CompressedKlassPointers::shift();\n+  if ((shifted_base & 0xffff0000ffffffff) == 0) {\n+    return KlassDecodeMovk;\n+  }\n@@ -4304,1 +4322,1 @@\n-  return (_klass_decode_mode = KlassDecodeMovk);\n+  return KlassDecodeNone;\n@@ -4308,0 +4326,2 @@\n+  assert (UseCompressedClassPointers, \"should only be used for compressed headers\");\n+  assert(CompressedKlassPointers::shift() != 0, \"not lilliput?\");\n@@ -4310,5 +4330,1 @@\n-    if (CompressedKlassPointers::shift() != 0) {\n-      lsr(dst, src, LogKlassAlignmentInBytes);\n-    } else {\n-      if (dst != src) mov(dst, src);\n-    }\n+    lsr(dst, src, LogKlassAlignmentInBytes);\n@@ -4318,6 +4334,2 @@\n-    if (CompressedKlassPointers::shift() != 0) {\n-      eor(dst, src, (uint64_t)CompressedKlassPointers::base());\n-      lsr(dst, dst, LogKlassAlignmentInBytes);\n-    } else {\n-      eor(dst, src, (uint64_t)CompressedKlassPointers::base());\n-    }\n+    eor(dst, src, (uint64_t)CompressedKlassPointers::base());\n+    lsr(dst, dst, LogKlassAlignmentInBytes);\n@@ -4327,5 +4339,1 @@\n-    if (CompressedKlassPointers::shift() != 0) {\n-      ubfx(dst, src, LogKlassAlignmentInBytes, 32);\n-    } else {\n-      movw(dst, src);\n-    }\n+    ubfx(dst, src, LogKlassAlignmentInBytes, MaxNarrowKlassPointerBits);\n@@ -4347,0 +4355,2 @@\n+  assert(CompressedKlassPointers::shift() != 0, \"not lilliput?\");\n+\n@@ -4349,5 +4359,1 @@\n-    if (CompressedKlassPointers::shift() != 0) {\n-      lsl(dst, src, LogKlassAlignmentInBytes);\n-    } else {\n-      if (dst != src) mov(dst, src);\n-    }\n+    if (dst != src) mov(dst, src);\n@@ -4357,6 +4363,2 @@\n-    if (CompressedKlassPointers::shift() != 0) {\n-      lsl(dst, src, LogKlassAlignmentInBytes);\n-      eor(dst, dst, (uint64_t)CompressedKlassPointers::base());\n-    } else {\n-      eor(dst, src, (uint64_t)CompressedKlassPointers::base());\n-    }\n+    lsl(dst, src, LogKlassAlignmentInBytes);\n+    eor(dst, dst, (uint64_t)CompressedKlassPointers::base());\n@@ -4369,0 +4371,3 @@\n+    \/\/ Invalid base should have been gracefully handled via klass_decode_mode() in VM initialization.\n+    assert((shifted_base & 0xffff0000ffffffff) == 0, \"incompatible base\");\n+\n@@ -4371,5 +4376,1 @@\n-\n-    if (CompressedKlassPointers::shift() != 0) {\n-      lsl(dst, dst, LogKlassAlignmentInBytes);\n-    }\n-\n+    lsl(dst, dst, LogKlassAlignmentInBytes);\n@@ -5957,0 +5958,55 @@\n+\n+\/\/ Attempt to fast-lock an object. Fall-through on success, branch to slow label\n+\/\/ on failure.\n+\/\/ Registers:\n+\/\/  - obj: the object to be locked\n+\/\/  - hdr: the header, already loaded from obj, will be destroyed\n+\/\/  - t1, t2, t3: temporary registers, will be destroyed\n+void MacroAssembler::fast_lock(Register obj, Register hdr, Register t1, Register t2, Label& slow, bool rt_check_stack) {\n+  assert(UseFastLocking, \"only used with fast-locking\");\n+  assert_different_registers(obj, hdr, t1, t2);\n+\n+  if (rt_check_stack) {\n+    \/\/ Check if we would have space on lock-stack for the object.\n+    ldr(t1, Address(rthread, JavaThread::lock_stack_current_offset()));\n+    ldr(t2, Address(rthread, JavaThread::lock_stack_limit_offset()));\n+    cmp(t1, t2);\n+    br(Assembler::GE, slow);\n+  }\n+\n+  \/\/ Load (object->mark() | 1) into hdr\n+  orr(hdr, hdr, markWord::unlocked_value);\n+  \/\/ Clear lock-bits, into t2\n+  eor(t2, hdr, markWord::unlocked_value);\n+  \/\/ Try to swing header from unlocked to locked\n+  cmpxchg(\/*addr*\/ obj, \/*expected*\/ hdr, \/*new*\/ t2, Assembler::xword,\n+          \/*acquire*\/ true, \/*release*\/ true, \/*weak*\/ false, t1);\n+  br(Assembler::NE, slow);\n+\n+  \/\/ After successful lock, push object on lock-stack\n+  ldr(t1, Address(rthread, JavaThread::lock_stack_current_offset()));\n+  str(obj, Address(t1, 0));\n+  add(t1, t1, oopSize);\n+  str(t1, Address(rthread, JavaThread::lock_stack_current_offset()));\n+}\n+\n+void MacroAssembler::fast_unlock(Register obj, Register hdr, Register t1, Register t2, Label& slow) {\n+  assert(UseFastLocking, \"only used with fast-locking\");\n+  assert_different_registers(obj, hdr, t1, t2);\n+\n+  \/\/ Load the expected old header (lock-bits cleared to indicate 'locked') into hdr\n+  andr(hdr, hdr, ~markWord::lock_mask_in_place);\n+\n+  \/\/ Load the new header (unlocked) into t1\n+  orr(t1, hdr, markWord::unlocked_value);\n+\n+  \/\/ Try to swing header from locked to unlocked\n+  cmpxchg(obj, hdr, t1, Assembler::xword,\n+          \/*acquire*\/ true, \/*release*\/ true, \/*weak*\/ false, t2);\n+  br(Assembler::NE, slow);\n+\n+  \/\/ After successful unlock, pop object from lock-stack\n+  ldr(t1, Address(rthread, JavaThread::lock_stack_current_offset()));\n+  sub(t1, t1, oopSize);\n+  str(t1, Address(rthread, JavaThread::lock_stack_current_offset()));\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":146,"deletions":90,"binary":false,"changes":236,"status":"modified"},{"patch":"@@ -89,0 +89,2 @@\n+ public:\n+\n@@ -96,1 +98,9 @@\n-  KlassDecodeMode klass_decode_mode();\n+  \/\/ Return the current narrow Klass pointer decode mode. Initialized on first call.\n+  static KlassDecodeMode klass_decode_mode();\n+\n+  \/\/ Given an arbitrary base address, return the KlassDecodeMode that would be used. Return KlassDecodeNone\n+  \/\/ if base address is not valid for encoding.\n+  static KlassDecodeMode klass_decode_mode_for_base(address base);\n+\n+  \/\/ Returns a static string\n+  static const char* describe_klass_decode_mode(KlassDecodeMode mode);\n@@ -99,0 +109,1 @@\n+\n@@ -843,0 +854,1 @@\n+  void load_nklass(Register dst, Register src);\n@@ -844,1 +856,0 @@\n-  void store_klass(Register dst, Register src);\n@@ -870,2 +881,0 @@\n-  void store_klass_gap(Register dst, Register src);\n-\n@@ -1565,0 +1574,3 @@\n+  void fast_lock(Register obj, Register hdr, Register t1, Register t2, Label& slow, bool rt_check_stack = true);\n+  void fast_unlock(Register obj, Register hdr, Register t1, Register t2, Label& slow);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":16,"deletions":4,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -1778,28 +1778,33 @@\n-      \/\/ Load (object->mark() | 1) into swap_reg %r0\n-      __ ldr(rscratch1, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ orr(swap_reg, rscratch1, 1);\n-\n-      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-      __ str(swap_reg, Address(lock_reg, mark_word_offset));\n-\n-      \/\/ src -> dest iff dest == r0 else r0 <- dest\n-      __ cmpxchg_obj_header(r0, lock_reg, obj_reg, rscratch1, count, \/*fallthrough*\/NULL);\n-\n-      \/\/ Hmm should this move to the slow path code area???\n-\n-      \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n-      \/\/  1) (mark & 3) == 0, and\n-      \/\/  2) sp <= mark < mark + os::pagesize()\n-      \/\/ These 3 tests can be done by evaluating the following\n-      \/\/ expression: ((mark - sp) & (3 - os::vm_page_size())),\n-      \/\/ assuming both stack pointer and pagesize have their\n-      \/\/ least significant 2 bits clear.\n-      \/\/ NOTE: the oopMark is in swap_reg %r0 as the result of cmpxchg\n-\n-      __ sub(swap_reg, sp, swap_reg);\n-      __ neg(swap_reg, swap_reg);\n-      __ ands(swap_reg, swap_reg, 3 - os::vm_page_size());\n-\n-      \/\/ Save the test result, for recursive case, the result is zero\n-      __ str(swap_reg, Address(lock_reg, mark_word_offset));\n-      __ br(Assembler::NE, slow_path_lock);\n+      if (UseFastLocking) {\n+        __ ldr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ fast_lock(obj_reg, swap_reg, tmp, rscratch1, slow_path_lock);\n+      } else {\n+        \/\/ Load (object->mark() | 1) into swap_reg %r0\n+        __ ldr(rscratch1, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ orr(swap_reg, rscratch1, 1);\n+\n+        \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+        __ str(swap_reg, Address(lock_reg, mark_word_offset));\n+\n+        \/\/ src -> dest iff dest == r0 else r0 <- dest\n+        __ cmpxchg_obj_header(r0, lock_reg, obj_reg, rscratch1, count, \/*fallthrough*\/NULL);\n+\n+        \/\/ Hmm should this move to the slow path code area???\n+\n+        \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n+        \/\/  1) (mark & 3) == 0, and\n+        \/\/  2) sp <= mark < mark + os::pagesize()\n+        \/\/ These 3 tests can be done by evaluating the following\n+        \/\/ expression: ((mark - sp) & (3 - os::vm_page_size())),\n+        \/\/ assuming both stack pointer and pagesize have their\n+        \/\/ least significant 2 bits clear.\n+        \/\/ NOTE: the oopMark is in swap_reg %r0 as the result of cmpxchg\n+\n+        __ sub(swap_reg, sp, swap_reg);\n+        __ neg(swap_reg, swap_reg);\n+        __ ands(swap_reg, swap_reg, 3 - os::vm_page_size());\n+\n+        \/\/ Save the test result, for recursive case, the result is zero\n+        __ str(swap_reg, Address(lock_reg, mark_word_offset));\n+        __ br(Assembler::NE, slow_path_lock);\n+      }\n@@ -1916,1 +1921,1 @@\n-    if (!UseHeavyMonitors) {\n+    if (!UseHeavyMonitors && !UseFastLocking) {\n@@ -1932,9 +1937,14 @@\n-      \/\/ get address of the stack lock\n-      __ lea(r0, Address(sp, lock_slot_offset * VMRegImpl::stack_slot_size));\n-      \/\/  get old displaced header\n-      __ ldr(old_hdr, Address(r0, 0));\n-\n-      \/\/ Atomic swap old header if oop still contains the stack lock\n-      Label count;\n-      __ cmpxchg_obj_header(r0, old_hdr, obj_reg, rscratch1, count, &slow_path_unlock);\n-      __ bind(count);\n+      if (UseFastLocking) {\n+        __ ldr(old_hdr, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ fast_unlock(obj_reg, old_hdr, swap_reg, rscratch1, slow_path_unlock);\n+      } else {\n+        \/\/ get address of the stack lock\n+        __ lea(r0, Address(sp, lock_slot_offset * VMRegImpl::stack_slot_size));\n+        \/\/  get old displaced header\n+        __ ldr(old_hdr, Address(r0, 0));\n+\n+        \/\/ Atomic swap old header if oop still contains the stack lock\n+        Label count;\n+        __ cmpxchg_obj_header(r0, old_hdr, obj_reg, rscratch1, count, &slow_path_unlock);\n+        __ bind(count);\n+      }\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":48,"deletions":38,"binary":false,"changes":86,"status":"modified"},{"patch":"@@ -592,2 +592,12 @@\n-    __ load_klass(r0, r0);  \/\/ get klass\n-    __ cbz(r0, error);      \/\/ if klass is NULL it is broken\n+    \/\/ NOTE: We used to load the Klass* here, and compare that to zero.\n+    \/\/ However, with current Lilliput implementation, that would require\n+    \/\/ checking the locking bits and calling into the runtime, which\n+    \/\/ clobbers the condition flags, which may be live around this call.\n+    \/\/ OTOH, this is a simple NULL-check, and we can simply load the upper\n+    \/\/ 32bit of the header as narrowKlass, and compare that to 0. The\n+    \/\/ worst that can happen (rarely) is that the object is locked and\n+    \/\/ we have lock pointer bits in the upper 32bits. We can't get a false\n+    \/\/ negative.\n+    assert(oopDesc::klass_offset_in_bytes() % 4 == 0, \"must be 4 byte aligned\");\n+    __ ldrw(r0, Address(r0, oopDesc::klass_offset_in_bytes()));  \/\/ get klass\n+    __ cbzw(r0, error);      \/\/ if klass is NULL it is broken\n@@ -5381,0 +5391,23 @@\n+  address generate_check_lock_stack() {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", \"check_lock_stack\");\n+\n+    address start = __ pc();\n+\n+    __ set_last_Java_frame(sp, rfp, lr, rscratch1);\n+    __ enter();\n+    __ push_call_clobbered_registers();\n+\n+    __ mov(c_rarg0, r9);\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, LockStack::ensure_lock_stack_size), 1);\n+\n+\n+    __ pop_call_clobbered_registers();\n+    __ leave();\n+    __ reset_last_Java_frame(true);\n+\n+    __ ret(lr);\n+\n+    return start;\n+  }\n+\n@@ -8013,0 +8046,3 @@\n+    if (UseFastLocking) {\n+      StubRoutines::aarch64::_check_lock_stack = generate_check_lock_stack();\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":38,"deletions":2,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -116,1 +116,1 @@\n-  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes());\n+  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes(), compilation()->max_monitors());\n","filename":"src\/hotspot\/cpu\/ppc\/c1_LIRAssembler_ppc.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n","filename":"src\/hotspot\/cpu\/ppc\/macroAssembler_ppc.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -218,1 +218,1 @@\n-  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes());\n+  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes(), compilation()->max_monitors());\n","filename":"src\/hotspot\/cpu\/riscv\/c1_LIRAssembler_riscv.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -812,28 +812,34 @@\n-    \/\/ Load (object->mark() | 1) into swap_reg\n-    ld(t0, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-    ori(swap_reg, t0, 1);\n-\n-    \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-    sd(swap_reg, Address(lock_reg, mark_offset));\n-\n-    assert(lock_offset == 0,\n-           \"displached header must be first word in BasicObjectLock\");\n-\n-    cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, t0, count, \/*fallthrough*\/NULL);\n-\n-    \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n-    \/\/  1) (mark & 7) == 0, and\n-    \/\/  2) sp <= mark < mark + os::pagesize()\n-    \/\/\n-    \/\/ These 3 tests can be done by evaluating the following\n-    \/\/ expression: ((mark - sp) & (7 - os::vm_page_size())),\n-    \/\/ assuming both stack pointer and pagesize have their\n-    \/\/ least significant 3 bits clear.\n-    \/\/ NOTE: the oopMark is in swap_reg x10 as the result of cmpxchg\n-    sub(swap_reg, swap_reg, sp);\n-    mv(t0, (int64_t)(7 - os::vm_page_size()));\n-    andr(swap_reg, swap_reg, t0);\n-\n-    \/\/ Save the test result, for recursive case, the result is zero\n-    sd(swap_reg, Address(lock_reg, mark_offset));\n-    beqz(swap_reg, count);\n+    if (UseFastLocking) {\n+      ld(tmp, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      fast_lock(obj_reg, tmp, t0, t1, slow_case);\n+      j(count);\n+    } else {\n+      \/\/ Load (object->mark() | 1) into swap_reg\n+      ld(t0, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      ori(swap_reg, t0, 1);\n+\n+      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+      sd(swap_reg, Address(lock_reg, mark_offset));\n+\n+      assert(lock_offset == 0,\n+             \"displached header must be first word in BasicObjectLock\");\n+\n+      cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, t0, count, \/*fallthrough*\/NULL);\n+\n+      \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n+      \/\/  1) (mark & 7) == 0, and\n+      \/\/  2) sp <= mark < mark + os::pagesize()\n+      \/\/\n+      \/\/ These 3 tests can be done by evaluating the following\n+      \/\/ expression: ((mark - sp) & (7 - os::vm_page_size())),\n+      \/\/ assuming both stack pointer and pagesize have their\n+      \/\/ least significant 3 bits clear.\n+      \/\/ NOTE: the oopMark is in swap_reg x10 as the result of cmpxchg\n+      sub(swap_reg, swap_reg, sp);\n+      mv(t0, (int64_t)(7 - os::vm_page_size()));\n+      andr(swap_reg, swap_reg, t0);\n+\n+      \/\/ Save the test result, for recursive case, the result is zero\n+      sd(swap_reg, Address(lock_reg, mark_offset));\n+      beqz(swap_reg, count);\n+    }\n@@ -846,1 +852,1 @@\n-            lock_reg);\n+            UseFastLocking ? obj_reg : lock_reg);\n@@ -884,3 +890,4 @@\n-    \/\/ Convert from BasicObjectLock structure to object and BasicLock\n-    \/\/ structure Store the BasicLock address into x10\n-    la(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n+    if (UseFastLocking) {\n+      Label slow_case;\n+      \/\/ Load oop into obj_reg(c_rarg3)\n+      ld(obj_reg, Address(lock_reg, BasicObjectLock::obj_offset_in_bytes()));\n@@ -888,2 +895,2 @@\n-    \/\/ Load oop into obj_reg(c_rarg3)\n-    ld(obj_reg, Address(lock_reg, BasicObjectLock::obj_offset_in_bytes()));\n+      \/\/ Free entry\n+      sd(zr, Address(lock_reg, BasicObjectLock::obj_offset_in_bytes()));\n@@ -891,2 +898,4 @@\n-    \/\/ Free entry\n-    sd(zr, Address(lock_reg, BasicObjectLock::obj_offset_in_bytes()));\n+      \/\/ Check for non-symmetric locking. This is allowed by the spec and the interpreter\n+      \/\/ must handle it.\n+      ld(header_reg, Address(xthread, JavaThread::lock_stack_current_offset()));\n+      bne(header_reg, obj_reg, slow_case);\n@@ -894,3 +903,9 @@\n-    \/\/ Load the old header from BasicLock structure\n-    ld(header_reg, Address(swap_reg,\n-                           BasicLock::displaced_header_offset_in_bytes()));\n+      ld(header_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      fast_unlock(obj_reg, header_reg, swap_reg, t0, slow_case);\n+      j(count);\n+\n+      bind(slow_case);\n+    } else {\n+      \/\/ Convert from BasicObjectLock structure to object and BasicLock\n+      \/\/ structure Store the BasicLock address into x10\n+      la(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n@@ -898,2 +913,2 @@\n-    \/\/ Test for recursion\n-    beqz(header_reg, count);\n+      \/\/ Load oop into obj_reg(c_rarg3)\n+      ld(obj_reg, Address(lock_reg, BasicObjectLock::obj_offset_in_bytes()));\n@@ -901,2 +916,13 @@\n-    \/\/ Atomic swap back the old header\n-    cmpxchg_obj_header(swap_reg, header_reg, obj_reg, t0, count, \/*fallthrough*\/NULL);\n+      \/\/ Free entry\n+      sd(zr, Address(lock_reg, BasicObjectLock::obj_offset_in_bytes()));\n+\n+      \/\/ Load the old header from BasicLock structure\n+      ld(header_reg, Address(swap_reg,\n+                             BasicLock::displaced_header_offset_in_bytes()));\n+\n+      \/\/ Test for recursion\n+      beqz(header_reg, count);\n+\n+      \/\/ Atomic swap back the old header\n+      cmpxchg_obj_header(swap_reg, header_reg, obj_reg, t0, count, \/*fallthrough*\/NULL);\n+    }\n","filename":"src\/hotspot\/cpu\/riscv\/interp_masm_riscv.cpp","additions":69,"deletions":43,"binary":false,"changes":112,"status":"modified"},{"patch":"@@ -4469,0 +4469,55 @@\n+\n+\/\/ Attempt to fast-lock an object. Fall-through on success, branch to slow label\n+\/\/ on failure.\n+\/\/ Registers:\n+\/\/  - obj: the object to be locked\n+\/\/  - hdr: the header, already loaded from obj, will be destroyed\n+\/\/  - tmp1, tmp2: temporary registers, will be destroyed\n+void MacroAssembler::fast_lock(Register obj, Register hdr, Register tmp1, Register tmp2, Label& slow) {\n+  assert(UseFastLocking, \"only used with fast-locking\");\n+  assert_different_registers(obj, hdr, tmp1, tmp2);\n+\n+  \/\/ Check if we would have space on lock-stack for the object.\n+  ld(tmp1, Address(xthread, JavaThread::lock_stack_current_offset()));\n+  ld(tmp2, Address(xthread, JavaThread::lock_stack_limit_offset()));\n+  bge(tmp1, tmp2, slow, true);\n+\n+  \/\/ Load (object->mark() | 1) into hdr\n+  ori(hdr, hdr, markWord::unlocked_value);\n+  \/\/ Clear lock-bits, into tmp2\n+  xori(tmp2, hdr, markWord::unlocked_value);\n+  \/\/ Try to swing header from unlocked to locked\n+  Label success;\n+  cmpxchgptr(hdr, tmp2, obj, tmp1, success, &slow);\n+  bind(success);\n+\n+  \/\/ After successful lock, push object on lock-stack\n+  \/\/ TODO: Can we avoid re-loading the current offset? The CAS above clobbers it.\n+  \/\/ Maybe we could ensure that we have enough space on the lock stack more cleverly.\n+  ld(tmp1, Address(xthread, JavaThread::lock_stack_current_offset()));\n+  sd(obj, Address(tmp1, 0));\n+  add(tmp1, tmp1, oopSize);\n+  sd(tmp1, Address(xthread, JavaThread::lock_stack_current_offset()));\n+}\n+\n+void MacroAssembler::fast_unlock(Register obj, Register hdr, Register tmp1, Register tmp2, Label& slow) {\n+  assert(UseFastLocking, \"only used with fast-locking\");\n+  assert_different_registers(obj, hdr, tmp1, tmp2);\n+\n+  \/\/ Load the expected old header (lock-bits cleared to indicate 'locked') into hdr\n+  mv(tmp1, ~markWord::lock_mask_in_place);\n+  andr(hdr, hdr, tmp1);\n+\n+  \/\/ Load the new header (unlocked) into tmp1\n+  ori(tmp1, hdr, markWord::unlocked_value);\n+\n+  \/\/ Try to swing header from locked to unlocked\n+  Label success;\n+  cmpxchgptr(hdr, tmp1, obj, tmp2, success, &slow);\n+  bind(success);\n+\n+  \/\/ After successful unlock, pop object from lock-stack\n+  ld(tmp1, Address(xthread, JavaThread::lock_stack_current_offset()));\n+  sub(tmp1, tmp1, oopSize);\n+  sd(tmp1, Address(xthread, JavaThread::lock_stack_current_offset()));\n+}\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.cpp","additions":55,"deletions":0,"binary":false,"changes":55,"status":"modified"},{"patch":"@@ -1385,0 +1385,4 @@\n+\n+public:\n+  void fast_lock(Register obj, Register hdr, Register tmp1, Register tmp2, Label& slow);\n+  void fast_unlock(Register obj, Register hdr, Register tmp1, Register tmp2, Label& slow);\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2406,30 +2406,40 @@\n-      \/\/ Set tmp to be (markWord of object | UNLOCK_VALUE).\n-      __ ori(tmp, disp_hdr, markWord::unlocked_value);\n-\n-      \/\/ Initialize the box. (Must happen before we update the object mark!)\n-      __ sd(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-\n-      \/\/ Compare object markWord with an unlocked value (tmp) and if\n-      \/\/ equal exchange the stack address of our box with object markWord.\n-      \/\/ On failure disp_hdr contains the possibly locked markWord.\n-      __ cmpxchg(\/*memory address*\/oop, \/*expected value*\/tmp, \/*new value*\/box, Assembler::int64, Assembler::aq,\n-                 Assembler::rl, \/*result*\/disp_hdr);\n-      __ mv(flag, zr);\n-      __ beq(disp_hdr, tmp, cont); \/\/ prepare zero flag and goto cont if we won the cas\n-\n-      assert(oopDesc::mark_offset_in_bytes() == 0, \"offset of _mark is not 0\");\n-\n-      \/\/ If the compare-and-exchange succeeded, then we found an unlocked\n-      \/\/ object, will have now locked it will continue at label cont\n-      \/\/ We did not see an unlocked object so try the fast recursive case.\n-\n-      \/\/ Check if the owner is self by comparing the value in the\n-      \/\/ markWord of object (disp_hdr) with the stack pointer.\n-      __ sub(disp_hdr, disp_hdr, sp);\n-      __ mv(tmp, (intptr_t) (~(os::vm_page_size()-1) | (uintptr_t)markWord::lock_mask_in_place));\n-      \/\/ If (mark & lock_mask) == 0 and mark - sp < page_size, we are stack-locking and goto cont,\n-      \/\/ hence we can store 0 as the displaced header in the box, which indicates that it is a\n-      \/\/ recursive lock.\n-      __ andr(tmp\/*==0?*\/, disp_hdr, tmp);\n-      __ sd(tmp\/*==0, perhaps*\/, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-      __ mv(flag, tmp); \/\/ we can use the value of tmp as the result here\n+      if (UseFastLocking) {\n+        Label slow;\n+        __ fast_lock(oop, disp_hdr, tmp, t0, slow);\n+        \/\/ Indicate success at cont.\n+        __ mv(flag, zr);\n+        __ j(cont);\n+        __ bind(slow);\n+        __ mv(flag, 1); \/\/ Set non-zero flag to indicate 'failure' -> take slow-path\n+      } else {\n+        \/\/ Set tmp to be (markWord of object | UNLOCK_VALUE).\n+        __ ori(tmp, disp_hdr, markWord::unlocked_value);\n+\n+        \/\/ Initialize the box. (Must happen before we update the object mark!)\n+        __ sd(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+\n+        \/\/ Compare object markWord with an unlocked value (tmp) and if\n+        \/\/ equal exchange the stack address of our box with object markWord.\n+        \/\/ On failure disp_hdr contains the possibly locked markWord.\n+        __ cmpxchg(\/*memory address*\/oop, \/*expected value*\/tmp, \/*new value*\/box, Assembler::int64, Assembler::aq,\n+                   Assembler::rl, \/*result*\/disp_hdr);\n+        __ mv(flag, zr);\n+        __ beq(disp_hdr, tmp, cont); \/\/ prepare zero flag and goto cont if we won the cas\n+\n+        assert(oopDesc::mark_offset_in_bytes() == 0, \"offset of _mark is not 0\");\n+\n+        \/\/ If the compare-and-exchange succeeded, then we found an unlocked\n+        \/\/ object, will have now locked it will continue at label cont\n+        \/\/ We did not see an unlocked object so try the fast recursive case.\n+\n+        \/\/ Check if the owner is self by comparing the value in the\n+        \/\/ markWord of object (disp_hdr) with the stack pointer.\n+        __ sub(disp_hdr, disp_hdr, sp);\n+        __ mv(tmp, (intptr_t) (~(os::vm_page_size()-1) | (uintptr_t)markWord::lock_mask_in_place));\n+        \/\/ If (mark & lock_mask) == 0 and mark - sp < page_size, we are stack-locking and goto cont,\n+        \/\/ hence we can store 0 as the displaced header in the box, which indicates that it is a\n+        \/\/ recursive lock.\n+        __ andr(tmp\/*==0?*\/, disp_hdr, tmp);\n+        __ sd(tmp\/*==0, perhaps*\/, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+        __ mv(flag, tmp); \/\/ we can use the value of tmp as the result here\n+      }\n@@ -2452,6 +2462,8 @@\n-    \/\/ Store a non-null value into the box to avoid looking like a re-entrant\n-    \/\/ lock. The fast-path monitor unlock code checks for\n-    \/\/ markWord::monitor_value so use markWord::unused_mark which has the\n-    \/\/ relevant bit set, and also matches ObjectSynchronizer::slow_enter.\n-    __ mv(tmp, (address)markWord::unused_mark().value());\n-    __ sd(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+    if (!UseFastLocking) {\n+      \/\/ Store a non-null value into the box to avoid looking like a re-entrant\n+      \/\/ lock. The fast-path monitor unlock code checks for\n+      \/\/ markWord::monitor_value so use markWord::unused_mark which has the\n+      \/\/ relevant bit set, and also matches ObjectSynchronizer::slow_enter.\n+      __ mv(tmp, (address)markWord::unused_mark().value());\n+      __ sd(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+    }\n@@ -2490,1 +2502,1 @@\n-    if (!UseHeavyMonitors) {\n+    if (!UseHeavyMonitors && !UseFastLocking) {\n@@ -2505,3 +2517,13 @@\n-      \/\/ Check if it is still a light weight lock, this is true if we\n-      \/\/ see the stack address of the basicLock in the markWord of the\n-      \/\/ object.\n+      if (UseFastLocking) {\n+        Label slow;\n+        __ fast_unlock(oop, tmp, box, disp_hdr, slow);\n+\n+        \/\/ Indicate success at cont.\n+        __ mv(flag, zr);\n+        __ j(cont);\n+        __ bind(slow);\n+        __ mv(flag, 1); \/\/ Set non-zero flag to indicate 'failure' -> take slow path\n+      } else {\n+        \/\/ Check if it is still a light weight lock, this is true if we\n+        \/\/ see the stack address of the basicLock in the markWord of the\n+        \/\/ object.\n@@ -2509,3 +2531,4 @@\n-      __ cmpxchg(\/*memory address*\/oop, \/*expected value*\/box, \/*new value*\/disp_hdr, Assembler::int64, Assembler::relaxed,\n-                 Assembler::rl, \/*result*\/tmp);\n-      __ xorr(flag, box, tmp); \/\/ box == tmp if cas succeeds\n+        __ cmpxchg(\/*memory address*\/oop, \/*expected value*\/box, \/*new value*\/disp_hdr, Assembler::int64, Assembler::relaxed,\n+                   Assembler::rl, \/*result*\/tmp);\n+        __ xorr(flag, box, tmp); \/\/ box == tmp if cas succeeds\n+      }\n@@ -2523,0 +2546,11 @@\n+\n+    if (UseFastLocking) {\n+      Label L;\n+      __ ld(disp_hdr, Address(tmp, ObjectMonitor::owner_offset_in_bytes()));\n+      __ mv(t0, (unsigned char)(intptr_t)ANONYMOUS_OWNER);\n+      __ bne(disp_hdr, t0, L);\n+      __ mv(flag, 1); \/\/ Indicate failure at cont -- dive into slow-path.\n+      __ j(cont);\n+      __ bind(L);\n+    }\n+\n","filename":"src\/hotspot\/cpu\/riscv\/riscv.ad","additions":77,"deletions":43,"binary":false,"changes":120,"status":"modified"},{"patch":"@@ -1675,25 +1675,30 @@\n-      \/\/ Load (object->mark() | 1) into swap_reg % x10\n-      __ ld(t0, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ ori(swap_reg, t0, 1);\n-\n-      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-      __ sd(swap_reg, Address(lock_reg, mark_word_offset));\n-\n-      \/\/ src -> dest if dest == x10 else x10 <- dest\n-      __ cmpxchg_obj_header(x10, lock_reg, obj_reg, t0, count, \/*fallthrough*\/NULL);\n-\n-      \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n-      \/\/  1) (mark & 3) == 0, and\n-      \/\/  2) sp <= mark < mark + os::pagesize()\n-      \/\/ These 3 tests can be done by evaluating the following\n-      \/\/ expression: ((mark - sp) & (3 - os::vm_page_size())),\n-      \/\/ assuming both stack pointer and pagesize have their\n-      \/\/ least significant 2 bits clear.\n-      \/\/ NOTE: the oopMark is in swap_reg % 10 as the result of cmpxchg\n-\n-      __ sub(swap_reg, swap_reg, sp);\n-      __ andi(swap_reg, swap_reg, 3 - os::vm_page_size());\n-\n-      \/\/ Save the test result, for recursive case, the result is zero\n-      __ sd(swap_reg, Address(lock_reg, mark_word_offset));\n-      __ bnez(swap_reg, slow_path_lock);\n+      if (UseFastLocking) {\n+        __ ld(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ fast_lock(obj_reg, swap_reg, tmp, t0, slow_path_lock);\n+      } else {\n+        \/\/ Load (object->mark() | 1) into swap_reg % x10\n+        __ ld(t0, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ ori(swap_reg, t0, 1);\n+\n+        \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+        __ sd(swap_reg, Address(lock_reg, mark_word_offset));\n+\n+        \/\/ src -> dest if dest == x10 else x10 <- dest\n+        __ cmpxchg_obj_header(x10, lock_reg, obj_reg, t0, count, \/*fallthrough*\/NULL);\n+\n+        \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n+        \/\/  1) (mark & 3) == 0, and\n+        \/\/  2) sp <= mark < mark + os::pagesize()\n+        \/\/ These 3 tests can be done by evaluating the following\n+        \/\/ expression: ((mark - sp) & (3 - os::vm_page_size())),\n+        \/\/ assuming both stack pointer and pagesize have their\n+        \/\/ least significant 2 bits clear.\n+        \/\/ NOTE: the oopMark is in swap_reg % 10 as the result of cmpxchg\n+\n+        __ sub(swap_reg, swap_reg, sp);\n+        __ andi(swap_reg, swap_reg, 3 - os::vm_page_size());\n+\n+        \/\/ Save the test result, for recursive case, the result is zero\n+        __ sd(swap_reg, Address(lock_reg, mark_word_offset));\n+        __ bnez(swap_reg, slow_path_lock);\n+      }\n@@ -1794,1 +1799,1 @@\n-    if (!UseHeavyMonitors) {\n+    if (!UseHeavyMonitors && !UseFastLocking) {\n@@ -1810,9 +1815,14 @@\n-      \/\/ get address of the stack lock\n-      __ la(x10, Address(sp, lock_slot_offset * VMRegImpl::stack_slot_size));\n-      \/\/  get old displaced header\n-      __ ld(old_hdr, Address(x10, 0));\n-\n-      \/\/ Atomic swap old header if oop still contains the stack lock\n-      Label count;\n-      __ cmpxchg_obj_header(x10, old_hdr, obj_reg, t0, count, &slow_path_unlock);\n-      __ bind(count);\n+      if (UseFastLocking) {\n+        __ ld(old_hdr, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ fast_unlock(obj_reg, old_hdr, swap_reg, t0, slow_path_unlock);\n+      } else {\n+        \/\/ get address of the stack lock\n+        __ la(x10, Address(sp, lock_slot_offset * VMRegImpl::stack_slot_size));\n+        \/\/  get old displaced header\n+        __ ld(old_hdr, Address(x10, 0));\n+\n+        \/\/ Atomic swap old header if oop still contains the stack lock\n+        Label count;\n+        __ cmpxchg_obj_header(x10, old_hdr, obj_reg, t0, count, &slow_path_unlock);\n+        __ bind(count);\n+      }\n","filename":"src\/hotspot\/cpu\/riscv\/sharedRuntime_riscv.cpp","additions":45,"deletions":35,"binary":false,"changes":80,"status":"modified"},{"patch":"@@ -48,1 +48,1 @@\n-void C2_MacroAssembler::verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub) {\n+void C2_MacroAssembler::verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub, int max_monitors) {\n@@ -130,0 +130,14 @@\n+#ifdef _LP64\n+  if (UseFastLocking && max_monitors > 0) {\n+    C2CheckLockStackStub* stub = new (Compile::current()->comp_arena()) C2CheckLockStackStub();\n+    Compile::current()->output()->add_stub(stub);\n+    assert(!is_stub, \"only methods have monitors\");\n+    Register thread = r15_thread;\n+    movptr(rax, Address(thread, JavaThread::lock_stack_current_offset()));\n+    addptr(rax, max_monitors * oopSize);\n+    cmpptr(rax, Address(thread, JavaThread::lock_stack_limit_offset()));\n+    jcc(Assembler::greaterEqual, stub->entry());\n+    bind(stub->continuation());\n+  }\n+#endif\n+\n@@ -551,1 +565,1 @@\n-                                 Register scrReg, Register cx1Reg, Register cx2Reg,\n+                                 Register scrReg, Register cx1Reg, Register cx2Reg, Register thread,\n@@ -604,14 +618,32 @@\n-    \/\/ Attempt stack-locking ...\n-    orptr (tmpReg, markWord::unlocked_value);\n-    movptr(Address(boxReg, 0), tmpReg);          \/\/ Anticipate successful CAS\n-    lock();\n-    cmpxchgptr(boxReg, Address(objReg, oopDesc::mark_offset_in_bytes()));      \/\/ Updates tmpReg\n-    jcc(Assembler::equal, COUNT);           \/\/ Success\n-\n-    \/\/ Recursive locking.\n-    \/\/ The object is stack-locked: markword contains stack pointer to BasicLock.\n-    \/\/ Locked by current thread if difference with current SP is less than one page.\n-    subptr(tmpReg, rsp);\n-    \/\/ Next instruction set ZFlag == 1 (Success) if difference is less then one page.\n-    andptr(tmpReg, (int32_t) (NOT_LP64(0xFFFFF003) LP64_ONLY(7 - os::vm_page_size())) );\n-    movptr(Address(boxReg, 0), tmpReg);\n+    if (UseFastLocking) {\n+#ifdef _LP64\n+      fast_lock_impl(objReg, tmpReg, thread, scrReg, NO_COUNT, false);\n+      jmp(COUNT);\n+#else\n+      \/\/ We can not emit the lock-stack-check in verified_entry() because we don't have enough\n+      \/\/ registers (for thread ptr). Therefor we have to emit the lock-stack-check in\n+      \/\/ fast_lock_impl(). However, that check can take a slow-path with ZF=1, therefore\n+      \/\/ we need to handle it specially and force ZF=0 before taking the actual slow-path.\n+      Label slow;\n+      fast_lock_impl(objReg, tmpReg, thread, scrReg, slow);\n+      jmp(COUNT);\n+      bind(slow);\n+      testptr(objReg, objReg); \/\/ ZF=0 to indicate failure\n+      jmp(NO_COUNT);\n+#endif\n+    } else {\n+      \/\/ Attempt stack-locking ...\n+      orptr (tmpReg, markWord::unlocked_value);\n+      movptr(Address(boxReg, 0), tmpReg);          \/\/ Anticipate successful CAS\n+      lock();\n+      cmpxchgptr(boxReg, Address(objReg, oopDesc::mark_offset_in_bytes()));      \/\/ Updates tmpReg\n+      jcc(Assembler::equal, COUNT);           \/\/ Success\n+\n+      \/\/ Recursive locking.\n+      \/\/ The object is stack-locked: markword contains stack pointer to BasicLock.\n+      \/\/ Locked by current thread if difference with current SP is less than one page.\n+      subptr(tmpReg, rsp);\n+      \/\/ Next instruction set ZFlag == 1 (Success) if difference is less then one page.\n+      andptr(tmpReg, (int32_t) (NOT_LP64(0xFFFFF003) LP64_ONLY(7 - os::vm_page_size())) );\n+      movptr(Address(boxReg, 0), tmpReg);\n+    }\n@@ -661,1 +693,1 @@\n-  cmpxchgptr(scrReg, Address(boxReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));\n+  cmpxchgptr(thread, Address(boxReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));\n@@ -663,7 +695,0 @@\n-  \/\/ If we weren't able to swing _owner from NULL to the BasicLock\n-  \/\/ then take the slow path.\n-  jccb  (Assembler::notZero, NO_COUNT);\n-  \/\/ update _owner from BasicLock to thread\n-  get_thread (scrReg);                    \/\/ beware: clobbers ICCs\n-  movptr(Address(boxReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), scrReg);\n-  xorptr(boxReg, boxReg);                 \/\/ set icc.ZFlag = 1 to indicate success\n@@ -775,1 +800,1 @@\n-  if (!UseHeavyMonitors) {\n+  if (!UseHeavyMonitors && !UseFastLocking) {\n@@ -782,1 +807,11 @@\n-    jccb   (Assembler::zero, Stacked);\n+#if INCLUDE_RTM_OPT\n+    if (UseFastLocking && use_rtm) {\n+      jcc(Assembler::zero, Stacked);\n+    } else\n+#endif\n+    jccb(Assembler::zero, Stacked);\n+    if (UseFastLocking) {\n+      \/\/ If the owner is ANONYMOUS, we need to fix it - in the slow-path.\n+      testptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), (int32_t) (intptr_t) ANONYMOUS_OWNER);\n+      jcc(Assembler::notEqual, NO_COUNT);\n+    }\n@@ -794,1 +829,1 @@\n-    jmpb(DONE_LABEL);\n+    jmp(DONE_LABEL);\n@@ -908,3 +943,9 @@\n-    movptr(tmpReg, Address (boxReg, 0));      \/\/ re-fetch\n-    lock();\n-    cmpxchgptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); \/\/ Uses RAX which is box\n+    if (UseFastLocking) {\n+      mov(boxReg, tmpReg);\n+      fast_unlock_impl(objReg, boxReg, tmpReg, NO_COUNT);\n+      jmp(COUNT);\n+    } else {\n+      movptr(tmpReg, Address (boxReg, 0));      \/\/ re-fetch\n+      lock();\n+      cmpxchgptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); \/\/ Uses RAX which is box\n+    }\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":71,"deletions":30,"binary":false,"changes":101,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n-  void verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub);\n+  void verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub, int max_monitors);\n@@ -39,1 +39,1 @@\n-                 Register scr, Register cx1, Register cx2,\n+                 Register scr, Register cx1, Register cx2, Register thread,\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n@@ -4173,1 +4174,1 @@\n-  assert((offset_in_bytes & (BytesPerWord - 1)) == 0, \"offset must be a multiple of BytesPerWord\");\n+  assert((offset_in_bytes & (BytesPerInt - 1)) == 0, \"offset must be a multiple of BytesPerInt\");\n@@ -4179,0 +4180,13 @@\n+  \/\/ Emit single 32bit store to clear leading bytes, if necessary.\n+  xorptr(temp, temp);    \/\/ use _zero reg to clear memory (shorter code)\n+#ifdef _LP64\n+  if (!is_aligned(offset_in_bytes, BytesPerWord)) {\n+    movl(Address(address, offset_in_bytes), temp);\n+    offset_in_bytes += BytesPerInt;\n+    decrement(length_in_bytes, BytesPerInt);\n+  }\n+  assert((offset_in_bytes & (BytesPerWord - 1)) == 0, \"offset must be a multiple of BytesPerWord\");\n+  testptr(length_in_bytes, length_in_bytes);\n+  jcc(Assembler::zero, done);\n+#endif\n+\n@@ -4191,1 +4205,0 @@\n-  xorptr(temp, temp);    \/\/ use _zero reg to clear memory (shorter code)\n@@ -5123,9 +5136,13 @@\n-void MacroAssembler::load_klass(Register dst, Register src, Register tmp) {\n-  assert_different_registers(src, tmp);\n-  assert_different_registers(dst, tmp);\n-  if (UseCompressedClassPointers) {\n-    movl(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n-    decode_klass_not_null(dst, tmp);\n-  } else\n-#endif\n-    movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+void MacroAssembler::load_nklass(Register dst, Register src) {\n+  assert(UseCompressedClassPointers, \"expect compressed class pointers\");\n+\n+  Label fast;\n+  movq(dst, Address(src, oopDesc::mark_offset_in_bytes()));\n+  testb(dst, markWord::monitor_value);\n+  jccb(Assembler::zero, fast);\n+\n+  \/\/ Fetch displaced header\n+  movq(dst, Address(dst, OM_OFFSET_NO_MONITOR_VALUE_TAG(header)));\n+\n+  bind(fast);\n+  shrq(dst, markWord::klass_shift);\n@@ -5134,0 +5151,1 @@\n+#endif\n@@ -5135,1 +5153,1 @@\n-void MacroAssembler::store_klass(Register dst, Register src, Register tmp) {\n+void MacroAssembler::load_klass(Register dst, Register src, Register tmp, bool null_check_src) {\n@@ -5139,4 +5157,11 @@\n-  if (UseCompressedClassPointers) {\n-    encode_klass_not_null(src, tmp);\n-    movl(Address(dst, oopDesc::klass_offset_in_bytes()), src);\n-  } else\n+  assert(UseCompressedClassPointers, \"expect compressed class pointers\");\n+  if (null_check_src) {\n+    null_check(src, oopDesc::mark_offset_in_bytes());\n+  }\n+  load_nklass(dst, src);\n+  decode_klass_not_null(dst, tmp);\n+#else\n+  if (null_check_src) {\n+    null_check(src, oopDesc::klass_offset_in_bytes());\n+  }\n+  movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n@@ -5144,1 +5169,0 @@\n-    movptr(Address(dst, oopDesc::klass_offset_in_bytes()), src);\n@@ -5147,0 +5171,6 @@\n+#ifndef _LP64\n+void MacroAssembler::store_klass(Register dst, Register src) {\n+  movptr(Address(dst, oopDesc::klass_offset_in_bytes()), src);\n+}\n+#endif\n+\n@@ -5193,7 +5223,0 @@\n-void MacroAssembler::store_klass_gap(Register dst, Register src) {\n-  if (UseCompressedClassPointers) {\n-    \/\/ Store to klass gap in destination\n-    movl(Address(dst, oopDesc::klass_gap_offset_in_bytes()), src);\n-  }\n-}\n-\n@@ -5351,0 +5374,62 @@\n+MacroAssembler::KlassDecodeMode MacroAssembler::_klass_decode_mode = KlassDecodeNone;\n+\n+\/\/ Returns a static string\n+const char* MacroAssembler::describe_klass_decode_mode(MacroAssembler::KlassDecodeMode mode) {\n+  switch (mode) {\n+  case KlassDecodeNone: return \"none\";\n+  case KlassDecodeZero: return \"zero\";\n+  case KlassDecodeXor:  return \"xor\";\n+  case KlassDecodeAdd:  return \"add\";\n+  default:\n+    ShouldNotReachHere();\n+  }\n+  return NULL;\n+}\n+\n+\/\/ Return the current narrow Klass pointer decode mode.\n+MacroAssembler::KlassDecodeMode MacroAssembler::klass_decode_mode() {\n+  if (_klass_decode_mode == KlassDecodeNone) {\n+    \/\/ First time initialization\n+    assert(UseCompressedClassPointers, \"not using compressed class pointers\");\n+    assert(Metaspace::initialized(), \"metaspace not initialized yet\");\n+\n+    _klass_decode_mode = klass_decode_mode_for_base(CompressedKlassPointers::base());\n+    guarantee(_klass_decode_mode != KlassDecodeNone,\n+              PTR_FORMAT \" is not a valid encoding base on aarch64\",\n+              p2i(CompressedKlassPointers::base()));\n+    log_info(metaspace)(\"klass decode mode initialized: %s\", describe_klass_decode_mode(_klass_decode_mode));\n+  }\n+  return _klass_decode_mode;\n+}\n+\n+\/\/ Given an arbitrary base address, return the KlassDecodeMode that would be used. Return KlassDecodeNone\n+\/\/ if base address is not valid for encoding.\n+MacroAssembler::KlassDecodeMode MacroAssembler::klass_decode_mode_for_base(address base) {\n+  assert(CompressedKlassPointers::shift() != 0, \"not lilliput?\");\n+\n+  const uint64_t base_u64 = (uint64_t) base;\n+\n+  if (base_u64 == 0) {\n+    return KlassDecodeZero;\n+  }\n+\n+  if ((base_u64 & (KlassEncodingMetaspaceMax - 1)) == 0) {\n+    return KlassDecodeXor;\n+  }\n+\n+  \/\/ Note that there is no point in optimizing for shift=3 since lilliput\n+  \/\/ will use larger shifts\n+\n+  \/\/ The add+shift mode for decode_and_move_klass_not_null() requires the base to be\n+  \/\/  shiftable-without-loss. So, this is the minimum restriction on x64 for a valid\n+  \/\/  encoding base. This does not matter in reality since the shift values we use for\n+  \/\/  Lilliput, while large, won't be larger than a page size. And the encoding base\n+  \/\/  will be quite likely page aligned since it usually falls to the beginning of\n+  \/\/  either CDS or CCS.\n+  if ((base_u64 & (KlassAlignmentInBytes - 1)) == 0) {\n+    return KlassDecodeAdd;\n+  }\n+\n+  return KlassDecodeNone;\n+}\n+\n@@ -5353,1 +5438,12 @@\n-  if (CompressedKlassPointers::base() != NULL) {\n+  switch (klass_decode_mode()) {\n+  case KlassDecodeZero: {\n+    shrq(r, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  case KlassDecodeXor: {\n+    mov64(tmp, (int64_t)CompressedKlassPointers::base());\n+    xorq(r, tmp);\n+    shrq(r, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  case KlassDecodeAdd: {\n@@ -5356,0 +5452,2 @@\n+    shrq(r, CompressedKlassPointers::shift());\n+    break;\n@@ -5357,3 +5455,2 @@\n-  if (CompressedKlassPointers::shift() != 0) {\n-    assert (LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-    shrq(r, LogKlassAlignmentInBytes);\n+  default:\n+    ShouldNotReachHere();\n@@ -5365,1 +5462,13 @@\n-  if (CompressedKlassPointers::base() != NULL) {\n+  switch (klass_decode_mode()) {\n+  case KlassDecodeZero: {\n+    movptr(dst, src);\n+    shrq(dst, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  case KlassDecodeXor: {\n+    mov64(dst, (int64_t)CompressedKlassPointers::base());\n+    xorq(dst, src);\n+    shrq(dst, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  case KlassDecodeAdd: {\n@@ -5368,2 +5477,2 @@\n-  } else {\n-    movptr(dst, src);\n+    shrq(dst, CompressedKlassPointers::shift());\n+    break;\n@@ -5371,3 +5480,2 @@\n-  if (CompressedKlassPointers::shift() != 0) {\n-    assert (LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-    shrq(dst, LogKlassAlignmentInBytes);\n+  default:\n+    ShouldNotReachHere();\n@@ -5379,8 +5487,5 @@\n-  \/\/ Note: it will change flags\n-  assert(UseCompressedClassPointers, \"should only be used for compressed headers\");\n-  \/\/ Cannot assert, unverified entry point counts instructions (see .ad file)\n-  \/\/ vtableStubs also counts instructions in pd_code_size_limit.\n-  \/\/ Also do not verify_oop as this is called by verify_oop.\n-  if (CompressedKlassPointers::shift() != 0) {\n-    assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-    shlq(r, LogKlassAlignmentInBytes);\n+  const uint64_t base_u64 = (uint64_t)CompressedKlassPointers::base();\n+  switch (klass_decode_mode()) {\n+  case KlassDecodeZero: {\n+    shlq(r, CompressedKlassPointers::shift());\n+    break;\n@@ -5388,2 +5493,11 @@\n-  if (CompressedKlassPointers::base() != NULL) {\n-    mov64(tmp, (int64_t)CompressedKlassPointers::base());\n+  case KlassDecodeXor: {\n+    assert((base_u64 & (KlassEncodingMetaspaceMax - 1)) == 0,\n+           \"base \" UINT64_FORMAT_X \" invalid for xor mode\", base_u64); \/\/ should have been handled at VM init.\n+    shlq(r, CompressedKlassPointers::shift());\n+    mov64(tmp, base_u64);\n+    xorq(r, tmp);\n+    break;\n+  }\n+  case KlassDecodeAdd: {\n+    shlq(r, CompressedKlassPointers::shift());\n+    mov64(tmp, base_u64);\n@@ -5391,0 +5505,4 @@\n+    break;\n+  }\n+  default:\n+    ShouldNotReachHere();\n@@ -5396,3 +5514,1 @@\n-  \/\/ Note: it will change flags\n-  assert (UseCompressedClassPointers, \"should only be used for compressed headers\");\n-  \/\/ Cannot assert, unverified entry point counts instructions (see .ad file)\n+  \/\/ Note: Cannot assert, unverified entry point counts instructions (see .ad file)\n@@ -5402,18 +5518,28 @@\n-  if (CompressedKlassPointers::base() == NULL &&\n-      CompressedKlassPointers::shift() == 0) {\n-    \/\/ The best case scenario is that there is no base or shift. Then it is already\n-    \/\/ a pointer that needs nothing but a register rename.\n-    movl(dst, src);\n-  } else {\n-    if (CompressedKlassPointers::base() != NULL) {\n-      mov64(dst, (int64_t)CompressedKlassPointers::base());\n-    } else {\n-      xorq(dst, dst);\n-    }\n-    if (CompressedKlassPointers::shift() != 0) {\n-      assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-      assert(LogKlassAlignmentInBytes == Address::times_8, \"klass not aligned on 64bits?\");\n-      leaq(dst, Address(dst, src, Address::times_8, 0));\n-    } else {\n-      addq(dst, src);\n-    }\n+  const uint64_t base_u64 = (uint64_t)CompressedKlassPointers::base();\n+\n+  switch (klass_decode_mode()) {\n+  case KlassDecodeZero: {\n+    movq(dst, src);\n+    shlq(dst, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  case KlassDecodeXor: {\n+    assert((base_u64 & (KlassEncodingMetaspaceMax - 1)) == 0,\n+           \"base \" UINT64_FORMAT_X \" invalid for xor mode\", base_u64); \/\/ should have been handled at VM init.\n+    const uint64_t base_right_shifted = base_u64 >> CompressedKlassPointers::shift();\n+    mov64(dst, base_right_shifted);\n+    xorq(dst, src);\n+    shlq(dst, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  case KlassDecodeAdd: {\n+    assert((base_u64 & (KlassAlignmentInBytes - 1)) == 0,\n+           \"base \" UINT64_FORMAT_X \" invalid for add mode\", base_u64); \/\/ should have been handled at VM init.\n+    const uint64_t base_right_shifted = base_u64 >> CompressedKlassPointers::shift();\n+    mov64(dst, base_right_shifted);\n+    addq(dst, src);\n+    shlq(dst, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  default:\n+    ShouldNotReachHere();\n@@ -9674,0 +9800,58 @@\n+\n+void MacroAssembler::fast_lock_impl(Register obj, Register hdr, Register thread, Register tmp, Label& slow, bool rt_check_stack) {\n+  assert(hdr == rax, \"header must be in rax for cmpxchg\");\n+  assert_different_registers(obj, hdr, thread, tmp);\n+\n+  \/\/ First we need to check if the lock-stack has room for pushing the object reference.\n+  if (rt_check_stack) {\n+    movptr(tmp, Address(thread, JavaThread::lock_stack_current_offset()));\n+    cmpptr(tmp, Address(thread, JavaThread::lock_stack_limit_offset()));\n+    jcc(Assembler::greaterEqual, slow);\n+  }\n+#ifdef ASSERT\n+  else {\n+    Label ok;\n+    movptr(tmp, Address(thread, JavaThread::lock_stack_current_offset()));\n+    cmpptr(tmp, Address(thread, JavaThread::lock_stack_limit_offset()));\n+    jcc(Assembler::less, ok);\n+    stop(\"Not enough room in lock stack; should have been checked in the method prologue\");\n+    bind(ok);\n+  }\n+#endif\n+\n+  \/\/ Now we attempt to take the fast-lock.\n+  \/\/ Clear lowest two header bits (locked state).\n+  andptr(hdr, ~(int32_t )markWord::lock_mask_in_place);\n+  movptr(tmp, hdr);\n+  \/\/ Set lowest bit (unlocked state).\n+  orptr(hdr, markWord::unlocked_value);\n+  lock();\n+  cmpxchgptr(tmp, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  jcc(Assembler::notEqual, slow);\n+\n+  \/\/ If successful, push object to lock-stack.\n+  movptr(tmp, Address(thread, JavaThread::lock_stack_current_offset()));\n+  movptr(Address(tmp, 0), obj);\n+  increment(tmp, oopSize);\n+  movptr(Address(thread, JavaThread::lock_stack_current_offset()), tmp);\n+}\n+\n+void MacroAssembler::fast_unlock_impl(Register obj, Register hdr, Register tmp, Label& slow) {\n+  assert(hdr == rax, \"header must be in rax for cmpxchg\");\n+  assert_different_registers(obj, hdr, tmp);\n+\n+  \/\/ Mark-word must be 00 now, try to swing it back to 01 (unlocked)\n+  movptr(tmp, hdr); \/\/ The expected old value\n+  orptr(tmp, markWord::unlocked_value);\n+  lock();\n+  cmpxchgptr(tmp, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  jcc(Assembler::notEqual, slow);\n+  \/\/ Pop the lock object from the lock-stack.\n+#ifdef _LP64\n+  const Register thread = r15_thread;\n+#else\n+  const Register thread = rax;\n+  get_thread(rax);\n+#endif\n+  subptr(Address(thread, JavaThread::lock_stack_current_offset()), oopSize);\n+}\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":249,"deletions":65,"binary":false,"changes":314,"status":"modified"},{"patch":"@@ -82,0 +82,23 @@\n+ public:\n+\n+  enum KlassDecodeMode {\n+    KlassDecodeNone,\n+    KlassDecodeZero,\n+    KlassDecodeXor,\n+    KlassDecodeAdd\n+  };\n+\n+  \/\/ Return the current narrow Klass pointer decode mode. Initialized on first call.\n+  static KlassDecodeMode klass_decode_mode();\n+\n+  \/\/ Given an arbitrary base address, return the KlassDecodeMode that would be used. Return KlassDecodeNone\n+  \/\/ if base address is not valid for encoding.\n+  static KlassDecodeMode klass_decode_mode_for_base(address base);\n+\n+  \/\/ Returns a static string\n+  static const char* describe_klass_decode_mode(KlassDecodeMode mode);\n+\n+ private:\n+\n+  static KlassDecodeMode _klass_decode_mode;\n+\n@@ -351,2 +374,6 @@\n-  void load_klass(Register dst, Register src, Register tmp);\n-  void store_klass(Register dst, Register src, Register tmp);\n+  void load_klass(Register dst, Register src, Register tmp, bool null_check_src = false);\n+#ifdef _LP64\n+  void load_nklass(Register dst, Register src);\n+#else\n+  void store_klass(Register dst, Register src);\n+#endif\n@@ -371,2 +398,0 @@\n-  void store_klass_gap(Register dst, Register src);\n-\n@@ -853,0 +878,1 @@\n+  void testptr(Address  src, int32_t imm32) {  LP64_ONLY(testq(src, imm32)) NOT_LP64(testl(src, imm32)); }\n@@ -1997,0 +2023,2 @@\n+  void fast_lock_impl(Register obj, Register hdr, Register thread, Register tmp, Label& slow, bool rt_check_stack = true);\n+  void fast_unlock_impl(Register obj, Register hdr, Register tmp, Label& slow);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":32,"deletions":4,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -2148,0 +2148,7 @@\n+      if (UseFastLocking) {\n+        \/\/ Load object header\n+        __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ fast_lock_impl(obj_reg, swap_reg, r15_thread, rscratch1, slow_path_lock);\n+      } else {\n+        \/\/ Load immediate 1 into swap_reg %rax\n+        __ movl(swap_reg, 1);\n@@ -2149,5 +2156,2 @@\n-      \/\/ Load immediate 1 into swap_reg %rax\n-      __ movl(swap_reg, 1);\n-\n-      \/\/ Load (object->mark() | 1) into swap_reg %rax\n-      __ orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        \/\/ Load (object->mark() | 1) into swap_reg %rax\n+        __ orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n@@ -2155,2 +2159,2 @@\n-      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-      __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n+        \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+        __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n@@ -2158,4 +2162,4 @@\n-      \/\/ src -> dest iff dest == rax else rax <- dest\n-      __ lock();\n-      __ cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ jcc(Assembler::equal, count_mon);\n+        \/\/ src -> dest iff dest == rax else rax <- dest\n+        __ lock();\n+        __ cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ jcc(Assembler::equal, count_mon);\n@@ -2163,1 +2167,1 @@\n-      \/\/ Hmm should this move to the slow path code area???\n+        \/\/ Hmm should this move to the slow path code area???\n@@ -2165,8 +2169,8 @@\n-      \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n-      \/\/  1) (mark & 3) == 0, and\n-      \/\/  2) rsp <= mark < mark + os::pagesize()\n-      \/\/ These 3 tests can be done by evaluating the following\n-      \/\/ expression: ((mark - rsp) & (3 - os::vm_page_size())),\n-      \/\/ assuming both stack pointer and pagesize have their\n-      \/\/ least significant 2 bits clear.\n-      \/\/ NOTE: the oopMark is in swap_reg %rax as the result of cmpxchg\n+        \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n+        \/\/  1) (mark & 3) == 0, and\n+        \/\/  2) rsp <= mark < mark + os::pagesize()\n+        \/\/ These 3 tests can be done by evaluating the following\n+        \/\/ expression: ((mark - rsp) & (3 - os::vm_page_size())),\n+        \/\/ assuming both stack pointer and pagesize have their\n+        \/\/ least significant 2 bits clear.\n+        \/\/ NOTE: the oopMark is in swap_reg %rax as the result of cmpxchg\n@@ -2174,2 +2178,2 @@\n-      __ subptr(swap_reg, rsp);\n-      __ andptr(swap_reg, 3 - os::vm_page_size());\n+        __ subptr(swap_reg, rsp);\n+        __ andptr(swap_reg, 3 - os::vm_page_size());\n@@ -2177,3 +2181,4 @@\n-      \/\/ Save the test result, for recursive case, the result is zero\n-      __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n-      __ jcc(Assembler::notEqual, slow_path_lock);\n+        \/\/ Save the test result, for recursive case, the result is zero\n+        __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n+        __ jcc(Assembler::notEqual, slow_path_lock);\n+      }\n@@ -2293,1 +2298,1 @@\n-    if (!UseHeavyMonitors) {\n+    if (!UseHeavyMonitors && !UseFastLocking) {\n@@ -2309,9 +2314,15 @@\n-      \/\/ get address of the stack lock\n-      __ lea(rax, Address(rsp, lock_slot_offset * VMRegImpl::stack_slot_size));\n-      \/\/  get old displaced header\n-      __ movptr(old_hdr, Address(rax, 0));\n-\n-      \/\/ Atomic swap old header if oop still contains the stack lock\n-      __ lock();\n-      __ cmpxchgptr(old_hdr, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ jcc(Assembler::notEqual, slow_path_unlock);\n+      if (UseFastLocking) {\n+        __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ andptr(swap_reg, ~(int32_t)markWord::lock_mask_in_place);\n+        __ fast_unlock_impl(obj_reg, swap_reg, lock_reg, slow_path_unlock);\n+      } else {\n+        \/\/ get address of the stack lock\n+        __ lea(rax, Address(rsp, lock_slot_offset * VMRegImpl::stack_slot_size));\n+        \/\/  get old displaced header\n+        __ movptr(old_hdr, Address(rax, 0));\n+\n+        \/\/ Atomic swap old header if oop still contains the stack lock\n+        __ lock();\n+        __ cmpxchgptr(old_hdr, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ jcc(Assembler::notEqual, slow_path_unlock);\n+      }\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":46,"deletions":35,"binary":false,"changes":81,"status":"modified"},{"patch":"@@ -3041,0 +3041,49 @@\n+\/\/ Call runtime to ensure lock-stack size.\n+\/\/ Arguments:\n+\/\/ - c_rarg0: the required _limit pointer\n+address StubGenerator::generate_check_lock_stack() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"check_lock_stack\");\n+  address start = __ pc();\n+\n+  BLOCK_COMMENT(\"Entry:\");\n+  __ enter(); \/\/ save rbp\n+\n+  __ pusha();\n+\n+  \/\/ The method may have floats as arguments, and we must spill them before calling\n+  \/\/ the VM runtime.\n+  assert(Argument::n_float_register_parameters_j == 8, \"Assumption\");\n+  const int xmm_size = wordSize * 2;\n+  const int xmm_spill_size = xmm_size * Argument::n_float_register_parameters_j;\n+  __ subptr(rsp, xmm_spill_size);\n+  __ movdqu(Address(rsp, xmm_size * 7), xmm7);\n+  __ movdqu(Address(rsp, xmm_size * 6), xmm6);\n+  __ movdqu(Address(rsp, xmm_size * 5), xmm5);\n+  __ movdqu(Address(rsp, xmm_size * 4), xmm4);\n+  __ movdqu(Address(rsp, xmm_size * 3), xmm3);\n+  __ movdqu(Address(rsp, xmm_size * 2), xmm2);\n+  __ movdqu(Address(rsp, xmm_size * 1), xmm1);\n+  __ movdqu(Address(rsp, xmm_size * 0), xmm0);\n+\n+  __ call_VM_leaf(CAST_FROM_FN_PTR(address, static_cast<void (*)(oop*)>(LockStack::ensure_lock_stack_size)), rax);\n+\n+  __ movdqu(xmm0, Address(rsp, xmm_size * 0));\n+  __ movdqu(xmm1, Address(rsp, xmm_size * 1));\n+  __ movdqu(xmm2, Address(rsp, xmm_size * 2));\n+  __ movdqu(xmm3, Address(rsp, xmm_size * 3));\n+  __ movdqu(xmm4, Address(rsp, xmm_size * 4));\n+  __ movdqu(xmm5, Address(rsp, xmm_size * 5));\n+  __ movdqu(xmm6, Address(rsp, xmm_size * 6));\n+  __ movdqu(xmm7, Address(rsp, xmm_size * 7));\n+  __ addptr(rsp, xmm_spill_size);\n+\n+  __ popa();\n+\n+  __ leave();\n+\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n@@ -3878,0 +3927,3 @@\n+  if (UseFastLocking) {\n+    StubRoutines::x86::_check_lock_stack = generate_check_lock_stack();\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":52,"deletions":0,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -85,0 +85,1 @@\n+address StubRoutines::x86::_check_lock_stack = NULL;\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -129,0 +129,2 @@\n+  static address _check_lock_stack;\n+\n@@ -216,0 +218,2 @@\n+  static address check_lock_stack() { return _check_lock_stack; }\n+\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -925,1 +925,2 @@\n-  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, false, C->stub_function() != NULL);\n+  int max_monitors = C->method() != NULL ? C->max_monitors() : 0;\n+  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, false, C->stub_function() != NULL, max_monitors);\n@@ -5315,1 +5316,1 @@\n-instruct loadNKlass(rRegN dst, memory mem)\n+instruct loadNKlass(rRegN dst, indOffset8 mem, rFlagsReg cr)\n@@ -5318,1 +5319,1 @@\n-\n+  effect(TEMP_DEF dst, KILL cr);\n@@ -5322,1 +5323,11 @@\n-    __ movl($dst$$Register, $mem$$Address);\n+    assert($mem$$disp == oopDesc::klass_offset_in_bytes(), \"expect correct offset 4, but got: %d\", $mem$$disp);\n+    assert($mem$$index == 4, \"expect no index register: %d\", $mem$$index);\n+    Register dst = $dst$$Register;\n+    Register obj = $mem$$base$$Register;\n+    C2LoadNKlassStub* stub = new (Compile::current()->comp_arena()) C2LoadNKlassStub(dst);\n+    Compile::current()->output()->add_stub(stub);\n+    __ movq(dst, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    __ testb(dst, markWord::monitor_value);\n+    __ jcc(Assembler::notZero, stub->entry());\n+    __ bind(stub->continuation());\n+    __ shrq(dst, markWord::klass_shift);\n@@ -5324,1 +5335,1 @@\n-  ins_pipe(ialu_reg_mem); \/\/ XXX\n+  ins_pipe(pipe_slow); \/\/ XXX\n@@ -12676,0 +12687,3 @@\n+\/\/ Disabled because the compressed Klass* in header cannot be safely\n+\/\/ accessed. TODO: Re-enable it as soon as synchronization does not\n+\/\/ overload the upper header bits anymore.\n@@ -12678,0 +12692,1 @@\n+  predicate(false);\n@@ -13364,1 +13379,1 @@\n-                 $scr$$Register, $cx1$$Register, $cx2$$Register,\n+                 $scr$$Register, $cx1$$Register, $cx2$$Register, r15_thread,\n@@ -13380,1 +13395,1 @@\n-                 $scr$$Register, $cx1$$Register, noreg, NULL, NULL, NULL, false, false);\n+                 $scr$$Register, $cx1$$Register, noreg, r15_thread, NULL, NULL, NULL, false, false);\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":22,"deletions":7,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -1244,1 +1244,2 @@\n-  __ load_klass(obj, klass, null_check_info);\n+  CodeStub* slow_path = new LoadKlassStub(klass);\n+  __ load_klass(obj, klass, null_check_info, slow_path);\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n@@ -43,0 +44,1 @@\n+#include \"oops\/klass.inline.hpp\"\n@@ -222,2 +224,4 @@\n-    \/\/ See RunTimeClassInfo::get_for()\n-    _estimated_metaspaceobj_bytes += align_up(BytesPerWord, SharedSpaceObjectAlignment);\n+    \/\/ See ArchiveBuilder::make_shallow_copies: make sure we have enough space for both maximum\n+    \/\/ Klass alignment as well as the RuntimeInfo* pointer we will embed in front of a Klass.\n+    _estimated_metaspaceobj_bytes += align_up(BytesPerWord, KlassAlignmentInBytes) +\n+        align_up(sizeof(void*), SharedSpaceObjectAlignment);\n@@ -620,4 +624,5 @@\n-    \/\/ Save a pointer immediate in front of an InstanceKlass, so\n-    \/\/ we can do a quick lookup from InstanceKlass* -> RunTimeClassInfo*\n-    \/\/ without building another hashtable. See RunTimeClassInfo::get_for()\n-    \/\/ in systemDictionaryShared.cpp.\n+    \/\/ Reserve space for a pointer immediately in front of an InstanceKlass. That space will\n+    \/\/ later be used to store the RuntimeClassInfo* pointer directly in front of the archived\n+    \/\/ InstanceKlass, in order to have a quick lookup InstanceKlass* -> RunTimeClassInfo*\n+    \/\/ without building another hashtable. See RunTimeClassInfo::get_for()\/::set_for() for\n+    \/\/ details.\n@@ -629,0 +634,3 @@\n+    dest = dump_region->allocate(bytes, KlassAlignmentInBytes);\n+  } else {\n+    dest = dump_region->allocate(bytes);\n@@ -630,1 +638,0 @@\n-  dest = dump_region->allocate(bytes);\n@@ -649,1 +656,2 @@\n-  log_trace(cds)(\"Copy: \" PTR_FORMAT \" ==> \" PTR_FORMAT \" %d\", p2i(src), p2i(dest), bytes);\n+  log_trace(cds)(\"Copy: \" PTR_FORMAT \" ==> \" PTR_FORMAT \" %d (%s)\", p2i(src), p2i(dest), bytes,\n+                 MetaspaceObj::type_name(ref->msotype()));\n@@ -653,0 +661,2 @@\n+\n+  DEBUG_ONLY(_alloc_stats.verify((int)dump_region->used(), src_info->read_only()));\n@@ -749,0 +759,7 @@\n+    Klass* requested_k = to_requested(k);\n+#ifdef _LP64\n+    narrowKlass nk = CompressedKlassPointers::encode_not_null(requested_k, _requested_static_archive_bottom);\n+    k->set_prototype_header(markWord::prototype().set_narrow_klass(nk));\n+#else\n+    k->set_prototype_header(markWord::prototype());\n+#endif\n@@ -840,1 +857,3 @@\n-  o->set_narrow_klass(nk);\n+#ifdef _LP64\n+  o->set_mark(o->mark().set_narrow_klass(nk));\n+#endif\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.cpp","additions":28,"deletions":9,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"memory\/metaspace\/metaspaceAlignment.hpp\"\n@@ -46,3 +47,13 @@\n-\/\/ Metaspace::allocate() requires that all blocks must be aligned with KlassAlignmentInBytes.\n-\/\/ We enforce the same alignment rule in blocks allocated from the shared space.\n-const int SharedSpaceObjectAlignment = KlassAlignmentInBytes;\n+\/\/ CDS has three alignments to deal with:\n+\/\/ - SharedSpaceObjectAlignment, always 8 bytes: used for placing arbitrary structures.\n+\/\/   These may contain 64-bit members (not larger, we know that much). Therefore we\n+\/\/   need to use 64-bit alignment on both 32-bit and 64-bit platforms. We reuse metaspace\n+\/\/   minimal alignment for this, which follows the same logic.\n+\/\/ - With CompressedClassPointers=1, we need to store Klass structures with a large\n+\/\/   alignment (Lilliput specific narrow Klass pointer encoding) - KlassAlignmentInBytes.\n+\/\/ - Header data and tags are squeezed in with word alignment, which happens to be 4 bytes\n+\/\/   on 32-bit. See ReadClosure::do_xxx() and DumpRegion::append_intptr().\n+const int SharedSpaceObjectAlignment = metaspace::MetaspaceMinAlignmentBytes;\n+\n+\/\/ standard alignment should be sufficient for storing 64-bit values.\n+STATIC_ASSERT(SharedSpaceObjectAlignment >= sizeof(uint64_t));\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.hpp","additions":14,"deletions":3,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -208,3 +208,13 @@\n-char* DumpRegion::allocate(size_t num_bytes) {\n-  char* p = (char*)align_up(_top, (size_t)SharedSpaceObjectAlignment);\n-  char* newtop = p + align_up(num_bytes, (size_t)SharedSpaceObjectAlignment);\n+char* DumpRegion::allocate(size_t num_bytes, size_t alignment) {\n+  \/\/ We align the starting address of each allocation.\n+  char* p = (char*)align_up(_top, alignment);\n+  char* newtop = p + num_bytes;\n+  \/\/ Leave _top always SharedSpaceObjectAlignment aligned. But not more -\n+  \/\/  if we allocate with large alignments, lets not waste the gaps.\n+  \/\/ Ideally we would not need to align _top to anything here but CDS has\n+  \/\/  a number of implicit alignment assumptions. Leaving this unaligned\n+  \/\/  here will trip of at least ReadClosure (assuming word alignment) and\n+  \/\/  DumpAllocStats (will get confused about counting bytes on 32-bit\n+  \/\/  platforms if we align to anything less than SharedSpaceObjectAlignment\n+  \/\/  here).\n+  newtop = align_up(newtop, SharedSpaceObjectAlignment);\n@@ -212,1 +222,1 @@\n-  memset(p, 0, newtop - p);\n+  memset(p, 0, newtop - p); \/\/ todo: needed? debug_only?\n@@ -216,0 +226,4 @@\n+char* DumpRegion::allocate(size_t num_bytes) {\n+  return allocate(num_bytes, SharedSpaceObjectAlignment);\n+}\n+\n@@ -312,1 +326,1 @@\n-  assert(tag == old_tag, \"old tag doesn't match\");\n+  assert(tag == old_tag, \"tag doesn't match (%d, expected %d)\", old_tag, tag);\n","filename":"src\/hotspot\/share\/cds\/archiveUtils.cpp","additions":19,"deletions":5,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -149,0 +149,1 @@\n+  \/\/ Allocate with default alignment (SharedSpaceObjectAlignment)\n@@ -150,0 +151,2 @@\n+  \/\/ Allocate with an arbitrary alignment.\n+  char* allocate(size_t num_bytes, size_t alignment);\n","filename":"src\/hotspot\/share\/cds\/archiveUtils.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -91,0 +91,2 @@\n+  DEBUG_ONLY(void verify(int expected_byte_size, bool read_only) const;)\n+\n","filename":"src\/hotspot\/share\/cds\/dumpAllocStats.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -58,0 +58,1 @@\n+#include \"runtime\/safepoint.hpp\"\n@@ -331,1 +332,8 @@\n-    archived_oop->set_mark(markWord::prototype().copy_set_hash(hash_original));\n+\n+    assert(SafepointSynchronize::is_at_safepoint(), \"resolving displaced headers only at safepoint\");\n+    markWord mark = obj->mark();\n+    if (mark.has_displaced_mark_helper()) {\n+      mark = mark.displaced_mark_helper();\n+    }\n+    narrowKlass nklass = mark.narrow_klass();\n+    archived_oop->set_mark(markWord::prototype().copy_set_hash(hash_original) LP64_ONLY(.set_narrow_klass(nklass)));\n@@ -711,1 +719,2 @@\n-    oopDesc::set_mark(mem, markWord::prototype());\n+    oopDesc::set_mark(mem, k->prototype_header());\n+#ifndef _LP64\n@@ -713,0 +722,1 @@\n+#endif\n","filename":"src\/hotspot\/share\/cds\/heapShared.cpp","additions":12,"deletions":2,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -61,0 +61,1 @@\n+#include \"oops\/compressedKlass.hpp\"\n@@ -1314,0 +1315,5 @@\n+#ifdef ASSERT\n+    if (UseCompressedClassPointers) {\n+      CompressedKlassPointers::verify_klass_pointer(record->_klass);\n+    }\n+#endif\n@@ -1315,1 +1321,0 @@\n-    assert(check_alignment(record->_klass), \"Address not aligned\");\n","filename":"src\/hotspot\/share\/classfile\/systemDictionaryShared.cpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -105,2 +105,2 @@\n-  static const int first_vtableStub_size =  64;\n-  static const int first_itableStub_size = 256;\n+  static const int first_vtableStub_size = 256;\n+  static const int first_itableStub_size = 512;\n","filename":"src\/hotspot\/share\/code\/vtableStubs.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -90,0 +90,2 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n+#include \"gc\/shared\/suspendibleThreadSet.hpp\"\n@@ -1566,0 +1568,2 @@\n+  _forwarding = new SlidingForwarding(heap_rs.region(), HeapRegion::LogOfHRGrainBytes - LogHeapWordSize);\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -89,0 +89,1 @@\n+class SlidingForwarding;\n@@ -233,0 +234,2 @@\n+  SlidingForwarding* _forwarding;\n+\n@@ -246,0 +249,4 @@\n+  SlidingForwarding* forwarding() const {\n+    return _forwarding;\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -44,0 +44,1 @@\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n@@ -323,0 +324,2 @@\n+  _heap->forwarding()->clear();\n+\n@@ -330,3 +333,4 @@\n-  if (!has_free_compaction_targets) {\n-    phase2c_prepare_serial_compaction();\n-  }\n+  \/\/ TODO: Disabled for now because it violates sliding-forwarding assumption.\n+  \/\/ if (!has_free_compaction_targets) {\n+  \/\/   phase2c_prepare_serial_compaction();\n+  \/\/ }\n@@ -352,1 +356,2 @@\n-  GCTraceTime(Debug, gc, phases) debug(\"Phase 2: Prepare serial compaction\", scope()->timer());\n+  ShouldNotReachHere(); \/\/ Disabled in Lilliput.\n+  \/\/GCTraceTime(Debug, gc, phases) debug(\"Phase 2: Prepare serial compaction\", scope()->timer());\n@@ -358,0 +363,1 @@\n+  \/*\n@@ -364,0 +370,1 @@\n+  *\/\n@@ -367,0 +374,1 @@\n+  \/*\n@@ -382,0 +390,1 @@\n+  *\/\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullCollector.cpp","additions":13,"deletions":4,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -112,1 +112,1 @@\n-              touched_words = MIN2((size_t)align_object_size(typeArrayOopDesc::header_size(T_INT)),\n+              touched_words = MIN2((size_t)align_object_size(align_up(typeArrayOopDesc::base_offset_in_bytes(T_INT), HeapWordSize) \/ HeapWordSize),\n","filename":"src\/hotspot\/share\/gc\/parallel\/mutableNUMASpace.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -398,1 +398,1 @@\n-    HeapWord* test_addr = cast_from_oop<HeapWord*>(obj) + 1;\n+    HeapWord* test_addr = cast_from_oop<HeapWord*>(obj);\n","filename":"src\/hotspot\/share\/gc\/parallel\/psOldGen.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -293,1 +293,0 @@\n-  assert(old->is_objArray(), \"invariant\");\n@@ -331,1 +330,1 @@\n-  if (obj->forward_to_atomic(obj, obj_mark) == nullptr) {\n+  if (obj->forward_to_self_atomic(obj_mark) == nullptr) {\n","filename":"src\/hotspot\/share\/gc\/parallel\/psPromotionManager.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -66,1 +66,1 @@\n-inline void PSPromotionManager::promotion_trace_event(oop new_obj, oop old_obj,\n+inline void PSPromotionManager::promotion_trace_event(oop new_obj, oop old_obj, Klass* klass,\n@@ -79,1 +79,1 @@\n-        gc_tracer->report_promotion_in_new_plab_event(old_obj->klass(), obj_bytes,\n+        gc_tracer->report_promotion_in_new_plab_event(klass, obj_bytes,\n@@ -86,1 +86,1 @@\n-        gc_tracer->report_promotion_outside_plab_event(old_obj->klass(), obj_bytes,\n+        gc_tracer->report_promotion_outside_plab_event(klass, obj_bytes,\n@@ -150,1 +150,1 @@\n-    return cast_to_oop(m.decode_pointer());\n+    return o->forwardee(m);\n@@ -166,1 +166,6 @@\n-  size_t new_obj_size = o->size();\n+#ifdef _LP64\n+  Klass* klass = test_mark.safe_klass();\n+#else\n+  Klass* klass = o->klass();\n+#endif\n+  size_t new_obj_size = o->size_given_klass(klass);\n@@ -181,1 +186,1 @@\n-          promotion_trace_event(new_obj, o, new_obj_size, age, false, nullptr);\n+          promotion_trace_event(new_obj, o, klass, new_obj_size, age, false, nullptr);\n@@ -191,1 +196,1 @@\n-            promotion_trace_event(new_obj, o, new_obj_size, age, false, &_young_lab);\n+            promotion_trace_event(new_obj, o, klass, new_obj_size, age, false, &_young_lab);\n@@ -217,1 +222,1 @@\n-          promotion_trace_event(new_obj, o, new_obj_size, age, true, nullptr);\n+          promotion_trace_event(new_obj, o, klass, new_obj_size, age, true, nullptr);\n@@ -227,1 +232,1 @@\n-            promotion_trace_event(new_obj, o, new_obj_size, age, true, &_old_lab);\n+            promotion_trace_event(new_obj, o, klass, new_obj_size, age, true, &_old_lab);\n@@ -250,3 +255,5 @@\n-  \/\/ Parallel GC claims with a release - so other threads might access this object\n-  \/\/ after claiming and they should see the \"completed\" object.\n-  ContinuationGCSupport::transform_stack_chunk(new_obj);\n+  if (!new_obj->mark().is_marked()) {\n+    \/\/ Parallel GC claims with a release - so other threads might access this object\n+    \/\/ after claiming and they should see the \"completed\" object.\n+    ContinuationGCSupport::transform_stack_chunk(new_obj);\n+  }\n","filename":"src\/hotspot\/share\/gc\/parallel\/psPromotionManager.inline.hpp","additions":19,"deletions":12,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -671,0 +671,10 @@\n+#ifdef _LP64\n+        oop forwardee = obj->forwardee();\n+        markWord header = forwardee->mark();\n+        if (header.has_displaced_mark_helper()) {\n+          header = header.displaced_mark_helper();\n+        }\n+        assert(UseCompressedClassPointers, \"assume +UseCompressedClassPointers\");\n+        narrowKlass nklass = header.narrow_klass();\n+        obj->set_mark(markWord::prototype().set_narrow_klass(nklass));\n+#else\n@@ -672,0 +682,1 @@\n+#endif\n@@ -695,1 +706,1 @@\n-  old->forward_to(old);\n+  old->forward_to_self();\n","filename":"src\/hotspot\/share\/gc\/serial\/defNewGeneration.cpp","additions":12,"deletions":1,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -259,0 +259,2 @@\n+  AdjustPointerClosure adjust_pointer_closure(gch->forwarding());\n+  CLDToOopClosure      adjust_cld_closure(&adjust_pointer_closure, ClassLoaderData::_claim_stw_fullgc_adjust);\n","filename":"src\/hotspot\/share\/gc\/serial\/genMarkSweep.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shared\/genCollectedHeap.hpp\"\n@@ -66,1 +67,0 @@\n-CLDToOopClosure    MarkSweep::adjust_cld_closure(&adjust_pointer_closure, ClassLoaderData::_claim_stw_fullgc_adjust);\n@@ -148,2 +148,2 @@\n-void PreservedMark::adjust_pointer() {\n-  MarkSweep::adjust_pointer(&_obj);\n+void PreservedMark::adjust_pointer(const SlidingForwarding* const forwarding) {\n+  MarkSweep::adjust_pointer(forwarding, &_obj);\n@@ -183,0 +183,2 @@\n+  ContinuationGCSupport::transform_stack_chunk(obj);\n+\n@@ -186,0 +188,8 @@\n+#ifdef _LP64\n+  markWord real_mark = mark;\n+  if (real_mark.has_displaced_mark_helper()) {\n+    real_mark = real_mark.displaced_mark_helper();\n+  }\n+  Klass* klass = real_mark.klass();\n+  obj->set_mark(klass->prototype_header().set_marked());\n+#else\n@@ -187,2 +197,1 @@\n-\n-  ContinuationGCSupport::transform_stack_chunk(obj);\n+#endif\n@@ -211,2 +220,2 @@\n-AdjustPointerClosure MarkSweep::adjust_pointer_closure;\n-\n+  const SlidingForwarding* const forwarding = GenCollectedHeap::heap()->forwarding();\n+\n@@ -216,1 +225,1 @@\n-    _preserved_marks[i].adjust_pointer();\n+    _preserved_marks[i].adjust_pointer(forwarding);\n@@ -223,1 +232,1 @@\n-    p->adjust_pointer();\n+    p->adjust_pointer(forwarding);\n","filename":"src\/hotspot\/share\/gc\/serial\/markSweep.cpp","additions":18,"deletions":9,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -226,4 +226,0 @@\n-  if (is_in(object->klass_or_null())) {\n-    return false;\n-  }\n-\n@@ -256,2 +252,4 @@\n-  _filler_array_max_size = align_object_size(filler_array_hdr_size() +\n-                                             max_len \/ elements_per_word);\n+  int header_size_in_bytes = arrayOopDesc::base_offset_in_bytes(T_INT);\n+  assert(header_size_in_bytes % sizeof(jint) == 0, \"must be aligned to int\");\n+  int header_size_in_ints = header_size_in_bytes \/ sizeof(jint);\n+  _filler_array_max_size = align_object_size((header_size_in_ints + max_len) \/ elements_per_word);\n@@ -419,1 +417,3 @@\n-  size_t max_int_size = typeArrayOopDesc::header_size(T_INT) +\n+  int header_size_in_bytes = typeArrayOopDesc::base_offset_in_bytes(T_INT);\n+  assert(header_size_in_bytes % sizeof(jint) == 0, \"header size must align to int\");\n+  size_t max_int_size = header_size_in_bytes \/ HeapWordSize +\n@@ -425,5 +425,2 @@\n-size_t CollectedHeap::filler_array_hdr_size() {\n-  return align_object_offset(arrayOopDesc::header_size(T_INT)); \/\/ align to Long\n-}\n-\n-  return align_object_size(filler_array_hdr_size()); \/\/ align to MinObjAlignment\n+  int aligned_header_size_words = align_up(arrayOopDesc::base_offset_in_bytes(T_INT), HeapWordSize) \/ HeapWordSize;\n+  return align_object_size(aligned_header_size_words); \/\/ align to MinObjAlignment\n@@ -434,2 +431,3 @@\n-  Copy::fill_to_words(start + filler_array_hdr_size(),\n-                      words - filler_array_hdr_size(), value);\n+  int payload_start = align_up(arrayOopDesc::base_offset_in_bytes(T_INT), HeapWordSize) \/ HeapWordSize;\n+  Copy::fill_to_words(start + payload_start,\n+                      words - payload_start, value);\n@@ -459,2 +457,3 @@\n-  const size_t payload_size = words - filler_array_hdr_size();\n-  const size_t len = payload_size * HeapWordSize \/ sizeof(jint);\n+  const size_t payload_size_bytes = words * HeapWordSize - arrayOopDesc::base_offset_in_bytes(T_INT);\n+  assert(payload_size_bytes % sizeof(jint) == 0, \"must be int aligned\");\n+  const size_t len = payload_size_bytes \/ sizeof(jint);\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.cpp","additions":15,"deletions":16,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -166,1 +166,0 @@\n-  static inline size_t filler_array_hdr_size();\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -58,0 +58,1 @@\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n@@ -120,0 +121,1 @@\n+  _forwarding = new SlidingForwarding(_reserved);\n@@ -1039,0 +1041,1 @@\n+  _forwarding->clear();\n","filename":"src\/hotspot\/share\/gc\/shared\/genCollectedHeap.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+class SlidingForwarding;\n@@ -90,0 +91,2 @@\n+  SlidingForwarding* _forwarding;\n+\n@@ -315,0 +318,4 @@\n+  SlidingForwarding* forwarding() const {\n+    return _forwarding;\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/genCollectedHeap.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"runtime\/objectMonitor.hpp\"\n@@ -265,0 +266,1 @@\n+        ObjectMonitor::maybe_deflate_dead(ptr);\n","filename":"src\/hotspot\/share\/gc\/shared\/oopStorage.inline.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -256,1 +257,1 @@\n-                                    CompactPoint* cp, HeapWord* compact_top) {\n+                                    CompactPoint* cp, HeapWord* compact_top, SlidingForwarding* const forwarding) {\n@@ -279,1 +280,1 @@\n-    q->forward_to(cast_to_oop(compact_top));\n+    forwarding->forward_to(q, cast_to_oop(compact_top));\n@@ -327,0 +328,1 @@\n+  SlidingForwarding* const forwarding = GenCollectedHeap::heap()->forwarding();\n@@ -332,1 +334,1 @@\n-      compact_top = cp->space->forward(cast_to_oop(cur_obj), size, cp, compact_top);\n+      compact_top = cp->space->forward(cast_to_oop(cur_obj), size, cp, compact_top, forwarding);\n@@ -348,1 +350,1 @@\n-        compact_top = cp->space->forward(obj, obj->size(), cp, compact_top);\n+        compact_top = cp->space->forward(obj, obj->size(), cp, compact_top, forwarding);\n@@ -391,0 +393,1 @@\n+  const SlidingForwarding* const forwarding = GenCollectedHeap::heap()->forwarding();\n@@ -402,1 +405,1 @@\n-      size_t size = MarkSweep::adjust_pointers(cast_to_oop(cur_obj));\n+      size_t size = MarkSweep::adjust_pointers(forwarding, cast_to_oop(cur_obj));\n@@ -443,0 +446,2 @@\n+  const SlidingForwarding* const forwarding = GenCollectedHeap::heap()->forwarding();\n+\n@@ -456,1 +461,1 @@\n-      HeapWord* compaction_top = cast_from_oop<HeapWord*>(cast_to_oop(cur_obj)->forwardee());\n+      HeapWord* compaction_top = cast_from_oop<HeapWord*>(forwarding->forwardee(cast_to_oop(cur_obj)));\n","filename":"src\/hotspot\/share\/gc\/shared\/space.cpp","additions":11,"deletions":6,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -59,0 +59,1 @@\n+class SlidingForwarding;\n@@ -387,1 +388,1 @@\n-                    HeapWord* compact_top);\n+                    HeapWord* compact_top, SlidingForwarding* const forwarding);\n","filename":"src\/hotspot\/share\/gc\/shared\/space.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n@@ -197,0 +198,2 @@\n+  _forwarding = new SlidingForwarding(_heap_region, ShenandoahHeapRegion::region_size_words_shift());\n+\n@@ -953,1 +956,1 @@\n-    if (!p->is_forwarded()) {\n+    if (!ShenandoahForwarding::is_forwarded(p)) {\n@@ -1298,0 +1301,1 @@\n+    shenandoah_assert_not_in_cset_except(NULL, obj, cancelled_gc());\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -65,0 +65,1 @@\n+class SlidingForwarding;\n@@ -231,0 +232,1 @@\n+  SlidingForwarding* _forwarding;\n@@ -247,0 +249,2 @@\n+  SlidingForwarding* forwarding() const { return _forwarding; }\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -738,0 +738,9 @@\n+  if (!UseHeavyMonitors && UseFastLocking) {\n+    \/\/ This is a hack to get around the limitation of registers in x86_32. We really\n+    \/\/ send an oopDesc* instead of a BasicObjectLock*.\n+    Handle h_obj(current, oop((reinterpret_cast<oopDesc*>(elem))));\n+    assert(Universe::heap()->is_in_or_null(h_obj()),\n+           \"must be NULL or an object\");\n+    ObjectSynchronizer::enter(h_obj, NULL, current);\n+    return;\n+  }\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.cpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2002,0 +2002,3 @@\n+#ifdef _LP64\n+              oopDesc::release_set_mark(result, ik->prototype_header());\n+#else\n@@ -2003,2 +2006,1 @@\n-              oopDesc::set_klass_gap(result, 0);\n-\n+#endif\n","filename":"src\/hotspot\/share\/interpreter\/zero\/bytecodeInterpreter.cpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2207,1 +2207,1 @@\n-  return arrayOopDesc::header_size(type) * HeapWordSize;\n+  return arrayOopDesc::base_offset_in_bytes(type);\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -271,1 +271,0 @@\n-  volatile_nonstatic_field(oopDesc,            _metadata._klass,                              Klass*)                                \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"memory\/metaspace\/metaspaceAlignment.hpp\"\n@@ -39,0 +40,2 @@\n+#include \"oops\/compressedKlass.hpp\"\n+#include \"utilities\/align.hpp\"\n@@ -40,0 +43,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -59,0 +63,2 @@\n+  const int klass_alignment_words = KlassAlignmentInBytes \/ BytesPerWord;\n+\n@@ -63,0 +69,1 @@\n+      metaspace::MetaspaceMinAlignmentWords,\n@@ -69,0 +76,1 @@\n+    \/\/ Klass instances live in class space and must be aligned correctly.\n@@ -74,0 +82,1 @@\n+        klass_alignment_words,\n@@ -77,0 +86,5 @@\n+  } else {\n+    \/\/ note for lilliput, this path should be restricted to 32bit only. There, klass alignment\n+    \/\/  should be compatible with metaspace minimal alignment since we store Klass structures\n+    \/\/  in regular metaspace.\n+    NOT_LP64(STATIC_ASSERT(metaspace::MetaspaceMinAlignmentBytes == KlassAlignmentInBytes));\n","filename":"src\/hotspot\/share\/memory\/classLoaderMetaspace.cpp","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"memory\/metaspace\/metaspaceAlignment.hpp\"\n@@ -870,0 +871,14 @@\n+#ifdef _LP64\n+\/\/ The largest allowed size for class space\n+size_t Metaspace::max_class_space_size() {\n+  \/\/ This is a bit fuzzy. Max value of class space size depends on narrow klass pointer\n+  \/\/ encoding range size and CDS, since class space shares encoding range with CDS. CDS\n+  \/\/ archives are usually pretty small though, so to keep matters simple, for now we\n+  \/\/ just assume a reasonable default (this is hackish; improve!).\n+  const size_t slice_for_cds = M * 128;\n+  assert(KlassEncodingMetaspaceMax >= (slice_for_cds * 2), \"rethink this\");\n+  const size_t max_class_space_size = KlassEncodingMetaspaceMax - slice_for_cds;\n+  return max_class_space_size;\n+}\n+#endif \/\/ _LP64\n+\n","filename":"src\/hotspot\/share\/memory\/metaspace.cpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -112,0 +112,3 @@\n+  \/\/ The largest allowed size for class space\n+  LP64_ONLY(static size_t max_class_space_size();)\n+\n","filename":"src\/hotspot\/share\/memory\/metaspace.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -55,0 +55,5 @@\n+    \/\/ Attention alignment: the resulting block must have the right alignment\n+    \/\/  for the enclosing arena. ATM this works, since the arena aligns allocated block\n+    \/\/  size. If we ever switch to a different model (e.g. aligning the start\n+    \/\/  address of allocated blocks instead of the request size) this should be\n+    \/\/  rewritten).\n","filename":"src\/hotspot\/share\/memory\/metaspace\/freeBlocks.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"memory\/metaspace\/metaspaceAlignment.hpp\"\n@@ -110,1 +111,1 @@\n-MetaspaceArena::MetaspaceArena(ChunkManager* chunk_manager, const ArenaGrowthPolicy* growth_policy,\n+MetaspaceArena::MetaspaceArena(ChunkManager* chunk_manager, const ArenaGrowthPolicy* growth_policy, int alignment_words,\n@@ -117,0 +118,1 @@\n+  _alignment_words(alignment_words),\n@@ -123,0 +125,1 @@\n+\n@@ -227,1 +230,1 @@\n-  const size_t raw_word_size = get_raw_word_size_for_requested_word_size(requested_word_size);\n+  const size_t raw_word_size = get_raw_word_size_for_requested_word_size(requested_word_size, _alignment_words);\n@@ -271,1 +274,1 @@\n-  const size_t raw_word_size = get_raw_word_size_for_requested_word_size(requested_word_size);\n+  const size_t raw_word_size = get_raw_word_size_for_requested_word_size(requested_word_size, _alignment_words);\n@@ -371,1 +374,1 @@\n-  size_t raw_word_size = get_raw_word_size_for_requested_word_size(word_size);\n+  size_t raw_word_size = get_raw_word_size_for_requested_word_size(word_size, _alignment_words);\n@@ -485,2 +488,2 @@\n-  st->print_cr(\"growth-policy \" PTR_FORMAT \", lock \" PTR_FORMAT \", cm \" PTR_FORMAT \", fbl \" PTR_FORMAT,\n-                p2i(_growth_policy), p2i(_lock), p2i(_chunk_manager), p2i(_fbl));\n+  st->print_cr(\"growth-policy \" PTR_FORMAT \", alignment %d, lock \" PTR_FORMAT \", cm \" PTR_FORMAT \", fbl \" PTR_FORMAT,\n+                p2i(_growth_policy), _alignment_words * BytesPerWord, p2i(_lock), p2i(_chunk_manager), p2i(_fbl));\n","filename":"src\/hotspot\/share\/memory\/metaspace\/metaspaceArena.cpp","additions":9,"deletions":6,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -97,0 +97,3 @@\n+  \/\/ Alignment alignment, in words.\n+  const int _alignment_words;\n+\n@@ -167,1 +170,1 @@\n-  MetaspaceArena(ChunkManager* chunk_manager, const ArenaGrowthPolicy* growth_policy,\n+  MetaspaceArena(ChunkManager* chunk_manager, const ArenaGrowthPolicy* growth_policy, int alignment_words,\n","filename":"src\/hotspot\/share\/memory\/metaspace\/metaspaceArena.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"memory\/metaspace\/metaspaceAlignment.hpp\"\n@@ -171,18 +172,0 @@\n-\/\/ Given a net allocation word size, return the raw word size we actually allocate.\n-\/\/ Note: externally visible for gtests.\n-\/\/static\n-size_t get_raw_word_size_for_requested_word_size(size_t word_size) {\n-  size_t byte_size = word_size * BytesPerWord;\n-\n-  \/\/ Deallocated metablocks are kept in a binlist which limits their minimal\n-  \/\/  size to at least the size of a binlist item (2 words).\n-  byte_size = MAX2(byte_size, FreeBlocks::MinWordSize * BytesPerWord);\n-\n-  \/\/ Metaspace allocations are aligned to word size.\n-  byte_size = align_up(byte_size, AllocationAlignmentByteSize);\n-\n-  size_t raw_word_size = byte_size \/ BytesPerWord;\n-  assert(raw_word_size * BytesPerWord == byte_size, \"Sanity\");\n-  return raw_word_size;\n-}\n-\n","filename":"src\/hotspot\/share\/memory\/metaspace\/metaspaceCommon.cpp","additions":1,"deletions":18,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"oops\/compressedOops.hpp\"\n@@ -108,0 +109,6 @@\n+    out->cr();\n+    out->print_cr(\"KlassAlignmentInBytes: %d\", KlassAlignmentInBytes);\n+    out->print(\"KlassEncodingMetaspaceMax: \");\n+    print_human_readable_size(out, KlassEncodingMetaspaceMax, scale);\n+    out->cr();\n+    CompressedKlassPointers::print_mode(out);\n@@ -109,1 +116,1 @@\n-    out->print(\"No class space\");\n+    out->print_cr(\"No class space\");\n@@ -111,1 +118,0 @@\n-  out->cr();\n","filename":"src\/hotspot\/share\/memory\/metaspace\/metaspaceReporter.cpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"memory\/metaspace\/metaspaceAlignment.hpp\"\n@@ -100,1 +101,2 @@\n-    arena = new MetaspaceArena(_context->cm(), growth_policy, lock, &_used_words_counter, _name);\n+    arena = new MetaspaceArena(_context->cm(), growth_policy, MetaspaceMinAlignmentWords,\n+                               lock, &_used_words_counter, _name);\n","filename":"src\/hotspot\/share\/memory\/metaspace\/testHelpers.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -550,0 +550,4 @@\n+    \/\/ Note Lilliput: the advantages of this strategy were questionable before\n+    \/\/  (since CDS=off + Compressed oops + heap large enough to suffocate us out of lower 32g\n+    \/\/  is rare) and with Lilliput the encoding range drastically shrank. We may just do away\n+    \/\/  with this altogether.\n","filename":"src\/hotspot\/share\/memory\/virtualspace.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -53,0 +53,1 @@\n+#include \"runtime\/globals.hpp\"\n@@ -54,0 +55,1 @@\n+#include \"utilities\/align.hpp\"\n@@ -195,1 +197,3 @@\n-  return Metaspace::allocate(loader_data, word_size, MetaspaceObj::ClassType, THREAD);\n+  MetaWord* p = Metaspace::allocate(loader_data, word_size, MetaspaceObj::ClassType, THREAD);\n+  assert(is_aligned(p, KlassAlignmentInBytes), \"metaspace returned badly aligned memory.\");\n+  return p;\n@@ -203,0 +207,1 @@\n+                           _prototype_header(markWord::prototype() LP64_ONLY(.set_klass(this))),\n@@ -747,0 +752,2 @@\n+     st->print(BULLET\"prototype_header: \" INTPTR_FORMAT, _prototype_header.value());\n+     st->cr();\n@@ -770,0 +777,4 @@\n+  if (UseCompressedClassPointers) {\n+    assert(is_aligned(this, KlassAlignmentInBytes), \"misaligned Klass structure\");\n+  }\n+\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":12,"deletions":1,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -168,0 +168,2 @@\n+  markWord _prototype_header;   \/\/ Used to initialize objects' header\n+\n@@ -671,0 +673,4 @@\n+  markWord prototype_header() const      { return _prototype_header; }\n+  inline void set_prototype_header(markWord header);\n+  static ByteSize prototype_header_offset() { return in_ByteSize(offset_of(Klass, _prototype_header)); }\n+\n","filename":"src\/hotspot\/share\/oops\/klass.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n-#include \"oops\/oopsHierarchy.hpp\"\n+#include \"oops\/compressedKlass.hpp\"\n@@ -44,1 +44,1 @@\n-\/\/  unused:25 hash:31 -->| unused_gap:1  age:4  unused_gap:1  lock:2 (normal object)\n+\/\/  nklass:32 hash:25 -->| unused_gap:1  age:4  unused_gap:1  lock:2 (normal object)\n@@ -105,4 +105,6 @@\n-  static const int first_unused_gap_bits          = 1;\n-  static const int max_hash_bits                  = BitsPerWord - age_bits - lock_bits - first_unused_gap_bits;\n-  static const int hash_bits                      = max_hash_bits > 31 ? 31 : max_hash_bits;\n-  static const int second_unused_gap_bits         = LP64_ONLY(1) NOT_LP64(0);\n+  static const int self_forwarded_bits            = 1;\n+  static const int max_hash_bits                  = BitsPerWord - age_bits - lock_bits - self_forwarded_bits;\n+  static const int hash_bits                      = max_hash_bits > 25 ? 25 : max_hash_bits;\n+#ifdef _LP64\n+  static const int klass_bits                     = 32;\n+#endif\n@@ -111,2 +113,6 @@\n-  static const int age_shift                      = lock_bits + first_unused_gap_bits;\n-  static const int hash_shift                     = age_shift + age_bits + second_unused_gap_bits;\n+  static const int self_forwarded_shift           = lock_shift + lock_bits;\n+  static const int age_shift                      = self_forwarded_shift + self_forwarded_bits;\n+  static const int hash_shift                     = age_shift + age_bits;\n+#ifdef _LP64\n+  static const int klass_shift                    = hash_shift + hash_bits;\n+#endif\n@@ -116,0 +122,2 @@\n+  static const uintptr_t self_forwarded_mask      = right_n_bits(self_forwarded_bits);\n+  static const uintptr_t self_forwarded_mask_in_place = self_forwarded_mask << self_forwarded_shift;\n@@ -121,0 +129,5 @@\n+#ifdef _LP64\n+  static const uintptr_t klass_mask               = right_n_bits(klass_bits);\n+  static const uintptr_t klass_mask_in_place      = klass_mask << klass_shift;\n+#endif\n+\n@@ -173,1 +186,1 @@\n-    return ((value() & lock_mask_in_place) == locked_value);\n+    return !UseFastLocking && ((value() & lock_mask_in_place) == locked_value);\n@@ -179,0 +192,8 @@\n+\n+  bool is_fast_locked() const {\n+    return UseFastLocking && ((value() & lock_mask_in_place) == locked_value);\n+  }\n+  markWord set_fast_locked() const {\n+    return markWord(value() & ~lock_mask_in_place);\n+  }\n+\n@@ -188,1 +209,3 @@\n-    return ((value() & unlocked_value) == 0);\n+    intptr_t lockbits = value() & lock_mask_in_place;\n+    return UseFastLocking ? lockbits == monitor_value   \/\/ monitor?\n+                    : (lockbits & unlocked_value) == 0; \/\/ monitor | stack-locked?\n@@ -235,0 +258,9 @@\n+#ifdef _LP64\n+  inline Klass* klass() const;\n+  inline Klass* klass_or_null() const;\n+  inline Klass* safe_klass() const;\n+  inline markWord set_klass(const Klass* klass) const;\n+  inline narrowKlass narrow_klass() const;\n+  inline markWord set_narrow_klass(const narrowKlass klass) const;\n+#endif\n+\n@@ -248,0 +280,8 @@\n+\n+  inline bool self_forwarded() const {\n+    return mask_bits(value(), self_forwarded_mask_in_place) != 0;\n+  }\n+\n+  inline markWord set_self_forwarded() const {\n+    return markWord(value() | self_forwarded_mask_in_place | marked_value);\n+  }\n","filename":"src\/hotspot\/share\/oops\/markWord.hpp","additions":50,"deletions":10,"binary":false,"changes":60,"status":"modified"},{"patch":"@@ -54,23 +54,3 @@\n-  \/\/ Give size of objArrayOop in HeapWords minus the header\n-  static int array_size(int length) {\n-    const uint OopsPerHeapWord = HeapWordSize\/heapOopSize;\n-    assert(OopsPerHeapWord >= 1 && (HeapWordSize % heapOopSize == 0),\n-           \"Else the following (new) computation would be in error\");\n-    uint res = ((uint)length + OopsPerHeapWord - 1)\/OopsPerHeapWord;\n-#ifdef ASSERT\n-    \/\/ The old code is left in for sanity-checking; it'll\n-    \/\/ go away pretty soon. XXX\n-    \/\/ Without UseCompressedOops, this is simply:\n-    \/\/ oop->length() * HeapWordsPerOop;\n-    \/\/ With narrowOops, HeapWordsPerOop is 1\/2 or equal 0 as an integer.\n-    \/\/ The oop elements are aligned up to wordSize\n-    const uint HeapWordsPerOop = heapOopSize\/HeapWordSize;\n-    uint old_res;\n-    if (HeapWordsPerOop > 0) {\n-      old_res = length * HeapWordsPerOop;\n-    } else {\n-      old_res = align_up((uint)length, OopsPerHeapWord)\/OopsPerHeapWord;\n-    }\n-    assert(res == old_res, \"Inconsistency between old and new.\");\n-#endif  \/\/ ASSERT\n-    return res;\n+  \/\/ Give size of objArrayOop in bytes minus the header\n+  static size_t array_size_in_bytes(int length) {\n+    return (size_t)length * heapOopSize;\n@@ -96,1 +76,0 @@\n-  static int header_size()    { return arrayOopDesc::header_size(T_OBJECT); }\n@@ -101,5 +80,5 @@\n-    uint asz = array_size(length);\n-    uint osz = align_object_size(header_size() + asz);\n-    assert(osz >= asz,   \"no overflow\");\n-    assert((int)osz > 0, \"no overflow\");\n-    return (size_t)osz;\n+    size_t asz = array_size_in_bytes(length);\n+    size_t size_words = align_up(base_offset_in_bytes() + asz, HeapWordSize) \/ HeapWordSize;\n+    size_t osz = align_object_size(size_words);\n+    assert(osz < max_jint, \"no overflow\");\n+    return osz;\n","filename":"src\/hotspot\/share\/oops\/objArrayOop.hpp","additions":8,"deletions":29,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n@@ -37,0 +38,1 @@\n+#include \"runtime\/interfaceSupport.inline.hpp\"\n@@ -115,1 +117,1 @@\n-  if (ignore_mark_word) {\n+  if (ignore_mark_word || UseFastLocking) {\n@@ -147,20 +149,8 @@\n-bool oopDesc::has_klass_gap() {\n-  \/\/ Only has a klass gap when compressed class pointers are used.\n-  return UseCompressedClassPointers;\n-}\n-\n-#if INCLUDE_CDS_JAVA_HEAP\n-void oopDesc::set_narrow_klass(narrowKlass nk) {\n-  assert(DumpSharedSpaces, \"Used by CDS only. Do not abuse!\");\n-  assert(UseCompressedClassPointers, \"must be\");\n-  _metadata._compressed_klass = nk;\n-}\n-#endif\n-\n-  if (UseCompressedClassPointers) {\n-    narrowKlass narrow_klass = obj->_metadata._compressed_klass;\n-    if (narrow_klass == 0) return NULL;\n-    return (void*)CompressedKlassPointers::decode_raw(narrow_klass);\n-  } else {\n-    return obj->_metadata._klass;\n-  }\n+  \/\/ TODO: Remove method altogether and replace with calls to obj->klass() ?\n+  \/\/ OTOH, we may eventually get rid of locking in header, and then no\n+  \/\/ longer have to deal with that anymore.\n+#ifdef _LP64\n+  return obj->klass();\n+#else\n+  return obj->_klass;\n+#endif\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":11,"deletions":21,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n@@ -37,1 +38,1 @@\n-#include \"oops\/markWord.hpp\"\n+#include \"oops\/markWord.inline.hpp\"\n@@ -41,0 +42,3 @@\n+#include \"runtime\/objectMonitor.inline.hpp\"\n+#include \"runtime\/safepoint.hpp\"\n+#include \"runtime\/synchronizer.hpp\"\n@@ -73,0 +77,4 @@\n+void oopDesc::release_set_mark(HeapWord* mem, markWord m) {\n+  Atomic::release_store((markWord*)(((char*)mem) + mark_offset_in_bytes()), m);\n+}\n+\n@@ -81,0 +89,9 @@\n+markWord oopDesc::resolve_mark() const {\n+  markWord hdr = mark();\n+  if (hdr.has_monitor()) {\n+    ObjectMonitor* monitor = hdr.monitor();\n+    return monitor->header();\n+  }\n+  return hdr;\n+}\n+\n@@ -82,1 +99,8 @@\n-  set_mark(markWord::prototype());\n+#ifdef _LP64\n+  markWord header = resolve_mark();\n+  assert(UseCompressedClassPointers, \"expect compressed klass pointers\");\n+  header = markWord((header.value() & markWord::klass_mask_in_place) | markWord::prototype().value());\n+#else\n+  markWord header = markWord::prototype();\n+#endif\n+  set_mark(header);\n@@ -86,5 +110,7 @@\n-  if (UseCompressedClassPointers) {\n-    return CompressedKlassPointers::decode_not_null(_metadata._compressed_klass);\n-  } else {\n-    return _metadata._klass;\n-  }\n+#ifdef _LP64\n+  assert(UseCompressedClassPointers, \"only with compressed class pointers\");\n+  markWord header = resolve_mark();\n+  return header.klass();\n+#else\n+  return _klass;\n+#endif\n@@ -94,5 +120,7 @@\n-  if (UseCompressedClassPointers) {\n-    return CompressedKlassPointers::decode(_metadata._compressed_klass);\n-  } else {\n-    return _metadata._klass;\n-  }\n+#ifdef _LP64\n+  assert(UseCompressedClassPointers, \"only with compressed class pointers\");\n+  markWord header = resolve_mark();\n+  return header.klass_or_null();\n+#else\n+  return _klass;\n+#endif\n@@ -102,5 +130,5 @@\n-  if (UseCompressedClassPointers) {\n-    narrowKlass nklass = Atomic::load_acquire(&_metadata._compressed_klass);\n-    return CompressedKlassPointers::decode(nklass);\n-  } else {\n-    return Atomic::load_acquire(&_metadata._klass);\n+#ifdef _LP64\n+  assert(UseCompressedClassPointers, \"only with compressed class pointers\");\n+  markWord header = mark_acquire();\n+  if (header.has_monitor()) {\n+    header = header.monitor()->header();\n@@ -108,0 +136,4 @@\n+  return header.klass_or_null();\n+#else\n+  return Atomic::load_acquire(&_klass);\n+#endif\n@@ -110,0 +142,1 @@\n+#ifndef _LP64\n@@ -112,5 +145,1 @@\n-  if (UseCompressedClassPointers) {\n-    _metadata._compressed_klass = CompressedKlassPointers::encode_not_null(k);\n-  } else {\n-    _metadata._klass = k;\n-  }\n+  _klass = k;\n@@ -129,6 +158,1 @@\n-\n-void oopDesc::set_klass_gap(HeapWord* mem, int v) {\n-  if (UseCompressedClassPointers) {\n-    *(int*)(((char*)mem) + klass_gap_offset_in_bytes()) = v;\n-  }\n-}\n+#endif\n@@ -263,1 +287,15 @@\n-  assert(m.decode_pointer() == p, \"encoding must be reversible\");\n+  assert(forwardee(m) == p, \"encoding must be reversable\");\n+  set_mark(m);\n+}\n+\n+void oopDesc::forward_to_self() {\n+#ifdef _LP64\n+  verify_forwardee(this);\n+  markWord m = mark();\n+  \/\/ If mark is displaced, we need to preserve the Klass* from real header.\n+  assert(SafepointSynchronize::is_at_safepoint(), \"we can only safely fetch the displaced header at safepoint\");\n+  if (m.has_displaced_mark_helper()) {\n+    m = m.displaced_mark_helper();\n+  }\n+  m = m.set_self_forwarded();\n+  assert(forwardee(m) == cast_to_oop(this), \"encoding must be reversable\");\n@@ -265,0 +303,3 @@\n+#else\n+  forward_to(oop(this));\n+#endif\n@@ -270,1 +311,20 @@\n-  assert(m.decode_pointer() == p, \"encoding must be reversible\");\n+  assert(forwardee(m) == p, \"encoding must be reversable\");\n+  markWord old_mark = cas_set_mark(m, compare, order);\n+  if (old_mark == compare) {\n+    return NULL;\n+  } else {\n+    return forwardee(old_mark);\n+  }\n+}\n+\n+oop oopDesc::forward_to_self_atomic(markWord compare, atomic_memory_order order) {\n+#ifdef _LP64\n+  verify_forwardee(this);\n+  markWord m = compare;\n+  \/\/ If mark is displaced, we need to preserve the Klass* from real header.\n+  assert(SafepointSynchronize::is_at_safepoint(), \"we can only safely fetch the displaced header at safepoint\");\n+  if (m.has_displaced_mark_helper()) {\n+    m = m.displaced_mark_helper();\n+  }\n+  m = m.set_self_forwarded();\n+  assert(forwardee(m) == cast_to_oop(this), \"encoding must be reversable\");\n@@ -275,1 +335,2 @@\n-    return cast_to_oop(old_mark.decode_pointer());\n+    assert(old_mark.is_marked(), \"must be marked here\");\n+    return forwardee(old_mark);\n@@ -277,0 +338,3 @@\n+#else\n+  return forward_to_atomic(oop(this), compare, order);\n+#endif\n@@ -283,2 +347,14 @@\n-  assert(is_forwarded(), \"only decode when actually forwarded\");\n-  return cast_to_oop(mark().decode_pointer());\n+  return forwardee(mark());\n+}\n+\n+oop oopDesc::forwardee(markWord header) const {\n+  assert(header.is_marked(), \"must be forwarded\");\n+#ifdef _LP64\n+  if (header.self_forwarded()) {\n+    return cast_to_oop(this);\n+  } else\n+#endif\n+  {\n+    assert(header.is_marked(), \"only decode when actually forwarded\");\n+    return cast_to_oop(header.decode_pointer());\n+  }\n@@ -339,1 +415,0 @@\n-  assert(k == klass(), \"wrong klass\");\n","filename":"src\/hotspot\/share\/oops\/oop.inline.hpp","additions":109,"deletions":34,"binary":false,"changes":143,"status":"modified"},{"patch":"@@ -40,3 +40,0 @@\n-\/\/ If compressed klass pointers then use narrowKlass.\n-typedef juint  narrowKlass;\n-\n","filename":"src\/hotspot\/share\/oops\/oopsHierarchy.hpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1575,3 +1575,3 @@\n-  Node* mark_node = NULL;\n-  \/\/ For now only enable fast locking for non-array types\n-  mark_node = phase->MakeConX(markWord::prototype().value());\n+  Node* klass_node = in(AllocateNode::KlassNode);\n+  Node* proto_adr = phase->transform(new AddPNode(klass_node, klass_node, phase->MakeConX(in_bytes(Klass::prototype_header_offset()))));\n+  Node* mark_node = LoadNode::make(*phase, control, mem, proto_adr, TypeRawPtr::BOTTOM, TypeX_X, TypeX_X->basic_type(), MemNode::unordered);\n","filename":"src\/hotspot\/share\/opto\/callnode.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1021,0 +1021,1 @@\n+  reset_max_monitors();\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -339,0 +339,1 @@\n+  uint                  _max_monitors;          \/\/ Keep track of maximum number of active monitors in this compilation\n@@ -633,0 +634,4 @@\n+  void          push_monitor() { _max_monitors++; }\n+  void          reset_max_monitors() { _max_monitors = 0; }\n+  uint          max_monitors() { return _max_monitors; }\n+\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -426,0 +426,1 @@\n+    C->push_monitor();\n","filename":"src\/hotspot\/share\/opto\/parse1.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -5008,1 +5008,2 @@\n-    int header_size = objArrayOopDesc::header_size() * wordSize;\n+    BasicType basic_elem_type = elem()->basic_type();\n+    int header_size = arrayOopDesc::base_offset_in_bytes(basic_elem_type);\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -70,1 +70,1 @@\n-  ( arrayOopDesc::header_size(T_DOUBLE) * HeapWordSize \\\n+  ( arrayOopDesc::base_offset_in_bytes(T_DOUBLE) \\\n","filename":"src\/hotspot\/share\/prims\/unsafe.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1504,3 +1504,0 @@\n-      if (COMPRESSED_CLASS_POINTERS_DEPENDS_ON_COMPRESSED_OOPS) {\n-        FLAG_SET_DEFAULT(UseCompressedClassPointers, false);\n-      }\n@@ -1512,1 +1509,0 @@\n-\n@@ -1517,25 +1513,14 @@\n-  \/\/ On some architectures, the use of UseCompressedClassPointers implies the use of\n-  \/\/ UseCompressedOops. The reason is that the rheap_base register of said platforms\n-  \/\/ is reused to perform some optimized spilling, in order to use rheap_base as a\n-  \/\/ temp register. But by treating it as any other temp register, spilling can typically\n-  \/\/ be completely avoided instead. So it is better not to perform this trick. And by\n-  \/\/ not having that reliance, large heaps, or heaps not supporting compressed oops,\n-  \/\/ can still use compressed class pointers.\n-  if (COMPRESSED_CLASS_POINTERS_DEPENDS_ON_COMPRESSED_OOPS && !UseCompressedOops) {\n-    if (UseCompressedClassPointers) {\n-      warning(\"UseCompressedClassPointers requires UseCompressedOops\");\n-    }\n-    FLAG_SET_DEFAULT(UseCompressedClassPointers, false);\n-  } else {\n-    \/\/ Turn on UseCompressedClassPointers too\n-    if (FLAG_IS_DEFAULT(UseCompressedClassPointers)) {\n-      FLAG_SET_ERGO(UseCompressedClassPointers, true);\n-    }\n-    \/\/ Check the CompressedClassSpaceSize to make sure we use compressed klass ptrs.\n-    if (UseCompressedClassPointers) {\n-      if (CompressedClassSpaceSize > KlassEncodingMetaspaceMax) {\n-        warning(\"CompressedClassSpaceSize is too large for UseCompressedClassPointers\");\n-        FLAG_SET_DEFAULT(UseCompressedClassPointers, false);\n-      }\n-    }\n-  }\n+  if (!UseCompressedClassPointers) {\n+    \/\/ Lilliput requires compressed class pointers. Default shall reflect that.\n+    \/\/ If user specifies -UseCompressedClassPointers, it should be reverted with\n+    \/\/ a warning.\n+    assert(!FLAG_IS_DEFAULT(UseCompressedClassPointers), \"Wrong default for UseCompressedClassPointers\");\n+    warning(\"Lilliput reqires compressed class pointers.\");\n+    FLAG_SET_ERGO(UseCompressedClassPointers, true);\n+  }\n+  \/\/ Assert validity of compressed class space size. User arg should have been checked at this point\n+  \/\/ (see CompressedClassSpaceSizeConstraintFunc()), so no need to be nice about it, this fires in\n+  \/\/ case the default is wrong.\n+  assert(CompressedClassSpaceSize <= Metaspace::max_class_space_size(),\n+         \"CompressedClassSpaceSize \" SIZE_FORMAT \" too large (max: \" SIZE_FORMAT \")\",\n+         CompressedClassSpaceSize, Metaspace::max_class_space_size());\n@@ -1704,3 +1689,0 @@\n-          if (COMPRESSED_CLASS_POINTERS_DEPENDS_ON_COMPRESSED_OOPS) {\n-            FLAG_SET_ERGO(UseCompressedClassPointers, false);\n-          }\n@@ -3125,0 +3107,3 @@\n+  \/\/ Lilliput requires fast-locking.\n+  FLAG_SET_DEFAULT(UseFastLocking, true);\n+\n@@ -4096,0 +4081,1 @@\n+\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":18,"deletions":32,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -128,1 +128,1 @@\n-  product(bool, UseCompressedClassPointers, false,                          \\\n+  product(bool, UseCompressedClassPointers, true,                           \\\n@@ -1059,1 +1059,1 @@\n-  develop(bool, UseHeavyMonitors, false,                                    \\\n+  product(bool, UseHeavyMonitors, false, DIAGNOSTIC,                        \\\n@@ -1420,1 +1420,1 @@\n-          range(1*M, 3*G)                                                   \\\n+          constraint(CompressedClassSpaceSizeConstraintFunc,AtParse)        \\\n@@ -1422,1 +1422,1 @@\n-  develop(size_t, CompressedClassSpaceBaseAddress, 0,                       \\\n+  product(size_t, CompressedClassSpaceBaseAddress, 0, DIAGNOSTIC,           \\\n@@ -1985,0 +1985,9 @@\n+                                                                            \\\n+  product(bool, HeapObjectStats, false, DIAGNOSTIC,                         \\\n+             \"Enable gathering of heap object statistics\")                  \\\n+                                                                            \\\n+  product(size_t, HeapObjectStatsSamplingInterval, 500, DIAGNOSTIC,         \\\n+             \"Heap object statistics sampling interval (ms)\")               \\\n+                                                                            \\\n+  product(bool, UseFastLocking, false, EXPERIMENTAL,                        \\\n+                \"Use fast-locking instead of stack-locking\")                \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":13,"deletions":4,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -73,0 +73,1 @@\n+#include \"runtime\/lockStack.inline.hpp\"\n@@ -490,2 +491,3 @@\n-  _SleepEvent(ParkEvent::Allocate(this))\n-{\n+  _SleepEvent(ParkEvent::Allocate(this)),\n+\n+  _lock_stack() {\n@@ -988,0 +990,1 @@\n+  assert(!UseFastLocking, \"should not be called with fast-locking\");\n@@ -1381,0 +1384,4 @@\n+\n+  if (!UseHeavyMonitors && UseFastLocking) {\n+    lock_stack().oops_do(f);\n+  }\n","filename":"src\/hotspot\/share\/runtime\/javaThread.cpp","additions":9,"deletions":2,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"runtime\/lockStack.hpp\"\n@@ -1143,0 +1144,10 @@\n+private:\n+  LockStack _lock_stack;\n+\n+public:\n+  LockStack& lock_stack() { return _lock_stack; }\n+\n+  static ByteSize lock_stack_current_offset()    { return byte_offset_of(JavaThread, _lock_stack) + LockStack::current_offset(); }\n+  static ByteSize lock_stack_limit_offset()    { return byte_offset_of(JavaThread, _lock_stack) + LockStack::limit_offset(); }\n+\n+\n","filename":"src\/hotspot\/share\/runtime\/javaThread.hpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -337,1 +337,1 @@\n-  if (current->is_lock_owned((address)cur)) {\n+  if (!UseFastLocking && current->is_lock_owned((address)cur)) {\n@@ -603,0 +603,16 @@\n+\/\/ We might access the dead object headers for parsable heap walk, make sure\n+\/\/ headers are in correct shape, e.g. monitors deflated.\n+void ObjectMonitor::maybe_deflate_dead(oop* p) {\n+  oop obj = *p;\n+  assert(obj != NULL, \"must not yet been cleared\");\n+  markWord mark = obj->mark();\n+  if (mark.has_monitor()) {\n+    ObjectMonitor* monitor = mark.monitor();\n+    if (p == monitor->_object.ptr_raw()) {\n+      assert(monitor->object_peek() == obj, \"lock object must match\");\n+      markWord dmw = monitor->header();\n+      obj->set_mark(dmw);\n+    }\n+  }\n+}\n+\n@@ -1138,1 +1154,1 @@\n-    if (current->is_lock_owned((address)cur)) {\n+    if (!UseFastLocking && current->is_lock_owned((address)cur)) {\n@@ -1358,1 +1374,1 @@\n-    if (current->is_lock_owned((address)cur)) {\n+    if (!UseFastLocking && current->is_lock_owned((address)cur)) {\n@@ -1407,0 +1423,1 @@\n+  assert(cur != ANONYMOUS_OWNER, \"no anon owner here\");\n@@ -1410,1 +1427,1 @@\n-  if (current->is_lock_owned((address)cur)) {\n+  if (!UseFastLocking && current->is_lock_owned((address)cur)) {\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.cpp","additions":21,"deletions":4,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -147,2 +147,10 @@\n-  \/\/ Used by async deflation as a marker in the _owner field:\n-  #define DEFLATER_MARKER reinterpret_cast<void*>(-1)\n+  \/\/ Used by async deflation as a marker in the _owner field.\n+  \/\/ Note that the choice of the two markers is peculiar:\n+  \/\/ - They need to represent values that cannot be pointers. In particular,\n+  \/\/   we achieve this by using the lowest two bits\n+  \/\/ - ANONYMOUS_OWNER should be a small value, it is used in generated code\n+  \/\/   and small values encode much better\n+  \/\/ - We test for anonymous owner by testing for the lowest bit, therefore\n+  \/\/   DEFLATER_MARKER must *not* have that bit set.\n+  #define DEFLATER_MARKER reinterpret_cast<void*>(2)\n+  #define ANONYMOUS_OWNER reinterpret_cast<void*>(1)\n@@ -207,0 +215,1 @@\n+  static int header_offset_in_bytes()      { return offset_of(ObjectMonitor, _header); }\n@@ -266,0 +275,12 @@\n+  void set_owner_anonymous() {\n+    set_owner_from(NULL, ANONYMOUS_OWNER);\n+  }\n+\n+  bool is_owner_anonymous() const {\n+    return owner_raw() == ANONYMOUS_OWNER;\n+  }\n+\n+  void set_owner_from_anonymous(Thread* owner) {\n+    set_owner_from(ANONYMOUS_OWNER, owner);\n+  }\n+\n@@ -326,0 +347,2 @@\n+  static void maybe_deflate_dead(oop* p);\n+\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.hpp","additions":25,"deletions":2,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"runtime\/lockStack.inline.hpp\"\n@@ -36,3 +37,11 @@\n-  void* owner = owner_raw();\n-  if (current == owner || current->is_lock_owned((address)owner)) {\n-    return 1;\n+  if (UseFastLocking) {\n+    if (is_owner_anonymous()) {\n+      return current->lock_stack().contains(object()) ? 1 : 0;\n+    } else {\n+      return current == owner_raw() ? 1 : 0;\n+    }\n+  } else {\n+    void* owner = owner_raw();\n+    if (current == owner || current->is_lock_owned((address)owner)) {\n+      return 1;\n+    }\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.inline.hpp","additions":12,"deletions":3,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+#include \"oops\/compressedKlass.hpp\"\n","filename":"src\/hotspot\/share\/runtime\/os.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"gc\/shared\/suspendibleThreadSet.hpp\"\n@@ -42,0 +43,1 @@\n+#include \"runtime\/lockStack.inline.hpp\"\n@@ -314,1 +316,2 @@\n-  if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n+  if ((mark.is_fast_locked() && current->lock_stack().contains(oop(obj))) ||\n+      (mark.has_locker() && current->is_lock_owned((address)mark.locker()))) {\n@@ -397,1 +400,3 @@\n-    lock->set_displaced_header(markWord::unused_mark());\n+    if (!UseFastLocking) {\n+      lock->set_displaced_header(markWord::unused_mark());\n+    }\n@@ -486,6 +491,37 @@\n-    markWord mark = obj->mark();\n-    if (mark.is_neutral()) {\n-      \/\/ Anticipate successful CAS -- the ST of the displaced mark must\n-      \/\/ be visible <= the ST performed by the CAS.\n-      lock->set_displaced_header(mark);\n-      if (mark == obj()->cas_set_mark(markWord::from_pointer(lock), mark)) {\n+    if (UseFastLocking) {\n+      LockStack& lock_stack = current->lock_stack();\n+\n+      markWord header = obj()->mark_acquire();\n+      while (true) {\n+        if (header.is_neutral()) {\n+          assert(!lock_stack.contains(obj()), \"thread must not already hold the lock\");\n+          \/\/ Try to swing into 'fast-locked' state without inflating.\n+          markWord locked_header = header.set_fast_locked();\n+          markWord witness = obj()->cas_set_mark(locked_header, header);\n+          if (witness == header) {\n+            \/\/ Successfully fast-locked, push object to lock-stack and return.\n+            lock_stack.push(obj());\n+            return;\n+          }\n+          \/\/ Otherwise retry.\n+          header = witness;\n+        } else {\n+          \/\/ Fall-through to inflate-enter.\n+          break;\n+        }\n+      }\n+    } else {\n+      markWord mark = obj->mark();\n+      if (mark.is_neutral()) {\n+        \/\/ Anticipate successful CAS -- the ST of the displaced mark must\n+        \/\/ be visible <= the ST performed by the CAS.\n+        lock->set_displaced_header(mark);\n+        if (mark == obj()->cas_set_mark(markWord::from_pointer(lock), mark)) {\n+          return;\n+        }\n+        \/\/ Fall through to inflate() ...\n+      } else if (mark.has_locker() &&\n+                 current->is_lock_owned((address)mark.locker())) {\n+        assert(lock != mark.locker(), \"must not re-lock the same lock\");\n+        assert(lock != (BasicLock*)obj->mark().value(), \"don't relock with same BasicLock\");\n+        lock->set_displaced_header(markWord::from_pointer(nullptr));\n@@ -494,13 +530,6 @@\n-      \/\/ Fall through to inflate() ...\n-    } else if (mark.has_locker() &&\n-               current->is_lock_owned((address)mark.locker())) {\n-      assert(lock != mark.locker(), \"must not re-lock the same lock\");\n-      assert(lock != (BasicLock*)obj->mark().value(), \"don't relock with same BasicLock\");\n-      lock->set_displaced_header(markWord::from_pointer(nullptr));\n-      return;\n-    }\n-    \/\/ The object header will never be displaced to this lock,\n-    \/\/ so it does not matter what the value is, except that it\n-    \/\/ must be non-zero to avoid looking like a re-entrant lock,\n-    \/\/ and must not look locked either.\n-    lock->set_displaced_header(markWord::unused_mark());\n+      \/\/ The object header will never be displaced to this lock,\n+      \/\/ so it does not matter what the value is, except that it\n+      \/\/ must be non-zero to avoid looking like a re-entrant lock,\n+      \/\/ and must not look locked either.\n+      lock->set_displaced_header(markWord::unused_mark());\n+    }\n@@ -509,1 +538,1 @@\n-    guarantee(!obj->mark().has_locker(), \"must not be stack-locked\");\n+    guarantee(!obj->mark().has_locker() && !obj->mark().is_fast_locked(), \"must not be stack-locked\");\n@@ -528,25 +557,12 @@\n-\n-    markWord dhw = lock->displaced_header();\n-    if (dhw.value() == 0) {\n-      \/\/ If the displaced header is null, then this exit matches up with\n-      \/\/ a recursive enter. No real work to do here except for diagnostics.\n-#ifndef PRODUCT\n-      if (mark != markWord::INFLATING()) {\n-        \/\/ Only do diagnostics if we are not racing an inflation. Simply\n-        \/\/ exiting a recursive enter of a Java Monitor that is being\n-        \/\/ inflated is safe; see the has_monitor() comment below.\n-        assert(!mark.is_neutral(), \"invariant\");\n-        assert(!mark.has_locker() ||\n-        current->is_lock_owned((address)mark.locker()), \"invariant\");\n-        if (mark.has_monitor()) {\n-          \/\/ The BasicLock's displaced_header is marked as a recursive\n-          \/\/ enter and we have an inflated Java Monitor (ObjectMonitor).\n-          \/\/ This is a special case where the Java Monitor was inflated\n-          \/\/ after this thread entered the stack-lock recursively. When a\n-          \/\/ Java Monitor is inflated, we cannot safely walk the Java\n-          \/\/ Monitor owner's stack and update the BasicLocks because a\n-          \/\/ Java Monitor can be asynchronously inflated by a thread that\n-          \/\/ does not own the Java Monitor.\n-          ObjectMonitor* m = mark.monitor();\n-          assert(m->object()->mark() == mark, \"invariant\");\n-          assert(m->is_entered(current), \"invariant\");\n+    if (UseFastLocking) {\n+      if (mark.is_fast_locked()) {\n+        markWord unlocked_header = mark.set_unlocked();\n+        markWord witness = object->cas_set_mark(unlocked_header, mark);\n+        if (witness != mark) {\n+          \/\/ Another thread beat us, it can only have installed an anonymously locked monitor at this point.\n+          \/\/ Fetch that monitor, set owner correctly to this thread, and exit it (allowing waiting threads to enter).\n+          assert(witness.has_monitor(), \"must have monitor\");\n+          ObjectMonitor* monitor = witness.monitor();\n+          assert(monitor->is_owner_anonymous(), \"must be anonymous owner\");\n+          monitor->set_owner_from_anonymous(current);\n+          monitor->exit(current);\n@@ -554,0 +570,3 @@\n+        LockStack& lock_stack = current->lock_stack();\n+        lock_stack.remove(object);\n+        return;\n@@ -555,0 +574,27 @@\n+    } else {\n+      markWord dhw = lock->displaced_header();\n+      if (dhw.value() == 0) {\n+        \/\/ If the displaced header is null, then this exit matches up with\n+        \/\/ a recursive enter. No real work to do here except for diagnostics.\n+#ifndef PRODUCT\n+        if (mark != markWord::INFLATING()) {\n+          \/\/ Only do diagnostics if we are not racing an inflation. Simply\n+          \/\/ exiting a recursive enter of a Java Monitor that is being\n+          \/\/ inflated is safe; see the has_monitor() comment below.\n+          assert(!mark.is_neutral(), \"invariant\");\n+          assert(!mark.has_locker() ||\n+                 current->is_lock_owned((address)mark.locker()), \"invariant\");\n+          if (mark.has_monitor()) {\n+            \/\/ The BasicLock's displaced_header is marked as a recursive\n+            \/\/ enter and we have an inflated Java Monitor (ObjectMonitor).\n+            \/\/ This is a special case where the Java Monitor was inflated\n+            \/\/ after this thread entered the stack-lock recursively. When a\n+            \/\/ Java Monitor is inflated, we cannot safely walk the Java\n+            \/\/ Monitor owner's stack and update the BasicLocks because a\n+            \/\/ Java Monitor can be asynchronously inflated by a thread that\n+            \/\/ does not own the Java Monitor.\n+            ObjectMonitor* m = mark.monitor();\n+            assert(m->object()->mark() == mark, \"invariant\");\n+            assert(m->is_entered(current), \"invariant\");\n+          }\n+        }\n@@ -556,8 +602,0 @@\n-      return;\n-    }\n-\n-    if (mark == markWord::from_pointer(lock)) {\n-      \/\/ If the object is stack-locked by the current thread, try to\n-      \/\/ swing the displaced header from the BasicLock back to the mark.\n-      assert(dhw.is_neutral(), \"invariant\");\n-      if (object->cas_set_mark(dhw, mark) == mark) {\n@@ -566,0 +604,9 @@\n+\n+      if (mark == markWord::from_pointer(lock)) {\n+        \/\/ If the object is stack-locked by the current thread, try to\n+        \/\/ swing the displaced header from the BasicLock back to the mark.\n+        assert(dhw.is_neutral(), \"invariant\");\n+        if (object->cas_set_mark(dhw, mark) == mark) {\n+          return;\n+        }\n+      }\n@@ -575,0 +622,7 @@\n+  if (UseFastLocking && monitor->is_owner_anonymous()) {\n+    \/\/ It must be us. Pop lock object from lock stack.\n+    LockStack& lock_stack = current->lock_stack();\n+    oop popped = lock_stack.pop();\n+    assert(popped == object, \"must be owned by this thread\");\n+    monitor->set_owner_from_anonymous(current);\n+  }\n@@ -701,1 +755,2 @@\n-  if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n+  if ((mark.is_fast_locked() && current->lock_stack().contains(obj())) ||\n+      (mark.has_locker() && current->is_lock_owned((address)mark.locker()))) {\n@@ -716,1 +771,2 @@\n-  if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n+  if ((mark.is_fast_locked() && current->lock_stack().contains(obj())) ||\n+      (mark.has_locker() && current->is_lock_owned((address)mark.locker()))) {\n@@ -744,1 +800,1 @@\n-  if (!mark.is_being_inflated()) {\n+  if (!mark.is_being_inflated() || UseFastLocking) {\n@@ -859,0 +915,5 @@\n+static bool is_lock_owned(Thread* thread, oop obj) {\n+  assert(UseFastLocking, \"only call this with fast-locking enabled\");\n+  return thread->is_Java_thread() ? reinterpret_cast<JavaThread*>(thread)->lock_stack().contains(obj) : false;\n+}\n+\n@@ -913,1 +974,8 @@\n-    } else if (current->is_lock_owned((address)mark.locker())) {\n+    } else if (mark.is_fast_locked() && is_lock_owned(current, obj)) {\n+      \/\/ This is a fast lock owned by the calling thread so use the\n+      \/\/ markWord from the object.\n+      hash = mark.hash();\n+      if (hash != 0) {                  \/\/ if it has a hash, just return it\n+        return hash;\n+      }\n+    } else if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n@@ -982,0 +1050,6 @@\n+\n+  \/\/ Fast-locking case.\n+  if (mark.is_fast_locked()) {\n+    return current->lock_stack().contains(h_obj());\n+  }\n+\n@@ -996,2 +1070,0 @@\n-  address owner = nullptr;\n-\n@@ -1002,1 +1074,5 @@\n-    owner = (address) mark.locker();\n+    return Threads::owning_thread_from_monitor_owner(t_list, (address) mark.locker());\n+  }\n+\n+  if (mark.is_fast_locked()) {\n+    return Threads::owning_thread_from_object(t_list, h_obj());\n@@ -1006,1 +1082,1 @@\n-  else if (mark.has_monitor()) {\n+  if (mark.has_monitor()) {\n@@ -1011,1 +1087,1 @@\n-    owner = (address) monitor->owner();\n+    return Threads::owning_thread_from_monitor(t_list, monitor);\n@@ -1014,10 +1090,0 @@\n-  if (owner != nullptr) {\n-    \/\/ owning_thread_from_monitor_owner() may also return null here\n-    return Threads::owning_thread_from_monitor_owner(t_list, owner);\n-  }\n-\n-  \/\/ Unlocked case, header in place\n-  \/\/ Cannot have assertion since this object may have been\n-  \/\/ locked by another thread when reaching here.\n-  \/\/ assert(mark.is_neutral(), \"sanity check\");\n-\n@@ -1221,0 +1287,5 @@\n+      if (UseFastLocking && inf->is_owner_anonymous() && is_lock_owned(current, object)) {\n+        inf->set_owner_from_anonymous(current);\n+        assert(current->is_Java_thread(), \"must be Java thread\");\n+        reinterpret_cast<JavaThread*>(current)->lock_stack().remove(object);\n+      }\n@@ -1230,1 +1301,4 @@\n-    if (mark == markWord::INFLATING()) {\n+    \/\/ NOTE: We need to check UseFastLocking here, because with fast-locking, the header\n+    \/\/ may legitimately be zero: cleared lock-bits and all upper header bits zero.\n+    \/\/ With fast-locking, the INFLATING protocol is not used.\n+    if (mark == markWord::INFLATING() && !UseFastLocking) {\n@@ -1246,0 +1320,42 @@\n+    if (mark.is_fast_locked()) {\n+      assert(UseFastLocking, \"can only happen with fast-locking\");\n+      ObjectMonitor* monitor = new ObjectMonitor(object);\n+      monitor->set_header(mark.set_unlocked());\n+      bool own = is_lock_owned(current, object);\n+      if (own) {\n+        \/\/ Owned by us.\n+        monitor->set_owner_from(nullptr, current);\n+      } else {\n+        \/\/ Owned by somebody else.\n+        monitor->set_owner_anonymous();\n+      }\n+      markWord monitor_mark = markWord::encode(monitor);\n+      markWord witness = object->cas_set_mark(monitor_mark, mark);\n+      if (witness == mark) {\n+        \/\/ Success! Return inflated monitor.\n+        if (own) {\n+          assert(current->is_Java_thread(), \"must be: checked in is_lock_owned()\");\n+          reinterpret_cast<JavaThread*>(current)->lock_stack().remove(object);\n+        }\n+        \/\/ Once the ObjectMonitor is configured and object is associated\n+        \/\/ with the ObjectMonitor, it is safe to allow async deflation:\n+        _in_use_list.add(monitor);\n+\n+        \/\/ Hopefully the performance counters are allocated on distinct\n+        \/\/ cache lines to avoid false sharing on MP systems ...\n+        OM_PERFDATA_OP(Inflations, inc());\n+        if (log_is_enabled(Trace, monitorinflation)) {\n+          ResourceMark rm(current);\n+          lsh.print_cr(\"inflate(has_locker): object=\" INTPTR_FORMAT \", mark=\"\n+                       INTPTR_FORMAT \", type='%s'\", p2i(object),\n+                       object->mark().value(), object->klass()->external_name());\n+        }\n+        if (event.should_commit()) {\n+          post_monitor_inflate_event(&event, object, cause);\n+        }\n+        return monitor;\n+      } else {\n+        delete monitor;\n+        continue;\n+      }\n+    }\n@@ -1248,0 +1364,1 @@\n+      assert(!UseFastLocking, \"can not happen with fast-locking\");\n@@ -1461,0 +1578,10 @@\n+class VM_RendezvousGCThreads : public VM_Operation {\n+public:\n+  bool evaluate_at_safepoint() const override { return false; }\n+  VMOp_Type type() const override { return VMOp_RendezvousGCThreads; }\n+  void doit() override {\n+    SuspendibleThreadSet::synchronize();\n+    SuspendibleThreadSet::desynchronize();\n+  };\n+};\n+\n@@ -1514,0 +1641,3 @@\n+      \/\/ Also, we sync and desync GC threads around the handshake, so that they can\n+      \/\/ safely read the mark-word and look-through to the object-monitor, without\n+      \/\/ being afraid that the object-monitor is going away.\n@@ -1516,0 +1646,2 @@\n+      VM_RendezvousGCThreads sync_gc;\n+      VMThread::execute(&sync_gc);\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":207,"deletions":75,"binary":false,"changes":282,"status":"modified"},{"patch":"@@ -536,0 +536,1 @@\n+  assert(!UseFastLocking, \"should not be called with fast-locking\");\n","filename":"src\/hotspot\/share\/runtime\/thread.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -71,0 +71,1 @@\n+#include \"runtime\/lockStack.inline.hpp\"\n@@ -1391,0 +1392,1 @@\n+  assert(!UseFastLocking, \"only with stack-locking\");\n@@ -1420,0 +1422,10 @@\n+JavaThread* Threads::owning_thread_from_object(ThreadsList * t_list, oop obj) {\n+  assert(UseFastLocking, \"Only with fast-locking\");\n+  for (JavaThread* q : *t_list) {\n+    if (q->lock_stack().contains(obj)) {\n+      return q;\n+    }\n+  }\n+  return nullptr;\n+}\n+\n@@ -1421,2 +1433,12 @@\n-  address owner = (address)monitor->owner();\n-  return owning_thread_from_monitor_owner(t_list, owner);\n+  if (UseFastLocking) {\n+    if (monitor->is_owner_anonymous()) {\n+      return owning_thread_from_object(t_list, monitor->object());\n+    } else {\n+      Thread* owner = reinterpret_cast<Thread*>(monitor->owner());\n+      assert(owner == nullptr || owner->is_Java_thread(), \"only JavaThreads own monitors\");\n+      return reinterpret_cast<JavaThread*>(owner);\n+    }\n+  } else {\n+    address owner = (address)monitor->owner();\n+    return owning_thread_from_monitor_owner(t_list, owner);\n+  }\n","filename":"src\/hotspot\/share\/runtime\/threads.cpp","additions":24,"deletions":2,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -141,0 +141,1 @@\n+  static JavaThread* owning_thread_from_object(ThreadsList* t_list, oop obj);\n","filename":"src\/hotspot\/share\/runtime\/threads.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -83,0 +83,1 @@\n+  template(HeapObjectStatistics)                  \\\n@@ -92,0 +93,1 @@\n+  template(RendezvousGCThreads)                   \\\n","filename":"src\/hotspot\/share\/runtime\/vmOperation.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -201,2 +201,1 @@\n-  volatile_nonstatic_field(oopDesc,            _metadata._klass,                              Klass*)                                \\\n-  volatile_nonstatic_field(oopDesc,            _metadata._compressed_klass,                   narrowKlass)                           \\\n+  NOT_LP64(volatile_nonstatic_field(oopDesc,   _klass,                                        Klass*))                               \\\n@@ -380,2 +379,2 @@\n-     static_field(CompressedKlassPointers,     _narrow_klass._base,                           address)                               \\\n-     static_field(CompressedKlassPointers,     _narrow_klass._shift,                          int)                                   \\\n+     static_field(CompressedKlassPointers,     _base,                           address)                                             \\\n+     static_field(CompressedKlassPointers,     _shift_copy,                          int)                                            \\\n@@ -702,0 +701,3 @@\n+  nonstatic_field(JavaThread,                  _lock_stack,                                   LockStack)                             \\\n+  nonstatic_field(LockStack,                   _current,                                      oop*)                                  \\\n+  nonstatic_field(LockStack,                   _base,                                         oop*)                                  \\\n@@ -1315,0 +1317,1 @@\n+  declare_toplevel_type(LockStack)                                        \\\n@@ -2605,0 +2608,1 @@\n+  LP64_ONLY(declare_constant(markWord::klass_shift))                      \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":8,"deletions":4,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -598,5 +598,0 @@\n-const int LogKlassAlignmentInBytes = 3;\n-const int LogKlassAlignment        = LogKlassAlignmentInBytes - LogHeapWordSize;\n-const int KlassAlignmentInBytes    = 1 << LogKlassAlignmentInBytes;\n-const int KlassAlignment           = KlassAlignmentInBytes \/ HeapWordSize;\n-\n@@ -610,5 +605,0 @@\n-\/\/ Maximal size of compressed class space. Above this limit compression is not possible.\n-\/\/ Also upper bound for placement of zero based class space. (Class space is further limited\n-\/\/ to be < 3G, see arguments.cpp.)\n-const  uint64_t KlassEncodingMetaspaceMax = (uint64_t(max_juint) + 1) << LogKlassAlignmentInBytes;\n-\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":0,"deletions":10,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"oops\/compressedKlass.hpp\"\n","filename":"src\/hotspot\/share\/utilities\/vmError.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -76,1 +76,2 @@\n-    final int hubOffset = getFieldOffset(\"oopDesc::_metadata._klass\", Integer.class, \"Klass*\");\n+    \/\/ TODO: Lilliput. Probably ok.\n+    final int hubOffset = 4; \/\/ getFieldOffset(\"oopDesc::_metadata._klass\", Integer.class, \"Klass*\");\n","filename":"src\/jdk.internal.vm.ci\/share\/classes\/jdk.vm.ci.hotspot\/src\/jdk\/vm\/ci\/hotspot\/HotSpotVMConfig.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -172,0 +172,4 @@\n+\n+\n+Lilliput temporary:\n+compiler\/c2\/irTests\/TestVectorizationNotRun.java 8301785 generic-all\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -0,0 +1,260 @@\n+\/*\n+ * Copyright (c) 2013, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+\/\/ These tests test that narrow Klass pointer encoding\/decoding work.\n+\/\/\n+\/\/ Note that we do not enforce the encoding base directly. We enforce base and size of the compressed class space.\n+\/\/ The hotspot then decides on the best encoding range and scheme to chose for the given range.\n+\/\/\n+\/\/ So what we really test here is that for a given range-to-encode:\n+\/\/  - the chosen encoding range and architecture-specific mode makes sense - e.g. if range fits into low address\n+\/\/    space, use base=0 and zero-based encoding.\n+\/\/  - and that the chosen encoding actually works by starting a simple program which loads a bunch of classes.\n+\/\/\n+\/\/  In order for that to work, we have to switch of CDS. Switching off CDS means the hotspot choses the encoding base\n+\/\/  based on the class space base address (we just know this - see CompressedKlassPointers::initialize() - and if this\n+\/\/  changes, we may have to adapt this test).\n+\/\/\n+\/\/  Switching off CDS also means we use the class space much more fully. More Klass structures stored in that range\n+\/\/  and we exercise the ability of Metaspace to allocate Klass structures with the correct alignment, compatible to\n+\/\/  encoding.\n+\n+\/*\n+ * @test id=x64-area-beyond-encoding-range-use-xor\n+ * @requires os.arch==\"amd64\" | os.arch==\"x86_64\"\n+ * @requires vm.flagless\n+ * @library \/test\/lib\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.management\n+ * @run driver CompressedClassPointerEncoding\n+ *\/\n+\n+import jdk.test.lib.Platform;\n+import jdk.test.lib.process.OutputAnalyzer;\n+import jdk.test.lib.process.ProcessTools;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+\n+public class CompressedClassPointerEncoding {\n+\n+    \/\/ Replace:\n+    \/\/ $1 with force base address\n+    \/\/ $2 with compressed class space size\n+    final static String[] vmOptionsTemplate = new String[] {\n+      \"-XX:CompressedClassSpaceBaseAddress=$1\",\n+      \"-XX:CompressedClassSpaceSize=$2\",\n+      \"-Xshare:off\",                         \/\/ Disable CDS\n+      \"-Xlog:metaspace*\",                    \/\/ for analysis\n+      \"-XX:+PrintMetaspaceStatisticsAtExit\", \/\/ for analysis\n+      \"-version\"\n+    };\n+\n+    \/\/ Replace:\n+    \/\/ $1 with expected ccs base address (extended hex printed)\n+    \/\/ $2 with expected encoding base (extended hex printed)\n+    \/\/ $3 with expected encoding shift\n+    \/\/ $4 with expected encoding range\n+    \/\/ $5 with expected encoding mode\n+    final String[] expectedOutputTemplate = new String[] {\n+            \".*Sucessfully forced class space address to $1.*\",\n+            \".*CDS archive(s) not mapped.*\",\n+            \".*Narrow klass base: $2, Narrow klass shift: $3, Narrow klass range: $4, Encoding mode $5.*\"\n+    };\n+\n+    final static long M = 1024 * 1024;\n+    final static long G = 1024 * M;\n+\n+    final static long expectedShift = 9;\n+    final static long expectedEncodingRange = 2 * G;\n+    final static long defaultCCSSize = 32 * M;\n+\n+    enum EPlatform {\n+        \/\/ Add more where needed\n+        \/\/ (Note: this would be useful in Platform.java)\n+        linux_aarch64,\n+        linux_x64,\n+        unknown\n+    };\n+\n+    static EPlatform getCurrentPlatform() {\n+        if (Platform.isAArch64() && Platform.isLinux()) {\n+            return EPlatform.linux_aarch64;\n+        } else if (Platform.isX64() && Platform.isLinux()) {\n+            return EPlatform.linux_x64;\n+        }\n+        return EPlatform.unknown;\n+    }\n+\n+    static class TestDetails {\n+        public final EPlatform platform;\n+        public final String name;\n+        public final long[] baseAdressesToTry;\n+        public final long compressedClassSpaceSize;\n+        public final long expectedEncodingBase;\n+        public final String expectedEncodingMode;\n+\n+        public TestDetails(EPlatform platform, String name, long[] baseAdressesToTry,\n+                           long compressedClassSpaceSize, long expectedEncodingBase, String expectedEncodingMode) {\n+            this.platform = platform;\n+            this.name = name;\n+            this.baseAdressesToTry = baseAdressesToTry;\n+            this.compressedClassSpaceSize = compressedClassSpaceSize;\n+            this.expectedEncodingBase = expectedEncodingBase;\n+            this.expectedEncodingMode = expectedEncodingMode;\n+        }\n+\n+        \/\/ Simplified, common version: one base address (which we assume always works) and 32G ccs size\n+        public TestDetails(EPlatform platform, String name, long baseAdress,\n+                           long expectedEncodingBase, String expectedEncodingMode) {\n+            this(platform, name, new long[]{ baseAdress }, defaultCCSSize,\n+                 expectedEncodingBase, expectedEncodingMode);\n+        }\n+    };\n+\n+    static TestDetails[] testDetails = new TestDetails[] {\n+\n+            \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+            \/\/\/\/\/\/ x64 \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+            \/\/ CCS base beyond encoding range (base=2G). Base does does not intersect the uncompressed klass pointer\n+            \/\/ bits. Encoding cannot be zero, and we should use xor+shift mode.\n+            new TestDetails(EPlatform.linux_x64,\n+                    \"x64-area-beyond-encoding-range-use-xor\",\n+                    2 * G,\n+                    2 * G,\n+                    \"xor\"),\n+\n+            \/\/ CCS partly contained in encoding range. We cannot use zero based encoding. We cannot use xor either,\n+            \/\/ since the first part of the ccs intersects the encoding range. Encoding hould use add+shift.\n+            \/*\n+            new TestDetails(EPlatform.linux_x64,\n+                    \"x64-area-partly-within-encoding-range-use-add\",\n+                    0x7fc00000,\n+                    2 * G,\n+                    \"add\"),\n+            *\/\n+\n+            \/\/ CCS (just) fully contained in encoding range (base=2G-ccs size). Expect zero-based encoding.\n+            new TestDetails(EPlatform.linux_x64,\n+                    \"x64-area-within-encoding-range-use-zero\",\n+                    0x7e000000, \/\/ 2G - 32M (ccs size)\n+                    0,\n+                    \"zero\"),\n+\n+            \/\/ CCS located far beyond the zero-based limit. Base does not intersect with narrow Klass pointer bits.\n+            \/\/ We should use xor.\n+            new TestDetails(EPlatform.linux_x64,\n+                    \"x64-area-far-out-no-low-bits-use-xor\",\n+                    0x800000000L, \/\/ 32G\n+                    0x800000000L,\n+                    \"xor\"),\n+\n+            \/\/ CCS located far beyond the zero-based limit. Base address intersects with narrow Klass pointer bits.\n+            \/\/ We should use add.\n+            \/*\n+            new TestDetails(EPlatform.linux_x64,\n+                    \"x64-area-far-out-with-low-bits-use-add\",\n+                    0x800800000L, \/\/ 32G + 8M (4M is minimum ccs alignment)\n+                    0x800800000L,\n+                    \"xor\"),\n+            *\/\n+\n+            \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+            \/\/\/\/\/\/ aarch64 \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+\n+            \/\/ CCS with a base which is a valid immediate, does not intersect the uncompressed klass pointer bits,\n+            \/\/ should use xor+shift\n+            new TestDetails(EPlatform.linux_aarch64,\n+                    \"aarch64-area-beyond-encoding-range-base-valid-immediate-use-xor\",\n+                    0x800000000L, \/\/ 32G\n+                    800000000L,\n+                    \"xor\")\n+\n+            \/\/ ... add more\n+\n+    };\n+\n+    \/\/ Helper function. Given a string, replace $1 ... $n with\n+    \/\/ replacement_strings[0] ... replacement_strings[n]\n+    static private String replacePlaceholdersInString(String original, String ...replacement_strings) {\n+        String result = original;\n+        int repl_id = 1; \/\/ 1 based\n+        for (String replacement : replacement_strings) {\n+            String placeholder = \"$\" + repl_id;\n+            result = result.replace(placeholder, replacement);\n+            repl_id ++;\n+        }\n+        return result;\n+    }\n+\n+    \/\/ Helper function. Given a string array, replace $1 ... $n with\n+    \/\/ replacement_strings[0] ... replacement_strings[n]\n+    static private String[] replacePlaceholdersInArray(String[] original, String ...replacement_strings) {\n+        String[] copy = new String[original.length];\n+        for (int n = 0; n < copy.length; n ++) {\n+            copy[n] = replacePlaceholdersInString(original[n], replacement_strings);\n+        }\n+        return copy;\n+    }\n+\n+    static void runTest(TestDetails details) throws IOException {\n+        System.err.println(\"----------------------------------------------------\");\n+        System.err.println(\"Running Test: \" + details.name);\n+        System.err.println(details);\n+\n+        long ccsBaseAddress = details.baseAdressesToTry[0];\n+        String ccsBaseAddressAsHex = String.format(\"0x%016x\", ccsBaseAddress);\n+\n+        \/\/ VM Options: replace:\n+        \/\/ $1 with force base address\n+        \/\/ $2 with compressed class space size\n+        String[] vmOptions = replacePlaceholdersInArray(vmOptionsTemplate,\n+                ccsBaseAddressAsHex,              \/\/ $1\n+                (details.compressedClassSpaceSize \/ M) + \"M\");    \/\/ $2\n+\n+        ProcessBuilder pb = ProcessTools.createJavaProcessBuilder(vmOptions);\n+        OutputAnalyzer output = new OutputAnalyzer(pb.start());\n+\n+        System.err.println(\"----------------------------------------------------\");\n+        System.err.println(Arrays.toString(vmOptions));\n+        output.reportDiagnosticSummary();\n+        System.err.println(\"----------------------------------------------------\");\n+\n+        output.shouldHaveExitValue(0);\n+\n+    }\n+\n+    static void runTestsForPlatform(EPlatform platform) throws IOException {\n+        for (TestDetails details : testDetails) {\n+            if (details.platform == platform) {\n+                runTest(details);\n+            }\n+        }\n+    }\n+\n+    public static void main(String[] args) throws Exception {\n+        runTestsForPlatform(getCurrentPlatform());\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/runtime\/CompressedOops\/CompressedClassPointerEncoding.java","additions":260,"deletions":0,"binary":false,"changes":260,"status":"added"},{"patch":"@@ -41,0 +41,7 @@\n+    \/\/ Sizes beyond this will be rejected by hotspot arg parsing\n+    \/\/ (Lilliput: see Metaspace::max_class_space_size() for details)\n+    static final long max_class_space_size = 2013265920;\n+\n+    \/\/ Below this size class space will be silently enlarged to a multiple of this size\n+    static final long min_class_space_size = 16777216;\n+\n@@ -44,6 +51,0 @@\n-        \/\/ Minimum size is 1MB\n-        pb = ProcessTools.createJavaProcessBuilder(\"-XX:CompressedClassSpaceSize=0\",\n-                                                   \"-version\");\n-        output = new OutputAnalyzer(pb.start());\n-        output.shouldContain(\"outside the allowed range\")\n-              .shouldHaveExitValue(1);\n@@ -58,0 +59,1 @@\n+        \/\/\/\/\/\/\/\/\/\/\/\n@@ -59,2 +61,4 @@\n-        \/\/ Maximum size is 3GB\n-        pb = ProcessTools.createJavaProcessBuilder(\"-XX:CompressedClassSpaceSize=4g\",\n+        \/\/ Going below the minimum size for class space (one root chunk size = atm 4M) should be transparently\n+        \/\/ handled by the hotspot, which should round up class space size and not report an error.\n+        pb = ProcessTools.createJavaProcessBuilder(\"-XX:CompressedClassSpaceSize=1m\",\n+                                                   \"-Xlog:gc+metaspace=trace\",\n@@ -63,2 +67,2 @@\n-        output.shouldContain(\"outside the allowed range\")\n-              .shouldHaveExitValue(1);\n+        output.shouldMatch(\"Compressed class space.*\" + min_class_space_size)\n+              .shouldHaveExitValue(0);\n@@ -66,0 +70,1 @@\n+        \/\/\/\/\/\/\/\/\/\/\/\n@@ -67,7 +72,4 @@\n-        \/\/ Make sure the minimum size is set correctly and printed\n-        \/\/ (Note: ccs size are rounded up to the next larger root chunk boundary (16m).\n-        \/\/ Note that this is **reserved** size and does not affect rss.\n-        pb = ProcessTools.createJavaProcessBuilder(\"-XX:+UnlockDiagnosticVMOptions\",\n-                                                   \"-XX:CompressedClassSpaceSize=1m\",\n-                                                   \"-Xlog:gc+metaspace=trace\",\n-                                                   \"-version\");\n+        \/\/ Try 0. Same result expected.\n+        pb = ProcessTools.createJavaProcessBuilder(\"-XX:CompressedClassSpaceSize=0\",\n+                \"-Xlog:gc+metaspace=trace\",\n+                \"-version\");\n@@ -75,2 +77,2 @@\n-        output.shouldMatch(\"Compressed class space.*16777216\")\n-              .shouldHaveExitValue(0);\n+        output.shouldMatch(\"Compressed class space.*\" + min_class_space_size)\n+                .shouldHaveExitValue(0);\n@@ -78,0 +80,1 @@\n+        \/\/\/\/\/\/\/\/\/\/\/\n@@ -79,1 +82,1 @@\n-        \/\/ Make sure the maximum size is set correctly and printed\n+        \/\/ Try max allowed size, which should be accepted\n@@ -81,1 +84,1 @@\n-                                                   \"-XX:CompressedClassSpaceSize=3g\",\n+                                                   \"-XX:CompressedClassSpaceSize=\" + max_class_space_size,\n@@ -85,1 +88,1 @@\n-        output.shouldMatch(\"Compressed class space.*3221225472\")\n+        output.shouldMatch(\"Compressed class space.*\" + max_class_space_size)\n@@ -88,0 +91,1 @@\n+        \/\/\/\/\/\/\/\/\/\/\/\n@@ -89,3 +93,5 @@\n-        pb = ProcessTools.createJavaProcessBuilder(\"-XX:-UseCompressedClassPointers\",\n-                                                   \"-XX:CompressedClassSpaceSize=1m\",\n-                                                   \"-version\");\n+        \/\/ Set max allowed size + 1, which should graciously fail\n+        pb = ProcessTools.createJavaProcessBuilder(\"-XX:+UnlockDiagnosticVMOptions\",\n+                \"-XX:CompressedClassSpaceSize=\" + (max_class_space_size + 1),\n+                \"-Xlog:gc+metaspace=trace\",\n+                \"-version\");\n@@ -93,2 +99,3 @@\n-        output.shouldContain(\"Setting CompressedClassSpaceSize has no effect when compressed class pointers are not used\")\n-              .shouldHaveExitValue(0);\n+        output.shouldContain(\"CompressedClassSpaceSize \" + (max_class_space_size + 1) + \" too large (max: \" + max_class_space_size)\n+              .shouldHaveExitValue(1);\n+\n","filename":"test\/hotspot\/jtreg\/runtime\/CompressedOops\/CompressedClassSpaceSize.java","additions":34,"deletions":27,"binary":false,"changes":61,"status":"modified"}]}