{"files":[{"patch":"@@ -124,0 +124,1 @@\n+          --variant=minbase\n","filename":".github\/workflows\/build-cross-compile.yml","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -803,0 +803,1 @@\n+  # The minidumps are disabled by default on client Windows, so enable them\n@@ -805,0 +806,1 @@\n+    $1_JTREG_BASIC_OPTIONS += -vmoption:-XX:+CreateCoredumpOnCrash\n","filename":"make\/RunTests.gmk","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2288,1 +2288,1 @@\n-const bool Matcher::match_rule_supported(int opcode) {\n+bool Matcher::match_rule_supported(int opcode) {\n@@ -2292,1 +2292,0 @@\n-  bool ret_value = true;\n@@ -2300,1 +2299,1 @@\n-        ret_value = false;\n+        return false;\n@@ -2305,2 +2304,10 @@\n-      if (!(UseSVE > 1 && VM_Version::supports_svebitperm())) {\n-        ret_value = false;\n+      if (!VM_Version::supports_svebitperm()) {\n+        return false;\n+      }\n+      break;\n+    case Op_FmaF:\n+    case Op_FmaD:\n+    case Op_FmaVF:\n+    case Op_FmaVD:\n+      if (!UseFMA) {\n+        return false;\n@@ -2311,1 +2318,1 @@\n-  return ret_value; \/\/ Per default match rules are supported.\n+  return true; \/\/ Per default match rules are supported.\n@@ -2323,1 +2330,1 @@\n-const bool Matcher::supports_vector_calling_convention(void) {\n+bool Matcher::supports_vector_calling_convention(void) {\n@@ -2343,1 +2350,1 @@\n-const int Matcher::vector_width_in_bytes(BasicType bt) {\n+int Matcher::vector_width_in_bytes(BasicType bt) {\n@@ -2354,1 +2361,1 @@\n-const int Matcher::max_vector_size(const BasicType bt) {\n+int Matcher::max_vector_size(const BasicType bt) {\n@@ -2358,1 +2365,1 @@\n-const int Matcher::min_vector_size(const BasicType bt) {\n+int Matcher::min_vector_size(const BasicType bt) {\n@@ -2373,1 +2380,1 @@\n-const int Matcher::superword_max_vector_size(const BasicType bt) {\n+int Matcher::superword_max_vector_size(const BasicType bt) {\n@@ -2378,1 +2385,1 @@\n-const int Matcher::scalable_vector_reg_size(const BasicType bt) {\n+int Matcher::scalable_vector_reg_size(const BasicType bt) {\n@@ -2383,1 +2390,1 @@\n-const uint Matcher::vector_ideal_reg(int len) {\n+uint Matcher::vector_ideal_reg(int len) {\n@@ -14334,1 +14341,0 @@\n-  predicate(UseFMA);\n@@ -14340,0 +14346,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n@@ -14351,1 +14358,0 @@\n-  predicate(UseFMA);\n@@ -14357,0 +14363,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n@@ -14366,1 +14373,2 @@\n-\/\/ -src1 * src2 + src3\n+\/\/ src1 * (-src2) + src3\n+\/\/ \"(-src1) * src2 + src3\" has been idealized to \"src2 * (-src1) + src3\"\n@@ -14368,2 +14376,0 @@\n-  predicate(UseFMA);\n-  match(Set dst (FmaF src3 (Binary (NegF src1) src2)));\n@@ -14375,0 +14381,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n@@ -14384,1 +14391,2 @@\n-\/\/ -src1 * src2 + src3\n+\/\/ src1 * (-src2) + src3\n+\/\/ \"(-src1) * src2 + src3\" has been idealized to \"src2 * (-src1) + src3\"\n@@ -14386,2 +14394,0 @@\n-  predicate(UseFMA);\n-  match(Set dst (FmaD src3 (Binary (NegD src1) src2)));\n@@ -14393,0 +14399,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n@@ -14402,1 +14409,2 @@\n-\/\/ -src1 * src2 - src3\n+\/\/ src1 * (-src2) - src3\n+\/\/ \"(-src1) * src2 - src3\" has been idealized to \"src2 * (-src1) - src3\"\n@@ -14404,2 +14412,0 @@\n-  predicate(UseFMA);\n-  match(Set dst (FmaF (NegF src3) (Binary (NegF src1) src2)));\n@@ -14411,0 +14417,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n@@ -14420,1 +14427,2 @@\n-\/\/ -src1 * src2 - src3\n+\/\/ src1 * (-src2) - src3\n+\/\/ \"(-src1) * src2 - src3\" has been idealized to \"src2 * (-src1) - src3\"\n@@ -14422,2 +14430,0 @@\n-  predicate(UseFMA);\n-  match(Set dst (FmaD (NegD src3) (Binary (NegD src1) src2)));\n@@ -14429,0 +14435,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n@@ -14440,1 +14447,0 @@\n-  predicate(UseFMA);\n@@ -14446,0 +14452,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n@@ -14457,1 +14464,0 @@\n-  predicate(UseFMA);\n@@ -14463,1 +14469,2 @@\n-  \/\/ n.b. insn name should be fnmsubd\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n+    \/\/ n.b. insn name should be fnmsubd\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":37,"deletions":30,"binary":false,"changes":67,"status":"modified"},{"patch":"@@ -41,1 +41,0 @@\n-#include \"oops\/compressedKlass.hpp\"\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -480,0 +480,9 @@\n+  if (AbortVMOnException) {\n+    __ mov(rscratch1, exception_oop);\n+    __ enter();\n+    save_live_registers(sasm);\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, check_abort_on_vm_exception), rscratch1);\n+    restore_live_registers(sasm);\n+    __ leave();\n+  }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_Runtime1_aarch64.cpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -1023,1 +1023,1 @@\n-  assert(arg_1 != c_rarg2, \"smashed arg\");\n+  assert_different_registers(arg_1, c_rarg2);\n@@ -1035,2 +1035,2 @@\n-  assert(arg_1 != c_rarg3, \"smashed arg\");\n-  assert(arg_2 != c_rarg3, \"smashed arg\");\n+  assert_different_registers(arg_1, c_rarg2, c_rarg3);\n+  assert_different_registers(arg_2, c_rarg3);\n@@ -1039,1 +1039,0 @@\n-  assert(arg_1 != c_rarg2, \"smashed arg\");\n@@ -1070,1 +1069,1 @@\n-  assert(arg_1 != c_rarg2, \"smashed arg\");\n+  assert_different_registers(arg_1, c_rarg2);\n@@ -1083,2 +1082,2 @@\n-  assert(arg_1 != c_rarg3, \"smashed arg\");\n-  assert(arg_2 != c_rarg3, \"smashed arg\");\n+  assert_different_registers(arg_1, c_rarg2, c_rarg3);\n+  assert_different_registers(arg_2, c_rarg3);\n@@ -1086,1 +1085,0 @@\n-  assert(arg_1 != c_rarg2, \"smashed arg\");\n@@ -1590,0 +1588,1 @@\n+  assert_different_registers(arg_1, c_rarg0);\n@@ -1597,0 +1596,2 @@\n+  assert_different_registers(arg_1, c_rarg0);\n+  assert_different_registers(arg_2, c_rarg0, c_rarg1);\n@@ -1610,1 +1611,1 @@\n-  assert(arg_0 != c_rarg1, \"smashed arg\");\n+  assert_different_registers(arg_0, c_rarg1);\n@@ -1617,2 +1618,2 @@\n-  assert(arg_0 != c_rarg2, \"smashed arg\");\n-  assert(arg_1 != c_rarg2, \"smashed arg\");\n+  assert_different_registers(arg_0, c_rarg1, c_rarg2);\n+  assert_different_registers(arg_1, c_rarg2);\n@@ -1620,1 +1621,0 @@\n-  assert(arg_0 != c_rarg1, \"smashed arg\");\n@@ -1627,3 +1627,3 @@\n-  assert(arg_0 != c_rarg3, \"smashed arg\");\n-  assert(arg_1 != c_rarg3, \"smashed arg\");\n-  assert(arg_2 != c_rarg3, \"smashed arg\");\n+  assert_different_registers(arg_0, c_rarg1, c_rarg2, c_rarg3);\n+  assert_different_registers(arg_1, c_rarg2, c_rarg3);\n+  assert_different_registers(arg_2, c_rarg3);\n@@ -1631,3 +1631,0 @@\n-  assert(arg_0 != c_rarg2, \"smashed arg\");\n-  assert(arg_1 != c_rarg2, \"smashed arg\");\n-  assert(arg_0 != c_rarg1, \"smashed arg\");\n@@ -4560,18 +4557,2 @@\n-\/\/ Returns a static string\n-const char* MacroAssembler::describe_klass_decode_mode(MacroAssembler::KlassDecodeMode mode) {\n-  switch (mode) {\n-  case KlassDecodeNone: return \"none\";\n-  case KlassDecodeZero: return \"zero\";\n-  case KlassDecodeXor:  return \"xor\";\n-  case KlassDecodeMovk: return \"movk\";\n-  default:\n-    ShouldNotReachHere();\n-  }\n-  return NULL;\n-}\n-\n-\/\/ Return the current narrow Klass pointer decode mode.\n-  if (_klass_decode_mode == KlassDecodeNone) {\n-    \/\/ First time initialization\n-    assert(UseCompressedClassPointers, \"not using compressed class pointers\");\n-    assert(Metaspace::initialized(), \"metaspace not initialized yet\");\n+  assert(UseCompressedClassPointers, \"not using compressed class pointers\");\n+  assert(Metaspace::initialized(), \"metaspace not initialized yet\");\n@@ -4580,5 +4561,2 @@\n-    _klass_decode_mode = klass_decode_mode_for_base(CompressedKlassPointers::base());\n-    guarantee(_klass_decode_mode != KlassDecodeNone,\n-              PTR_FORMAT \" is not a valid encoding base on aarch64\",\n-              p2i(CompressedKlassPointers::base()));\n-    log_info(metaspace)(\"klass decode mode initialized: %s\", describe_klass_decode_mode(_klass_decode_mode));\n+  if (_klass_decode_mode != KlassDecodeNone) {\n+    return _klass_decode_mode;\n@@ -4586,7 +4564,2 @@\n-  return _klass_decode_mode;\n-}\n-\n-\/\/ Given an arbitrary base address, return the KlassDecodeMode that would be used. Return KlassDecodeNone\n-\/\/ if base address is not valid for encoding.\n-MacroAssembler::KlassDecodeMode MacroAssembler::klass_decode_mode_for_base(address base) {\n-  const uint64_t base_u64 = (uint64_t) base;\n+  assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift()\n+         || 0 == CompressedKlassPointers::shift(), \"decode alg wrong\");\n@@ -4595,2 +4568,2 @@\n-  if (base_u64 == 0) {\n-    return KlassDecodeZero;\n+  if (CompressedKlassPointers::base() == nullptr) {\n+    return (_klass_decode_mode = KlassDecodeZero);\n@@ -4599,3 +4572,7 @@\n-  if (operand_valid_for_logical_immediate(false, base_u64) &&\n-      ((base_u64 & (KlassEncodingMetaspaceMax - 1)) == 0)) {\n-    return KlassDecodeXor;\n+  if (operand_valid_for_logical_immediate(\n+        \/*is32*\/false, (uint64_t)CompressedKlassPointers::base())) {\n+    const uint64_t range_mask =\n+      (1ULL << log2i(CompressedKlassPointers::range())) - 1;\n+    if (((uint64_t)CompressedKlassPointers::base() & range_mask) == 0) {\n+      return (_klass_decode_mode = KlassDecodeXor);\n+    }\n@@ -4604,4 +4581,4 @@\n-  const uint64_t shifted_base = base_u64 >> LogKlassAlignmentInBytes;\n-  if ((shifted_base & 0xffff0000ffffffff) == 0) {\n-    return KlassDecodeMovk;\n-  }\n+  const uint64_t shifted_base =\n+    (uint64_t)CompressedKlassPointers::base() >> CompressedKlassPointers::shift();\n+  guarantee((shifted_base & 0xffff0000ffffffff) == 0,\n+            \"compressed class base bad alignment\");\n@@ -4609,1 +4586,1 @@\n-  return KlassDecodeNone;\n+  return (_klass_decode_mode = KlassDecodeMovk);\n@@ -4613,2 +4590,0 @@\n-  assert (UseCompressedClassPointers, \"should only be used for compressed headers\");\n-  assert(CompressedKlassPointers::shift() != 0, \"not lilliput?\");\n@@ -4617,1 +4592,5 @@\n-    lsr(dst, src, LogKlassAlignmentInBytes);\n+    if (CompressedKlassPointers::shift() != 0) {\n+      lsr(dst, src, LogKlassAlignmentInBytes);\n+    } else {\n+      if (dst != src) mov(dst, src);\n+    }\n@@ -4621,2 +4600,6 @@\n-    eor(dst, src, (uint64_t)CompressedKlassPointers::base());\n-    lsr(dst, dst, LogKlassAlignmentInBytes);\n+    if (CompressedKlassPointers::shift() != 0) {\n+      eor(dst, src, (uint64_t)CompressedKlassPointers::base());\n+      lsr(dst, dst, LogKlassAlignmentInBytes);\n+    } else {\n+      eor(dst, src, (uint64_t)CompressedKlassPointers::base());\n+    }\n@@ -4626,1 +4609,5 @@\n-    ubfx(dst, src, LogKlassAlignmentInBytes, MaxNarrowKlassPointerBits);\n+    if (CompressedKlassPointers::shift() != 0) {\n+      ubfx(dst, src, LogKlassAlignmentInBytes, 32);\n+    } else {\n+      movw(dst, src);\n+    }\n@@ -4642,2 +4629,0 @@\n-  assert(CompressedKlassPointers::shift() != 0, \"not lilliput?\");\n-\n@@ -4646,1 +4631,5 @@\n-    if (dst != src) mov(dst, src);\n+    if (CompressedKlassPointers::shift() != 0) {\n+      lsl(dst, src, LogKlassAlignmentInBytes);\n+    } else {\n+      if (dst != src) mov(dst, src);\n+    }\n@@ -4650,2 +4639,6 @@\n-    lsl(dst, src, LogKlassAlignmentInBytes);\n-    eor(dst, dst, (uint64_t)CompressedKlassPointers::base());\n+    if (CompressedKlassPointers::shift() != 0) {\n+      lsl(dst, src, LogKlassAlignmentInBytes);\n+      eor(dst, dst, (uint64_t)CompressedKlassPointers::base());\n+    } else {\n+      eor(dst, src, (uint64_t)CompressedKlassPointers::base());\n+    }\n@@ -4658,3 +4651,0 @@\n-    \/\/ Invalid base should have been gracefully handled via klass_decode_mode() in VM initialization.\n-    assert((shifted_base & 0xffff0000ffffffff) == 0, \"incompatible base\");\n-\n@@ -4663,1 +4653,5 @@\n-    lsl(dst, dst, LogKlassAlignmentInBytes);\n+\n+    if (CompressedKlassPointers::shift() != 0) {\n+      lsl(dst, dst, LogKlassAlignmentInBytes);\n+    }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":67,"deletions":73,"binary":false,"changes":140,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"oops\/compressedKlass.hpp\"\n@@ -89,2 +90,0 @@\n- public:\n-\n@@ -98,9 +97,1 @@\n-  \/\/ Return the current narrow Klass pointer decode mode. Initialized on first call.\n-  static KlassDecodeMode klass_decode_mode();\n-\n-  \/\/ Given an arbitrary base address, return the KlassDecodeMode that would be used. Return KlassDecodeNone\n-  \/\/ if base address is not valid for encoding.\n-  static KlassDecodeMode klass_decode_mode_for_base(address base);\n-\n-  \/\/ Returns a static string\n-  static const char* describe_klass_decode_mode(KlassDecodeMode mode);\n+  KlassDecodeMode klass_decode_mode();\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":2,"deletions":11,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"oops\/resolvedFieldEntry.hpp\"\n@@ -190,1 +191,8 @@\n-      __ get_cache_and_index_and_bytecode_at_bcp(temp_reg, bc_reg, temp_reg, byte_no, 1);\n+      __ load_field_entry(temp_reg, bc_reg);\n+      if (byte_no == f1_byte) {\n+        __ lea(temp_reg, Address(temp_reg, in_bytes(ResolvedFieldEntry::get_code_offset())));\n+      } else {\n+        __ lea(temp_reg, Address(temp_reg, in_bytes(ResolvedFieldEntry::put_code_offset())));\n+      }\n+      \/\/ Load-acquire the bytecode to match store-release in ResolvedFieldEntry::fill_in()\n+      __ ldarb(temp_reg, temp_reg);\n@@ -2250,5 +2258,0 @@\n-  switch (code) {\n-  case Bytecodes::_nofast_getfield: code = Bytecodes::_getfield; break;\n-  case Bytecodes::_nofast_putfield: code = Bytecodes::_putfield; break;\n-  default: break;\n-  }\n@@ -2282,0 +2285,63 @@\n+void TemplateTable::resolve_cache_and_index_for_field(int byte_no,\n+                                            Register Rcache,\n+                                            Register index) {\n+  const Register temp = r19;\n+  assert_different_registers(Rcache, index, temp);\n+\n+  Label resolved;\n+\n+  Bytecodes::Code code = bytecode();\n+  switch (code) {\n+  case Bytecodes::_nofast_getfield: code = Bytecodes::_getfield; break;\n+  case Bytecodes::_nofast_putfield: code = Bytecodes::_putfield; break;\n+  default: break;\n+  }\n+\n+  assert(byte_no == f1_byte || byte_no == f2_byte, \"byte_no out of range\");\n+  __ load_field_entry(Rcache, index);\n+  if (byte_no == f1_byte) {\n+    __ lea(temp, Address(Rcache, in_bytes(ResolvedFieldEntry::get_code_offset())));\n+  } else {\n+    __ lea(temp, Address(Rcache, in_bytes(ResolvedFieldEntry::put_code_offset())));\n+  }\n+  \/\/ Load-acquire the bytecode to match store-release in ResolvedFieldEntry::fill_in()\n+  __ ldarb(temp, temp);\n+  __ subs(zr, temp, (int) code);  \/\/ have we resolved this bytecode?\n+  __ br(Assembler::EQ, resolved);\n+\n+  \/\/ resolve first time through\n+  address entry = CAST_FROM_FN_PTR(address, InterpreterRuntime::resolve_from_cache);\n+  __ mov(temp, (int) code);\n+  __ call_VM(noreg, entry, temp);\n+\n+  \/\/ Update registers with resolved info\n+  __ load_field_entry(Rcache, index);\n+  __ bind(resolved);\n+}\n+\n+void TemplateTable::load_resolved_field_entry(Register obj,\n+                                              Register cache,\n+                                              Register tos_state,\n+                                              Register offset,\n+                                              Register flags,\n+                                              bool is_static = false) {\n+  assert_different_registers(cache, tos_state, flags, offset);\n+\n+  \/\/ Field offset\n+  __ load_sized_value(offset, Address(cache, in_bytes(ResolvedFieldEntry::field_offset_offset())), sizeof(int), true \/*is_signed*\/);\n+\n+  \/\/ Flags\n+  __ load_unsigned_byte(flags, Address(cache, in_bytes(ResolvedFieldEntry::flags_offset())));\n+\n+  \/\/ TOS state\n+  __ load_unsigned_byte(tos_state, Address(cache, in_bytes(ResolvedFieldEntry::type_offset())));\n+\n+  \/\/ Klass overwrite register\n+  if (is_static) {\n+    __ ldr(obj, Address(cache, ResolvedFieldEntry::field_holder_offset()));\n+    const int mirror_offset = in_bytes(Klass::java_mirror_offset());\n+    __ ldr(obj, Address(obj, mirror_offset));\n+    __ resolve_oop_handle(obj, r5, rscratch2);\n+  }\n+}\n+\n@@ -2433,2 +2499,1 @@\n-    __ get_cache_and_index_at_bcp(c_rarg2, c_rarg3, 1);\n-    __ lea(c_rarg2, Address(c_rarg2, in_bytes(ConstantPoolCache::base_offset())));\n+    __ load_field_entry(c_rarg2, index);\n@@ -2444,1 +2509,0 @@\n-    \/\/ c_rarg3: jvalue object on the stack\n@@ -2447,2 +2511,2 @@\n-               c_rarg1, c_rarg2, c_rarg3);\n-    __ get_cache_and_index_at_bcp(cache, index, 1);\n+               c_rarg1, c_rarg2);\n+    __ load_field_entry(cache, index);\n@@ -2462,7 +2526,7 @@\n-  const Register cache = r2;\n-  const Register index = r3;\n-  const Register obj   = r4;\n-  const Register off   = r19;\n-  const Register flags = r0;\n-  const Register raw_flags = r6;\n-  const Register bc    = r4; \/\/ uses same reg as obj, so don't mix them\n+  const Register cache     = r4;\n+  const Register obj       = r4;\n+  const Register index     = r3;\n+  const Register tos_state = r3;\n+  const Register off       = r19;\n+  const Register flags     = r6;\n+  const Register bc        = r4; \/\/ uses same reg as obj, so don't mix them\n@@ -2470,1 +2534,1 @@\n-  resolve_cache_and_index(byte_no, cache, index, sizeof(u2));\n+  resolve_cache_and_index_for_field(byte_no, cache, index);\n@@ -2472,1 +2536,1 @@\n-  load_field_cp_cache_entry(obj, cache, index, off, raw_flags, is_static);\n+  load_resolved_field_entry(obj, cache, tos_state, off, flags, is_static);\n@@ -2487,1 +2551,1 @@\n-    __ tbz(raw_flags, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);\n+    __ tbz(flags, ResolvedFieldEntry::is_volatile_shift, notVolatile);\n@@ -2497,6 +2561,1 @@\n-  \/\/ x86 uses a shift and mask or wings it with a shift plus assert\n-  \/\/ the mask is not needed. aarch64 just uses bitfield extract\n-  __ ubfxw(flags, raw_flags, ConstantPoolCacheEntry::tos_state_shift,\n-           ConstantPoolCacheEntry::tos_state_bits);\n-\n-  __ cbnz(flags, notByte);\n+  __ cbnz(tos_state, notByte);\n@@ -2518,1 +2577,1 @@\n-  __ cmp(flags, (u1)ztos);\n+  __ cmp(tos_state, (u1)ztos);\n@@ -2532,1 +2591,1 @@\n-  __ cmp(flags, (u1)atos);\n+  __ cmp(tos_state, (u1)atos);\n@@ -2543,1 +2602,1 @@\n-  __ cmp(flags, (u1)itos);\n+  __ cmp(tos_state, (u1)itos);\n@@ -2555,1 +2614,1 @@\n-  __ cmp(flags, (u1)ctos);\n+  __ cmp(tos_state, (u1)ctos);\n@@ -2567,1 +2626,1 @@\n-  __ cmp(flags, (u1)stos);\n+  __ cmp(tos_state, (u1)stos);\n@@ -2579,1 +2638,1 @@\n-  __ cmp(flags, (u1)ltos);\n+  __ cmp(tos_state, (u1)ltos);\n@@ -2591,1 +2650,1 @@\n-  __ cmp(flags, (u1)ftos);\n+  __ cmp(tos_state, (u1)ftos);\n@@ -2604,1 +2663,1 @@\n-  __ cmp(flags, (u1)dtos);\n+  __ cmp(tos_state, (u1)dtos);\n@@ -2624,1 +2683,1 @@\n-  __ tbz(raw_flags, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);\n+  __ tbz(flags, ResolvedFieldEntry::is_volatile_shift, notVolatile);\n@@ -2649,2 +2708,0 @@\n-  ByteSize cp_base_offset = ConstantPoolCache::base_offset();\n-\n@@ -2660,1 +2717,1 @@\n-    __ get_cache_and_index_at_bcp(c_rarg2, rscratch1, 1);\n+    __ mov(c_rarg2, cache);\n@@ -2670,6 +2727,1 @@\n-      __ ldrw(c_rarg3, Address(c_rarg2,\n-                               in_bytes(cp_base_offset +\n-                                        ConstantPoolCacheEntry::flags_offset())));\n-      __ lsr(c_rarg3, c_rarg3,\n-             ConstantPoolCacheEntry::tos_state_shift);\n-      ConstantPoolCacheEntry::verify_tos_state_shift();\n+      __ load_unsigned_byte(c_rarg3, Address(c_rarg2, in_bytes(ResolvedFieldEntry::type_offset())));\n@@ -2686,2 +2738,0 @@\n-    \/\/ cache entry pointer\n-    __ add(c_rarg2, c_rarg2, in_bytes(cp_base_offset));\n@@ -2697,1 +2747,1 @@\n-    __ get_cache_and_index_at_bcp(cache, index, 1);\n+    __ load_field_entry(cache, index);\n@@ -2705,6 +2755,7 @@\n-  const Register cache = r2;\n-  const Register index = r3;\n-  const Register obj   = r2;\n-  const Register off   = r19;\n-  const Register flags = r0;\n-  const Register bc    = r4;\n+  const Register cache     = r2;\n+  const Register index     = r3;\n+  const Register tos_state = r3;\n+  const Register obj       = r2;\n+  const Register off       = r19;\n+  const Register flags     = r0;\n+  const Register bc        = r4;\n@@ -2712,1 +2763,1 @@\n-  resolve_cache_and_index(byte_no, cache, index, sizeof(u2));\n+  resolve_cache_and_index_for_field(byte_no, cache, index);\n@@ -2714,1 +2765,1 @@\n-  load_field_cp_cache_entry(obj, cache, index, off, flags, is_static);\n+  load_resolved_field_entry(obj, cache, tos_state, off, flags, is_static);\n@@ -2721,1 +2772,1 @@\n-    __ tbz(r5, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);\n+    __ tbz(r5, ResolvedFieldEntry::is_volatile_shift, notVolatile);\n@@ -2732,5 +2783,1 @@\n-  \/\/ x86 uses a shift and mask or wings it with a shift plus assert\n-  \/\/ the mask is not needed. aarch64 just uses bitfield extract\n-  __ ubfxw(flags, flags, ConstantPoolCacheEntry::tos_state_shift,  ConstantPoolCacheEntry::tos_state_bits);\n-\n-  __ cbnz(flags, notByte);\n+  __ cbnz(tos_state, notByte);\n@@ -2754,1 +2801,1 @@\n-  __ cmp(flags, (u1)ztos);\n+  __ cmp(tos_state, (u1)ztos);\n@@ -2769,1 +2816,1 @@\n-  __ cmp(flags, (u1)atos);\n+  __ cmp(tos_state, (u1)atos);\n@@ -2785,1 +2832,1 @@\n-  __ cmp(flags, (u1)itos);\n+  __ cmp(tos_state, (u1)itos);\n@@ -2800,1 +2847,1 @@\n-  __ cmp(flags, (u1)ctos);\n+  __ cmp(tos_state, (u1)ctos);\n@@ -2815,1 +2862,1 @@\n-  __ cmp(flags, (u1)stos);\n+  __ cmp(tos_state, (u1)stos);\n@@ -2830,1 +2877,1 @@\n-  __ cmp(flags, (u1)ltos);\n+  __ cmp(tos_state, (u1)ltos);\n@@ -2845,1 +2892,1 @@\n-  __ cmp(flags, (u1)ftos);\n+  __ cmp(tos_state, (u1)ftos);\n@@ -2861,1 +2908,1 @@\n-  __ cmp(flags, (u1)dtos);\n+  __ cmp(tos_state, (u1)dtos);\n@@ -2886,1 +2933,1 @@\n-    __ tbz(r5, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);\n+    __ tbz(r5, ResolvedFieldEntry::is_volatile_shift, notVolatile);\n@@ -2905,2 +2952,1 @@\n-void TemplateTable::jvmti_post_fast_field_mod()\n-{\n+void TemplateTable::jvmti_post_fast_field_mod() {\n@@ -2936,1 +2982,1 @@\n-    __ get_cache_entry_pointer_at_bcp(c_rarg2, r0, 1);\n+    __ load_field_entry(c_rarg2, r0);\n@@ -2971,1 +3017,5 @@\n-  __ get_cache_and_index_at_bcp(r2, r1, 1);\n+  __ load_field_entry(r2, r1);\n+  __ push(r0);\n+  \/\/ R1: field offset, R2: TOS, R3: flags\n+  load_resolved_field_entry(r2, r2, r0, r1, r3);\n+  __ pop(r0);\n@@ -2976,7 +3026,0 @@\n-  \/\/ test for volatile with r3\n-  __ ldrw(r3, Address(r2, in_bytes(base +\n-                                   ConstantPoolCacheEntry::flags_offset())));\n-\n-  \/\/ replace index with field offset from cache entry\n-  __ ldr(r1, Address(r2, in_bytes(base + ConstantPoolCacheEntry::f2_offset())));\n-\n@@ -2985,1 +3028,1 @@\n-    __ tbz(r3, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);\n+    __ tbz(r3, ResolvedFieldEntry::is_volatile_shift, notVolatile);\n@@ -3033,1 +3076,1 @@\n-    __ tbz(r3, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);\n+    __ tbz(r3, ResolvedFieldEntry::is_volatile_shift, notVolatile);\n@@ -3052,1 +3095,1 @@\n-    __ get_cache_entry_pointer_at_bcp(c_rarg2, rscratch2, 1);\n+    __ load_field_entry(c_rarg2, rscratch2);\n@@ -3067,1 +3110,1 @@\n-  __ get_cache_and_index_at_bcp(r2, r1, 1);\n+  __ load_field_entry(r2, r1);\n@@ -3072,4 +3115,2 @@\n-  __ ldr(r1, Address(r2, in_bytes(ConstantPoolCache::base_offset() +\n-                                  ConstantPoolCacheEntry::f2_offset())));\n-  __ ldrw(r3, Address(r2, in_bytes(ConstantPoolCache::base_offset() +\n-                                   ConstantPoolCacheEntry::flags_offset())));\n+  __ load_sized_value(r1, Address(r2, in_bytes(ResolvedFieldEntry::field_offset_offset())), sizeof(int), true \/*is_signed*\/);\n+  __ load_unsigned_byte(r3, Address(r2, in_bytes(ResolvedFieldEntry::flags_offset())));\n@@ -3090,1 +3131,1 @@\n-    __ tbz(r3, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);\n+    __ tbz(r3, ResolvedFieldEntry::is_volatile_shift, notVolatile);\n@@ -3127,1 +3168,1 @@\n-    __ tbz(r3, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);\n+    __ tbz(r3, ResolvedFieldEntry::is_volatile_shift, notVolatile);\n@@ -3140,3 +3181,2 @@\n-  __ get_cache_and_index_at_bcp(r2, r3, 2);\n-  __ ldr(r1, Address(r2, in_bytes(ConstantPoolCache::base_offset() +\n-                                  ConstantPoolCacheEntry::f2_offset())));\n+  __ load_field_entry(r2, r3, 2);\n+  __ load_sized_value(r1, Address(r2, in_bytes(ResolvedFieldEntry::field_offset_offset())), sizeof(int), true \/*is_signed*\/);\n@@ -3152,3 +3192,2 @@\n-    __ ldrw(r3, Address(r2, in_bytes(ConstantPoolCache::base_offset() +\n-                                     ConstantPoolCacheEntry::flags_offset())));\n-    __ tbz(r3, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);\n+    __ load_unsigned_byte(r3, Address(r2, in_bytes(ResolvedFieldEntry::flags_offset())));\n+    __ tbz(r3, ResolvedFieldEntry::is_volatile_shift, notVolatile);\n@@ -3180,3 +3219,2 @@\n-    __ ldrw(r3, Address(r2, in_bytes(ConstantPoolCache::base_offset() +\n-                                     ConstantPoolCacheEntry::flags_offset())));\n-    __ tbz(r3, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);\n+    __ load_unsigned_byte(r3, Address(r2, in_bytes(ResolvedFieldEntry::flags_offset())));\n+    __ tbz(r3, ResolvedFieldEntry::is_volatile_shift, notVolatile);\n","filename":"src\/hotspot\/cpu\/aarch64\/templateTable_aarch64.cpp","additions":139,"deletions":101,"binary":false,"changes":240,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n@@ -139,1 +140,1 @@\n-  assert(arg_1 != c_rarg2, \"smashed arg\");\n+  assert_different_registers(arg_1, c_rarg2);\n@@ -151,2 +152,2 @@\n-  assert(arg_1 != c_rarg3, \"smashed arg\");\n-  assert(arg_2 != c_rarg3, \"smashed arg\");\n+  assert_different_registers(arg_1, c_rarg2, c_rarg3);\n+  assert_different_registers(arg_2, c_rarg3);\n@@ -155,1 +156,0 @@\n-  assert(arg_1 != c_rarg2, \"smashed arg\");\n@@ -186,1 +186,1 @@\n-  assert(arg_1 != c_rarg2, \"smashed arg\");\n+  assert_different_registers(arg_1, c_rarg2);\n@@ -199,2 +199,2 @@\n-  assert(arg_1 != c_rarg3, \"smashed arg\");\n-  assert(arg_2 != c_rarg3, \"smashed arg\");\n+  assert_different_registers(arg_1, c_rarg2, c_rarg3);\n+  assert_different_registers(arg_2, c_rarg3);\n@@ -202,1 +202,0 @@\n-  assert(arg_1 != c_rarg2, \"smashed arg\");\n@@ -671,0 +670,1 @@\n+  assert_different_registers(arg_1, c_rarg0);\n@@ -678,0 +678,2 @@\n+  assert_different_registers(arg_1, c_rarg0);\n+  assert_different_registers(arg_2, c_rarg0, c_rarg1);\n@@ -691,1 +693,1 @@\n-  assert(arg_0 != c_rarg1, \"smashed arg\");\n+  assert_different_registers(arg_0, c_rarg1);\n@@ -698,2 +700,2 @@\n-  assert(arg_0 != c_rarg2, \"smashed arg\");\n-  assert(arg_1 != c_rarg2, \"smashed arg\");\n+  assert_different_registers(arg_0, c_rarg1, c_rarg2);\n+  assert_different_registers(arg_1, c_rarg2);\n@@ -701,1 +703,0 @@\n-  assert(arg_0 != c_rarg1, \"smashed arg\");\n@@ -708,3 +709,4 @@\n-  assert(arg_0 != c_rarg3, \"smashed arg\");\n-  assert(arg_1 != c_rarg3, \"smashed arg\");\n-  assert(arg_2 != c_rarg3, \"smashed arg\");\n+  assert_different_registers(arg_0, c_rarg1, c_rarg2, c_rarg3);\n+  assert_different_registers(arg_1, c_rarg2, c_rarg3);\n+  assert_different_registers(arg_2, c_rarg3);\n+\n@@ -712,3 +714,0 @@\n-  assert(arg_0 != c_rarg2, \"smashed arg\");\n-  assert(arg_1 != c_rarg2, \"smashed arg\");\n-  assert(arg_0 != c_rarg1, \"smashed arg\");\n@@ -1658,0 +1657,22 @@\n+\/\/ Rd = Rs1 & (~Rd2)\n+void MacroAssembler::andn(Register Rd, Register Rs1, Register Rs2) {\n+  if (UseZbb) {\n+    Assembler::andn(Rd, Rs1, Rs2);\n+    return;\n+  }\n+\n+  notr(Rd, Rs2);\n+  andr(Rd, Rs1, Rd);\n+}\n+\n+\/\/ Rd = Rs1 | (~Rd2)\n+void MacroAssembler::orn(Register Rd, Register Rs1, Register Rs2) {\n+  if (UseZbb) {\n+    Assembler::orn(Rd, Rs1, Rs2);\n+    return;\n+  }\n+\n+  notr(Rd, Rs2);\n+  orr(Rd, Rs1, Rd);\n+}\n+\n@@ -1703,1 +1724,18 @@\n-\/\/ granularity is 1, 2 bytes per load\n+\/\/ granularity is 1 OR 2 bytes per load. dst and src.base() allowed to be the same register\n+void MacroAssembler::load_short_misaligned(Register dst, Address src, Register tmp, bool is_signed, int granularity) {\n+  if (granularity != 1 && granularity != 2) {\n+    ShouldNotReachHere();\n+  }\n+  if (AvoidUnalignedAccesses && (granularity != 2)) {\n+    assert_different_registers(dst, tmp);\n+    assert_different_registers(tmp, src.base());\n+    is_signed ? lb(tmp, Address(src.base(), src.offset() + 1)) : lbu(tmp, Address(src.base(), src.offset() + 1));\n+    slli(tmp, tmp, 8);\n+    lbu(dst, src);\n+    add(dst, dst, tmp);\n+  } else {\n+    is_signed ? lh(dst, src) : lhu(dst, src);\n+  }\n+}\n+\n+\/\/ granularity is 1, 2 OR 4 bytes per load, if granularity 2 or 4 then dst and src.base() allowed to be the same register\n@@ -1706,1 +1744,0 @@\n-    assert_different_registers(dst, tmp, src.base());\n@@ -1709,0 +1746,1 @@\n+        assert_different_registers(dst, tmp, src.base());\n@@ -1721,1 +1759,2 @@\n-        lhu(dst, src);\n+        assert_different_registers(dst, tmp);\n+        assert_different_registers(tmp, src.base());\n@@ -1724,0 +1763,1 @@\n+        lhu(dst, src);\n@@ -1734,1 +1774,1 @@\n-\/\/ granularity is 1, 2 or 4 bytes per load\n+\/\/ granularity is 1, 2, 4 or 8 bytes per load, if granularity 4 or 8 then dst and src.base() allowed to be same register\n@@ -1737,1 +1777,0 @@\n-    assert_different_registers(dst, tmp, src.base());\n@@ -1740,0 +1779,1 @@\n+        assert_different_registers(dst, tmp, src.base());\n@@ -1764,0 +1804,1 @@\n+        assert_different_registers(dst, tmp, src.base());\n@@ -1776,1 +1817,2 @@\n-        lwu(dst, src);\n+        assert_different_registers(dst, tmp);\n+        assert_different_registers(tmp, src.base());\n@@ -1779,0 +1821,1 @@\n+        lwu(dst, src);\n@@ -1950,0 +1993,16 @@\n+\/\/ rotate left with shift bits, 32-bit version\n+void MacroAssembler::rolw_imm(Register dst, Register src, uint32_t shift, Register tmp) {\n+  if (UseZbb) {\n+    \/\/ no roliw available\n+    roriw(dst, src, 32 - shift);\n+    return;\n+  }\n+\n+  assert_different_registers(dst, tmp);\n+  assert_different_registers(src, tmp);\n+  assert(shift < 32, \"shift amount must be < 32\");\n+  srliw(tmp, src, 32 - shift);\n+  slliw(dst, src, shift);\n+  orr(dst, dst, tmp);\n+}\n+\n@@ -3949,3 +4008,5 @@\n-  mv(tmp1, 0xFF);\n-  mv(Rd, zr);\n-  for (int i = 0; i <= 3; i++) {\n+  mv(tmp1, 0xFF000000); \/\/ first byte mask at lower word\n+  andr(Rd, Rs, tmp1);\n+  for (int i = 0; i < 2; i++) {\n+    slli(Rd, Rd, wordSize);\n+    srli(tmp1, tmp1, wordSize);\n@@ -3953,6 +4014,3 @@\n-    if (i) {\n-      slli(tmp2, tmp2, i * 8);\n-    }\n-    if (i != 3) {\n-      slli(tmp1, tmp1, 8);\n-    }\n+  slli(Rd, Rd, wordSize);\n+  andi(tmp2, Rs, 0xFF); \/\/ last byte mask at lower word\n+  orr(Rd, Rd, tmp2);\n@@ -3969,11 +4027,2 @@\n-\n-  mv(tmp1, 0xFF00000000);\n-  mv(Rd, zr);\n-  for (int i = 0; i <= 3; i++) {\n-    andr(tmp2, Rs, tmp1);\n-    orr(Rd, Rd, tmp2);\n-    srli(Rd, Rd, 8);\n-    if (i != 3) {\n-      slli(tmp1, tmp1, 8);\n-    }\n-  }\n+  srli(Rs, Rs, 32);   \/\/ only upper 32 bits are needed\n+  inflate_lo32(Rd, Rs, tmp1, tmp2);\n@@ -4310,0 +4359,1 @@\n+    assert_different_registers(Rs2, tmp);\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.cpp","additions":94,"deletions":44,"binary":false,"changes":138,"status":"modified"},{"patch":"@@ -1395,1 +1395,1 @@\n-  LP64_ONLY(assert(arg_1 != c_rarg2, \"smashed arg\"));\n+  LP64_ONLY(assert_different_registers(arg_1, c_rarg2));\n@@ -1417,2 +1417,2 @@\n-  LP64_ONLY(assert(arg_1 != c_rarg3, \"smashed arg\"));\n-  LP64_ONLY(assert(arg_2 != c_rarg3, \"smashed arg\"));\n+  LP64_ONLY(assert_different_registers(arg_1, c_rarg2, c_rarg3));\n+  LP64_ONLY(assert_different_registers(arg_2, c_rarg3));\n@@ -1420,3 +1420,0 @@\n-\n-  LP64_ONLY(assert(arg_1 != c_rarg2, \"smashed arg\"));\n-\n@@ -1456,1 +1453,1 @@\n-  LP64_ONLY(assert(arg_1 != c_rarg2, \"smashed arg\"));\n+  LP64_ONLY(assert_different_registers(arg_1, c_rarg2));\n@@ -1469,2 +1466,2 @@\n-  LP64_ONLY(assert(arg_1 != c_rarg3, \"smashed arg\"));\n-  LP64_ONLY(assert(arg_2 != c_rarg3, \"smashed arg\"));\n+  LP64_ONLY(assert_different_registers(arg_1, c_rarg2, c_rarg3));\n+  LP64_ONLY(assert_different_registers(arg_2, c_rarg3));\n@@ -1472,1 +1469,0 @@\n-  LP64_ONLY(assert(arg_1 != c_rarg2, \"smashed arg\"));\n@@ -1503,1 +1499,1 @@\n-  LP64_ONLY(assert(arg_1 != c_rarg2, \"smashed arg\"));\n+  LP64_ONLY(assert_different_registers(arg_1, c_rarg2));\n@@ -1516,2 +1512,2 @@\n-  LP64_ONLY(assert(arg_1 != c_rarg3, \"smashed arg\"));\n-  LP64_ONLY(assert(arg_2 != c_rarg3, \"smashed arg\"));\n+  LP64_ONLY(assert_different_registers(arg_1, c_rarg2, c_rarg3));\n+  LP64_ONLY(assert_different_registers(arg_2, c_rarg3));\n@@ -1519,1 +1515,0 @@\n-  LP64_ONLY(assert(arg_1 != c_rarg2, \"smashed arg\"));\n@@ -1661,1 +1656,1 @@\n-  LP64_ONLY(assert(arg_0 != c_rarg1, \"smashed arg\"));\n+  LP64_ONLY(assert_different_registers(arg_0, c_rarg1));\n@@ -1668,2 +1663,2 @@\n-  LP64_ONLY(assert(arg_0 != c_rarg2, \"smashed arg\"));\n-  LP64_ONLY(assert(arg_1 != c_rarg2, \"smashed arg\"));\n+  LP64_ONLY(assert_different_registers(arg_0, c_rarg1, c_rarg2));\n+  LP64_ONLY(assert_different_registers(arg_1, c_rarg2));\n@@ -1671,1 +1666,0 @@\n-  LP64_ONLY(assert(arg_0 != c_rarg1, \"smashed arg\"));\n@@ -1678,3 +1672,3 @@\n-  LP64_ONLY(assert(arg_0 != c_rarg3, \"smashed arg\"));\n-  LP64_ONLY(assert(arg_1 != c_rarg3, \"smashed arg\"));\n-  LP64_ONLY(assert(arg_2 != c_rarg3, \"smashed arg\"));\n+  LP64_ONLY(assert_different_registers(arg_0, c_rarg1, c_rarg2, c_rarg3));\n+  LP64_ONLY(assert_different_registers(arg_1, c_rarg2, c_rarg3));\n+  LP64_ONLY(assert_different_registers(arg_2, c_rarg3));\n@@ -1682,3 +1676,0 @@\n-  LP64_ONLY(assert(arg_0 != c_rarg2, \"smashed arg\"));\n-  LP64_ONLY(assert(arg_1 != c_rarg2, \"smashed arg\"));\n-  LP64_ONLY(assert(arg_0 != c_rarg1, \"smashed arg\"));\n@@ -1697,2 +1688,1 @@\n-\n-  LP64_ONLY(assert(arg_0 != c_rarg1, \"smashed arg\"));\n+  LP64_ONLY(assert_different_registers(arg_0, c_rarg1));\n@@ -1705,2 +1695,2 @@\n-  LP64_ONLY(assert(arg_0 != c_rarg2, \"smashed arg\"));\n-  LP64_ONLY(assert(arg_1 != c_rarg2, \"smashed arg\"));\n+  LP64_ONLY(assert_different_registers(arg_0, c_rarg1, c_rarg2));\n+  LP64_ONLY(assert_different_registers(arg_1, c_rarg2));\n@@ -1708,1 +1698,0 @@\n-  LP64_ONLY(assert(arg_0 != c_rarg1, \"smashed arg\"));\n@@ -1715,3 +1704,3 @@\n-  LP64_ONLY(assert(arg_0 != c_rarg3, \"smashed arg\"));\n-  LP64_ONLY(assert(arg_1 != c_rarg3, \"smashed arg\"));\n-  LP64_ONLY(assert(arg_2 != c_rarg3, \"smashed arg\"));\n+  LP64_ONLY(assert_different_registers(arg_0, c_rarg1, c_rarg2, c_rarg3));\n+  LP64_ONLY(assert_different_registers(arg_1, c_rarg2, c_rarg3));\n+  LP64_ONLY(assert_different_registers(arg_2, c_rarg3));\n@@ -1719,3 +1708,0 @@\n-  LP64_ONLY(assert(arg_0 != c_rarg2, \"smashed arg\"));\n-  LP64_ONLY(assert(arg_1 != c_rarg2, \"smashed arg\"));\n-  LP64_ONLY(assert(arg_0 != c_rarg1, \"smashed arg\"));\n@@ -2039,4 +2025,4 @@\n-  emit_int8((int8_t)0x0f);\n-  emit_int8((int8_t)0x1f);\n-  emit_int8((int8_t)0x84);\n-  emit_int8((int8_t)0x00);\n+  emit_int8((uint8_t)0x0f);\n+  emit_int8((uint8_t)0x1f);\n+  emit_int8((uint8_t)0x84);\n+  emit_int8((uint8_t)0x00);\n@@ -2051,5 +2037,5 @@\n-    emit_int8((int8_t)0x26); \/\/ es:\n-    emit_int8((int8_t)0x2e); \/\/ cs:\n-    emit_int8((int8_t)0x64); \/\/ fs:\n-    emit_int8((int8_t)0x65); \/\/ gs:\n-    emit_int8((int8_t)0x90);\n+    emit_int8((uint8_t)0x26); \/\/ es:\n+    emit_int8((uint8_t)0x2e); \/\/ cs:\n+    emit_int8((uint8_t)0x64); \/\/ fs:\n+    emit_int8((uint8_t)0x65); \/\/ gs:\n+    emit_int8((uint8_t)0x90);\n@@ -4605,1 +4591,1 @@\n-  int* pst_counter = &SharedRuntime::_partial_subtype_ctr;\n+  uint* pst_counter = &SharedRuntime::_partial_subtype_ctr;\n@@ -5544,61 +5530,0 @@\n-MacroAssembler::KlassDecodeMode MacroAssembler::_klass_decode_mode = KlassDecodeNone;\n-\n-\/\/ Returns a static string\n-const char* MacroAssembler::describe_klass_decode_mode(MacroAssembler::KlassDecodeMode mode) {\n-  switch (mode) {\n-  case KlassDecodeNone: return \"none\";\n-  case KlassDecodeZero: return \"zero\";\n-  case KlassDecodeXor:  return \"xor\";\n-  case KlassDecodeAdd:  return \"add\";\n-  default:\n-    ShouldNotReachHere();\n-  }\n-  return NULL;\n-}\n-\n-\/\/ Return the current narrow Klass pointer decode mode.\n-MacroAssembler::KlassDecodeMode MacroAssembler::klass_decode_mode() {\n-  if (_klass_decode_mode == KlassDecodeNone) {\n-    \/\/ First time initialization\n-    assert(UseCompressedClassPointers, \"not using compressed class pointers\");\n-    assert(Metaspace::initialized(), \"metaspace not initialized yet\");\n-\n-    _klass_decode_mode = klass_decode_mode_for_base(CompressedKlassPointers::base());\n-    guarantee(_klass_decode_mode != KlassDecodeNone,\n-              PTR_FORMAT \" is not a valid encoding base on aarch64\",\n-              p2i(CompressedKlassPointers::base()));\n-    log_info(metaspace)(\"klass decode mode initialized: %s\", describe_klass_decode_mode(_klass_decode_mode));\n-  }\n-  return _klass_decode_mode;\n-}\n-\n-\/\/ Given an arbitrary base address, return the KlassDecodeMode that would be used. Return KlassDecodeNone\n-\/\/ if base address is not valid for encoding.\n-MacroAssembler::KlassDecodeMode MacroAssembler::klass_decode_mode_for_base(address base) {\n-\n-  const uint64_t base_u64 = (uint64_t) base;\n-\n-  if (base_u64 == 0) {\n-    return KlassDecodeZero;\n-  }\n-\n-  if ((base_u64 & (KlassEncodingMetaspaceMax - 1)) == 0) {\n-    return KlassDecodeXor;\n-  }\n-\n-  \/\/ Note that there is no point in optimizing for shift=3 since lilliput\n-  \/\/ will use larger shifts\n-\n-  \/\/ The add+shift mode for decode_and_move_klass_not_null() requires the base to be\n-  \/\/  shiftable-without-loss. So, this is the minimum restriction on x64 for a valid\n-  \/\/  encoding base. This does not matter in reality since the shift values we use for\n-  \/\/  Lilliput, while large, won't be larger than a page size. And the encoding base\n-  \/\/  will be quite likely page aligned since it usually falls to the beginning of\n-  \/\/  either CDS or CCS.\n-  if ((base_u64 & (KlassAlignmentInBytes - 1)) == 0) {\n-    return KlassDecodeAdd;\n-  }\n-\n-  return KlassDecodeNone;\n-}\n-\n@@ -5607,12 +5532,1 @@\n-  switch (klass_decode_mode()) {\n-  case KlassDecodeZero: {\n-    shrq(r, CompressedKlassPointers::shift());\n-    break;\n-  }\n-  case KlassDecodeXor: {\n-    mov64(tmp, (int64_t)CompressedKlassPointers::base());\n-    xorq(r, tmp);\n-    shrq(r, CompressedKlassPointers::shift());\n-    break;\n-  }\n-  case KlassDecodeAdd: {\n+  if (CompressedKlassPointers::base() != nullptr) {\n@@ -5621,4 +5535,3 @@\n-    shrq(r, CompressedKlassPointers::shift());\n-    break;\n-  default:\n-    ShouldNotReachHere();\n+  if (CompressedKlassPointers::shift() != 0) {\n+    assert (LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n+    shrq(r, LogKlassAlignmentInBytes);\n@@ -5631,13 +5544,1 @@\n-  switch (klass_decode_mode()) {\n-  case KlassDecodeZero: {\n-    movptr(dst, src);\n-    shrq(dst, CompressedKlassPointers::shift());\n-    break;\n-  }\n-  case KlassDecodeXor: {\n-    mov64(dst, (int64_t)CompressedKlassPointers::base());\n-    xorq(dst, src);\n-    shrq(dst, CompressedKlassPointers::shift());\n-    break;\n-  }\n-  case KlassDecodeAdd: {\n+  if (CompressedKlassPointers::base() != nullptr) {\n@@ -5646,2 +5547,2 @@\n-    shrq(dst, CompressedKlassPointers::shift());\n-    break;\n+  } else {\n+    movptr(dst, src);\n@@ -5649,2 +5550,3 @@\n-  default:\n-    ShouldNotReachHere();\n+  if (CompressedKlassPointers::shift() != 0) {\n+    assert (LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n+    shrq(dst, LogKlassAlignmentInBytes);\n@@ -5656,13 +5558,8 @@\n-  const uint64_t base_u64 = (uint64_t)CompressedKlassPointers::base();\n-  switch (klass_decode_mode()) {\n-  case KlassDecodeZero: {\n-    shlq(r, CompressedKlassPointers::shift());\n-    break;\n-  }\n-  case KlassDecodeXor: {\n-    assert((base_u64 & (KlassEncodingMetaspaceMax - 1)) == 0,\n-           \"base \" UINT64_FORMAT_X \" invalid for xor mode\", base_u64); \/\/ should have been handled at VM init.\n-    shlq(r, CompressedKlassPointers::shift());\n-    mov64(tmp, base_u64);\n-    xorq(r, tmp);\n-    break;\n+  \/\/ Note: it will change flags\n+  assert(UseCompressedClassPointers, \"should only be used for compressed headers\");\n+  \/\/ Cannot assert, unverified entry point counts instructions (see .ad file)\n+  \/\/ vtableStubs also counts instructions in pd_code_size_limit.\n+  \/\/ Also do not verify_oop as this is called by verify_oop.\n+  if (CompressedKlassPointers::shift() != 0) {\n+    assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n+    shlq(r, LogKlassAlignmentInBytes);\n@@ -5670,3 +5567,2 @@\n-  case KlassDecodeAdd: {\n-    shlq(r, CompressedKlassPointers::shift());\n-    mov64(tmp, base_u64);\n+  if (CompressedKlassPointers::base() != nullptr) {\n+    mov64(tmp, (int64_t)CompressedKlassPointers::base());\n@@ -5674,4 +5570,0 @@\n-    break;\n-  }\n-  default:\n-    ShouldNotReachHere();\n@@ -5683,1 +5575,3 @@\n-  \/\/ Note: Cannot assert, unverified entry point counts instructions (see .ad file)\n+  \/\/ Note: it will change flags\n+  assert (UseCompressedClassPointers, \"should only be used for compressed headers\");\n+  \/\/ Cannot assert, unverified entry point counts instructions (see .ad file)\n@@ -5687,28 +5581,18 @@\n-  const uint64_t base_u64 = (uint64_t)CompressedKlassPointers::base();\n-\n-  switch (klass_decode_mode()) {\n-  case KlassDecodeZero: {\n-    movq(dst, src);\n-    shlq(dst, CompressedKlassPointers::shift());\n-    break;\n-  }\n-  case KlassDecodeXor: {\n-    assert((base_u64 & (KlassEncodingMetaspaceMax - 1)) == 0,\n-           \"base \" UINT64_FORMAT_X \" invalid for xor mode\", base_u64); \/\/ should have been handled at VM init.\n-    const uint64_t base_right_shifted = base_u64 >> CompressedKlassPointers::shift();\n-    mov64(dst, base_right_shifted);\n-    xorq(dst, src);\n-    shlq(dst, CompressedKlassPointers::shift());\n-    break;\n-  }\n-  case KlassDecodeAdd: {\n-    assert((base_u64 & (KlassAlignmentInBytes - 1)) == 0,\n-           \"base \" UINT64_FORMAT_X \" invalid for add mode\", base_u64); \/\/ should have been handled at VM init.\n-    const uint64_t base_right_shifted = base_u64 >> CompressedKlassPointers::shift();\n-    mov64(dst, base_right_shifted);\n-    addq(dst, src);\n-    shlq(dst, CompressedKlassPointers::shift());\n-    break;\n-  }\n-  default:\n-    ShouldNotReachHere();\n+  if (CompressedKlassPointers::base() == nullptr &&\n+      CompressedKlassPointers::shift() == 0) {\n+    \/\/ The best case scenario is that there is no base or shift. Then it is already\n+    \/\/ a pointer that needs nothing but a register rename.\n+    movl(dst, src);\n+  } else {\n+    if (CompressedKlassPointers::base() != nullptr) {\n+      mov64(dst, (int64_t)CompressedKlassPointers::base());\n+    } else {\n+      xorq(dst, dst);\n+    }\n+    if (CompressedKlassPointers::shift() != 0) {\n+      assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n+      assert(LogKlassAlignmentInBytes == Address::times_8, \"klass not aligned on 64bits?\");\n+      leaq(dst, Address(dst, src, Address::times_8, 0));\n+    } else {\n+      addq(dst, src);\n+    }\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":72,"deletions":188,"binary":false,"changes":260,"status":"modified"},{"patch":"@@ -82,22 +82,0 @@\n- public:\n-\n-  enum KlassDecodeMode {\n-    KlassDecodeNone,\n-    KlassDecodeZero,\n-    KlassDecodeXor,\n-    KlassDecodeAdd\n-  };\n-\n-  \/\/ Return the current narrow Klass pointer decode mode. Initialized on first call.\n-  static KlassDecodeMode klass_decode_mode();\n-\n-  \/\/ Given an arbitrary base address, return the KlassDecodeMode that would be used. Return KlassDecodeNone\n-  \/\/ if base address is not valid for encoding.\n-  static KlassDecodeMode klass_decode_mode_for_base(address base);\n-\n-  \/\/ Returns a static string\n-  static const char* describe_klass_decode_mode(KlassDecodeMode mode);\n-\n- private:\n-  static KlassDecodeMode _klass_decode_mode;\n-\n@@ -142,1 +120,1 @@\n-      int imm8 = target - (address) &disp[1];\n+      int imm8 = checked_cast<int>(target - (address) &disp[1]);\n@@ -145,1 +123,1 @@\n-      *disp = imm8;\n+      *disp = (char)imm8;\n@@ -148,1 +126,1 @@\n-      int imm32 = target - (address) &disp[1];\n+      int imm32 = checked_cast<int>(target - (address) &disp[1]);\n@@ -785,1 +763,1 @@\n-    if (src.is_constant()) addptr(dst, src.as_constant());\n+    if (src.is_constant()) addptr(dst, checked_cast<int>(src.as_constant()));\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":4,"deletions":26,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"oops\/resolvedFieldEntry.hpp\"\n@@ -200,1 +201,7 @@\n-      __ get_cache_and_index_and_bytecode_at_bcp(temp_reg, bc_reg, temp_reg, byte_no, 1);\n+      __ load_field_entry(temp_reg, bc_reg);\n+      if (byte_no == f1_byte) {\n+        __ load_unsigned_byte(temp_reg, Address(temp_reg, in_bytes(ResolvedFieldEntry::get_code_offset())));\n+      } else {\n+        __ load_unsigned_byte(temp_reg, Address(temp_reg, in_bytes(ResolvedFieldEntry::put_code_offset())));\n+      }\n+\n@@ -2659,5 +2666,0 @@\n-  switch (code) {\n-  case Bytecodes::_nofast_getfield: code = Bytecodes::_getfield; break;\n-  case Bytecodes::_nofast_putfield: code = Bytecodes::_putfield; break;\n-  default: break;\n-  }\n@@ -2694,0 +2696,62 @@\n+void TemplateTable::resolve_cache_and_index_for_field(int byte_no,\n+                                            Register cache,\n+                                            Register index) {\n+  const Register temp = rbx;\n+  assert_different_registers(cache, index, temp);\n+\n+  Label resolved;\n+\n+  Bytecodes::Code code = bytecode();\n+  switch (code) {\n+    case Bytecodes::_nofast_getfield: code = Bytecodes::_getfield; break;\n+    case Bytecodes::_nofast_putfield: code = Bytecodes::_putfield; break;\n+    default: break;\n+  }\n+\n+  assert(byte_no == f1_byte || byte_no == f2_byte, \"byte_no out of range\");\n+  __ load_field_entry(cache, index);\n+  if (byte_no == f1_byte) {\n+    __ load_unsigned_byte(temp, Address(cache, in_bytes(ResolvedFieldEntry::get_code_offset())));\n+  } else {\n+    __ load_unsigned_byte(temp, Address(cache, in_bytes(ResolvedFieldEntry::put_code_offset())));\n+  }\n+  __ cmpl(temp, code);  \/\/ have we resolved this bytecode?\n+  __ jcc(Assembler::equal, resolved);\n+\n+  \/\/ resolve first time through\n+  address entry = CAST_FROM_FN_PTR(address, InterpreterRuntime::resolve_from_cache);\n+  __ movl(temp, code);\n+  __ call_VM(noreg, entry, temp);\n+  \/\/ Update registers with resolved info\n+  __ load_field_entry(cache, index);\n+\n+  __ bind(resolved);\n+}\n+\n+void TemplateTable::load_resolved_field_entry(Register obj,\n+                                              Register cache,\n+                                              Register tos_state,\n+                                              Register offset,\n+                                              Register flags,\n+                                              bool is_static = false) {\n+  assert_different_registers(cache, tos_state, flags, offset);\n+\n+  \/\/ Field offset\n+  __ load_sized_value(offset, Address(cache, in_bytes(ResolvedFieldEntry::field_offset_offset())), sizeof(int), true \/*is_signed*\/);\n+\n+  \/\/ Flags\n+  __ load_unsigned_byte(flags, Address(cache, in_bytes(ResolvedFieldEntry::flags_offset())));\n+\n+  \/\/ TOS state\n+  __ load_unsigned_byte(tos_state, Address(cache, in_bytes(ResolvedFieldEntry::type_offset())));\n+\n+  \/\/ Klass overwrite register\n+  if (is_static) {\n+    __ movptr(obj, Address(cache, ResolvedFieldEntry::field_holder_offset()));\n+    const int mirror_offset = in_bytes(Klass::java_mirror_offset());\n+    __ movptr(obj, Address(obj, mirror_offset));\n+    __ resolve_oop_handle(obj, rscratch2);\n+  }\n+\n+}\n+\n@@ -2841,3 +2905,1 @@\n-    __ addptr(cache, in_bytes(ConstantPoolCache::base_offset()));\n-    __ shll(index, LogBytesPerWord);\n-    __ addptr(cache, index);\n+    __ load_field_entry(cache, index);\n@@ -2854,2 +2916,3 @@\n-               rax, cache);\n-    __ get_cache_and_index_at_bcp(cache, index, 1);\n+              rax, cache);\n+\n+    __ load_field_entry(cache, index);\n@@ -2869,0 +2932,1 @@\n+  const Register obj   = LP64_ONLY(c_rarg3) NOT_LP64(rcx);\n@@ -2871,2 +2935,2 @@\n-  const Register obj   = LP64_ONLY(c_rarg3) NOT_LP64(rcx);\n-  const Register flags = rax;\n+  const Register tos_state   = rax;\n+  const Register flags = rdx;\n@@ -2876,1 +2940,1 @@\n-  resolve_cache_and_index(byte_no, cache, index, sizeof(u2));\n+  resolve_cache_and_index_for_field(byte_no, cache, index);\n@@ -2878,1 +2942,1 @@\n-  load_field_cp_cache_entry(obj, cache, index, off, flags, is_static);\n+  load_resolved_field_entry(obj, cache, tos_state, off, flags, is_static);\n@@ -2886,1 +2950,0 @@\n-  __ shrl(flags, ConstantPoolCacheEntry::tos_state_shift);\n@@ -2889,3 +2952,1 @@\n-\n-  __ andl(flags, ConstantPoolCacheEntry::tos_state_mask);\n-\n+  __ testl(tos_state, tos_state);\n@@ -2893,0 +2954,1 @@\n+\n@@ -2903,1 +2965,1 @@\n-  __ cmpl(flags, ztos);\n+  __ cmpl(tos_state, ztos);\n@@ -2917,1 +2979,1 @@\n-  __ cmpl(flags, atos);\n+  __ cmpl(tos_state, atos);\n@@ -2928,1 +2990,1 @@\n-  __ cmpl(flags, itos);\n+  __ cmpl(tos_state, itos);\n@@ -2940,1 +3002,1 @@\n-  __ cmpl(flags, ctos);\n+  __ cmpl(tos_state, ctos);\n@@ -2952,1 +3014,1 @@\n-  __ cmpl(flags, stos);\n+  __ cmpl(tos_state, stos);\n@@ -2964,1 +3026,1 @@\n-  __ cmpl(flags, ltos);\n+  __ cmpl(tos_state, ltos);\n@@ -2976,1 +3038,1 @@\n-  __ cmpl(flags, ftos);\n+  __ cmpl(tos_state, ftos);\n@@ -2991,1 +3053,1 @@\n-  __ cmpl(flags, dtos);\n+  __ cmpl(tos_state, dtos);\n@@ -3031,7 +3093,4 @@\n-\n-  const Register robj = LP64_ONLY(c_rarg2)   NOT_LP64(rax);\n-  const Register RBX  = LP64_ONLY(c_rarg1)   NOT_LP64(rbx);\n-  const Register RCX  = LP64_ONLY(c_rarg3)   NOT_LP64(rcx);\n-  const Register RDX  = LP64_ONLY(rscratch1) NOT_LP64(rdx);\n-\n-  ByteSize cp_base_offset = ConstantPoolCache::base_offset();\n+  \/\/ Cache is rcx and index is rdx\n+  const Register entry = LP64_ONLY(c_rarg2) NOT_LP64(rax); \/\/ ResolvedFieldEntry\n+  const Register obj = LP64_ONLY(c_rarg1) NOT_LP64(rbx);   \/\/ Object pointer\n+  const Register value = LP64_ONLY(c_rarg3) NOT_LP64(rcx); \/\/ JValue object\n@@ -3043,1 +3102,1 @@\n-    assert_different_registers(cache, index, rax);\n+    assert_different_registers(cache, obj, rax);\n@@ -3048,2 +3107,1 @@\n-    __ get_cache_and_index_at_bcp(robj, RDX, 1);\n-\n+    __ mov(entry, cache);\n@@ -3053,1 +3111,1 @@\n-      __ xorl(RBX, RBX);\n+      __ xorl(obj, obj);\n@@ -3063,9 +3121,1 @@\n-      __ movl(RCX, Address(robj, RDX,\n-                           Address::times_ptr,\n-                           in_bytes(cp_base_offset +\n-                                     ConstantPoolCacheEntry::flags_offset())));\n-      NOT_LP64(__ mov(rbx, rsp));\n-      __ shrl(RCX, ConstantPoolCacheEntry::tos_state_shift);\n-\n-      \/\/ Make sure we don't need to mask rcx after the above shift\n-      ConstantPoolCacheEntry::verify_tos_state_shift();\n+      __ load_unsigned_byte(value, Address(entry, in_bytes(ResolvedFieldEntry::type_offset())));\n@@ -3073,2 +3123,2 @@\n-      __ movptr(c_rarg1, at_tos_p1());  \/\/ initially assume a one word jvalue\n-      __ cmpl(c_rarg3, ltos);\n+      __ movptr(obj, at_tos_p1());  \/\/ initially assume a one word jvalue\n+      __ cmpl(value, ltos);\n@@ -3076,2 +3126,2 @@\n-                 c_rarg1, at_tos_p2()); \/\/ ltos (two word jvalue)\n-      __ cmpl(c_rarg3, dtos);\n+                 obj, at_tos_p2()); \/\/ ltos (two word jvalue)\n+      __ cmpl(value, dtos);\n@@ -3079,1 +3129,1 @@\n-                 c_rarg1, at_tos_p2()); \/\/ dtos (two word jvalue)\n+                 obj, at_tos_p2()); \/\/ dtos (two word jvalue)\n@@ -3081,1 +3131,2 @@\n-      __ cmpl(rcx, ltos);\n+      __ mov(obj, rsp);\n+      __ cmpl(value, ltos);\n@@ -3083,1 +3134,1 @@\n-      __ cmpl(rcx, dtos);\n+      __ cmpl(value, dtos);\n@@ -3085,1 +3136,1 @@\n-      __ addptr(rbx, Interpreter::expr_offset_in_bytes(1)); \/\/ one word jvalue (not ltos, dtos)\n+      __ addptr(obj, Interpreter::expr_offset_in_bytes(1)); \/\/ one word jvalue (not ltos, dtos)\n@@ -3089,1 +3140,1 @@\n-      __ addptr(rbx, Interpreter::expr_offset_in_bytes(2)); \/\/ two words jvalue\n+      __ addptr(obj, Interpreter::expr_offset_in_bytes(2)); \/\/ two words jvalue\n@@ -3093,1 +3144,1 @@\n-      __ movptr(rbx, Address(rbx, 0));\n+      __ movptr(obj, Address(obj, 0));\n@@ -3096,4 +3147,1 @@\n-    \/\/ cache entry pointer\n-    __ addptr(robj, in_bytes(cp_base_offset));\n-    __ shll(RDX, LogBytesPerWord);\n-    __ addptr(robj, RDX);\n+\n@@ -3101,4 +3149,4 @@\n-    __ mov(RCX, rsp);\n-    \/\/ c_rarg1: object pointer set up above (null if static)\n-    \/\/ c_rarg2: cache entry pointer\n-    \/\/ c_rarg3: jvalue object on the stack\n+    __ mov(value, rsp);\n+    \/\/ obj: object pointer set up above (null if static)\n+    \/\/ cache: field entry pointer\n+    \/\/ value: jvalue object on the stack\n@@ -3106,4 +3154,5 @@\n-               CAST_FROM_FN_PTR(address,\n-                                InterpreterRuntime::post_field_modification),\n-               RBX, robj, RCX);\n-    __ get_cache_and_index_at_bcp(cache, index, 1);\n+              CAST_FROM_FN_PTR(address,\n+                              InterpreterRuntime::post_field_modification),\n+              obj, entry, value);\n+    \/\/ Reload field entry\n+    __ load_field_entry(cache, index);\n@@ -3117,0 +3166,1 @@\n+  const Register obj = rcx;\n@@ -3119,1 +3169,1 @@\n-  const Register obj   = rcx;\n+  const Register tos_state   = rdx;\n@@ -3123,1 +3173,1 @@\n-  resolve_cache_and_index(byte_no, cache, index, sizeof(u2));\n+  resolve_cache_and_index_for_field(byte_no, cache, index);\n@@ -3125,1 +3175,1 @@\n-  load_field_cp_cache_entry(obj, cache, index, off, flags, is_static);\n+  load_resolved_field_entry(obj, cache, tos_state, off, flags, is_static);\n@@ -3132,3 +3182,0 @@\n-  __ movl(rdx, flags);\n-  __ shrl(rdx, ConstantPoolCacheEntry::is_volatile_shift);\n-  __ andl(rdx, 0x1);\n@@ -3137,1 +3184,2 @@\n-  __ testl(rdx, rdx);\n+  __ andl(flags, (1 << ResolvedFieldEntry::is_volatile_shift));\n+  __ testl(flags, flags);\n@@ -3140,1 +3188,1 @@\n-  putfield_or_static_helper(byte_no, is_static, rc, obj, off, flags);\n+  putfield_or_static_helper(byte_no, is_static, rc, obj, off, tos_state);\n@@ -3146,1 +3194,1 @@\n-  putfield_or_static_helper(byte_no, is_static, rc, obj, off, flags);\n+  putfield_or_static_helper(byte_no, is_static, rc, obj, off, tos_state);\n@@ -3152,1 +3200,1 @@\n-                                              Register obj, Register off, Register flags) {\n+                                              Register obj, Register off, Register tos_state) {\n@@ -3164,4 +3212,2 @@\n-  __ shrl(flags, ConstantPoolCacheEntry::tos_state_shift);\n-\n-  assert(btos == 0, \"change code, btos != 0\");\n-  __ andl(flags, ConstantPoolCacheEntry::tos_state_mask);\n+  \/\/ Test TOS state\n+  __ testl(tos_state, tos_state);\n@@ -3182,1 +3228,1 @@\n-  __ cmpl(flags, ztos);\n+  __ cmpl(tos_state, ztos);\n@@ -3197,1 +3243,1 @@\n-  __ cmpl(flags, atos);\n+  __ cmpl(tos_state, atos);\n@@ -3213,1 +3259,1 @@\n-  __ cmpl(flags, itos);\n+  __ cmpl(tos_state, itos);\n@@ -3228,1 +3274,1 @@\n-  __ cmpl(flags, ctos);\n+  __ cmpl(tos_state, ctos);\n@@ -3243,1 +3289,1 @@\n-  __ cmpl(flags, stos);\n+  __ cmpl(tos_state, stos);\n@@ -3258,1 +3304,1 @@\n-  __ cmpl(flags, ltos);\n+  __ cmpl(tos_state, ltos);\n@@ -3276,1 +3322,1 @@\n-  __ cmpl(flags, ftos);\n+  __ cmpl(tos_state, ftos);\n@@ -3293,1 +3339,1 @@\n-  __ cmpl(flags, dtos);\n+  __ cmpl(tos_state, dtos);\n@@ -3363,2 +3409,2 @@\n-    LP64_ONLY(__ get_cache_entry_pointer_at_bcp(c_rarg2, rax, 1));\n-    NOT_LP64(__ get_cache_entry_pointer_at_bcp(rax, rdx, 1));\n+    LP64_ONLY(__ load_field_entry(c_rarg2, rax));\n+    NOT_LP64(__ load_field_entry(rax, rdx));\n@@ -3391,1 +3437,1 @@\n-  ByteSize base = ConstantPoolCache::base_offset();\n+  Register cache = rcx;\n@@ -3393,13 +3439,1 @@\n-  jvmti_post_fast_field_mod();\n-\n-  \/\/ access constant pool cache\n-  __ get_cache_and_index_at_bcp(rcx, rbx, 1);\n-\n-  \/\/ test for volatile with rdx but rdx is tos register for lputfield.\n-  __ movl(rdx, Address(rcx, rbx, Address::times_ptr,\n-                       in_bytes(base +\n-                                ConstantPoolCacheEntry::flags_offset())));\n-\n-  \/\/ replace index with field offset from cache entry\n-  __ movptr(rbx, Address(rcx, rbx, Address::times_ptr,\n-                         in_bytes(base + ConstantPoolCacheEntry::f2_offset())));\n+  Label notVolatile, Done;\n@@ -3407,3 +3441,1 @@\n-  \/\/ [jk] not needed currently\n-  \/\/ volatile_barrier(Assembler::Membar_mask_bits(Assembler::LoadStore |\n-  \/\/                                              Assembler::StoreStore));\n+  jvmti_post_fast_field_mod();\n@@ -3411,3 +3443,6 @@\n-  Label notVolatile, Done;\n-  __ shrl(rdx, ConstantPoolCacheEntry::is_volatile_shift);\n-  __ andl(rdx, 0x1);\n+  __ push(rax);\n+  __ load_field_entry(rcx, rax);\n+  load_resolved_field_entry(noreg, cache, rax, rbx, rdx);\n+  \/\/ RBX: field offset, RCX: RAX: TOS, RDX: flags\n+  __ andl(rdx, (1 << ResolvedFieldEntry::is_volatile_shift));\n+  __ pop(rax);\n@@ -3488,2 +3523,2 @@\n-    LP64_ONLY(__ get_cache_entry_pointer_at_bcp(c_rarg2, rcx, 1));\n-    NOT_LP64(__ get_cache_entry_pointer_at_bcp(rcx, rdx, 1));\n+    LP64_ONLY(__ load_field_entry(c_rarg2, rcx));\n+    NOT_LP64(__ load_field_entry(rcx, rdx));\n@@ -3502,12 +3537,2 @@\n-  __ get_cache_and_index_at_bcp(rcx, rbx, 1);\n-  \/\/ replace index with field offset from cache entry\n-  \/\/ [jk] not needed currently\n-  \/\/ __ movl(rdx, Address(rcx, rbx, Address::times_8,\n-  \/\/                      in_bytes(ConstantPoolCache::base_offset() +\n-  \/\/                               ConstantPoolCacheEntry::flags_offset())));\n-  \/\/ __ shrl(rdx, ConstantPoolCacheEntry::is_volatile_shift);\n-  \/\/ __ andl(rdx, 0x1);\n-  \/\/\n-  __ movptr(rbx, Address(rcx, rbx, Address::times_ptr,\n-                         in_bytes(ConstantPoolCache::base_offset() +\n-                                  ConstantPoolCacheEntry::f2_offset())));\n+  __ load_field_entry(rcx, rbx);\n+  __ load_sized_value(rbx, Address(rcx, in_bytes(ResolvedFieldEntry::field_offset_offset())), sizeof(int), true \/*is_signed*\/);\n@@ -3568,5 +3593,3 @@\n-  __ get_cache_and_index_at_bcp(rcx, rdx, 2);\n-  __ movptr(rbx,\n-            Address(rcx, rdx, Address::times_ptr,\n-                    in_bytes(ConstantPoolCache::base_offset() +\n-                             ConstantPoolCacheEntry::f2_offset())));\n+  __ load_field_entry(rcx, rdx, 2);\n+  __ load_sized_value(rbx, Address(rcx, in_bytes(ResolvedFieldEntry::field_offset_offset())), sizeof(int), true \/*is_signed*\/);\n+\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.cpp","additions":155,"deletions":132,"binary":false,"changes":287,"status":"modified"},{"patch":"@@ -1721,1 +1721,1 @@\n-const bool Matcher::supports_vector_calling_convention(void) {\n+bool Matcher::supports_vector_calling_convention(void) {\n@@ -8339,1 +8339,1 @@\n-instruct xaddB_no_res( memory mem, Universe dummy, immI add, rFlagsReg cr) %{\n+instruct xaddB_reg_no_res(memory mem, Universe dummy, rRegI add, rFlagsReg cr) %{\n@@ -8343,1 +8343,13 @@\n-  format %{ \"ADDB  [$mem],$add\" %}\n+  format %{ \"addb_lock   $mem, $add\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ addb($mem$$Address, $add$$Register);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct xaddB_imm_no_res(memory mem, Universe dummy, immI add, rFlagsReg cr) %{\n+  predicate(n->as_LoadStore()->result_not_used());\n+  match(Set dummy (GetAndAddB mem add));\n+  effect(KILL cr);\n+  format %{ \"addb_lock   $mem, $add\" %}\n@@ -8348,1 +8360,1 @@\n-  ins_pipe( pipe_cmpxchg );\n+  ins_pipe(pipe_cmpxchg);\n@@ -8351,1 +8363,2 @@\n-instruct xaddB( memory mem, rRegI newval, rFlagsReg cr) %{\n+instruct xaddB(memory mem, rRegI newval, rFlagsReg cr) %{\n+  predicate(!n->as_LoadStore()->result_not_used());\n@@ -8354,1 +8367,1 @@\n-  format %{ \"XADDB  [$mem],$newval\" %}\n+  format %{ \"xaddb_lock  $mem, $newval\" %}\n@@ -8359,1 +8372,1 @@\n-  ins_pipe( pipe_cmpxchg );\n+  ins_pipe(pipe_cmpxchg);\n@@ -8362,1 +8375,1 @@\n-instruct xaddS_no_res( memory mem, Universe dummy, immI add, rFlagsReg cr) %{\n+instruct xaddS_reg_no_res(memory mem, Universe dummy, rRegI add, rFlagsReg cr) %{\n@@ -8366,1 +8379,13 @@\n-  format %{ \"ADDW  [$mem],$add\" %}\n+  format %{ \"addw_lock   $mem, $add\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ addw($mem$$Address, $add$$Register);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct xaddS_imm_no_res(memory mem, Universe dummy, immI add, rFlagsReg cr) %{\n+  predicate(UseStoreImmI16 && n->as_LoadStore()->result_not_used());\n+  match(Set dummy (GetAndAddS mem add));\n+  effect(KILL cr);\n+  format %{ \"addw_lock   $mem, $add\" %}\n@@ -8371,1 +8396,1 @@\n-  ins_pipe( pipe_cmpxchg );\n+  ins_pipe(pipe_cmpxchg);\n@@ -8374,1 +8399,2 @@\n-instruct xaddS( memory mem, rRegI newval, rFlagsReg cr) %{\n+instruct xaddS(memory mem, rRegI newval, rFlagsReg cr) %{\n+  predicate(!n->as_LoadStore()->result_not_used());\n@@ -8377,1 +8403,1 @@\n-  format %{ \"XADDW  [$mem],$newval\" %}\n+  format %{ \"xaddw_lock  $mem, $newval\" %}\n@@ -8382,1 +8408,1 @@\n-  ins_pipe( pipe_cmpxchg );\n+  ins_pipe(pipe_cmpxchg);\n@@ -8385,1 +8411,1 @@\n-instruct xaddI_no_res( memory mem, Universe dummy, immI add, rFlagsReg cr) %{\n+instruct xaddI_reg_no_res(memory mem, Universe dummy, rRegI add, rFlagsReg cr) %{\n@@ -8389,1 +8415,13 @@\n-  format %{ \"ADDL  [$mem],$add\" %}\n+  format %{ \"addl_lock   $mem, $add\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ addl($mem$$Address, $add$$Register);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct xaddI_imm_no_res(memory mem, Universe dummy, immI add, rFlagsReg cr) %{\n+  predicate(n->as_LoadStore()->result_not_used());\n+  match(Set dummy (GetAndAddI mem add));\n+  effect(KILL cr);\n+  format %{ \"addl_lock   $mem, $add\" %}\n@@ -8394,1 +8432,1 @@\n-  ins_pipe( pipe_cmpxchg );\n+  ins_pipe(pipe_cmpxchg);\n@@ -8397,1 +8435,2 @@\n-instruct xaddI( memory mem, rRegI newval, rFlagsReg cr) %{\n+instruct xaddI(memory mem, rRegI newval, rFlagsReg cr) %{\n+  predicate(!n->as_LoadStore()->result_not_used());\n@@ -8400,1 +8439,1 @@\n-  format %{ \"XADDL  [$mem],$newval\" %}\n+  format %{ \"xaddl_lock  $mem, $newval\" %}\n@@ -8405,1 +8444,1 @@\n-  ins_pipe( pipe_cmpxchg );\n+  ins_pipe(pipe_cmpxchg);\n@@ -8408,1 +8447,1 @@\n-instruct xaddL_no_res( memory mem, Universe dummy, immL32 add, rFlagsReg cr) %{\n+instruct xaddL_reg_no_res(memory mem, Universe dummy, rRegL add, rFlagsReg cr) %{\n@@ -8412,1 +8451,13 @@\n-  format %{ \"ADDQ  [$mem],$add\" %}\n+  format %{ \"addq_lock   $mem, $add\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ addq($mem$$Address, $add$$Register);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct xaddL_imm_no_res(memory mem, Universe dummy, immL32 add, rFlagsReg cr) %{\n+  predicate(n->as_LoadStore()->result_not_used());\n+  match(Set dummy (GetAndAddL mem add));\n+  effect(KILL cr);\n+  format %{ \"addq_lock   $mem, $add\" %}\n@@ -8417,1 +8468,1 @@\n-  ins_pipe( pipe_cmpxchg );\n+  ins_pipe(pipe_cmpxchg);\n@@ -8420,1 +8471,2 @@\n-instruct xaddL( memory mem, rRegL newval, rFlagsReg cr) %{\n+instruct xaddL(memory mem, rRegL newval, rFlagsReg cr) %{\n+  predicate(!n->as_LoadStore()->result_not_used());\n@@ -8423,1 +8475,1 @@\n-  format %{ \"XADDQ  [$mem],$newval\" %}\n+  format %{ \"xaddq_lock  $mem, $newval\" %}\n@@ -8428,1 +8480,1 @@\n-  ins_pipe( pipe_cmpxchg );\n+  ins_pipe(pipe_cmpxchg);\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":77,"deletions":25,"binary":false,"changes":102,"status":"modified"},{"patch":"@@ -446,1 +446,1 @@\n-    _bytes_to_copy = masm->pc() - pc_start();\n+    _bytes_to_copy = pointer_delta_as_int(masm->pc(), pc_start());\n","filename":"src\/hotspot\/share\/c1\/c1_CodeStubs.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -81,0 +81,1 @@\n+ , _loop(nullptr)\n@@ -1210,1 +1211,2 @@\n-                 reference, LIR_OprFact::intConst(referent_offset), result);\n+                 reference, LIR_OprFact::intConst(referent_offset), result,\n+                 nullptr, info);\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -50,1 +50,1 @@\n-GrowableArrayCHeap<u1, mtClassShared>* ArchiveHeapWriter::_buffer;\n+GrowableArrayCHeap<u1, mtClassShared>* ArchiveHeapWriter::_buffer = nullptr;\n@@ -54,1 +54,1 @@\n-size_t ArchiveHeapWriter::_heap_roots_bottom_offset;\n+size_t ArchiveHeapWriter::_heap_roots_offset;\n@@ -67,0 +67,7 @@\n+\n+typedef ResourceHashtable<address, size_t,\n+      127, \/\/ prime number\n+      AnyObj::C_HEAP,\n+      mtClassShared> FillersTable;\n+static FillersTable* _fillers;\n+\n@@ -72,1 +79,1 @@\n-\n+    _fillers = new FillersTable();\n@@ -156,1 +163,1 @@\n-  return cast_to_oop(_requested_bottom + _heap_roots_bottom_offset);\n+  return cast_to_oop(_requested_bottom + _heap_roots_offset);\n@@ -221,1 +228,1 @@\n-  _heap_roots_bottom_offset = _buffer_used;\n+  _heap_roots_offset = _buffer_used;\n@@ -263,1 +270,1 @@\n-void ArchiveHeapWriter::init_filler_array_at_buffer_top(int array_length, size_t fill_bytes) {\n+HeapWord* ArchiveHeapWriter::init_filler_array_at_buffer_top(int array_length, size_t fill_bytes) {\n@@ -276,0 +283,1 @@\n+  return mem;\n@@ -305,2 +313,1 @@\n-    init_filler_array_at_buffer_top(array_length, fill_bytes);\n-\n+    HeapWord* filler = init_filler_array_at_buffer_top(array_length, fill_bytes);\n@@ -308,0 +315,11 @@\n+    _fillers->put((address)filler, fill_bytes);\n+  }\n+}\n+\n+size_t ArchiveHeapWriter::get_filler_size_at(address buffered_addr) {\n+  size_t* p = _fillers->get(buffered_addr);\n+  if (p != nullptr) {\n+    assert(*p > 0, \"filler must be larger than zero bytes\");\n+    return *p;\n+  } else {\n+    return 0; \/\/ buffered_addr is not a filler\n@@ -351,1 +369,12 @@\n-  _requested_bottom = align_down(heap_end - heap_region_byte_size, HeapRegion::GrainBytes);\n+\n+  if (UseCompressedOops) {\n+    _requested_bottom = align_down(heap_end - heap_region_byte_size, HeapRegion::GrainBytes);\n+  } else {\n+    \/\/ We always write the objects as if the heap started at this address. This\n+    \/\/ makes the contents of the archive heap deterministic.\n+    \/\/\n+    \/\/ Note that at runtime, the heap address is selected by the OS, so the archive\n+    \/\/ heap will not be mapped at 0x10000000, and the contents need to be patched.\n+    _requested_bottom = (address)NOCOOPS_REQUESTED_BASE;\n+  }\n+\n@@ -356,2 +385,3 @@\n-  info->set_memregion(MemRegion(offset_to_buffered_address<HeapWord*>(0),\n-                                offset_to_buffered_address<HeapWord*>(_buffer_used)));\n+  info->set_buffer_region(MemRegion(offset_to_buffered_address<HeapWord*>(0),\n+                                    offset_to_buffered_address<HeapWord*>(_buffer_used)));\n+  info->set_heap_roots_offset(_heap_roots_offset);\n@@ -383,3 +413,2 @@\n-void ArchiveHeapWriter::store_oop_in_buffer(oop* buffered_addr, oop requested_obj) {\n-  \/\/ Make heap content deterministic. See comments inside HeapShared::to_requested_address.\n-  *buffered_addr = HeapShared::to_requested_address(requested_obj);\n+inline void ArchiveHeapWriter::store_oop_in_buffer(oop* buffered_addr, oop requested_obj) {\n+  *buffered_addr = requested_obj;\n@@ -388,3 +417,1 @@\n-void ArchiveHeapWriter::store_oop_in_buffer(narrowOop* buffered_addr, oop requested_obj) {\n-  \/\/ Note: HeapShared::to_requested_address() is not necessary because\n-  \/\/ the heap always starts at a deterministic address with UseCompressedOops==true.\n+inline void ArchiveHeapWriter::store_oop_in_buffer(narrowOop* buffered_addr, oop requested_obj) {\n@@ -497,1 +524,1 @@\n-  oop requested_roots = requested_obj_from_buffer_offset(_heap_roots_bottom_offset);\n+  oop requested_roots = requested_obj_from_buffer_offset(_heap_roots_offset);\n@@ -521,0 +548,14 @@\n+\/\/ Do we have a jlong\/jint field that's actually a pointer to a MetaspaceObj?\n+bool ArchiveHeapWriter::is_marked_as_native_pointer(ArchiveHeapInfo* heap_info, oop src_obj, int field_offset) {\n+  HeapShared::CachedOopInfo* p = HeapShared::archived_object_cache()->get(src_obj);\n+  assert(p != nullptr, \"must be\");\n+\n+  \/\/ requested_field_addr = the address of this field in the requested space\n+  oop requested_obj = requested_obj_from_buffer_offset(p->buffer_offset());\n+  Metadata** requested_field_addr = (Metadata**)(cast_from_oop<address>(requested_obj) + field_offset);\n+  assert((Metadata**)_requested_bottom <= requested_field_addr && requested_field_addr < (Metadata**) _requested_top, \"range check\");\n+\n+  BitMap::idx_t idx = requested_field_addr - (Metadata**) _requested_bottom;\n+  return (idx < heap_info->ptrmap()->size()) && (heap_info->ptrmap()->at(idx) == true);\n+}\n+\n","filename":"src\/hotspot\/share\/cds\/archiveHeapWriter.cpp","additions":59,"deletions":18,"binary":false,"changes":77,"status":"modified"},{"patch":"@@ -211,11 +211,0 @@\n-    if (UseCompressedOops) {\n-      _heap_begin = CompressedOops::begin();\n-      _heap_end = CompressedOops::end();\n-    } else {\n-#if INCLUDE_G1GC\n-      address start = (address)G1CollectedHeap::heap()->reserved().start();\n-      address end = (address)G1CollectedHeap::heap()->reserved().end();\n-      _heap_begin = HeapShared::to_requested_address(start);\n-      _heap_end = HeapShared::to_requested_address(end);\n-#endif\n-    }\n@@ -226,1 +215,0 @@\n-  _narrow_klass_shift = CompressedKlassPointers::shift();\n@@ -288,1 +276,0 @@\n-  st->print_cr(\"- narrow_klass_shift:             %d\", _narrow_klass_shift);\n@@ -293,2 +280,0 @@\n-  st->print_cr(\"- heap_begin:                     \" INTPTR_FORMAT, p2i(_heap_begin));\n-  st->print_cr(\"- heap_end:                       \" INTPTR_FORMAT, p2i(_heap_end));\n@@ -307,0 +292,1 @@\n+  st->print_cr(\"- heap_roots_offset:              \" SIZE_FORMAT, _heap_roots_offset);\n@@ -1574,3 +1560,1 @@\n-#if INCLUDE_G1GC\n-      mapping_offset = requested_base - (char*)G1CollectedHeap::heap()->reserved().start();\n-#endif\n+      mapping_offset = 0; \/\/ not used with !UseCompressedOops\n@@ -1641,4 +1625,5 @@\n-  char* start = heap_info->start();\n-  size_t size = heap_info->byte_size();\n-  write_region(MetaspaceShared::hp, start, size, false, false);\n-  return size;\n+  char* buffer_start = heap_info->buffer_start();\n+  size_t buffer_size = heap_info->buffer_byte_size();\n+  write_region(MetaspaceShared::hp, buffer_start, buffer_size, false, false);\n+  header()->set_heap_roots_offset(heap_info->heap_roots_offset());\n+  return buffer_size;\n@@ -2021,0 +2006,7 @@\n+  \/\/ We pre-compute narrow Klass IDs with the runtime mapping start intended to be the base, and a shift of\n+  \/\/ ArchiveHeapWriter::precomputed_narrow_klass_shift. We enforce this encoding at runtime (see\n+  \/\/ CompressedKlassPointers::initialize_for_given_encoding()). Therefore, the following assertions must\n+  \/\/ hold:\n+  address archive_narrow_klass_base = (address)header()->mapped_base_address();\n+  const int archive_narrow_klass_shift = ArchiveHeapWriter::precomputed_narrow_klass_shift;\n+\n@@ -2023,2 +2015,2 @@\n-  log_info(cds)(\"    narrow_klass_base = \" PTR_FORMAT \", narrow_klass_shift = %d\",\n-                p2i(narrow_klass_base()), narrow_klass_shift());\n+  log_info(cds)(\"    narrow_klass_base at mapping start address, narrow_klass_shift = %d\",\n+                archive_narrow_klass_shift);\n@@ -2027,3 +2019,0 @@\n-  log_info(cds)(\"    heap range = [\" PTR_FORMAT \" - \"  PTR_FORMAT \"]\",\n-                p2i(header()->heap_begin()), p2i(header()->heap_end()));\n-\n@@ -2042,5 +2031,5 @@\n-  if (narrow_klass_base() != CompressedKlassPointers::base() ||\n-      narrow_klass_shift() != CompressedKlassPointers::shift()) {\n-    log_info(cds)(\"CDS heap data cannot be used because the archive was created with an incompatible narrow klass encoding mode.\");\n-    return false;\n-  }\n+  assert(archive_narrow_klass_base == CompressedKlassPointers::base(), \"Unexpected encoding base encountered \"\n+         \"(\" PTR_FORMAT \", expected \" PTR_FORMAT \")\", p2i(CompressedKlassPointers::base()), p2i(archive_narrow_klass_base));\n+  assert(archive_narrow_klass_shift == CompressedKlassPointers::shift(), \"Unexpected encoding shift encountered \"\n+         \"(%d, expected %d)\", CompressedKlassPointers::shift(), archive_narrow_klass_shift);\n+\n@@ -2084,3 +2073,4 @@\n-    \/\/ We can avoid relocation if each region is mapped into the exact same address\n-    \/\/ where it was at dump time.\n-    return \/*dumptime*\/header()->heap_begin() + r->mapping_offset();\n+    \/\/ This was the hard-coded requested base address used at dump time. With uncompressed oops,\n+    \/\/ the heap range is assigned by the OS so we will most likely have to relocate anyway, no matter\n+    \/\/ what base address was picked at duump time.\n+    return (address)ArchiveHeapWriter::NOCOOPS_REQUESTED_BASE;\n@@ -2172,1 +2162,1 @@\n-  ArchiveHeapLoader::init_mapped_heap_relocation(delta, narrow_oop_shift());\n+  ArchiveHeapLoader::init_mapped_heap_info(mapped_start, delta, narrow_oop_shift());\n","filename":"src\/hotspot\/share\/cds\/filemap.cpp","additions":26,"deletions":36,"binary":false,"changes":62,"status":"modified"},{"patch":"@@ -198,1 +198,0 @@\n-  int     _narrow_klass_shift;                    \/\/ save narrow klass base and shift\n@@ -203,2 +202,0 @@\n-  address _heap_begin;                            \/\/ heap begin at dump time.\n-  address _heap_end;                              \/\/ heap end at dump time.\n@@ -237,0 +234,2 @@\n+  size_t _heap_roots_offset;            \/\/ Offset of the HeapShared::roots() object, from the bottom\n+                                        \/\/ of the archived heap objects, in bytes.\n@@ -267,2 +266,0 @@\n-  int narrow_klass_shift()                 const { return _narrow_klass_shift; }\n-  address narrow_klass_base()              const { return (address)mapped_base_address(); }\n@@ -271,2 +268,0 @@\n-  address heap_begin()                     const { return _heap_begin; }\n-  address heap_end()                       const { return _heap_end; }\n@@ -281,0 +276,1 @@\n+  size_t heap_roots_offset()               const { return _heap_roots_offset; }\n@@ -292,0 +288,1 @@\n+  void set_heap_roots_offset(size_t n)           { _heap_roots_offset = n; }\n@@ -387,2 +384,1 @@\n-  address narrow_klass_base()  const { return header()->narrow_klass_base(); }\n-  int     narrow_klass_shift() const { return header()->narrow_klass_shift(); }\n+  size_t  heap_roots_offset()  const { return header()->heap_roots_offset(); }\n","filename":"src\/hotspot\/share\/cds\/filemap.hpp","additions":5,"deletions":9,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -137,1 +137,4 @@\n-KlassToOopHandleTable* HeapShared::_scratch_java_mirror_table = nullptr;\n+MetaspaceObjToOopHandleTable* HeapShared::_scratch_java_mirror_table = nullptr;\n+MetaspaceObjToOopHandleTable* HeapShared::_scratch_references_table = nullptr;\n+ClassLoaderData* HeapShared::_saved_java_platform_loader_data = nullptr;\n+ClassLoaderData* HeapShared::_saved_java_system_loader_data = nullptr;\n@@ -302,1 +305,1 @@\n-      \/\/ class_data will be restored explicitly at run time.\n+      \/\/ class_data will be restored explicitly at run time and after dumptime\n@@ -306,0 +309,5 @@\n+      if (obj == SystemDictionary::java_platform_loader()) {\n+        _saved_java_platform_loader_data = java_lang_ClassLoader::loader_data_acquire(SystemDictionary::java_platform_loader());\n+      } else if (obj == SystemDictionary::java_system_loader()) {\n+        _saved_java_system_loader_data = java_lang_ClassLoader::loader_data_acquire(SystemDictionary::java_system_loader());\n+      }\n@@ -313,1 +321,7 @@\n-class KlassToOopHandleTable: public ResourceHashtable<Klass*, OopHandle,\n+void HeapShared::restore_loader_data() {\n+  log_info(cds)(\"Restoring java platform and system loaders\");\n+  java_lang_ClassLoader::release_set_loader_data(SystemDictionary::java_platform_loader(), _saved_java_platform_loader_data);\n+  java_lang_ClassLoader::release_set_loader_data(SystemDictionary::java_system_loader(), _saved_java_system_loader_data);\n+}\n+\n+class MetaspaceObjToOopHandleTable: public ResourceHashtable<MetaspaceObj*, OopHandle,\n@@ -318,1 +332,1 @@\n-  oop get_oop(Klass* k) {\n+  oop get_oop(MetaspaceObj* ptr) {\n@@ -320,1 +334,1 @@\n-    OopHandle* handle = get(k);\n+    OopHandle* handle = get(ptr);\n@@ -327,1 +341,1 @@\n-  void set_oop(Klass* k, oop o) {\n+  void set_oop(MetaspaceObj* ptr, oop o) {\n@@ -330,1 +344,1 @@\n-    bool is_new = put(k, handle);\n+    bool is_new = put(ptr, handle);\n@@ -333,1 +347,1 @@\n-  void remove_oop(Klass* k) {\n+  void remove_oop(MetaspaceObj* ptr) {\n@@ -335,1 +349,1 @@\n-    OopHandle* handle = get(k);\n+    OopHandle* handle = get(ptr);\n@@ -338,1 +352,1 @@\n-      remove(k);\n+      remove(ptr);\n@@ -343,0 +357,8 @@\n+void HeapShared::add_scratch_resolved_references(ConstantPool* src, objArrayOop dest) {\n+  _scratch_references_table->set_oop(src, dest);\n+}\n+\n+objArrayOop HeapShared::scratch_resolved_references(ConstantPool* src) {\n+  return (objArrayOop)_scratch_references_table->get_oop(src);\n+}\n+\n@@ -351,1 +373,2 @@\n-  _scratch_java_mirror_table = new (mtClass)KlassToOopHandleTable();\n+  _scratch_java_mirror_table = new (mtClass)MetaspaceObjToOopHandleTable();\n+  _scratch_references_table = new (mtClass)MetaspaceObjToOopHandleTable();\n@@ -370,0 +393,3 @@\n+  if (k->is_instance_klass()) {\n+    _scratch_references_table->remove(InstanceKlass::cast(k)->constants());\n+  }\n@@ -836,17 +862,4 @@\n-void HeapShared::serialize_root(SerializeClosure* soc) {\n-  oop roots_oop = nullptr;\n-\n-  if (soc->reading()) {\n-    soc->do_oop(&roots_oop); \/\/ read from archive\n-    assert(oopDesc::is_oop_or_null(roots_oop), \"is oop\");\n-    \/\/ Create an OopHandle only if we have actually mapped or loaded the roots\n-    if (roots_oop != nullptr) {\n-      assert(ArchiveHeapLoader::is_in_use(), \"must be\");\n-      _roots = OopHandle(Universe::vm_global(), roots_oop);\n-    }\n-  } else {\n-    \/\/ writing\n-    if (HeapShared::can_write()) {\n-      roots_oop = ArchiveHeapWriter::heap_roots_requested_address();\n-    }\n-    soc->do_oop(&roots_oop); \/\/ write to archive\n+void HeapShared::init_roots(oop roots_oop) {\n+  if (roots_oop != nullptr) {\n+    assert(ArchiveHeapLoader::is_in_use(), \"must be\");\n+    _roots = OopHandle(Universe::vm_global(), roots_oop);\n@@ -1673,2 +1686,0 @@\n-      \/\/ Note: HeapShared::to_requested_address() is not necessary because\n-      \/\/ the heap always starts at a deterministic address with UseCompressedOops==true.\n@@ -1696,27 +1707,0 @@\n-address HeapShared::to_requested_address(address dumptime_addr) {\n-  assert(DumpSharedSpaces, \"static dump time only\");\n-  if (dumptime_addr == nullptr || UseCompressedOops) {\n-    return dumptime_addr;\n-  }\n-\n-  \/\/ With UseCompressedOops==false, actual_base is selected by the OS so\n-  \/\/ it's different across -Xshare:dump runs.\n-  address actual_base = (address)G1CollectedHeap::heap()->reserved().start();\n-  address actual_end  = (address)G1CollectedHeap::heap()->reserved().end();\n-  assert(actual_base <= dumptime_addr && dumptime_addr <= actual_end, \"must be an address in the heap\");\n-\n-  \/\/ We always write the objects as if the heap started at this address. This\n-  \/\/ makes the heap content deterministic.\n-  \/\/\n-  \/\/ Note that at runtime, the heap address is also selected by the OS, so\n-  \/\/ the archive heap will not be mapped at 0x10000000. Instead, we will call\n-  \/\/ HeapShared::patch_embedded_pointers() to relocate the heap contents\n-  \/\/ accordingly.\n-  const address REQUESTED_BASE = (address)0x10000000;\n-  intx delta = REQUESTED_BASE - actual_base;\n-\n-  address requested_addr = dumptime_addr + delta;\n-  assert(REQUESTED_BASE != 0 && requested_addr != nullptr, \"sanity\");\n-  return requested_addr;\n-}\n-\n","filename":"src\/hotspot\/share\/cds\/heapShared.cpp","additions":41,"deletions":57,"binary":false,"changes":98,"status":"modified"},{"patch":"@@ -1311,1 +1311,0 @@\n-                                          page_size,\n@@ -1314,1 +1313,2 @@\n-                                          rs.size());\n+                                          rs.size(),\n+                                          page_size);\n@@ -1404,2 +1404,2 @@\n-                       page_size,\n-                       heap_rs.size());\n+                       heap_rs.size(),\n+                       page_size);\n@@ -2470,0 +2470,6 @@\n+\n+  if (collector_state()->in_concurrent_start_gc()) {\n+    log_debug(gc, verify)(\"Marking state\");\n+    _verifier->verify_marking_state();\n+  }\n+\n@@ -2658,0 +2664,5 @@\n+void G1CollectedHeap::retain_region(HeapRegion* hr) {\n+  MutexLocker x(G1RareEvent_lock, Mutex::_no_safepoint_check_flag);\n+  collection_set()->candidates()->add_retained_region_unsorted(hr);\n+}\n+\n@@ -2983,3 +2994,0 @@\n-  if (collector_state()->in_concurrent_start_gc()) {\n-    _cm->add_to_liveness(worker_id, obj, obj_size);\n-  }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":15,"deletions":7,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -91,1 +91,2 @@\n-    _evac_failure_regions(evac_failure_regions)\n+    _evac_failure_regions(evac_failure_regions),\n+    _evac_failure_enqueued_cards(0)\n@@ -150,0 +151,4 @@\n+size_t G1ParScanThreadState::evac_failure_enqueued_cards() const {\n+  return _evac_failure_enqueued_cards;\n+}\n+\n@@ -609,0 +614,1 @@\n+    size_t evac_fail_enqueued_cards = pss->evac_failure_enqueued_cards();\n@@ -613,0 +619,1 @@\n+    p->record_or_add_thread_work_item(G1GCPhaseTimes::MergePSS, worker_id, evac_fail_enqueued_cards, G1GCPhaseTimes::MergePSSEvacFailExtra);\n@@ -654,6 +661,3 @@\n-    \/\/ existing closure to scan evacuated objects because:\n-    \/\/ - for objects referring into the collection set we do not need to gather\n-    \/\/ cards at this time. The regions they are in will be unconditionally turned\n-    \/\/ to old regions without remembered sets.\n-    \/\/ - since we are iterating from a collection set region (i.e. never a Survivor\n-    \/\/ region), we always need to gather cards for this case.\n+    \/\/ existing closure to scan evacuated objects; since we are iterating from a\n+    \/\/ collection set region (i.e. never a Survivor region), we always need to\n+    \/\/ gather cards for this case.\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.cpp","additions":11,"deletions":7,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -112,0 +112,10 @@\n+  \/\/ Number of additional cards into evacuation failed regions enqueued into\n+  \/\/ the local DCQS. This is an approximation, as cards that would be added later\n+  \/\/ outside of evacuation failure will not be subtracted again.\n+  size_t _evac_failure_enqueued_cards;\n+\n+  \/\/ Enqueue the card if not already in the set; this is a best-effort attempt on\n+  \/\/ detecting duplicates.\n+  template <class T> bool enqueue_if_new(T* p);\n+  \/\/ Enqueue the card of p into the (evacuation failed) region.\n+  template <class T> void enqueue_card_into_evac_fail_region(T* p, oop obj);\n@@ -155,0 +165,2 @@\n+  size_t evac_failure_enqueued_cards() const;\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.hpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -240,3 +240,8 @@\n-      if (!tq->try_push_to_taskqueue(task)) {\n-        process_popped_location_depth(task);\n-      }\n+      \/\/ In PSCardTable::scavenge_contents_parallel(), when work is distributed\n+      \/\/ among different workers, an object is never split between multiple workers.\n+      \/\/ Therefore, if a worker gets owned a large objArray, it may accumulate\n+      \/\/ many tasks (corresponding to every element in this array) in its\n+      \/\/ task queue. When there are too many overflow tasks, publishing them\n+      \/\/ (via try_push_to_taskqueue()) can incur noticeable overhead in Young GC\n+      \/\/ pause, so it is better to process them locally until large-objArray-splitting is implemented.\n+      process_popped_location_depth(task);\n","filename":"src\/hotspot\/share\/gc\/parallel\/psPromotionManager.cpp","additions":8,"deletions":3,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -1007,3 +1007,2 @@\n-void DefNewGeneration::contribute_scratch(ScratchBlock*& list, Generation* requestor,\n-                                         size_t max_alloc_words) {\n-  if (requestor == this || _promotion_failed) {\n+void DefNewGeneration::contribute_scratch(void*& scratch, size_t& num_words) {\n+  if (_promotion_failed) {\n@@ -1012,6 +1011,1 @@\n-  assert(GenCollectedHeap::heap()->is_old_gen(requestor), \"We should not call our own generation\");\n-  \/* $$$ Assert this?  \"trace\" is a \"MarkSweep\" function so that's not appropriate.\n-  if (to_space->top() > to_space->bottom()) {\n-    trace(\"to_space not empty when contribute_scratch called\");\n-  }\n-  *\/\n+  const size_t MinFreeScratchWords = 100;\n@@ -1021,2 +1015,1 @@\n-  assert(to_space->end() >= to_space->top(), \"pointers out of order\");\n-  size_t free_words = pointer_delta(to_space->end(), to_space->top());\n+  const size_t free_words = pointer_delta(to_space->end(), to_space->top());\n@@ -1024,4 +1017,2 @@\n-    ScratchBlock* sb = (ScratchBlock*)to_space->top();\n-    sb->num_words = free_words;\n-    sb->next = list;\n-    list = sb;\n+    scratch = to_space->top();\n+    num_words = free_words;\n","filename":"src\/hotspot\/share\/gc\/serial\/defNewGeneration.cpp","additions":6,"deletions":15,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"gc\/serial\/defNewGeneration.hpp\"\n@@ -134,3 +135,4 @@\n-  GenCollectedHeap* gch = GenCollectedHeap::heap();\n-  \/\/ Scratch request on behalf of old generation; will do no allocation.\n-  ScratchBlock* scratch = gch->gather_scratch(gch->old_gen(), 0);\n+  void* scratch = nullptr;\n+  size_t num_words;\n+  DefNewGeneration* young_gen = (DefNewGeneration*)GenCollectedHeap::heap()->young_gen();\n+  young_gen->contribute_scratch(scratch, num_words);\n@@ -138,4 +140,1 @@\n-  \/\/ $$$ To cut a corner, we'll only use the first scratch block, and then\n-  \/\/ revert to malloc.\n-    _preserved_count_max =\n-      scratch->num_words * HeapWordSize \/ sizeof(PreservedMark);\n+    _preserved_count_max = num_words * HeapWordSize \/ sizeof(PreservedMark);\n@@ -155,2 +154,4 @@\n-  GenCollectedHeap* gch = GenCollectedHeap::heap();\n-  gch->release_scratch();\n+  if (_preserved_count_max != 0) {\n+    DefNewGeneration* young_gen = (DefNewGeneration*)GenCollectedHeap::heap()->young_gen();\n+    young_gen->reset_scratch();\n+  }\n","filename":"src\/hotspot\/share\/gc\/serial\/genMarkSweep.cpp","additions":10,"deletions":9,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -138,1 +138,1 @@\n-  static ReferenceProcessor* const ref_processor() { return _ref_processor; }\n+  static ReferenceProcessor* ref_processor() { return _ref_processor; }\n","filename":"src\/hotspot\/share\/gc\/serial\/markSweep.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -165,2 +165,2 @@\n-                       used_page_size,\n-                       heap_rs.size());\n+                       heap_rs.size(),\n+                       used_page_size);\n@@ -911,50 +911,0 @@\n-\/\/ Requires \"*prev_ptr\" to be non-null.  Deletes and a block of minimal size\n-\/\/ from the list headed by \"*prev_ptr\".\n-static ScratchBlock *removeSmallestScratch(ScratchBlock **prev_ptr) {\n-  bool first = true;\n-  size_t min_size = 0;   \/\/ \"first\" makes this conceptually infinite.\n-  ScratchBlock **smallest_ptr, *smallest;\n-  ScratchBlock  *cur = *prev_ptr;\n-  while (cur) {\n-    assert(*prev_ptr == cur, \"just checking\");\n-    if (first || cur->num_words < min_size) {\n-      smallest_ptr = prev_ptr;\n-      smallest     = cur;\n-      min_size     = smallest->num_words;\n-      first        = false;\n-    }\n-    prev_ptr = &cur->next;\n-    cur     =  cur->next;\n-  }\n-  smallest      = *smallest_ptr;\n-  *smallest_ptr = smallest->next;\n-  return smallest;\n-}\n-\n-\/\/ Sort the scratch block list headed by res into decreasing size order,\n-\/\/ and set \"res\" to the result.\n-static void sort_scratch_list(ScratchBlock*& list) {\n-  ScratchBlock* sorted = nullptr;\n-  ScratchBlock* unsorted = list;\n-  while (unsorted) {\n-    ScratchBlock *smallest = removeSmallestScratch(&unsorted);\n-    smallest->next  = sorted;\n-    sorted          = smallest;\n-  }\n-  list = sorted;\n-}\n-\n-ScratchBlock* GenCollectedHeap::gather_scratch(Generation* requestor,\n-                                               size_t max_alloc_words) {\n-  ScratchBlock* res = nullptr;\n-  _young_gen->contribute_scratch(res, requestor, max_alloc_words);\n-  _old_gen->contribute_scratch(res, requestor, max_alloc_words);\n-  sort_scratch_list(res);\n-  return res;\n-}\n-\n-void GenCollectedHeap::release_scratch() {\n-  _young_gen->reset_scratch();\n-  _old_gen->reset_scratch();\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shared\/genCollectedHeap.cpp","additions":2,"deletions":52,"binary":false,"changes":54,"status":"modified"},{"patch":"@@ -59,1 +59,1 @@\n-  void notify_allocation(JavaThread* thread);\n+  void notify_allocation();\n@@ -63,1 +63,1 @@\n-  void notify_allocation_dtrace_sampler(JavaThread* thread);\n+  void notify_allocation_dtrace_sampler();\n@@ -73,1 +73,1 @@\n-      _thread(JavaThread::current()),\n+      _thread(JavaThread::cast(allocator._thread)), \/\/ Do not use Allocation in non-JavaThreads.\n@@ -80,0 +80,1 @@\n+    assert(Thread::current() == allocator._thread, \"do not pass MemAllocator across threads\");\n@@ -85,1 +86,1 @@\n-      notify_allocation(_thread);\n+      notify_allocation();\n@@ -159,1 +160,1 @@\n-  JavaThread::cast(_thread)->check_for_valid_safepoint_state();\n+  _thread->check_for_valid_safepoint_state();\n@@ -220,1 +221,1 @@\n-void MemAllocator::Allocation::notify_allocation_dtrace_sampler(JavaThread* thread) {\n+void MemAllocator::Allocation::notify_allocation_dtrace_sampler() {\n@@ -226,1 +227,1 @@\n-      SharedRuntime::dtrace_object_alloc(thread, obj(), word_size);\n+      SharedRuntime::dtrace_object_alloc(_thread, obj(), word_size);\n@@ -231,1 +232,1 @@\n-void MemAllocator::Allocation::notify_allocation(JavaThread* thread) {\n+void MemAllocator::Allocation::notify_allocation() {\n@@ -234,1 +235,1 @@\n-  notify_allocation_dtrace_sampler(thread);\n+  notify_allocation_dtrace_sampler();\n@@ -338,1 +339,1 @@\n-  debug_only(JavaThread::cast(_thread)->check_for_valid_safepoint_state());\n+  debug_only(allocation._thread->check_for_valid_safepoint_state());\n@@ -408,9 +409,0 @@\n-MemRegion ObjArrayAllocator::obj_memory_range(oop obj) const {\n-  if (_do_zero) {\n-    return MemAllocator::obj_memory_range(obj);\n-  }\n-  ArrayKlass* array_klass = ArrayKlass::cast(_klass);\n-  const size_t hs = align_up(arrayOopDesc::base_offset_in_bytes(array_klass->element_type()), HeapWordSize) \/ HeapWordSize;\n-  return MemRegion(cast_from_oop<HeapWord*>(obj) + hs, _word_size - hs);\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shared\/memAllocator.cpp","additions":11,"deletions":19,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -196,0 +196,4 @@\n+  os::trace_page_sizes_for_requested_size(\"Heap\",\n+                                          max_byte_size, heap_alignment,\n+                                          heap_rs.base(),\n+                                          heap_rs.size(), heap_rs.page_size());\n@@ -219,2 +223,2 @@\n-  _bitmap_size = ShenandoahMarkBitMap::compute_size(heap_rs.size());\n-  _bitmap_size = align_up(_bitmap_size, bitmap_page_size);\n+  size_t bitmap_size_orig = ShenandoahMarkBitMap::compute_size(heap_rs.size());\n+  _bitmap_size = align_up(bitmap_size_orig, bitmap_page_size);\n@@ -246,0 +250,4 @@\n+  os::trace_page_sizes_for_requested_size(\"Mark Bitmap\",\n+                                          bitmap_size_orig, bitmap_page_size,\n+                                          bitmap.base(),\n+                                          bitmap.size(), bitmap.page_size());\n@@ -262,0 +270,4 @@\n+    os::trace_page_sizes_for_requested_size(\"Verify Bitmap\",\n+                                            bitmap_size_orig, bitmap_page_size,\n+                                            verify_bitmap.base(),\n+                                            verify_bitmap.size(), verify_bitmap.page_size());\n@@ -273,1 +285,14 @@\n-  ReservedSpace aux_bitmap(_bitmap_size, bitmap_page_size);\n+  size_t aux_bitmap_page_size = bitmap_page_size;\n+#ifdef LINUX\n+  \/\/ In THP \"advise\" mode, we refrain from advising the system to use large pages\n+  \/\/ since we know these commits will be short lived, and there is no reason to trash\n+  \/\/ the THP area with this bitmap.\n+  if (UseTransparentHugePages) {\n+    aux_bitmap_page_size = os::vm_page_size();\n+  }\n+#endif\n+  ReservedSpace aux_bitmap(_bitmap_size, aux_bitmap_page_size);\n+  os::trace_page_sizes_for_requested_size(\"Aux Bitmap\",\n+                                          bitmap_size_orig, aux_bitmap_page_size,\n+                                          aux_bitmap.base(),\n+                                          aux_bitmap.size(), aux_bitmap.page_size());\n@@ -283,2 +308,3 @@\n-  size_t region_storage_size = align_up(region_align * _num_regions, region_page_size);\n-  region_storage_size = align_up(region_storage_size, os::vm_allocation_granularity());\n+  size_t region_storage_size_orig = region_align * _num_regions;\n+  size_t region_storage_size = align_up(region_storage_size_orig,\n+                                        MAX2(region_page_size, os::vm_allocation_granularity()));\n@@ -287,0 +313,4 @@\n+  os::trace_page_sizes_for_requested_size(\"Region Storage\",\n+                                          region_storage_size_orig, region_page_size,\n+                                          region_storage.base(),\n+                                          region_storage.size(), region_storage.page_size());\n@@ -297,2 +327,3 @@\n-    size_t cset_align = MAX2<size_t>(os::vm_page_size(), os::vm_allocation_granularity());\n-    size_t cset_size = align_up(((size_t) sh_rs.base() + sh_rs.size()) >> ShenandoahHeapRegion::region_size_bytes_shift(), cset_align);\n+    const size_t cset_align = MAX2<size_t>(os::vm_page_size(), os::vm_allocation_granularity());\n+    const size_t cset_size = align_up(((size_t) sh_rs.base() + sh_rs.size()) >> ShenandoahHeapRegion::region_size_bytes_shift(), cset_align);\n+    const size_t cset_page_size = os::vm_page_size();\n@@ -302,0 +333,1 @@\n+    ReservedSpace cset_rs;\n@@ -306,1 +338,1 @@\n-      ReservedSpace cset_rs(cset_size, cset_align, os::vm_page_size(), req_addr);\n+      cset_rs = ReservedSpace(cset_size, cset_align, cset_page_size, req_addr);\n@@ -315,1 +347,1 @@\n-      ReservedSpace cset_rs(cset_size, cset_align, os::vm_page_size());\n+      cset_rs = ReservedSpace(cset_size, cset_align, os::vm_page_size());\n@@ -318,0 +350,4 @@\n+    os::trace_page_sizes_for_requested_size(\"Collection Set\",\n+                                            cset_size, cset_page_size,\n+                                            cset_rs.base(),\n+                                            cset_rs.size(), cset_rs.page_size());\n@@ -641,0 +677,4 @@\n+size_t ShenandoahHeap::available() const {\n+  return free_set()->available();\n+}\n+\n@@ -1879,1 +1919,1 @@\n-size_t ShenandoahHeap::bytes_allocated_since_gc_start() {\n+size_t ShenandoahHeap::bytes_allocated_since_gc_start() const {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":50,"deletions":10,"binary":false,"changes":60,"status":"modified"},{"patch":"@@ -84,1 +84,1 @@\n-inline ShenandoahHeapRegion* const ShenandoahHeap::heap_region_containing(const void* addr) const {\n+inline ShenandoahHeapRegion* ShenandoahHeap::heap_region_containing(const void* addr) const {\n@@ -557,1 +557,1 @@\n-inline ShenandoahHeapRegion* const ShenandoahHeap::get_region(size_t region_idx) const {\n+inline ShenandoahHeapRegion* ShenandoahHeap::get_region(size_t region_idx) const {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -476,1 +476,1 @@\n-  const size_t in_place_count() const {\n+  size_t in_place_count() const {\n@@ -570,1 +570,1 @@\n-  const size_t in_place_count() const {\n+  size_t in_place_count() const {\n","filename":"src\/hotspot\/share\/gc\/z\/zRelocate.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -48,0 +48,1 @@\n+#include \"oops\/resolvedFieldEntry.hpp\"\n@@ -409,1 +410,1 @@\n-                                  target, cache),                   \\\n+                                  target, entry),                   \\\n@@ -429,1 +430,1 @@\n-                                  target, cache,                    \\\n+                                  target, entry,                    \\\n@@ -1725,1 +1726,1 @@\n-          ConstantPoolCacheEntry* cache;\n+          ResolvedFieldEntry* entry = cp->resolved_field_entry_at(index);\n@@ -1739,2 +1740,1 @@\n-          cache = cp->entry_at(index);\n-          if (!cache->is_resolved(code)) {\n+          if (!entry->is_resolved(code)) {\n@@ -1743,1 +1743,1 @@\n-            cache = cp->entry_at(index);\n+            entry = cp->resolved_field_entry_at(index);\n@@ -1748,1 +1748,1 @@\n-            Klass* k = cache->f1_as_klass();\n+            Klass* k = entry->field_holder();\n@@ -1755,1 +1755,1 @@\n-            if (REWRITE_BYTECODES && !cache->is_volatile() &&\n+            if (REWRITE_BYTECODES && !entry->is_volatile() &&\n@@ -1758,1 +1758,1 @@\n-              REWRITE_AT_PC(fast_get_type(cache->flag_state()));\n+              REWRITE_AT_PC(fast_get_type((TosState)(entry->tos_state())));\n@@ -1767,3 +1767,3 @@\n-          TosState tos_type = cache->flag_state();\n-          int field_offset = cache->f2_as_index();\n-          if (cache->is_volatile()) {\n+          TosState tos_type = (TosState)(entry->tos_state());\n+          int field_offset = entry->field_offset();\n+          if (entry->is_volatile()) {\n@@ -1845,1 +1845,1 @@\n-         }\n+        }\n@@ -1852,1 +1852,1 @@\n-          ConstantPoolCacheEntry* cache = cp->entry_at(index);\n+          ResolvedFieldEntry* entry = cp->resolved_field_entry_at(index);\n@@ -1861,1 +1861,1 @@\n-          if (!cache->is_resolved(code)) {\n+          if (!entry->is_resolved(code)) {\n@@ -1864,1 +1864,1 @@\n-            cache = cp->entry_at(index);\n+            entry = cp->resolved_field_entry_at(index);\n@@ -1872,1 +1872,1 @@\n-          TosState tos_type = cache->flag_state();\n+          TosState tos_type = (TosState)(entry->tos_state());\n@@ -1879,1 +1879,1 @@\n-            Klass* k = cache->f1_as_klass();\n+            Klass* k = entry->field_holder();\n@@ -1887,1 +1887,1 @@\n-            if (REWRITE_BYTECODES && !cache->is_volatile() &&\n+            if (REWRITE_BYTECODES && !entry->is_volatile() &&\n@@ -1890,1 +1890,1 @@\n-              REWRITE_AT_PC(fast_put_type(cache->flag_state()));\n+              REWRITE_AT_PC(fast_put_type((TosState)(entry->tos_state())));\n@@ -1899,2 +1899,2 @@\n-          int field_offset = cache->f2_as_index();\n-          if (cache->is_volatile()) {\n+          int field_offset = entry->field_offset();\n+          if (entry->is_volatile()) {\n@@ -2590,2 +2590,2 @@\n-        ConstantPoolCacheEntry* cache = cp->entry_at(index);\n-        int field_offset = cache->f2_as_index();\n+        ResolvedFieldEntry* entry = cp->resolved_field_entry_at(index);\n+        int field_offset = entry->field_offset();\n@@ -2605,2 +2605,2 @@\n-        ConstantPoolCacheEntry* cache = cp->entry_at(index);\n-        int field_offset = cache->f2_as_index();\n+        ResolvedFieldEntry* entry = cp->resolved_field_entry_at(index);\n+        int field_offset = entry->field_offset();\n@@ -2619,2 +2619,2 @@\n-        ConstantPoolCacheEntry* cache = cp->entry_at(index);\n-        int field_offset = cache->f2_as_index();\n+        ResolvedFieldEntry* entry = cp->resolved_field_entry_at(index);\n+        int field_offset = entry->field_offset();\n@@ -2633,2 +2633,2 @@\n-        ConstantPoolCacheEntry* cache = cp->entry_at(index);\n-        int field_offset = cache->f2_as_index();\n+        ResolvedFieldEntry* entry = cp->resolved_field_entry_at(index);\n+        int field_offset = entry->field_offset();\n@@ -2648,2 +2648,2 @@\n-        ConstantPoolCacheEntry* cache = cp->entry_at(index);\n-        int field_offset = cache->f2_as_index();\n+        ResolvedFieldEntry* entry = cp->resolved_field_entry_at(index);\n+        int field_offset = entry->field_offset();\n@@ -2662,2 +2662,2 @@\n-        ConstantPoolCacheEntry* cache = cp->entry_at(index);\n-        int field_offset = cache->f2_as_index();\n+        ResolvedFieldEntry* entry = cp->resolved_field_entry_at(index);\n+        int field_offset = entry->field_offset();\n@@ -2676,2 +2676,2 @@\n-        ConstantPoolCacheEntry* cache = cp->entry_at(index);\n-        int field_offset = cache->f2_as_index();\n+        ResolvedFieldEntry* entry = cp->resolved_field_entry_at(index);\n+        int field_offset = entry->field_offset();\n@@ -2691,2 +2691,2 @@\n-        ConstantPoolCacheEntry* cache = cp->entry_at(index);\n-        int field_offset = cache->f2_as_index();\n+        ResolvedFieldEntry* entry = cp->resolved_field_entry_at(index);\n+        int field_offset = entry->field_offset();\n@@ -2705,1 +2705,1 @@\n-        ConstantPoolCacheEntry* cache = cp->entry_at(index);\n+        ResolvedFieldEntry* entry = cp->resolved_field_entry_at(index);\n@@ -2712,1 +2712,1 @@\n-        int field_offset = cache->f2_as_index();\n+        int field_offset = entry->field_offset();\n@@ -2720,1 +2720,1 @@\n-        ConstantPoolCacheEntry* cache = cp->entry_at(index);\n+        ResolvedFieldEntry* entry = cp->resolved_field_entry_at(index);\n@@ -2727,1 +2727,1 @@\n-        int field_offset = cache->f2_as_index();\n+        int field_offset = entry->field_offset();\n@@ -2735,1 +2735,1 @@\n-        ConstantPoolCacheEntry* cache = cp->entry_at(index);\n+        ResolvedFieldEntry* entry = cp->resolved_field_entry_at(index);\n@@ -2742,1 +2742,1 @@\n-        int field_offset = cache->f2_as_index();\n+        int field_offset = entry->field_offset();\n@@ -2750,1 +2750,1 @@\n-        ConstantPoolCacheEntry* cache = cp->entry_at(index);\n+        ResolvedFieldEntry* entry = cp->resolved_field_entry_at(index);\n@@ -2757,1 +2757,1 @@\n-        int field_offset = cache->f2_as_index();\n+        int field_offset = entry->field_offset();\n@@ -2765,1 +2765,1 @@\n-        ConstantPoolCacheEntry* cache = cp->entry_at(index);\n+        ResolvedFieldEntry* entry = cp->resolved_field_entry_at(index);\n@@ -2772,1 +2772,1 @@\n-        int field_offset = cache->f2_as_index();\n+        int field_offset = entry->field_offset();\n@@ -2780,1 +2780,1 @@\n-        ConstantPoolCacheEntry* cache = cp->entry_at(index);\n+        ResolvedFieldEntry* entry = cp->resolved_field_entry_at(index);\n@@ -2787,1 +2787,1 @@\n-        int field_offset = cache->f2_as_index();\n+        int field_offset = entry->field_offset();\n@@ -2795,1 +2795,1 @@\n-        ConstantPoolCacheEntry* cache = cp->entry_at(index);\n+        ResolvedFieldEntry* entry = cp->resolved_field_entry_at(index);\n@@ -2802,1 +2802,1 @@\n-        int field_offset = cache->f2_as_index();\n+        int field_offset = entry->field_offset();\n@@ -2810,1 +2810,1 @@\n-        ConstantPoolCacheEntry* cache = cp->entry_at(index);\n+        ResolvedFieldEntry* entry = cp->resolved_field_entry_at(index);\n@@ -2817,1 +2817,1 @@\n-        int field_offset = cache->f2_as_index();\n+        int field_offset = entry->field_offset();\n@@ -2825,1 +2825,1 @@\n-        ConstantPoolCacheEntry* cache = cp->entry_at(index);\n+        ResolvedFieldEntry* entry = cp->resolved_field_entry_at(index);\n@@ -2832,1 +2832,1 @@\n-        int field_offset = cache->f2_as_index();\n+        int field_offset = entry->field_offset();\n@@ -2847,2 +2847,2 @@\n-        ConstantPoolCacheEntry* cache = cp->entry_at(index);\n-        int field_offset = cache->f2_as_index();\n+        ResolvedFieldEntry* entry = cp->resolved_field_entry_at(index);\n+        int field_offset = entry->field_offset();\n@@ -2863,2 +2863,2 @@\n-        ConstantPoolCacheEntry* cache = cp->entry_at(index);\n-        int field_offset = cache->f2_as_index();\n+        ResolvedFieldEntry* entry = cp->resolved_field_entry_at(index);\n+        int field_offset = entry->field_offset();\n@@ -2878,2 +2878,2 @@\n-        ConstantPoolCacheEntry* cache = cp->entry_at(index);\n-        int field_offset = cache->f2_as_index();\n+        ResolvedFieldEntry* entry = cp->resolved_field_entry_at(index);\n+        int field_offset = entry->field_offset();\n","filename":"src\/hotspot\/share\/interpreter\/zero\/bytecodeInterpreter.cpp","additions":62,"deletions":62,"binary":false,"changes":124,"status":"modified"},{"patch":"@@ -55,0 +55,1 @@\n+#include \"runtime\/arguments.hpp\"\n@@ -579,1 +580,1 @@\n-C2V_VMENTRY_NULL(jobject, lookupType, (JNIEnv* env, jobject, jstring jname, ARGUMENT_PAIR(accessing_klass), jboolean resolve))\n+C2V_VMENTRY_NULL(jobject, lookupType, (JNIEnv* env, jobject, jstring jname, ARGUMENT_PAIR(accessing_klass), jint accessing_klass_loader, jboolean resolve))\n@@ -585,1 +586,1 @@\n-    JVMCI_THROW_MSG_0(InternalError, err_msg(\"Primitive type %s should be handled in Java code\", class_name->as_C_string()));\n+    JVMCI_THROW_MSG_0(InternalError, err_msg(\"Primitive type %s should be handled in Java code\", str));\n@@ -588,0 +589,12 @@\n+#ifdef ASSERT\n+  const char* val = Arguments::PropertyList_get_value(Arguments::system_properties(), \"test.jvmci.lookupTypeException\");\n+  if (val != nullptr) {\n+    if (strstr(val, \"<trace>\") != nullptr) {\n+      tty->print_cr(\"CompilerToVM.lookupType: %s\", str);\n+    } else if (strstr(val, str) != nullptr) {\n+      THROW_MSG_0(vmSymbols::java_lang_Exception(),\n+                  err_msg(\"lookupTypeException: %s\", str));\n+    }\n+  }\n+#endif\n+\n@@ -596,2 +609,7 @@\n-    \/\/ Use the System class loader\n-    class_loader = Handle(THREAD, SystemDictionary::java_system_loader());\n+    switch (accessing_klass_loader) {\n+      case 0: break; \/\/ class_loader is already null, the boot loader\n+      case 1: class_loader = Handle(THREAD, SystemDictionary::java_platform_loader()); break;\n+      case 2: class_loader = Handle(THREAD, SystemDictionary::java_system_loader()); break;\n+      default:\n+        JVMCI_THROW_MSG_0(InternalError, err_msg(\"Illegal class loader value: %d\", accessing_klass_loader));\n+    }\n@@ -604,1 +622,1 @@\n-      JVMCI_THROW_MSG_NULL(ClassNotFoundException, str);\n+      JVMCI_THROW_MSG_NULL(NoClassDefFoundError, str);\n@@ -700,1 +718,1 @@\n-C2V_VMENTRY_NULL(jobject, resolvePossiblyCachedConstantInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint index))\n+C2V_VMENTRY_NULL(jobject, lookupConstantInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint cp_index, bool resolve))\n@@ -702,4 +720,13 @@\n-  oop obj = cp->resolve_possibly_cached_constant_at(index, CHECK_NULL);\n-  constantTag tag = cp->tag_at(index);\n-  if (tag.is_dynamic_constant() || tag.is_dynamic_constant_in_error()) {\n-    if (obj == Universe::the_null_sentinel()) {\n+  oop obj;\n+  if (!resolve) {\n+    bool found_it;\n+    obj = cp->find_cached_constant_at(cp_index, found_it, CHECK_NULL);\n+    if (!found_it) {\n+      return nullptr;\n+    }\n+  } else {\n+    obj = cp->resolve_possibly_cached_constant_at(cp_index, CHECK_NULL);\n+  }\n+  constantTag tag = cp->tag_at(cp_index);\n+  if (tag.is_dynamic_constant()) {\n+    if (obj == nullptr) {\n@@ -708,1 +735,1 @@\n-    BasicType bt = Signature::basic_type(cp->uncached_signature_ref_at(index));\n+    BasicType bt = Signature::basic_type(cp->uncached_signature_ref_at(cp_index));\n@@ -1501,0 +1528,1 @@\n+              assert(!value->is_object_merge(), \"Should not be.\");\n@@ -1580,3 +1608,3 @@\n-C2V_VMENTRY_0(int, resolveInvokeDynamicInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint index))\n-  if (!ConstantPool::is_invokedynamic_index(index)) {\n-    JVMCI_THROW_MSG_0(IllegalStateException, err_msg(\"not an invokedynamic index %d\", index));\n+C2V_VMENTRY_0(int, decodeIndyIndexToCPIndex, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint encoded_indy_index, jboolean resolve))\n+  if (!ConstantPool::is_invokedynamic_index(encoded_indy_index)) {\n+    JVMCI_THROW_MSG_0(IllegalStateException, err_msg(\"not an encoded indy index %d\", encoded_indy_index));\n@@ -1587,3 +1615,5 @@\n-  LinkResolver::resolve_invoke(callInfo, Handle(), cp, index, Bytecodes::_invokedynamic, CHECK_0);\n-  int indy_index = cp->decode_invokedynamic_index(index);\n-  cp->cache()->set_dynamic_call(callInfo, indy_index);\n+  int indy_index = cp->decode_invokedynamic_index(encoded_indy_index);\n+  if (resolve) {\n+    LinkResolver::resolve_invoke(callInfo, Handle(), cp, encoded_indy_index, Bytecodes::_invokedynamic, CHECK_0);\n+    cp->cache()->set_dynamic_call(callInfo, indy_index);\n+  }\n@@ -1593,0 +1623,8 @@\n+C2V_VMENTRY_0(int, decodeFieldIndexToCPIndex, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint field_index))\n+  constantPoolHandle cp(THREAD, UNPACK_PAIR(ConstantPool, cp));\n+  if (field_index < 0 || field_index >= cp->resolved_field_entries_length()) {\n+    JVMCI_THROW_MSG_0(IllegalStateException, err_msg(\"invalid field index %d\", field_index));\n+  }\n+  return cp->resolved_field_entry_at(field_index)->constant_pool_index();\n+C2V_END\n+\n@@ -1743,0 +1781,1 @@\n+        assert(!scopedValues->at(i2)->is_object_merge(), \"Should not be.\");\n@@ -1756,0 +1795,1 @@\n+        assert(!scopeExpressions->at(i2)->is_object_merge(), \"Should not be.\");\n@@ -3097,1 +3137,1 @@\n-  {CC \"lookupType\",                                   CC \"(\" STRING HS_KLASS2 \"Z)\" HS_RESOLVED_TYPE,                                        FN_PTR(lookupType)},\n+  {CC \"lookupType\",                                   CC \"(\" STRING HS_KLASS2 \"IZ)\" HS_RESOLVED_TYPE,                                       FN_PTR(lookupType)},\n@@ -3108,0 +3148,1 @@\n+  {CC \"lookupConstantInPool\",                         CC \"(\" HS_CONSTANT_POOL2 \"IZ)\" JAVACONSTANT,                                          FN_PTR(lookupConstantInPool)},\n@@ -3111,1 +3152,0 @@\n-  {CC \"resolvePossiblyCachedConstantInPool\",          CC \"(\" HS_CONSTANT_POOL2 \"I)\" JAVACONSTANT,                                           FN_PTR(resolvePossiblyCachedConstantInPool)},\n@@ -3114,1 +3154,2 @@\n-  {CC \"resolveInvokeDynamicInPool\",                   CC \"(\" HS_CONSTANT_POOL2 \"I)I\",                                                       FN_PTR(resolveInvokeDynamicInPool)},\n+  {CC \"decodeFieldIndexToCPIndex\",                    CC \"(\" HS_CONSTANT_POOL2 \"I)I\",                                                       FN_PTR(decodeFieldIndexToCPIndex)},\n+  {CC \"decodeIndyIndexToCPIndex\",                     CC \"(\" HS_CONSTANT_POOL2 \"IZ)I\",                                                      FN_PTR(decodeIndyIndexToCPIndex)},\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":61,"deletions":20,"binary":false,"changes":81,"status":"modified"},{"patch":"@@ -218,1 +218,1 @@\n-  nonstatic_field(JavaThread,                  _held_monitor_count,                           int64_t)                               \\\n+  nonstatic_field(JavaThread,                  _held_monitor_count,                           intx)                                  \\\n@@ -532,0 +532,1 @@\n+  declare_constant(CodeInstaller::REGISTER_VECTOR)                        \\\n@@ -535,0 +536,1 @@\n+  declare_constant(CodeInstaller::STACK_SLOT_VECTOR)                      \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -136,1 +136,1 @@\n-    assert(type >= 0 && type < T_CONFLICT, \"wrong type\");\n+    assert(type < T_CONFLICT, \"wrong type\");\n","filename":"src\/hotspot\/share\/oops\/arrayOop.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -544,1 +544,1 @@\n-  virtual Klass* array_klass(int rank, TRAPS) = 0;\n+  virtual ArrayKlass* array_klass(int rank, TRAPS) = 0;\n@@ -547,1 +547,1 @@\n-  virtual Klass* array_klass(TRAPS) = 0;\n+  virtual ArrayKlass* array_klass(TRAPS) = 0;\n@@ -550,2 +550,2 @@\n-  virtual Klass* array_klass_or_null(int rank) = 0;\n-  virtual Klass* array_klass_or_null() = 0;\n+  virtual ArrayKlass* array_klass_or_null(int rank) = 0;\n+  virtual ArrayKlass* array_klass_or_null() = 0;\n","filename":"src\/hotspot\/share\/oops\/klass.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,1 +29,2 @@\n-#include \"oops\/compressedKlass.hpp\"\n+#include \"oops\/compressedKlass.hpp\" \/\/ for narrowKlass\n+#include \"oops\/oopsHierarchy.hpp\"\n@@ -152,1 +153,1 @@\n-  static const uintptr_t no_hash_in_place         = (address_word)no_hash << hash_shift;\n+  static const uintptr_t no_hash_in_place         = (uintptr_t)no_hash << hash_shift;\n@@ -264,1 +265,1 @@\n-  uint     age()           const { return mask_bits(value() >> age_shift, age_mask); }\n+  uint     age()           const { return (uint) mask_bits(value() >> age_shift, age_mask); }\n","filename":"src\/hotspot\/share\/oops\/markWord.hpp","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -28,1 +28,0 @@\n-#include \"oops\/compressedKlass.inline.hpp\"\n","filename":"src\/hotspot\/share\/oops\/markWord.inline.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -171,3 +171,1 @@\n-  \/\/ Call to lower_dimension uses this pointer, so most be called before a\n-  \/\/ possible GC\n-  Klass* ld_klass = lower_dimension();\n+  ArrayKlass* ld_klass = lower_dimension();\n@@ -180,2 +178,1 @@\n-        ArrayKlass* ak = ArrayKlass::cast(ld_klass);\n-        oop sub_array = ak->multi_allocate(rank-1, &sizes[1], CHECK_NULL);\n+        oop sub_array = ld_klass->multi_allocate(rank - 1, &sizes[1], CHECK_NULL);\n@@ -310,1 +307,1 @@\n-Klass* ObjArrayKlass::array_klass(int n, TRAPS) {\n+ArrayKlass* ObjArrayKlass::array_klass(int n, TRAPS) {\n@@ -328,1 +325,1 @@\n-        Klass* k =\n+        ObjArrayKlass* ak =\n@@ -330,1 +327,0 @@\n-        ObjArrayKlass* ak = ObjArrayKlass::cast(k);\n@@ -339,1 +335,1 @@\n-  ObjArrayKlass *ak = ObjArrayKlass::cast(higher_dimension());\n+  ObjArrayKlass *ak = higher_dimension();\n@@ -344,1 +340,1 @@\n-Klass* ObjArrayKlass::array_klass_or_null(int n) {\n+ArrayKlass* ObjArrayKlass::array_klass_or_null(int n) {\n@@ -355,1 +351,1 @@\n-  ObjArrayKlass *ak = ObjArrayKlass::cast(higher_dimension());\n+  ObjArrayKlass *ak = higher_dimension();\n@@ -359,1 +355,1 @@\n-Klass* ObjArrayKlass::array_klass(TRAPS) {\n+ArrayKlass* ObjArrayKlass::array_klass(TRAPS) {\n@@ -363,1 +359,1 @@\n-Klass* ObjArrayKlass::array_klass_or_null() {\n+ArrayKlass* ObjArrayKlass::array_klass_or_null() {\n","filename":"src\/hotspot\/share\/oops\/objArrayKlass.cpp","additions":9,"deletions":13,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -30,1 +30,1 @@\n-#include \"oops\/accessDecorators.hpp\"\n+#include \"oops\/accessDecorators.hpp\"\n","filename":"src\/hotspot\/share\/oops\/oop.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -36,1 +36,0 @@\n-#include \"oops\/compressedOops.inline.hpp\"\n@@ -103,1 +102,1 @@\n-    assert(CompressedKlassPointers::use_compressed_class_pointers(), \"expect compressed klass pointers\");\n+    assert(UseCompressedClassPointers, \"expect compressed klass pointers\");\n@@ -112,2 +111,2 @@\n-  if (CompressedKlassPointers::use_compact_object_headers()) {\n-    assert(CompressedKlassPointers::use_compressed_class_pointers(), \"only with compressed class pointers\");\n+  if (UseCompactObjectHeaders) {\n+    assert(UseCompressedClassPointers, \"only with compressed class pointers\");\n@@ -116,1 +115,1 @@\n-  } else if (CompressedKlassPointers::use_compressed_class_pointers()) {\n+  } else if (UseCompressedClassPointers) {\n@@ -125,2 +124,2 @@\n-  if (CompressedKlassPointers::use_compact_object_headers()) {\n-    assert(CompressedKlassPointers::use_compressed_class_pointers(), \"only with compressed class pointers\");\n+  if (UseCompactObjectHeaders) {\n+    assert(UseCompressedClassPointers, \"only with compressed class pointers\");\n@@ -129,1 +128,1 @@\n-  } else if (CompressedKlassPointers::use_compressed_class_pointers()) {\n+  } else if (UseCompressedClassPointers) {\n@@ -138,2 +137,2 @@\n-  if (CompressedKlassPointers::use_compact_object_headers()) {\n-    assert(CompressedKlassPointers::use_compressed_class_pointers(), \"only with compressed class pointers\");\n+  if (UseCompactObjectHeaders) {\n+    assert(UseCompressedClassPointers, \"only with compressed class pointers\");\n@@ -145,1 +144,1 @@\n-  } else if (CompressedKlassPointers::use_compressed_class_pointers()) {\n+  } else if (UseCompressedClassPointers) {\n@@ -160,1 +159,1 @@\n-  if (CompressedKlassPointers::use_compressed_class_pointers()) {\n+  if (UseCompressedClassPointers) {\n@@ -171,1 +170,1 @@\n-  if (CompressedKlassPointers::use_compressed_class_pointers()) {\n+  if (UseCompressedClassPointers) {\n@@ -181,1 +180,1 @@\n-  if (CompressedKlassPointers::use_compressed_class_pointers()) {\n+  if (UseCompressedClassPointers) {\n@@ -387,3 +386,4 @@\n-  assert(!mark().is_marked(), \"Attempt to read age from forwarded mark\");\n-  if (has_displaced_mark()) {\n-    return displaced_mark().age();\n+  markWord m = mark();\n+  assert(!m.is_marked(), \"Attempt to read age from forwarded mark\");\n+  if (m.has_displaced_mark_helper()) {\n+    return m.displaced_mark_helper().age();\n@@ -391,1 +391,1 @@\n-    return mark().age();\n+    return m.age();\n@@ -396,3 +396,4 @@\n-  assert(!mark().is_marked(), \"Attempt to increment age of forwarded mark\");\n-  if (has_displaced_mark()) {\n-    set_displaced_mark(displaced_mark().incr_age());\n+  markWord m = mark();\n+  assert(!m.is_marked(), \"Attempt to increment age of forwarded mark\");\n+  if (m.has_displaced_mark_helper()) {\n+    m.set_displaced_mark_helper(m.displaced_mark_helper().incr_age());\n@@ -400,1 +401,1 @@\n-    set_mark(mark().incr_age());\n+    set_mark(m.incr_age());\n","filename":"src\/hotspot\/share\/oops\/oop.inline.hpp","additions":23,"deletions":22,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -174,1 +174,1 @@\n-Klass* TypeArrayKlass::array_klass(int n, TRAPS) {\n+ArrayKlass* TypeArrayKlass::array_klass(int n, TRAPS) {\n@@ -201,1 +201,1 @@\n-  ObjArrayKlass* h_ak = ObjArrayKlass::cast(higher_dimension());\n+  ObjArrayKlass* h_ak = higher_dimension();\n@@ -207,1 +207,1 @@\n-Klass* TypeArrayKlass::array_klass_or_null(int n) {\n+ArrayKlass* TypeArrayKlass::array_klass_or_null(int n) {\n@@ -218,1 +218,1 @@\n-  ObjArrayKlass* h_ak = ObjArrayKlass::cast(higher_dimension());\n+  ObjArrayKlass* h_ak = higher_dimension();\n@@ -222,1 +222,1 @@\n-Klass* TypeArrayKlass::array_klass(TRAPS) {\n+ArrayKlass* TypeArrayKlass::array_klass(TRAPS) {\n@@ -226,1 +226,1 @@\n-Klass* TypeArrayKlass::array_klass_or_null() {\n+ArrayKlass* TypeArrayKlass::array_klass_or_null() {\n@@ -278,2 +278,0 @@\n-#ifndef PRODUCT\n-\n@@ -343,1 +341,4 @@\n-  typeArrayOop ta = typeArrayOop(obj);\n+  oop_print_elements_on(typeArrayOop(obj), st);\n+}\n+\n+void TypeArrayKlass::oop_print_elements_on(typeArrayOop ta, outputStream* st) {\n@@ -362,2 +363,0 @@\n-#endif \/\/ PRODUCT\n-\n","filename":"src\/hotspot\/share\/oops\/typeArrayKlass.cpp","additions":10,"deletions":11,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -1106,0 +1106,6 @@\n+\/\/----------------------------is_uncommon_trap----------------------------\n+\/\/ Returns true if this is an uncommon trap.\n+bool CallStaticJavaNode::is_uncommon_trap() const {\n+  return (_name != nullptr && !strcmp(_name, \"uncommon_trap\"));\n+}\n+\n@@ -1109,4 +1115,1 @@\n-  if (_name != nullptr && !strcmp(_name, \"uncommon_trap\")) {\n-    return extract_uncommon_trap_request(this);\n-  }\n-  return 0;\n+  return is_uncommon_trap() ? extract_uncommon_trap_request(this) : 0;\n@@ -1463,6 +1466,1 @@\n-SafePointScalarObjectNode::SafePointScalarObjectNode(const TypeOopPtr* tp,\n-#ifdef ASSERT\n-                                                     Node* alloc,\n-#endif\n-                                                     uint first_index,\n-                                                     uint n_fields) :\n+SafePointScalarObjectNode::SafePointScalarObjectNode(const TypeOopPtr* tp, Node* alloc, uint first_index, uint n_fields) :\n@@ -1471,4 +1469,2 @@\n-  _n_fields(n_fields)\n-#ifdef ASSERT\n-  , _alloc(alloc)\n-#endif\n+  _n_fields(n_fields),\n+  _alloc(alloc)\n@@ -1477,2 +1473,1 @@\n-  if (!alloc->is_Allocate()\n-      && !(alloc->Opcode() == Op_VectorBox)) {\n+  if (!alloc->is_Allocate() && !(alloc->Opcode() == Op_VectorBox)) {\n@@ -1524,2 +1519,21 @@\n-  st->print(\" # fields@[%d..%d]\", first_index(),\n-             first_index() + n_fields() - 1);\n+  st->print(\" # fields@[%d..%d]\", first_index(), first_index() + n_fields() - 1);\n+}\n+#endif\n+\n+\/\/==============  SafePointScalarMergeNode  ==============\n+\n+SafePointScalarMergeNode::SafePointScalarMergeNode(const TypeOopPtr* tp, int merge_pointer_idx) :\n+  TypeNode(tp, 1), \/\/ 1 control input -- seems required.  Get from root.\n+  _merge_pointer_idx(merge_pointer_idx)\n+{\n+  init_class_id(Class_SafePointScalarMerge);\n+}\n+\n+\/\/ Do not allow value-numbering for SafePointScalarMerge node.\n+uint SafePointScalarMergeNode::hash() const { return NO_HASH; }\n+bool SafePointScalarMergeNode::cmp( const Node &n ) const {\n+  return (&n == this); \/\/ Always fail except on self\n+}\n+\n+uint SafePointScalarMergeNode::ideal_reg() const {\n+  return 0; \/\/ No matching to machine instruction\n@@ -1528,0 +1542,29 @@\n+const RegMask &SafePointScalarMergeNode::in_RegMask(uint idx) const {\n+  return *(Compile::current()->matcher()->idealreg2debugmask[in(idx)->ideal_reg()]);\n+}\n+\n+const RegMask &SafePointScalarMergeNode::out_RegMask() const {\n+  return RegMask::Empty;\n+}\n+\n+uint SafePointScalarMergeNode::match_edge(uint idx) const {\n+  return 0;\n+}\n+\n+SafePointScalarMergeNode*\n+SafePointScalarMergeNode::clone(Dict* sosn_map, bool& new_node) const {\n+  void* cached = (*sosn_map)[(void*)this];\n+  if (cached != nullptr) {\n+    new_node = false;\n+    return (SafePointScalarMergeNode*)cached;\n+  }\n+  new_node = true;\n+  SafePointScalarMergeNode* res = (SafePointScalarMergeNode*)Node::clone();\n+  sosn_map->Insert((void*)this, (void*)res);\n+  return res;\n+}\n+\n+#ifndef PRODUCT\n+void SafePointScalarMergeNode::dump_spec(outputStream *st) const {\n+  st->print(\" # merge_pointer_idx=%d, scalarized_objects=%d\", _merge_pointer_idx, req()-1);\n+}\n","filename":"src\/hotspot\/share\/opto\/callnode.cpp","additions":61,"deletions":18,"binary":false,"changes":79,"status":"modified"},{"patch":"@@ -522,0 +522,6 @@\n+  if (do_reduce_allocation_merges() != ReduceAllocationMerges && PrintOpto) {\n+    \/\/ Recompiling without reducing allocation merges\n+    tty->print_cr(\"*********************************************************\");\n+    tty->print_cr(\"** Bailout: Recompile without reduce allocation merges **\");\n+    tty->print_cr(\"*********************************************************\");\n+  }\n@@ -554,1 +560,0 @@\n-  ttyLocker ttyl;\n@@ -559,6 +564,6 @@\n-  if (xtty != nullptr) {\n-    xtty->head(\"ideal compile_id='%d'%s compile_phase='%s'\",\n-               compile_id(),\n-               is_osr_compilation() ? \" compile_kind='osr'\" : \"\",\n-               phase_name);\n-  }\n+\n+  \/\/ Node dumping can cause a safepoint, which can break the tty lock.\n+  \/\/ Buffer all node dumps, so that all safepoints happen before we lock.\n+  ResourceMark rm;\n+  stringStream ss;\n+\n@@ -566,1 +571,1 @@\n-    tty->print_cr(\"AFTER: %s\", phase_name);\n+    ss.print_cr(\"AFTER: %s\", phase_name);\n@@ -568,1 +573,1 @@\n-    root()->dump_bfs(MaxNodeLimit, nullptr, \"+S$\");\n+    root()->dump_bfs(MaxNodeLimit, nullptr, \"+S$\", &ss);\n@@ -571,1 +576,1 @@\n-    _output->print_scheduling();\n+    _output->print_scheduling(&ss);\n@@ -574,0 +579,3 @@\n+  \/\/ Check that the lock is not broken by a safepoint.\n+  NoSafepointVerifier nsv;\n+  ttyLocker ttyl;\n@@ -575,0 +583,5 @@\n+    xtty->head(\"ideal compile_id='%d'%s compile_phase='%s'\",\n+               compile_id(),\n+               is_osr_compilation() ? \" compile_kind='osr'\" : \"\",\n+               phase_name);\n+    xtty->print(\"%s\", ss.as_string()); \/\/ print to tty would use xml escape encoding\n@@ -576,0 +589,2 @@\n+  } else {\n+    tty->print(\"%s\", ss.as_string());\n@@ -627,1 +642,1 @@\n-                  _dead_node_list(comp_arena()),\n+                  _unique(0),\n@@ -629,2 +644,4 @@\n-                  _node_arena(mtCompiler),\n-                  _old_arena(mtCompiler),\n+                  _dead_node_list(comp_arena()),\n+                  _node_arena_one(mtCompiler),\n+                  _node_arena_two(mtCompiler),\n+                  _node_arena(&_node_arena_one),\n@@ -659,7 +676,0 @@\n-  if (CITimeVerbose) {\n-    tty->print(\" \");\n-    target->holder()->name()->print();\n-    tty->print(\".\");\n-    target->print_short_name();\n-    tty->print(\"  \");\n-  }\n@@ -770,6 +780,4 @@\n-      if (!failure_reason_is(C2Compiler::retry_class_loading_during_parsing())) {\n-        assert(failure_reason() != nullptr, \"expect reason for parse failure\");\n-        stringStream ss;\n-        ss.print(\"method parse failed: %s\", failure_reason());\n-        record_method_not_compilable(ss.as_string());\n-      }\n+      assert(failure_reason() != nullptr, \"expect reason for parse failure\");\n+      stringStream ss;\n+      ss.print(\"method parse failed: %s\", failure_reason());\n+      record_method_not_compilable(ss.as_string());\n@@ -830,1 +838,1 @@\n-    if (FLAG_IS_DEFAULT(StressSeed) || (FLAG_IS_ERGO(StressSeed) && RepeatCompilation)) {\n+    if (FLAG_IS_DEFAULT(StressSeed) || (FLAG_IS_ERGO(StressSeed) && directive->RepeatCompilationOption)) {\n@@ -916,1 +924,1 @@\n-    _dead_node_list(comp_arena()),\n+    _unique(0),\n@@ -918,2 +926,4 @@\n-    _node_arena(mtCompiler),\n-    _old_arena(mtCompiler),\n+    _dead_node_list(comp_arena()),\n+    _node_arena_one(mtCompiler),\n+    _node_arena_two(mtCompiler),\n+    _node_arena(&_node_arena_one),\n@@ -1934,1 +1944,1 @@\n-          if (Verbose) {\n+          if (PrintOpto && Verbose) {\n@@ -2297,1 +2307,0 @@\n-      if (major_progress()) print_method(PHASE_PHASEIDEAL_BEFORE_EA, 2);\n@@ -2301,0 +2310,1 @@\n+    print_method(PHASE_PHASEIDEAL_BEFORE_EA, 2);\n@@ -2318,1 +2328,1 @@\n-        igvn.set_delay_transform(false);\n+        if (failing()) return;\n@@ -2320,0 +2330,1 @@\n+        igvn.set_delay_transform(false);\n@@ -2322,2 +2333,4 @@\n-\n-        if (failing())  return;\n+\n+      ConnectionGraph::verify_ram_nodes(this, root());\n+      if (failing())  return;\n+\n@@ -2978,0 +2991,1 @@\n+    print_method(PHASE_FINAL_CODE, 1); \/\/ Compile::_output is not null here\n@@ -2980,2 +2994,0 @@\n-  print_method(PHASE_FINAL_CODE, 1);\n-\n@@ -4328,1 +4340,4 @@\n-    _phase_name(name), _dolog(CITimeVerbose)\n+    _compile(Compile::current()),\n+    _log(nullptr),\n+    _phase_name(name),\n+    _dolog(CITimeVerbose)\n@@ -4330,0 +4345,1 @@\n+  assert(_compile != nullptr, \"sanity check\");\n@@ -4331,5 +4347,1 @@\n-    C = Compile::current();\n-    _log = C->log();\n-  } else {\n-    C = nullptr;\n-    _log = nullptr;\n+    _log = _compile->log();\n@@ -4338,1 +4350,1 @@\n-    _log->begin_head(\"phase name='%s' nodes='%d' live='%d'\", _phase_name, C->unique(), C->live_nodes());\n+    _log->begin_head(\"phase name='%s' nodes='%d' live='%d'\", _phase_name, _compile->unique(), _compile->live_nodes());\n@@ -4345,8 +4357,0 @@\n-\n-  C = Compile::current();\n-  if (_dolog) {\n-    _log = C->log();\n-  } else {\n-    _log = nullptr;\n-  }\n-\n@@ -4356,1 +4360,1 @@\n-                  _phase_name, C->unique(), C->live_nodes(), C->count_live_nodes_by_graph_walk());\n+                  _phase_name, _compile->unique(), _compile->live_nodes(), _compile->count_live_nodes_by_graph_walk());\n@@ -4360,1 +4364,1 @@\n-    Compile::current()->print_missing_nodes();\n+    _compile->print_missing_nodes();\n@@ -4365,1 +4369,1 @@\n-    _log->done(\"phase name='%s' nodes='%d' live='%d'\", _phase_name, C->unique(), C->live_nodes());\n+    _log->done(\"phase name='%s' nodes='%d' live='%d'\", _phase_name, _compile->unique(), _compile->live_nodes());\n@@ -4896,1 +4900,10 @@\n-          assert(in_hash, \"node should be in igvn hash table\");\n+#ifdef ASSERT\n+          if (!in_hash) {\n+            tty->print_cr(\"current graph:\");\n+            n->dump_bfs(MaxNodeLimit, nullptr, \"S$\");\n+            tty->cr();\n+            tty->print_cr(\"erroneous node:\");\n+            n->dump();\n+            assert(false, \"node should be in igvn hash table\");\n+          }\n+#endif\n@@ -4986,2 +4999,2 @@\n-void NodeCloneInfo::dump() const {\n-  tty->print(\" {%d:%d} \", idx(), gen());\n+void NodeCloneInfo::dump_on(outputStream* st) const {\n+  st->print(\" {%d:%d} \", idx(), gen());\n@@ -5034,1 +5047,1 @@\n-void CloneMap::dump(node_idx_t key) const {\n+void CloneMap::dump(node_idx_t key, outputStream* st) const {\n@@ -5038,1 +5051,1 @@\n-    ni.dump();\n+    ni.dump_on(st);\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":73,"deletions":60,"binary":false,"changes":133,"status":"modified"},{"patch":"@@ -53,0 +53,1 @@\n+#include \"prims\/jvmtiExport.hpp\"\n@@ -497,0 +498,1 @@\n+  case vmIntrinsics::_jvm_commit:               return inline_native_jvm_commit();\n@@ -701,0 +703,2 @@\n+  case vmIntrinsics::_VectorShuffleIota:\n+    return inline_vector_shuffle_iota();\n@@ -703,0 +707,2 @@\n+  case vmIntrinsics::_VectorShuffleToVector:\n+    return inline_vector_shuffle_to_vector();\n@@ -2808,0 +2814,7 @@\n+\n+#if INCLUDE_JVMTI\n+  if (too_many_traps(Deoptimization::Reason_intrinsic)) {\n+    return false;\n+  }\n+#endif \/\/INCLUDE_JVMTI\n+\n@@ -2818,0 +2831,18 @@\n+#if INCLUDE_JVMTI\n+    \/\/ Don't try to access new allocated obj in the intrinsic.\n+    \/\/ It causes perfomance issues even when jvmti event VmObjectAlloc is disabled.\n+    \/\/ Deoptimize and allocate in interpreter instead.\n+    Node* addr = makecon(TypeRawPtr::make((address) &JvmtiExport::_should_notify_object_alloc));\n+    Node* should_post_vm_object_alloc = make_load(this->control(), addr, TypeInt::INT, T_INT, MemNode::unordered);\n+    Node* chk = _gvn.transform(new CmpINode(should_post_vm_object_alloc, intcon(0)));\n+    Node* tst = _gvn.transform(new BoolNode(chk, BoolTest::eq));\n+    {\n+      BuildCutout unless(this, tst, PROB_MAX);\n+      uncommon_trap(Deoptimization::Reason_intrinsic,\n+                    Deoptimization::Action_make_not_entrant);\n+    }\n+    if (stopped()) {\n+      return true;\n+    }\n+#endif \/\/INCLUDE_JVMTI\n+\n@@ -3002,0 +3033,130 @@\n+\/\/------------------------inline_native_jvm_commit------------------\n+bool LibraryCallKit::inline_native_jvm_commit() {\n+  enum { _true_path = 1, _false_path = 2, PATH_LIMIT };\n+\n+  \/\/ Save input memory and i_o state.\n+  Node* input_memory_state = reset_memory();\n+  set_all_memory(input_memory_state);\n+  Node* input_io_state = i_o();\n+\n+  \/\/ TLS.\n+  Node* tls_ptr = _gvn.transform(new ThreadLocalNode());\n+  \/\/ Jfr java buffer.\n+  Node* java_buffer_offset = _gvn.transform(new AddPNode(top(), tls_ptr, _gvn.transform(MakeConX(in_bytes(JAVA_BUFFER_OFFSET_JFR)))));\n+  Node* java_buffer = _gvn.transform(new LoadPNode(control(), input_memory_state, java_buffer_offset, TypePtr::BOTTOM, TypeRawPtr::NOTNULL, MemNode::unordered));\n+  Node* java_buffer_pos_offset = _gvn.transform(new AddPNode(top(), java_buffer, _gvn.transform(MakeConX(in_bytes(JFR_BUFFER_POS_OFFSET)))));\n+\n+  \/\/ Load the current value of the notified field in the JfrThreadLocal.\n+  Node* notified_offset = basic_plus_adr(top(), tls_ptr, in_bytes(NOTIFY_OFFSET_JFR));\n+  Node* notified = make_load(control(), notified_offset, TypeInt::BOOL, T_BOOLEAN, MemNode::unordered);\n+\n+  \/\/ Test for notification.\n+  Node* notified_cmp = _gvn.transform(new CmpINode(notified, _gvn.intcon(1)));\n+  Node* test_notified = _gvn.transform(new BoolNode(notified_cmp, BoolTest::eq));\n+  IfNode* iff_notified = create_and_map_if(control(), test_notified, PROB_MIN, COUNT_UNKNOWN);\n+\n+  \/\/ True branch, is notified.\n+  Node* is_notified = _gvn.transform(new IfTrueNode(iff_notified));\n+  set_control(is_notified);\n+\n+  \/\/ Reset notified state.\n+  Node* notified_reset_memory = store_to_memory(control(), notified_offset, _gvn.intcon(0), T_BOOLEAN, Compile::AliasIdxRaw, MemNode::unordered);\n+\n+  \/\/ Iff notified, the return address of the commit method is the current position of the backing java buffer. This is used to reset the event writer.\n+  Node* current_pos_X = _gvn.transform(new LoadXNode(control(), input_memory_state, java_buffer_pos_offset, TypeRawPtr::NOTNULL, TypeX_X, MemNode::unordered));\n+  \/\/ Convert the machine-word to a long.\n+  Node* current_pos = _gvn.transform(ConvX2L(current_pos_X));\n+\n+  \/\/ False branch, not notified.\n+  Node* not_notified = _gvn.transform(new IfFalseNode(iff_notified));\n+  set_control(not_notified);\n+  set_all_memory(input_memory_state);\n+\n+  \/\/ Arg is the next position as a long.\n+  Node* arg = argument(0);\n+  \/\/ Convert long to machine-word.\n+  Node* next_pos_X = _gvn.transform(ConvL2X(arg));\n+\n+  \/\/ Store the next_position to the underlying jfr java buffer.\n+  Node* commit_memory;\n+#ifdef _LP64\n+  commit_memory = store_to_memory(control(), java_buffer_pos_offset, next_pos_X, T_LONG, Compile::AliasIdxRaw, MemNode::release);\n+#else\n+  commit_memory = store_to_memory(control(), java_buffer_pos_offset, next_pos_X, T_INT, Compile::AliasIdxRaw, MemNode::release);\n+#endif\n+\n+  \/\/ Now load the flags from off the java buffer and decide if the buffer is a lease. If so, it needs to be returned post-commit.\n+  Node* java_buffer_flags_offset = _gvn.transform(new AddPNode(top(), java_buffer, _gvn.transform(MakeConX(in_bytes(JFR_BUFFER_FLAGS_OFFSET)))));\n+  Node* flags = make_load(control(), java_buffer_flags_offset, TypeInt::UBYTE, T_BYTE, MemNode::unordered);\n+  Node* lease_constant = _gvn.transform(_gvn.intcon(4));\n+\n+  \/\/ And flags with lease constant.\n+  Node* lease = _gvn.transform(new AndINode(flags, lease_constant));\n+\n+  \/\/ Branch on lease to conditionalize returning the leased java buffer.\n+  Node* lease_cmp = _gvn.transform(new CmpINode(lease, lease_constant));\n+  Node* test_lease = _gvn.transform(new BoolNode(lease_cmp, BoolTest::eq));\n+  IfNode* iff_lease = create_and_map_if(control(), test_lease, PROB_MIN, COUNT_UNKNOWN);\n+\n+  \/\/ False branch, not a lease.\n+  Node* not_lease = _gvn.transform(new IfFalseNode(iff_lease));\n+\n+  \/\/ True branch, is lease.\n+  Node* is_lease = _gvn.transform(new IfTrueNode(iff_lease));\n+  set_control(is_lease);\n+\n+  \/\/ Make a runtime call, which can safepoint, to return the leased buffer. This updates both the JfrThreadLocal and the Java event writer oop.\n+  Node* call_return_lease = make_runtime_call(RC_NO_LEAF,\n+                                              OptoRuntime::void_void_Type(),\n+                                              StubRoutines::jfr_return_lease(),\n+                                              \"return_lease\", TypePtr::BOTTOM);\n+  Node* call_return_lease_control = _gvn.transform(new ProjNode(call_return_lease, TypeFunc::Control));\n+\n+  RegionNode* lease_compare_rgn = new RegionNode(PATH_LIMIT);\n+  record_for_igvn(lease_compare_rgn);\n+  PhiNode* lease_compare_mem = new PhiNode(lease_compare_rgn, Type::MEMORY, TypePtr::BOTTOM);\n+  record_for_igvn(lease_compare_mem);\n+  PhiNode* lease_compare_io = new PhiNode(lease_compare_rgn, Type::ABIO);\n+  record_for_igvn(lease_compare_io);\n+  PhiNode* lease_result_value = new PhiNode(lease_compare_rgn, TypeLong::LONG);\n+  record_for_igvn(lease_result_value);\n+\n+  \/\/ Update control and phi nodes.\n+  lease_compare_rgn->init_req(_true_path, call_return_lease_control);\n+  lease_compare_rgn->init_req(_false_path, not_lease);\n+\n+  lease_compare_mem->init_req(_true_path, _gvn.transform(reset_memory()));\n+  lease_compare_mem->init_req(_false_path, commit_memory);\n+\n+  lease_compare_io->init_req(_true_path, i_o());\n+  lease_compare_io->init_req(_false_path, input_io_state);\n+\n+  lease_result_value->init_req(_true_path, null()); \/\/ if the lease was returned, return 0.\n+  lease_result_value->init_req(_false_path, arg); \/\/ if not lease, return new updated position.\n+\n+  RegionNode* result_rgn = new RegionNode(PATH_LIMIT);\n+  PhiNode* result_mem = new PhiNode(result_rgn, Type::MEMORY, TypePtr::BOTTOM);\n+  PhiNode* result_io = new PhiNode(result_rgn, Type::ABIO);\n+  PhiNode* result_value = new PhiNode(result_rgn, TypeLong::LONG);\n+\n+  \/\/ Update control and phi nodes.\n+  result_rgn->init_req(_true_path, is_notified);\n+  result_rgn->init_req(_false_path, _gvn.transform(lease_compare_rgn));\n+\n+  result_mem->init_req(_true_path, notified_reset_memory);\n+  result_mem->init_req(_false_path, _gvn.transform(lease_compare_mem));\n+\n+  result_io->init_req(_true_path, input_io_state);\n+  result_io->init_req(_false_path, _gvn.transform(lease_compare_io));\n+\n+  result_value->init_req(_true_path, current_pos);\n+  result_value->init_req(_false_path, _gvn.transform(lease_result_value));\n+\n+  \/\/ Set output state.\n+  set_control(_gvn.transform(result_rgn));\n+  set_all_memory(_gvn.transform(result_mem));\n+  set_i_o(_gvn.transform(result_io));\n+  set_result(result_rgn, result_value);\n+  return true;\n+}\n+\n@@ -3454,3 +3615,4 @@\n-Node* LibraryCallKit::scopedValueCache_helper() {\n-  ciKlass *objects_klass = ciObjArrayKlass::make(env()->Object_klass());\n-  const TypeOopPtr *etype = TypeOopPtr::make_from_klass(env()->Object_klass());\n+const Type* LibraryCallKit::scopedValueCache_type() {\n+  ciKlass* objects_klass = ciObjArrayKlass::make(env()->Object_klass());\n+  const TypeOopPtr* etype = TypeOopPtr::make_from_klass(env()->Object_klass());\n+  const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS);\n@@ -3458,0 +3620,2 @@\n+  \/\/ Because we create the scopedValue cache lazily we have to make the\n+  \/\/ type of the result BotPTR.\n@@ -3459,0 +3623,3 @@\n+  const Type* objects_type = TypeAryPtr::make(TypePtr::BotPTR, arr0, objects_klass, xk, 0);\n+  return objects_type;\n+}\n@@ -3460,0 +3627,1 @@\n+Node* LibraryCallKit::scopedValueCache_helper() {\n@@ -3472,8 +3640,1 @@\n-  ciKlass *objects_klass = ciObjArrayKlass::make(env()->Object_klass());\n-  const TypeOopPtr *etype = TypeOopPtr::make_from_klass(env()->Object_klass());\n-  const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS);\n-\n-  \/\/ Because we create the scopedValue cache lazily we have to make the\n-  \/\/ type of the result BotPTR.\n-  bool xk = etype->klass_is_exact();\n-  const Type* objects_type = TypeAryPtr::make(TypePtr::BotPTR, arr0, objects_klass, xk, 0);\n+  const Type* objects_type = scopedValueCache_type();\n@@ -3490,0 +3651,1 @@\n+  const Type* objects_type = scopedValueCache_type();\n@@ -3492,1 +3654,1 @@\n-  access_store_at(nullptr, cache_obj_handle, adr_type, arr, _gvn.type(arr), T_OBJECT, IN_NATIVE | MO_UNORDERED);\n+  access_store_at(nullptr, cache_obj_handle, adr_type, arr, objects_type, T_OBJECT, IN_NATIVE | MO_UNORDERED);\n@@ -3981,0 +4143,1 @@\n+    null_check_receiver();\n@@ -4015,1 +4178,1 @@\n-      slow_call = generate_method_call(vmIntrinsics::_allocateUninitializedArray, false, false);\n+      slow_call = generate_method_call(vmIntrinsics::_allocateUninitializedArray, false, false, true);\n@@ -4017,1 +4180,1 @@\n-      slow_call = generate_method_call_static(vmIntrinsics::_newArray);\n+      slow_call = generate_method_call_static(vmIntrinsics::_newArray, true);\n@@ -4040,1 +4203,1 @@\n-      AllocateArrayNode* alloc = AllocateArrayNode::Ideal_array_allocation(obj, &_gvn);\n+      AllocateArrayNode* alloc = AllocateArrayNode::Ideal_array_allocation(obj);\n@@ -4262,1 +4425,1 @@\n-LibraryCallKit::generate_method_call(vmIntrinsics::ID method_id, bool is_virtual, bool is_static) {\n+LibraryCallKit::generate_method_call(vmIntrinsicID method_id, bool is_virtual, bool is_static, bool res_not_null) {\n@@ -4271,0 +4434,8 @@\n+  if (res_not_null) {\n+    assert(tf->return_type() == T_OBJECT, \"\");\n+    const TypeTuple* range = tf->range();\n+    const Type** fields = TypeTuple::fields(range->cnt());\n+    fields[TypeFunc::Parms] = range->field_at(TypeFunc::Parms)->filter_speculative(TypePtr::NOTNULL);\n+    const TypeTuple* new_range = TypeTuple::make(range->cnt(), fields);\n+    tf = TypeFunc::make(tf->domain(), new_range);\n+  }\n@@ -4277,1 +4448,1 @@\n-    null_check_receiver();\n+    assert(!gvn().type(argument(0))->maybe_null(), \"should not be null\");\n@@ -4293,1 +4464,1 @@\n-    null_check_receiver();\n+    assert(!gvn().type(argument(0))->maybe_null(), \"should not be null\");\n@@ -4420,1 +4591,1 @@\n-    CallJavaNode* slow_call = generate_method_call(hashCode_id, is_virtual, is_static);\n+    CallJavaNode* slow_call = generate_method_call(hashCode_id, is_virtual, is_static, false);\n@@ -4754,1 +4925,1 @@\n-    alloc = AllocateNode::Ideal_allocation(alloc_obj, &_gvn);\n+    alloc = AllocateNode::Ideal_allocation(alloc_obj);\n@@ -4946,1 +5117,1 @@\n-      CallJavaNode* slow_call = generate_method_call(vmIntrinsics::_clone, is_virtual);\n+      CallJavaNode* slow_call = generate_method_call(vmIntrinsics::_clone, is_virtual, false, true);\n@@ -5463,1 +5634,1 @@\n-  AllocateArrayNode* alloc = AllocateArrayNode::Ideal_array_allocation(ptr, &_gvn);\n+  AllocateArrayNode* alloc = AllocateArrayNode::Ideal_array_allocation(ptr);\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":193,"deletions":22,"binary":false,"changes":215,"status":"modified"},{"patch":"@@ -288,1 +288,1 @@\n-    Node* adr = _igvn.transform(new AddPNode(base, base, MakeConX(offset)));\n+    Node* adr = _igvn.transform(new AddPNode(base, base, _igvn.MakeConX(offset)));\n@@ -307,1 +307,1 @@\n-        adr = _igvn.transform(new AddPNode(base, base, MakeConX(off)));\n+        adr = _igvn.transform(new AddPNode(base, base, _igvn.MakeConX(off)));\n@@ -318,1 +318,1 @@\n-        diff = _igvn.transform(new LShiftXNode(diff, intcon(shift)));\n+        diff = _igvn.transform(new LShiftXNode(diff, _igvn.intcon(shift)));\n@@ -320,1 +320,1 @@\n-        Node* off = _igvn.transform(new AddXNode(MakeConX(offset), diff));\n+        Node* off = _igvn.transform(new AddXNode(_igvn.MakeConX(offset), diff));\n@@ -553,1 +553,1 @@\n-bool PhaseMacroExpand::can_eliminate_allocation(AllocateNode *alloc, GrowableArray <SafePointNode *>& safepoints) {\n+bool PhaseMacroExpand::can_eliminate_allocation(PhaseIterGVN* igvn, AllocateNode *alloc, GrowableArray <SafePointNode *>* safepoints) {\n@@ -558,1 +558,2 @@\n-  bool  can_eliminate = true;\n+  bool can_eliminate = true;\n+  bool reduce_merge_precheck = (safepoints == nullptr);\n@@ -568,1 +569,1 @@\n-    res_type = _igvn.type(res)->isa_oopptr();\n+    res_type = igvn->type(res)->isa_oopptr();\n@@ -588,1 +589,1 @@\n-        const TypePtr* addp_type = _igvn.type(use)->is_ptr();\n+        const TypePtr* addp_type = igvn->type(use)->is_ptr();\n@@ -629,2 +630,2 @@\n-        } else {\n-          safepoints.append_if_missing(sfpt);\n+        } else if (!reduce_merge_precheck) {\n+          safepoints->append_if_missing(sfpt);\n@@ -632,0 +633,2 @@\n+      } else if (reduce_merge_precheck && (use->is_Phi() || use->is_EncodeP() || use->Opcode() == Op_MemBarRelease)) {\n+        \/\/ Nothing to do\n@@ -643,1 +646,1 @@\n-          }else {\n+          } else {\n@@ -654,1 +657,1 @@\n-  if (PrintEliminateAllocations) {\n+  if (PrintEliminateAllocations && safepoints != nullptr) {\n@@ -679,11 +682,1 @@\n-\/\/ Do scalar replacement.\n-bool PhaseMacroExpand::scalar_replacement(AllocateNode *alloc, GrowableArray <SafePointNode *>& safepoints) {\n-  GrowableArray <SafePointNode *> safepoints_done;\n-\n-  ciInstanceKlass* iklass = nullptr;\n-  int nfields = 0;\n-  int array_base = 0;\n-  int element_size = 0;\n-  BasicType basic_elem_type = T_ILLEGAL;\n-  const Type* field_type = nullptr;\n-\n+void PhaseMacroExpand::undo_previous_scalarizations(GrowableArray <SafePointNode *> safepoints_done, AllocateNode* alloc) {\n@@ -691,0 +684,1 @@\n+  int nfields = 0;\n@@ -692,0 +686,50 @@\n+\n+  if (res != nullptr) {\n+    const TypeOopPtr* res_type = _igvn.type(res)->isa_oopptr();\n+\n+    if (res_type->isa_instptr()) {\n+      \/\/ find the fields of the class which will be needed for safepoint debug information\n+      ciInstanceKlass* iklass = res_type->is_instptr()->instance_klass();\n+      nfields = iklass->nof_nonstatic_fields();\n+    } else {\n+      \/\/ find the array's elements which will be needed for safepoint debug information\n+      nfields = alloc->in(AllocateNode::ALength)->find_int_con(-1);\n+      assert(nfields >= 0, \"must be an array klass.\");\n+    }\n+  }\n+\n+  \/\/ rollback processed safepoints\n+  while (safepoints_done.length() > 0) {\n+    SafePointNode* sfpt_done = safepoints_done.pop();\n+    \/\/ remove any extra entries we added to the safepoint\n+    uint last = sfpt_done->req() - 1;\n+    for (int k = 0;  k < nfields; k++) {\n+      sfpt_done->del_req(last--);\n+    }\n+    JVMState *jvms = sfpt_done->jvms();\n+    jvms->set_endoff(sfpt_done->req());\n+    \/\/ Now make a pass over the debug information replacing any references\n+    \/\/ to SafePointScalarObjectNode with the allocated object.\n+    int start = jvms->debug_start();\n+    int end   = jvms->debug_end();\n+    for (int i = start; i < end; i++) {\n+      if (sfpt_done->in(i)->is_SafePointScalarObject()) {\n+        SafePointScalarObjectNode* scobj = sfpt_done->in(i)->as_SafePointScalarObject();\n+        if (scobj->first_index(jvms) == sfpt_done->req() &&\n+            scobj->n_fields() == (uint)nfields) {\n+          assert(scobj->alloc() == alloc, \"sanity\");\n+          sfpt_done->set_req(i, res);\n+        }\n+      }\n+    }\n+    _igvn._worklist.push(sfpt_done);\n+  }\n+}\n+\n+SafePointScalarObjectNode* PhaseMacroExpand::create_scalarized_object_description(AllocateNode *alloc, SafePointNode* sfpt) {\n+  \/\/ Fields of scalar objs are referenced only at the end\n+  \/\/ of regular debuginfo at the last (youngest) JVMS.\n+  \/\/ Record relative start index.\n+  ciInstanceKlass* iklass    = nullptr;\n+  BasicType basic_elem_type  = T_ILLEGAL;\n+  const Type* field_type     = nullptr;\n@@ -693,0 +737,9 @@\n+  int nfields                = 0;\n+  int array_base             = 0;\n+  int element_size           = 0;\n+  uint first_ind             = (sfpt->req() - sfpt->jvms()->scloff());\n+  Node* res                  = alloc->result_cast();\n+\n+  assert(res == nullptr || res->is_CheckCastPP(), \"unexpected AllocateNode result\");\n+  assert(sfpt->jvms() != nullptr, \"missed JVMS\");\n+\n@@ -695,2 +748,0 @@\n-  }\n-  if (res != nullptr) {\n@@ -712,47 +763,25 @@\n-  \/\/\n-  \/\/ Process the safepoint uses\n-  \/\/\n-  while (safepoints.length() > 0) {\n-    SafePointNode* sfpt = safepoints.pop();\n-    Node* mem = sfpt->memory();\n-    Node* ctl = sfpt->control();\n-    assert(sfpt->jvms() != nullptr, \"missed JVMS\");\n-    \/\/ Fields of scalar objs are referenced only at the end\n-    \/\/ of regular debuginfo at the last (youngest) JVMS.\n-    \/\/ Record relative start index.\n-    uint first_ind = (sfpt->req() - sfpt->jvms()->scloff());\n-    SafePointScalarObjectNode* sobj = new SafePointScalarObjectNode(res_type,\n-#ifdef ASSERT\n-                                                 alloc,\n-#endif\n-                                                 first_ind, nfields);\n-    sobj->init_req(0, C->root());\n-    transform_later(sobj);\n-\n-    \/\/ Scan object's fields adding an input to the safepoint for each field.\n-    for (int j = 0; j < nfields; j++) {\n-      intptr_t offset;\n-      ciField* field = nullptr;\n-      if (iklass != nullptr) {\n-        field = iklass->nonstatic_field_at(j);\n-        offset = field->offset_in_bytes();\n-        ciType* elem_type = field->type();\n-        basic_elem_type = field->layout_type();\n-\n-        \/\/ The next code is taken from Parse::do_get_xxx().\n-        if (is_reference_type(basic_elem_type)) {\n-          if (!elem_type->is_loaded()) {\n-            field_type = TypeInstPtr::BOTTOM;\n-          } else if (field != nullptr && field->is_static_constant()) {\n-            ciObject* con = field->constant_value().as_object();\n-            \/\/ Do not \"join\" in the previous type; it doesn't add value,\n-            \/\/ and may yield a vacuous result if the field is of interface type.\n-            field_type = TypeOopPtr::make_from_constant(con)->isa_oopptr();\n-            assert(field_type != nullptr, \"field singleton type must be consistent\");\n-          } else {\n-            field_type = TypeOopPtr::make_from_klass(elem_type->as_klass());\n-          }\n-          if (UseCompressedOops) {\n-            field_type = field_type->make_narrowoop();\n-            basic_elem_type = T_NARROWOOP;\n-          }\n+\n+  SafePointScalarObjectNode* sobj = new SafePointScalarObjectNode(res_type, alloc, first_ind, nfields);\n+  sobj->init_req(0, C->root());\n+  transform_later(sobj);\n+\n+  \/\/ Scan object's fields adding an input to the safepoint for each field.\n+  for (int j = 0; j < nfields; j++) {\n+    intptr_t offset;\n+    ciField* field = nullptr;\n+    if (iklass != nullptr) {\n+      field = iklass->nonstatic_field_at(j);\n+      offset = field->offset_in_bytes();\n+      ciType* elem_type = field->type();\n+      basic_elem_type = field->layout_type();\n+\n+      \/\/ The next code is taken from Parse::do_get_xxx().\n+      if (is_reference_type(basic_elem_type)) {\n+        if (!elem_type->is_loaded()) {\n+          field_type = TypeInstPtr::BOTTOM;\n+        } else if (field != nullptr && field->is_static_constant()) {\n+          ciObject* con = field->constant_value().as_object();\n+          \/\/ Do not \"join\" in the previous type; it doesn't add value,\n+          \/\/ and may yield a vacuous result if the field is of interface type.\n+          field_type = TypeOopPtr::make_from_constant(con)->isa_oopptr();\n+          assert(field_type != nullptr, \"field singleton type must be consistent\");\n@@ -760,1 +789,5 @@\n-          field_type = Type::get_const_basic_type(basic_elem_type);\n+          field_type = TypeOopPtr::make_from_klass(elem_type->as_klass());\n+        }\n+        if (UseCompressedOops) {\n+          field_type = field_type->make_narrowoop();\n+          basic_elem_type = T_NARROWOOP;\n@@ -763,1 +796,1 @@\n-        offset = array_base + j * (intptr_t)element_size;\n+        field_type = Type::get_const_basic_type(basic_elem_type);\n@@ -765,0 +798,3 @@\n+    } else {\n+      offset = array_base + j * (intptr_t)element_size;\n+    }\n@@ -766,1 +802,1 @@\n-      const TypeOopPtr *field_addr_type = res_type->add_offset(offset)->isa_oopptr();\n+    const TypeOopPtr *field_addr_type = res_type->add_offset(offset)->isa_oopptr();\n@@ -768,4 +804,10 @@\n-      Node *field_val = value_from_mem(mem, ctl, basic_elem_type, field_type, field_addr_type, alloc);\n-      if (field_val == nullptr) {\n-        \/\/ We weren't able to find a value for this field,\n-        \/\/ give up on eliminating this allocation.\n+    Node *field_val = value_from_mem(sfpt->memory(), sfpt->control(), basic_elem_type, field_type, field_addr_type, alloc);\n+\n+    \/\/ We weren't able to find a value for this field,\n+    \/\/ give up on eliminating this allocation.\n+    if (field_val == nullptr) {\n+      uint last = sfpt->req() - 1;\n+      for (int k = 0;  k < j; k++) {\n+        sfpt->del_req(last--);\n+      }\n+      _igvn._worklist.push(sfpt);\n@@ -773,48 +815,8 @@\n-        \/\/ Remove any extra entries we added to the safepoint.\n-        uint last = sfpt->req() - 1;\n-        for (int k = 0;  k < j; k++) {\n-          sfpt->del_req(last--);\n-        }\n-        _igvn._worklist.push(sfpt);\n-        \/\/ rollback processed safepoints\n-        while (safepoints_done.length() > 0) {\n-          SafePointNode* sfpt_done = safepoints_done.pop();\n-          \/\/ remove any extra entries we added to the safepoint\n-          last = sfpt_done->req() - 1;\n-          for (int k = 0;  k < nfields; k++) {\n-            sfpt_done->del_req(last--);\n-          }\n-          JVMState *jvms = sfpt_done->jvms();\n-          jvms->set_endoff(sfpt_done->req());\n-          \/\/ Now make a pass over the debug information replacing any references\n-          \/\/ to SafePointScalarObjectNode with the allocated object.\n-          int start = jvms->debug_start();\n-          int end   = jvms->debug_end();\n-          for (int i = start; i < end; i++) {\n-            if (sfpt_done->in(i)->is_SafePointScalarObject()) {\n-              SafePointScalarObjectNode* scobj = sfpt_done->in(i)->as_SafePointScalarObject();\n-              if (scobj->first_index(jvms) == sfpt_done->req() &&\n-                  scobj->n_fields() == (uint)nfields) {\n-                assert(scobj->alloc() == alloc, \"sanity\");\n-                sfpt_done->set_req(i, res);\n-              }\n-            }\n-          }\n-          _igvn._worklist.push(sfpt_done);\n-        }\n-        if (PrintEliminateAllocations) {\n-          if (field != nullptr) {\n-            tty->print(\"=== At SafePoint node %d can't find value of Field: \",\n-                       sfpt->_idx);\n-            field->print();\n-            int field_idx = C->get_alias_index(field_addr_type);\n-            tty->print(\" (alias_idx=%d)\", field_idx);\n-          } else { \/\/ Array's element\n-            tty->print(\"=== At SafePoint node %d can't find value of array element [%d]\",\n-                       sfpt->_idx, j);\n-          }\n-          tty->print(\", which prevents elimination of: \");\n-          if (res == nullptr)\n-            alloc->dump();\n-          else\n-            res->dump();\n+      if (PrintEliminateAllocations) {\n+        if (field != nullptr) {\n+          tty->print(\"=== At SafePoint node %d can't find value of field: \", sfpt->_idx);\n+          field->print();\n+          int field_idx = C->get_alias_index(field_addr_type);\n+          tty->print(\" (alias_idx=%d)\", field_idx);\n+        } else { \/\/ Array's element\n+          tty->print(\"=== At SafePoint node %d can't find value of array element [%d]\", sfpt->_idx, j);\n@@ -823,2 +825,5 @@\n-#endif\n-        return false;\n+        tty->print(\", which prevents elimination of: \");\n+        if (res == nullptr)\n+          alloc->dump();\n+        else\n+          res->dump();\n@@ -826,8 +831,12 @@\n-      if (UseCompressedOops && field_type->isa_narrowoop()) {\n-        \/\/ Enable \"DecodeN(EncodeP(Allocate)) --> Allocate\" transformation\n-        \/\/ to be able scalar replace the allocation.\n-        if (field_val->is_EncodeP()) {\n-          field_val = field_val->in(1);\n-        } else {\n-          field_val = transform_later(new DecodeNNode(field_val, field_val->get_ptr_type()));\n-        }\n+#endif\n+\n+      return nullptr;\n+    }\n+\n+    if (UseCompressedOops && field_type->isa_narrowoop()) {\n+      \/\/ Enable \"DecodeN(EncodeP(Allocate)) --> Allocate\" transformation\n+      \/\/ to be able scalar replace the allocation.\n+      if (field_val->is_EncodeP()) {\n+        field_val = field_val->in(1);\n+      } else {\n+        field_val = transform_later(new DecodeNNode(field_val, field_val->get_ptr_type()));\n@@ -835,3 +844,24 @@\n-      sfpt->add_req(field_val);\n-    JVMState *jvms = sfpt->jvms();\n-    jvms->set_endoff(sfpt->req());\n+    sfpt->add_req(field_val);\n+  }\n+\n+  sfpt->jvms()->set_endoff(sfpt->req());\n+\n+  return sobj;\n+}\n+\n+\/\/ Do scalar replacement.\n+bool PhaseMacroExpand::scalar_replacement(AllocateNode *alloc, GrowableArray <SafePointNode *>& safepoints) {\n+  GrowableArray <SafePointNode *> safepoints_done;\n+  Node* res = alloc->result_cast();\n+  assert(res == nullptr || res->is_CheckCastPP(), \"unexpected AllocateNode result\");\n+\n+  \/\/ Process the safepoint uses\n+  while (safepoints.length() > 0) {\n+    SafePointNode* sfpt = safepoints.pop();\n+    SafePointScalarObjectNode* sobj = create_scalarized_object_description(alloc, sfpt);\n+\n+    if (sobj == nullptr) {\n+      undo_previous_scalarizations(safepoints_done, alloc);\n+      return false;\n+    }\n+\n@@ -841,3 +871,2 @@\n-    int start = jvms->debug_start();\n-    int end   = jvms->debug_end();\n-    sfpt->replace_edges_in_range(res, sobj, start, end, &_igvn);\n+    JVMState *jvms = sfpt->jvms();\n+    sfpt->replace_edges_in_range(res, sobj, jvms->debug_start(), jvms->debug_end(), &_igvn);\n@@ -845,1 +874,3 @@\n-    safepoints_done.append_if_missing(sfpt); \/\/ keep it for rollback\n+\n+    \/\/ keep it for rollback\n+    safepoints_done.append_if_missing(sfpt);\n@@ -847,0 +878,1 @@\n+\n@@ -1033,1 +1065,1 @@\n-  if (!can_eliminate_allocation(alloc, safepoints)) {\n+  if (!can_eliminate_allocation(&_igvn, alloc, &safepoints)) {\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":177,"deletions":145,"binary":false,"changes":322,"status":"modified"},{"patch":"@@ -556,1 +556,1 @@\n-  ArrayCopyNode* ac = find_array_copy_clone(phase, ld_alloc, mem);\n+  ArrayCopyNode* ac = find_array_copy_clone(ld_alloc, mem);\n@@ -611,1 +611,1 @@\n-ArrayCopyNode* MemNode::find_array_copy_clone(PhaseValues* phase, Node* ld_alloc, Node* mem) const {\n+ArrayCopyNode* MemNode::find_array_copy_clone(Node* ld_alloc, Node* mem) const {\n@@ -632,1 +632,1 @@\n-        AllocateNode* alloc = AllocateNode::Ideal_allocation(ac->in(ArrayCopyNode::Dest), phase);\n+        AllocateNode* alloc = AllocateNode::Ideal_allocation(ac->in(ArrayCopyNode::Dest));\n@@ -660,1 +660,1 @@\n-  AllocateNode* alloc  = AllocateNode::Ideal_allocation(base, phase);\n+  AllocateNode* alloc  = AllocateNode::Ideal_allocation(base);\n@@ -708,1 +708,1 @@\n-                                  AllocateNode::Ideal_allocation(st_base, phase),\n+                                  AllocateNode::Ideal_allocation(st_base),\n@@ -1061,1 +1061,1 @@\n-  Node* ld_alloc = AllocateNode::Ideal_allocation(ld_base, phase);\n+  Node* ld_alloc = AllocateNode::Ideal_allocation(ld_base);\n@@ -1163,1 +1163,1 @@\n-        if (ReduceBulkZeroing || find_array_copy_clone(phase, ld_alloc, in(MemNode::Memory)) == nullptr) {\n+        if (ReduceBulkZeroing || find_array_copy_clone(ld_alloc, in(MemNode::Memory)) == nullptr) {\n@@ -1520,0 +1520,29 @@\n+\n+\/\/------------------------------split_through_phi------------------------------\n+\/\/ Check whether a call to 'split_through_phi' would split this load through the\n+\/\/ Phi *base*. This method is essentially a copy of the validations performed\n+\/\/ by 'split_through_phi'. The first use of this method was in EA code as part\n+\/\/ of simplification of allocation merges.\n+bool LoadNode::can_split_through_phi_base(PhaseGVN* phase) {\n+  Node* mem        = in(Memory);\n+  Node* address    = in(Address);\n+  intptr_t ignore  = 0;\n+  Node*    base    = AddPNode::Ideal_base_and_offset(address, phase, ignore);\n+  bool base_is_phi = (base != nullptr) && base->is_Phi();\n+\n+  if (req() > 3 || !base_is_phi) {\n+    return false;\n+  }\n+\n+  if (!mem->is_Phi()) {\n+    if (!MemNode::all_controls_dominate(mem, base->in(0)))\n+      return false;\n+  } else if (base->in(0) != mem->in(0)) {\n+    if (!MemNode::all_controls_dominate(mem, base->in(0))) {\n+      return false;\n+    }\n+  }\n+\n+  return true;\n+}\n+\n@@ -1522,1 +1551,1 @@\n-Node* LoadNode::split_through_phi(PhaseGVN* phase) {\n+Node* LoadNode::split_through_phi(PhaseGVN* phase, bool ignore_missing_instance_id) {\n@@ -1533,1 +1562,2 @@\n-         (t_oop->is_known_instance_field() ||\n+         (ignore_missing_instance_id ||\n+          t_oop->is_known_instance_field() ||\n@@ -1545,2 +1575,2 @@\n-        (load_boxed_values || t_oop->is_known_instance_field()))) {\n-    return nullptr; \/\/ memory is not Phi\n+        (ignore_missing_instance_id || load_boxed_values || t_oop->is_known_instance_field()))) {\n+    return nullptr; \/\/ Neither memory or base are Phi\n@@ -1590,1 +1620,1 @@\n-  assert(C->have_alias_type(t_oop), \"instance should have alias type\");\n+  assert(ignore_missing_instance_id || C->have_alias_type(t_oop), \"instance should have alias type\");\n@@ -1626,0 +1656,1 @@\n+  Node* phi = nullptr;\n@@ -1627,8 +1658,11 @@\n-  int this_index  = C->get_alias_index(t_oop);\n-  int this_offset = t_oop->offset();\n-  int this_iid    = t_oop->instance_id();\n-  if (!t_oop->is_known_instance() && load_boxed_values) {\n-    \/\/ Use _idx of address base for boxed values.\n-    this_iid = base->_idx;\n-  }\n-  Node* phi = new PhiNode(region, this_type, nullptr, mem->_idx, this_iid, this_index, this_offset);\n+  if (t_oop != nullptr && (t_oop->is_known_instance_field() || load_boxed_values)) {\n+    int this_index = C->get_alias_index(t_oop);\n+    int this_offset = t_oop->offset();\n+    int this_iid = t_oop->is_known_instance_field() ? t_oop->instance_id() : base->_idx;\n+    phi = new PhiNode(region, this_type, nullptr, mem->_idx, this_iid, this_index, this_offset);\n+  } else if (ignore_missing_instance_id) {\n+    phi = new PhiNode(region, this_type, nullptr, mem->_idx);\n+  } else {\n+    return nullptr;\n+  }\n+\n@@ -1716,1 +1750,1 @@\n-AllocateNode* LoadNode::is_new_object_mark_load(PhaseGVN *phase) const {\n+AllocateNode* LoadNode::is_new_object_mark_load() const {\n@@ -1719,1 +1753,1 @@\n-    AllocateNode* alloc = AllocateNode::Ideal_allocation(address, phase);\n+    AllocateNode* alloc = AllocateNode::Ideal_allocation(address);\n@@ -2147,1 +2181,1 @@\n-  Node* alloc = is_new_object_mark_load(phase);\n+  Node* alloc = is_new_object_mark_load();\n@@ -2395,1 +2429,1 @@\n-      return tkls->is_aryklassptr()->elem();\n+      return tkls->is_aryklassptr()->elem()->isa_klassptr()->cast_to_exactness(tkls->klass_is_exact());\n@@ -2534,1 +2568,1 @@\n-    AllocateArrayNode* alloc = AllocateArrayNode::Ideal_array_allocation(base, phase);\n+    AllocateArrayNode* alloc = AllocateArrayNode::Ideal_array_allocation(base);\n@@ -2566,1 +2600,1 @@\n-    AllocateArrayNode* alloc = AllocateArrayNode::Ideal_array_allocation(base, phase);\n+    AllocateArrayNode* alloc = AllocateArrayNode::Ideal_array_allocation(base);\n@@ -3030,0 +3064,5 @@\n+\/\/ This method conservatively checks if the result of a LoadStoreNode is\n+\/\/ used, that is, if it returns true, then it is definitely the case that\n+\/\/ the result of the node is not needed.\n+\/\/ For example, GetAndAdd can be matched into a lock_add instead of a\n+\/\/ lock_xadd if the result of LoadStoreNode::result_not_used() is true\n@@ -3031,1 +3070,1 @@\n-  for( DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++ ) {\n+  for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n@@ -3033,1 +3072,8 @@\n-    if (x->Opcode() == Op_SCMemProj) continue;\n+    if (x->Opcode() == Op_SCMemProj) {\n+      continue;\n+    }\n+    if (x->bottom_type() == TypeTuple::MEMBAR &&\n+        !x->is_Call() &&\n+        x->Opcode() != Op_Blackhole) {\n+      continue;\n+    }\n@@ -3365,1 +3411,1 @@\n-      Node* alloc = AllocateNode::Ideal_allocation(in(MemBarNode::Precedent), phase);\n+      Node* alloc = AllocateNode::Ideal_allocation(in(MemBarNode::Precedent));\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":75,"deletions":29,"binary":false,"changes":104,"status":"modified"},{"patch":"@@ -111,1 +111,1 @@\n-static inline jlong field_offset_from_byte_offset(jlong byte_offset) {\n+static inline int field_offset_from_byte_offset(int byte_offset) {\n","filename":"src\/hotspot\/share\/prims\/unsafe.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -44,0 +44,1 @@\n+#include \"oops\/compressedKlass.hpp\"\n@@ -88,1 +89,0 @@\n-bool   Arguments::_xdebug_mode                  = false;\n@@ -515,1 +515,0 @@\n-  { \"EnableWaitForParallelLoad\",    JDK_Version::jdk(20), JDK_Version::jdk(21), JDK_Version::jdk(22) },\n@@ -523,1 +522,0 @@\n-  { \"G1UsePreventiveGC\",            JDK_Version::undefined(), JDK_Version::jdk(21), JDK_Version::jdk(22) },\n@@ -1153,1 +1151,1 @@\n-                  \"Did you mean '%s%s%s'? \",\n+                  \"Did you mean '%s%s%s'?\\n\",\n@@ -1182,1 +1180,1 @@\n-  char quote_c        = 0;\n+  int  quote_c        = 0;\n@@ -1194,1 +1192,1 @@\n-          token[pos++] = c;\n+          token[pos++] = checked_cast<char>(c);\n@@ -1214,1 +1212,1 @@\n-        token[pos++] = c;\n+        token[pos++] = checked_cast<char>(c);\n@@ -1487,24 +1485,2 @@\n-\n-  if (UseCompactObjectHeaders) {\n-    \/\/ 512 byte alignment, 22-bit values (Lilliput)\n-    LogKlassAlignmentInBytes = 9;\n-    MaxNarrowKlassPointerBits = 22;\n-  } else {\n-    \/\/ Traditional: 8 byte alignment, 32-bit values\n-    LogKlassAlignmentInBytes = 3;\n-    MaxNarrowKlassPointerBits = 32;\n-  }\n-\n-  KlassAlignmentInBytes = 1 << LogKlassAlignmentInBytes;\n-  assert(is_aligned(KlassAlignmentInBytes, BytesPerWord), \"Must be at least word-sized\");\n-  KlassAlignmentInWords = KlassAlignmentInBytes \/ BytesPerWord;\n-  NarrowKlassPointerBitMask = ((((uint64_t)1) << MaxNarrowKlassPointerBits) - 1);\n-  KlassEncodingMetaspaceMax = UCONST64(1) << (MaxNarrowKlassPointerBits + LogKlassAlignmentInBytes);\n-\n-  \/\/ Assert validity of compressed class space size. User arg should have been checked at this point\n-  \/\/ (see CompressedClassSpaceSizeConstraintFunc()), so no need to be nice about it, this fires in\n-  \/\/ case the default is wrong.\n-  \/\/ TODO: This is placed wrong. The CompressedClassSpaceSizeFunc is done after ergo, but this\n-  \/\/ assert is during ergo.\n-  \/\/ assert(!UseCompressedClassPointers || CompressedClassSpaceSize <= KlassEncodingMetaspaceMax,\n-  \/\/       \"CompressedClassSpaceSize is too large for UseCompressedClassPointers\");\n+  assert(!UseCompressedClassPointers || CompressedClassSpaceSize <= KlassEncodingMetaspaceMax,\n+         \"CompressedClassSpaceSize is too large for UseCompressedClassPointers\");\n@@ -1592,1 +1568,1 @@\n-    MaxRAMPercentage = 100.0 \/ MaxRAMFraction;\n+    MaxRAMPercentage = 100.0 \/ (double)MaxRAMFraction;\n@@ -1596,1 +1572,1 @@\n-    MinRAMPercentage = 100.0 \/ MinRAMFraction;\n+    MinRAMPercentage = 100.0 \/ (double)MinRAMFraction;\n@@ -1600,1 +1576,1 @@\n-    InitialRAMPercentage = 100.0 \/ InitialRAMFraction;\n+    InitialRAMPercentage = 100.0 \/ (double)InitialRAMFraction;\n@@ -1606,2 +1582,2 @@\n-    julong reasonable_max = (julong)((phys_mem * MaxRAMPercentage) \/ 100);\n-    const julong reasonable_min = (julong)((phys_mem * MinRAMPercentage) \/ 100);\n+    julong reasonable_max = (julong)(((double)phys_mem * MaxRAMPercentage) \/ 100);\n+    const julong reasonable_min = (julong)(((double)phys_mem * MinRAMPercentage) \/ 100);\n@@ -1691,1 +1667,1 @@\n-      julong reasonable_initial = (julong)((phys_mem * InitialRAMPercentage) \/ 100);\n+      julong reasonable_initial = (julong)(((double)phys_mem * InitialRAMPercentage) \/ 100);\n@@ -1940,1 +1916,1 @@\n-                  \"Conflicting -XX:+UseHeavyMonitors and -XX:LockingMode=%d flags\", LockingMode);\n+                  \"Conflicting -XX:+UseHeavyMonitors and -XX:LockingMode=%d flags\\n\", LockingMode);\n@@ -1949,1 +1925,1 @@\n-                \"LockingMode == 0 (LM_MONITOR) is not fully implemented on this architecture\");\n+                \"LockingMode == 0 (LM_MONITOR) is not fully implemented on this architecture\\n\");\n@@ -1953,1 +1929,1 @@\n-#if (defined(X86) || defined(PPC64)) && !defined(ZERO)\n+#if defined(X86) && !defined(ZERO)\n@@ -1956,1 +1932,1 @@\n-                \"LockingMode == 0 (LM_MONITOR) and -XX:+UseRTMForStackLocks are mutually exclusive\");\n+                \"LockingMode == 0 (LM_MONITOR) and -XX:+UseRTMForStackLocks are mutually exclusive\\n\");\n@@ -1963,1 +1939,1 @@\n-                \"-XX:+VerifyHeavyMonitors requires LockingMode == 0 (LM_MONITOR)\");\n+                \"-XX:+VerifyHeavyMonitors requires LockingMode == 0 (LM_MONITOR)\\n\");\n@@ -1992,4 +1968,4 @@\n-bool Arguments::parse_uintx(const char* value,\n-                            uintx* uintx_arg,\n-                            uintx min_size) {\n-  uintx n;\n+bool Arguments::parse_uint(const char* value,\n+                           uint* uint_arg,\n+                           uint min_size) {\n+  uint n;\n@@ -2000,1 +1976,1 @@\n-    *uintx_arg = n;\n+    *uint_arg = n;\n@@ -2681,2 +2657,1 @@\n-      \/\/ note this flag has been used, then ignore\n-      set_xdebug_mode(true);\n+      warning(\"Option -Xdebug was deprecated in JDK 22 and will likely be removed in a future release.\");\n@@ -2685,1 +2660,1 @@\n-      \/\/ For compatibility with classic. HotSpot refuses to load the old style agent.dll.\n+      warning(\"Option -Xnoagent was deprecated in JDK 22 and will likely be removed in a future release.\");\n@@ -2756,2 +2731,2 @@\n-      uintx max_tenuring_thresh = 0;\n-      if (!parse_uintx(tail, &max_tenuring_thresh, 0)) {\n+      uint max_tenuring_thresh = 0;\n+      if (!parse_uint(tail, &max_tenuring_thresh, 0)) {\n@@ -2865,1 +2840,1 @@\n-              \"Value of jvmci.Compiler incompatible with +UseGraalJIT: %s\", jvmci_compiler);\n+              \"Value of jvmci.Compiler incompatible with +UseGraalJIT: %s\\n\", jvmci_compiler);\n@@ -2882,1 +2857,1 @@\n-            \"Unable to enable JVMCI in product mode\");\n+            \"Unable to enable JVMCI in product mode\\n\");\n@@ -3987,1 +3962,1 @@\n-                \"Syntax error, expecting -XX:NativeMemoryTracking=[off|summary|detail]\", nullptr);\n+                \"Syntax error, expecting -XX:NativeMemoryTracking=[off|summary|detail]\\n\");\n@@ -4000,0 +3975,6 @@\n+  bool log_class_load_cause = log_is_enabled(Info, class, load, cause, native) ||\n+                              log_is_enabled(Info, class, load, cause);\n+  if (log_class_load_cause && LogClassLoadingCauseFor == nullptr) {\n+    warning(\"class load cause logging will not produce output without LogClassLoadingCauseFor\");\n+  }\n+\n@@ -4075,1 +4056,1 @@\n-#ifdef COMPILER2\n+#if COMPILER2_OR_JVMCI\n@@ -4096,1 +4077,1 @@\n-#endif \/\/ COMPILER2\n+#endif \/\/ COMPILER2_OR_JVMCI\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":38,"deletions":57,"binary":false,"changes":95,"status":"modified"},{"patch":"@@ -70,2 +70,2 @@\n-\/\/ EXPERIMENTAL flags are in support of features that are not\n-\/\/    part of the officially supported product, but are available\n+\/\/ EXPERIMENTAL flags are in support of features that may not be\n+\/\/    an officially supported part of a product, but may be available\n@@ -78,0 +78,2 @@\n+\/\/    Refer to the documentation of any products using this code for details\n+\/\/    on support and fitness for production.\n@@ -83,2 +85,1 @@\n-\/\/    and they are not supported on production loads, except under explicit\n-\/\/    direction from support engineers.\n+\/\/    Refer to the documentation of any products using this code for details.\n@@ -449,1 +450,1 @@\n-  product(uintx, LogEventsBufferEntries, 20, DIAGNOSTIC,                    \\\n+  product(int, LogEventsBufferEntries, 20, DIAGNOSTIC,                      \\\n@@ -732,1 +733,1 @@\n-          \"at one time (minimum is 1024).\")                      \\\n+          \"at one time (minimum is 1024).\")                                 \\\n@@ -735,1 +736,1 @@\n-  product(intx, MonitorUsedDeflationThreshold, 90, DIAGNOSTIC,              \\\n+  product(int, MonitorUsedDeflationThreshold, 90, DIAGNOSTIC,               \\\n@@ -808,1 +809,1 @@\n-  product(intx, ContendedPaddingWidth, 128,                                 \\\n+  product(int, ContendedPaddingWidth, 128,                                  \\\n@@ -939,0 +940,5 @@\n+  product(ccstr, LogClassLoadingCauseFor, nullptr,                          \\\n+          \"Apply -Xlog:class+load+cause* to classes whose fully \"           \\\n+          \"qualified name contains this string (\\\"*\\\" matches \"             \\\n+          \"any class).\")                                                    \\\n+                                                                            \\\n@@ -1102,1 +1108,1 @@\n-  develop(intx, FastAllocateSizeLimit, 128*K,                               \\\n+  develop(int, FastAllocateSizeLimit, 128*K,                                \\\n@@ -1116,1 +1122,1 @@\n-  product(intx, TypeProfileArgsLimit,     2,                                \\\n+  product(int, TypeProfileArgsLimit,     2,                                 \\\n@@ -1120,1 +1126,1 @@\n-  product(intx, TypeProfileParmsLimit,    2,                                \\\n+  product(int, TypeProfileParmsLimit,    2,                                 \\\n@@ -1235,1 +1241,1 @@\n-  product(intx,  AllocatePrefetchStyle, 1,                                  \\\n+  product(int,  AllocatePrefetchStyle, 1,                                   \\\n@@ -1242,1 +1248,1 @@\n-  product(intx,  AllocatePrefetchDistance, -1,                              \\\n+  product(int,  AllocatePrefetchDistance, -1,                               \\\n@@ -1245,1 +1251,1 @@\n-          constraint(AllocatePrefetchDistanceConstraintFunc,AfterMemoryInit)\\\n+          range(-1, 512)                                                    \\\n@@ -1247,1 +1253,1 @@\n-  product(intx,  AllocatePrefetchLines, 3,                                  \\\n+  product(int,  AllocatePrefetchLines, 3,                                   \\\n@@ -1251,1 +1257,1 @@\n-  product(intx,  AllocateInstancePrefetchLines, 1,                          \\\n+  product(int,  AllocateInstancePrefetchLines, 1,                           \\\n@@ -1256,1 +1262,1 @@\n-  product(intx,  AllocatePrefetchStepSize, 16,                              \\\n+  product(int,  AllocatePrefetchStepSize, 16,                               \\\n@@ -1293,3 +1299,4 @@\n-  product(intx, SafepointTimeoutDelay, 10000,                               \\\n-          \"Delay in milliseconds for option SafepointTimeout\")              \\\n-          range(0, max_intx LP64_ONLY(\/MICROUNITS))                         \\\n+  product(double, SafepointTimeoutDelay, 10000,                             \\\n+          \"Delay in milliseconds for option SafepointTimeout; \"             \\\n+          \"supports sub-millisecond resolution with fractional values.\")    \\\n+          range(0, max_jlongDouble LP64_ONLY(\/MICROUNITS))                  \\\n@@ -1308,1 +1315,1 @@\n-  develop(intx, StackPrintLimit, 100,                                       \\\n+  develop(int, StackPrintLimit, 100,                                        \\\n@@ -1383,1 +1390,1 @@\n-  product(intx, SpecTrapLimitExtraEntries,  3, EXPERIMENTAL,                \\\n+  product(int, SpecTrapLimitExtraEntries,  3, EXPERIMENTAL,                 \\\n@@ -1413,1 +1420,1 @@\n-          constraint(CompressedClassSpaceSizeConstraintFunc, AfterErgo)     \\\n+          range(1*M, 3*G)                                                   \\\n@@ -1415,1 +1422,1 @@\n-  product(size_t, CompressedClassSpaceBaseAddress, 0, DIAGNOSTIC,           \\\n+  develop(size_t, CompressedClassSpaceBaseAddress, 0,                       \\\n@@ -1593,0 +1600,3 @@\n+  develop(intx, TraceBytecodesStopAt, 0,                                    \\\n+          \"Stop bytecode tracing at the specified bytecode number\")         \\\n+                                                                            \\\n@@ -1739,1 +1749,1 @@\n-  product(intx, PerfDataSamplingInterval, 50,                               \\\n+  product(int, PerfDataSamplingInterval, 50,                                \\\n@@ -1988,0 +1998,7 @@\n+                                                                            \\\n+  product(uint, TrimNativeHeapInterval, 0, EXPERIMENTAL,                    \\\n+          \"Interval, in ms, at which the JVM will trim the native heap if \" \\\n+          \"the platform supports that. Lower values will reclaim memory \"   \\\n+          \"more eagerly at the cost of higher overhead. A value of 0 \"      \\\n+          \"(default) disables native heap trimming.\")                       \\\n+          range(0, UINT_MAX)                                                \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":42,"deletions":25,"binary":false,"changes":67,"status":"modified"},{"patch":"@@ -239,1 +239,1 @@\n-    ((in_bytes(ObjectMonitor::f ## _offset())) - markWord::monitor_value)\n+    ((in_bytes(ObjectMonitor::f ## _offset())) - checked_cast<int>(markWord::monitor_value))\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -58,0 +58,1 @@\n+#include \"runtime\/trimNativeHeap.hpp\"\n@@ -1172,2 +1173,2 @@\n-    float remainder = (100.0 - MonitorUsedDeflationThreshold) \/ 100.0;\n-    size_t new_ceiling = ceiling + (ceiling * remainder) + 1;\n+    double remainder = (100.0 - MonitorUsedDeflationThreshold) \/ 100.0;\n+    size_t new_ceiling = ceiling + (size_t)((double)ceiling * remainder) + 1;\n@@ -1186,1 +1187,1 @@\n-                               \", monitor_usage=\" SIZE_FORMAT \", threshold=\" INTX_FORMAT,\n+                               \", monitor_usage=\" SIZE_FORMAT \", threshold=%d\",\n@@ -1650,0 +1651,1 @@\n+  NativeHeapTrimmer::SuspendMark sm(\"monitor deletion\");\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -90,0 +90,1 @@\n+#include \"runtime\/trimNativeHeap.hpp\"\n@@ -566,0 +567,4 @@\n+  \/\/ Create WatcherThread as soon as we can since we need it in case\n+  \/\/ of hangs during error reporting.\n+  WatcherThread::start();\n+\n@@ -688,1 +693,1 @@\n-  Chunk::start_chunk_pool_cleaner_task();\n+  Arena::start_chunk_pool_cleaner_task();\n@@ -768,0 +773,4 @@\n+  if (NativeHeapTrimmer::enabled()) {\n+    NativeHeapTrimmer::initialize();\n+  }\n+\n@@ -805,13 +814,5 @@\n-  {\n-    MutexLocker ml(PeriodicTask_lock);\n-    \/\/ Make sure the WatcherThread can be started by WatcherThread::start()\n-    \/\/ or by dynamic enrollment.\n-    WatcherThread::make_startable();\n-    \/\/ Start up the WatcherThread if there are any periodic tasks\n-    \/\/ NOTE:  All PeriodicTasks should be registered by now. If they\n-    \/\/   aren't, late joiners might appear to start slowly (we might\n-    \/\/   take a while to process their first tick).\n-    if (PeriodicTask::num_tasks() > 0) {\n-      WatcherThread::start();\n-    }\n-  }\n+  \/\/ Let WatcherThread run all registered periodic tasks now.\n+  \/\/ NOTE:  All PeriodicTasks should be registered by now. If they\n+  \/\/   aren't, late joiners might appear to start slowly (we might\n+  \/\/   take a while to process their first tick).\n+  WatcherThread::run_all_tasks();\n@@ -826,1 +827,0 @@\n-    ShouldNotReachHere();\n","filename":"src\/hotspot\/share\/runtime\/threads.cpp","additions":15,"deletions":15,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -51,0 +51,1 @@\n+  template(HeapDumpMerge)                         \\\n","filename":"src\/hotspot\/share\/runtime\/vmOperation.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -86,0 +86,1 @@\n+#include \"oops\/resolvedFieldEntry.hpp\"\n@@ -112,0 +113,1 @@\n+#include \"services\/attachListener.hpp\"\n@@ -218,2 +220,2 @@\n-  volatile_nonstatic_field(ArrayKlass,         _higher_dimension,                             Klass*)                                \\\n-  volatile_nonstatic_field(ArrayKlass,         _lower_dimension,                              Klass*)                                \\\n+  volatile_nonstatic_field(ArrayKlass,         _higher_dimension,                             ObjArrayKlass*)                        \\\n+  volatile_nonstatic_field(ArrayKlass,         _lower_dimension,                              ArrayKlass*)                           \\\n@@ -236,0 +238,2 @@\n+  nonstatic_field(ConstantPoolCache,           _resolved_field_entries,                       Array<ResolvedFieldEntry>*)            \\\n+  nonstatic_field(ResolvedFieldEntry,          _cpool_index,                                  u2)                                    \\\n@@ -247,0 +251,1 @@\n+  nonstatic_field(InstanceKlass,               _nest_members,                                 Array<jushort>*)                       \\\n@@ -254,0 +259,1 @@\n+  nonstatic_field(InstanceKlass,               _nest_host_index,                              u2)                                    \\\n@@ -338,0 +344,4 @@\n+  nonstatic_field(Annotations,                 _class_annotations,                            Array<u1>*)                            \\\n+  nonstatic_field(Annotations,                 _fields_annotations,                           Array<Array<u1>*>*)                    \\\n+  nonstatic_field(Annotations,                 _class_type_annotations,                       Array<u1>*)                            \\\n+  nonstatic_field(Annotations,                 _fields_type_annotations,                      Array<Array<u1>*>*)                    \\\n@@ -392,2 +402,2 @@\n-     static_field(CompressedKlassPointers,     _base_copy,                           address)                                        \\\n-     static_field(CompressedKlassPointers,     _shift_copy,                          int)                                            \\\n+     static_field(CompressedKlassPointers,     _base,                                         address)                               \\\n+     static_field(CompressedKlassPointers,     _shift,                                        int)                                   \\\n@@ -488,0 +498,2 @@\n+  nonstatic_field(Array<ResolvedFieldEntry>,   _length,                                       int)                                   \\\n+  nonstatic_field(Array<ResolvedFieldEntry>,   _data[0],                                      ResolvedFieldEntry)                    \\\n@@ -538,1 +550,1 @@\n-  \/* StubRoutines (NOTE: incomplete) *\/                                                                                              \\\n+  \/* StubRoutine for stack walking.  *\/                                                                                              \\\n@@ -541,69 +553,0 @@\n-     static_field(StubRoutines,                _verify_oop_count,                             jint)                                  \\\n-     static_field(StubRoutines,                _aescrypt_encryptBlock,                        address)                               \\\n-     static_field(StubRoutines,                _aescrypt_decryptBlock,                        address)                               \\\n-     static_field(StubRoutines,                _cipherBlockChaining_encryptAESCrypt,          address)                               \\\n-     static_field(StubRoutines,                _cipherBlockChaining_decryptAESCrypt,          address)                               \\\n-     static_field(StubRoutines,                _electronicCodeBook_encryptAESCrypt,           address)                               \\\n-     static_field(StubRoutines,                _electronicCodeBook_decryptAESCrypt,           address)                               \\\n-     static_field(StubRoutines,                _counterMode_AESCrypt,                         address)                               \\\n-     static_field(StubRoutines,                _galoisCounterMode_AESCrypt,                   address)                               \\\n-     static_field(StubRoutines,                _ghash_processBlocks,                          address)                               \\\n-     static_field(StubRoutines,                _chacha20Block,                                address)                               \\\n-     static_field(StubRoutines,                _base64_encodeBlock,                           address)                               \\\n-     static_field(StubRoutines,                _base64_decodeBlock,                           address)                               \\\n-     static_field(StubRoutines,                _poly1305_processBlocks,                       address)                               \\\n-     static_field(StubRoutines,                _updateBytesCRC32,                             address)                               \\\n-     static_field(StubRoutines,                _crc_table_adr,                                address)                               \\\n-     static_field(StubRoutines,                _crc32c_table_addr,                            address)                               \\\n-     static_field(StubRoutines,                _updateBytesCRC32C,                            address)                               \\\n-     static_field(StubRoutines,                _updateBytesAdler32,                           address)                               \\\n-     static_field(StubRoutines,                _multiplyToLen,                                address)                               \\\n-     static_field(StubRoutines,                _squareToLen,                                  address)                               \\\n-     static_field(StubRoutines,                _bigIntegerRightShiftWorker,                   address)                               \\\n-     static_field(StubRoutines,                _bigIntegerLeftShiftWorker,                    address)                               \\\n-     static_field(StubRoutines,                _mulAdd,                                       address)                               \\\n-     static_field(StubRoutines,                _dexp,                                         address)                               \\\n-     static_field(StubRoutines,                _dlog,                                         address)                               \\\n-     static_field(StubRoutines,                _dlog10,                                       address)                               \\\n-     static_field(StubRoutines,                _dpow,                                         address)                               \\\n-     static_field(StubRoutines,                _fmod,                                         address)                               \\\n-     static_field(StubRoutines,                _dsin,                                         address)                               \\\n-     static_field(StubRoutines,                _dcos,                                         address)                               \\\n-     static_field(StubRoutines,                _dtan,                                         address)                               \\\n-     static_field(StubRoutines,                _vectorizedMismatch,                           address)                               \\\n-     static_field(StubRoutines,                _jbyte_arraycopy,                              address)                               \\\n-     static_field(StubRoutines,                _jshort_arraycopy,                             address)                               \\\n-     static_field(StubRoutines,                _jint_arraycopy,                               address)                               \\\n-     static_field(StubRoutines,                _jlong_arraycopy,                              address)                               \\\n-     static_field(StubRoutines,                _oop_arraycopy,                                address)                               \\\n-     static_field(StubRoutines,                _oop_arraycopy_uninit,                         address)                               \\\n-     static_field(StubRoutines,                _jbyte_disjoint_arraycopy,                     address)                               \\\n-     static_field(StubRoutines,                _jshort_disjoint_arraycopy,                    address)                               \\\n-     static_field(StubRoutines,                _jint_disjoint_arraycopy,                      address)                               \\\n-     static_field(StubRoutines,                _jlong_disjoint_arraycopy,                     address)                               \\\n-     static_field(StubRoutines,                _oop_disjoint_arraycopy,                       address)                               \\\n-     static_field(StubRoutines,                _oop_disjoint_arraycopy_uninit,                address)                               \\\n-     static_field(StubRoutines,                _arrayof_jbyte_arraycopy,                      address)                               \\\n-     static_field(StubRoutines,                _arrayof_jshort_arraycopy,                     address)                               \\\n-     static_field(StubRoutines,                _arrayof_jint_arraycopy,                       address)                               \\\n-     static_field(StubRoutines,                _arrayof_jlong_arraycopy,                      address)                               \\\n-     static_field(StubRoutines,                _arrayof_oop_arraycopy,                        address)                               \\\n-     static_field(StubRoutines,                _arrayof_oop_arraycopy_uninit,                 address)                               \\\n-     static_field(StubRoutines,                _arrayof_jbyte_disjoint_arraycopy,             address)                               \\\n-     static_field(StubRoutines,                _arrayof_jshort_disjoint_arraycopy,            address)                               \\\n-     static_field(StubRoutines,                _arrayof_jint_disjoint_arraycopy,              address)                               \\\n-     static_field(StubRoutines,                _arrayof_jlong_disjoint_arraycopy,             address)                               \\\n-     static_field(StubRoutines,                _arrayof_oop_disjoint_arraycopy,               address)                               \\\n-     static_field(StubRoutines,                _arrayof_oop_disjoint_arraycopy_uninit,        address)                               \\\n-     static_field(StubRoutines,                _checkcast_arraycopy,                          address)                               \\\n-     static_field(StubRoutines,                _checkcast_arraycopy_uninit,                   address)                               \\\n-     static_field(StubRoutines,                _unsafe_arraycopy,                             address)                               \\\n-     static_field(StubRoutines,                _generic_arraycopy,                            address)                               \\\n-                                                                                                                                     \\\n-  \/*****************\/                                                                                                                \\\n-  \/* SharedRuntime *\/                                                                                                                \\\n-  \/*****************\/                                                                                                                \\\n-                                                                                                                                     \\\n-     static_field(SharedRuntime,               _wrong_method_blob,                            RuntimeStub*)                          \\\n-     static_field(SharedRuntime,               _ic_miss_blob,                                 RuntimeStub*)                          \\\n-     static_field(SharedRuntime,               _deopt_blob,                                   DeoptimizationBlob*)                   \\\n@@ -1017,3 +960,0 @@\n-     static_field(JDK_Version,                 _current,                                      JDK_Version)                           \\\n-  nonstatic_field(JDK_Version,                 _major,                                        unsigned char)                         \\\n-                                                                                                                                     \\\n@@ -1040,7 +980,9 @@\n-  nonstatic_field(Array<int>,                         _length,                                int)                                   \\\n-  unchecked_nonstatic_field(Array<int>,               _data,                                  sizeof(int))                           \\\n-  unchecked_nonstatic_field(Array<u1>,                _data,                                  sizeof(u1))                            \\\n-  unchecked_nonstatic_field(Array<u2>,                _data,                                  sizeof(u2))                            \\\n-  unchecked_nonstatic_field(Array<Method*>,           _data,                                  sizeof(Method*))                       \\\n-  unchecked_nonstatic_field(Array<Klass*>,            _data,                                  sizeof(Klass*))                        \\\n-  unchecked_nonstatic_field(Array<ResolvedIndyEntry>, _data,                                  sizeof(ResolvedIndyEntry))             \\\n+  nonstatic_field(Array<int>,                          _length,                               int)                                   \\\n+  unchecked_nonstatic_field(Array<int>,                _data,                                 sizeof(int))                           \\\n+  unchecked_nonstatic_field(Array<u1>,                 _data,                                 sizeof(u1))                            \\\n+  unchecked_nonstatic_field(Array<u2>,                 _data,                                 sizeof(u2))                            \\\n+  unchecked_nonstatic_field(Array<Method*>,            _data,                                 sizeof(Method*))                       \\\n+  unchecked_nonstatic_field(Array<Klass*>,             _data,                                 sizeof(Klass*))                        \\\n+  unchecked_nonstatic_field(Array<ResolvedFieldEntry>, _data,                                 sizeof(ResolvedFieldEntry))            \\\n+  unchecked_nonstatic_field(Array<ResolvedIndyEntry>,  _data,                                 sizeof(ResolvedIndyEntry))             \\\n+  unchecked_nonstatic_field(Array<Array<u1>*>,         _data,                                 sizeof(Array<u1>*))                    \\\n@@ -1255,0 +1197,1 @@\n+    declare_type(Annotations, MetaspaceObj)                               \\\n@@ -1332,0 +1275,1 @@\n+        declare_type(AttachListenerThread, JavaThread)                    \\\n@@ -1941,1 +1885,0 @@\n-  declare_toplevel_type(JDK_Version)                                      \\\n@@ -1977,0 +1920,1 @@\n+            declare_type(Array<ResolvedFieldEntry>, MetaspaceObj)         \\\n@@ -1978,0 +1922,1 @@\n+            declare_type(Array<Array<u1>*>, MetaspaceObj)                 \\\n@@ -1994,0 +1939,1 @@\n+  declare_toplevel_type(ResolvedFieldEntry)                               \\\n@@ -3107,1 +3053,1 @@\n-    int len = end - start + 1;\n+    int len = pointer_delta_as_int(end, start) + 1;\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":32,"deletions":86,"binary":false,"changes":118,"status":"modified"},{"patch":"@@ -409,5 +409,0 @@\n-    \/\/ Todo: Lilliput: this is a hack. The real problem is the assumption that size\n-    \/\/  of a narrow Klass pointer can be expressed in number of bytes (getKlassPtrSize).\n-    \/\/  That assumption is present in a number of files here. Better would be\n-    \/\/  to change this to getKlassPtrSizeInBits, or to do it some other way.\n-    value &= (1 << 22) - 1; \/\/ narrow klass pointer size is 22 bits.\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/debugger\/DebuggerBase.java","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -61,2 +61,2 @@\n-    baseField = type.getAddressField(\"_base_copy\");\n-    shiftField = type.getCIntegerField(\"_shift_copy\");\n+    baseField = type.getAddressField(\"_base\");\n+    shiftField = type.getCIntegerField(\"_shift\");\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/CompressedKlassPointers.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -58,1 +58,1 @@\n-compiler\/rtm\/locking\/TestRTMDeoptOnLowAbortRatio.java 8183263,8307907 generic-x64,generic-i586,aix-ppc64\n+compiler\/rtm\/locking\/TestRTMDeoptOnLowAbortRatio.java 8183263 generic-x64,generic-i586\n@@ -60,1 +60,1 @@\n-compiler\/rtm\/locking\/TestRTMLockingThreshold.java 8183263,8307907 generic-x64,generic-i586,aix-ppc64\n+compiler\/rtm\/locking\/TestRTMLockingThreshold.java 8183263 generic-x64,generic-i586\n@@ -63,2 +63,2 @@\n-compiler\/rtm\/locking\/TestUseRTMXendForLockBusy.java 8183263,8307907 generic-x64,generic-i586,aix-ppc64\n-compiler\/rtm\/print\/TestPrintPreciseRTMLockingStatistics.java 8183263,8307907 generic-x64,generic-i586,aix-ppc64\n+compiler\/rtm\/locking\/TestUseRTMXendForLockBusy.java 8183263 generic-x64,generic-i586\n+compiler\/rtm\/print\/TestPrintPreciseRTMLockingStatistics.java 8183263 generic-x64,generic-i586\n@@ -72,2 +72,0 @@\n-compiler\/c2\/irTests\/TestVectorConditionalMove.java 8306922 generic-all\n-\n@@ -107,0 +105,1 @@\n+runtime\/ErrorHandling\/MachCodeFramesInErrorFile.java 8313315 linux-ppc64le\n@@ -124,7 +123,7 @@\n-serviceability\/sa\/ClhsdbCDSCore.java 8294316,8267433 macosx-x64\n-serviceability\/sa\/ClhsdbFindPC.java#xcomp-core 8294316,8267433 macosx-x64\n-serviceability\/sa\/ClhsdbFindPC.java#no-xcomp-core 8294316,8267433 macosx-x64\n-serviceability\/sa\/ClhsdbPmap.java#core 8294316,8267433 macosx-x64\n-serviceability\/sa\/ClhsdbPstack.java#core 8294316,8267433 macosx-x64\n-serviceability\/sa\/TestJmapCore.java 8294316,8267433 macosx-x64\n-serviceability\/sa\/TestJmapCoreMetaspace.java 8294316,8267433 macosx-x64\n+serviceability\/sa\/ClhsdbCDSCore.java 8267433 macosx-x64\n+serviceability\/sa\/ClhsdbFindPC.java#xcomp-core 8267433 macosx-x64\n+serviceability\/sa\/ClhsdbFindPC.java#no-xcomp-core 8267433 macosx-x64\n+serviceability\/sa\/ClhsdbPmap.java#core 8267433 macosx-x64\n+serviceability\/sa\/ClhsdbPstack.java#core 8267433 macosx-x64\n+serviceability\/sa\/TestJmapCore.java 8267433 macosx-x64\n+serviceability\/sa\/TestJmapCoreMetaspace.java 8267433 macosx-x64\n@@ -154,1 +153,0 @@\n-vmTestbase\/nsk\/jvmti\/AttachOnDemand\/attach002a\/TestDescription.java 8307462 generic-all\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":12,"deletions":14,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -348,1 +348,0 @@\n-  gc\/startup_warnings\/TestShenandoah.java \\\n","filename":"test\/hotspot\/jtreg\/TEST.groups","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -160,1 +160,1 @@\n-    @IR(counts = { IRNode.LOAD_VECTOR, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n+    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n@@ -173,1 +173,1 @@\n-    @IR(counts = { IRNode.LOAD_VECTOR, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n+    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n@@ -186,1 +186,1 @@\n-    @IR(counts = { IRNode.LOAD_VECTOR, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n+    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n@@ -199,1 +199,1 @@\n-    @IR(counts = { IRNode.LOAD_VECTOR, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n+    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n@@ -213,1 +213,1 @@\n-    @IR(counts = { IRNode.LOAD_VECTOR, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n+    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n@@ -227,1 +227,1 @@\n-    @IR(counts = { IRNode.LOAD_VECTOR, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n+    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n@@ -239,2 +239,1 @@\n-    \/\/ It would be legal to vectorize this one but it's not currently\n-    \/\/@IR(counts = { IRNode.LOAD_VECTOR, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n+    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n@@ -254,1 +253,1 @@\n-    @IR(failOn = { IRNode.LOAD_VECTOR, IRNode.STORE_VECTOR })\n+    @IR(failOn = { IRNode.LOAD_VECTOR_L, IRNode.STORE_VECTOR })\n@@ -267,1 +266,1 @@\n-    @IR(failOn = { IRNode.LOAD_VECTOR, IRNode.STORE_VECTOR })\n+    @IR(failOn = { IRNode.LOAD_VECTOR_L, IRNode.STORE_VECTOR })\n@@ -281,1 +280,1 @@\n-    @IR(failOn = { IRNode.LOAD_VECTOR, IRNode.STORE_VECTOR })\n+    @IR(failOn = { IRNode.LOAD_VECTOR_L, IRNode.STORE_VECTOR })\n@@ -295,1 +294,1 @@\n-    @IR(counts = { IRNode.LOAD_VECTOR, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n+    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n@@ -308,1 +307,1 @@\n-    @IR(counts = { IRNode.LOAD_VECTOR, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n+    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n@@ -321,1 +320,1 @@\n-    @IR(counts = { IRNode.LOAD_VECTOR, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n+    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n@@ -334,1 +333,1 @@\n-    @IR(counts = { IRNode.LOAD_VECTOR, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n+    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n","filename":"test\/hotspot\/jtreg\/compiler\/c2\/irTests\/TestVectorizationMismatchedAccess.java","additions":14,"deletions":15,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -151,1 +151,1 @@\n-    @IR(counts = {IRNode.ADD_VI, \"> 0\", IRNode.MUL_VF, \"> 0\", IRNode.VECTOR_CAST_F2X, \"> 0\", IRNode.VECTOR_CAST_I2X, \"> 0\"},\n+    @IR(counts = {IRNode.ADD_VI, \"> 0\", IRNode.MUL_VF, \"> 0\", IRNode.VECTOR_CAST_F2I, \"> 0\", IRNode.VECTOR_CAST_I2F, \"> 0\"},\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/TestIndependentPacksWithCyclicDependency.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -331,1 +331,0 @@\n-\n","filename":"test\/hotspot\/jtreg\/gc\/stress\/TestMultiThreadStressRSet.java","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -58,0 +58,1 @@\n+ * @requires vm.debug\n@@ -69,0 +70,1 @@\n+ * @requires vm.debug\n@@ -79,0 +81,1 @@\n+ * @requires vm.debug\n@@ -89,0 +92,1 @@\n+ * @requires vm.debug\n","filename":"test\/hotspot\/jtreg\/gtest\/MetaspaceGtests.java","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -25,1 +25,2 @@\n- * @test 8232069 for ZGC\n+ * @test id=ZSinglegen\n+ * @bug 8232069\n@@ -28,1 +29,1 @@\n- * @requires vm.gc.Z\n+ * @requires vm.gc.ZSinglegen\n@@ -33,1 +34,14 @@\n- * @run driver TestZGCWithCDS\n+ * @run driver TestZGCWithCDS -XX:-ZGenerational\n+ *\/\n+\n+\/*\n+ * @test id=ZGenerational\n+ * @bug 8232069\n+ * @requires vm.cds\n+ * @requires vm.bits == 64\n+ * @requires vm.gc.ZGenerational\n+ * @requires vm.gc.Serial\n+ * @requires vm.gc == null\n+ * @library \/test\/lib \/test\/hotspot\/jtreg\/runtime\/cds\/appcds\n+ * @compile test-classes\/Hello.java\n+ * @run driver TestZGCWithCDS -XX:+ZGenerational\n@@ -44,0 +58,1 @@\n+         String zGenerational = args[0];\n@@ -50,0 +65,1 @@\n+                                        zGenerational,\n@@ -58,0 +74,1 @@\n+                         zGenerational,\n@@ -102,0 +119,1 @@\n+                         zGenerational,\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/TestZGCWithCDS.java","additions":21,"deletions":3,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -103,1 +103,1 @@\n-        runCheck(BadCountsConstraint.create(BadCount.class, \"bad1()\", 1, 1, \"Load\"),\n+        runCheck(BadCountsConstraint.create(BadCount.class, \"bad1()\", 1, 2, \"Load\"),\n@@ -106,3 +106,3 @@\n-                 BadCountsConstraint.create(BadCount.class, \"bad2()\", 2,  1, \"Store\"),\n-                 BadCountsConstraint.create(BadCount.class, \"bad3()\", 1,  1, \"Load\"),\n-                 BadCountsConstraint.create(BadCount.class, \"bad3()\", 2,  1, \"Store\")\n+                 BadCountsConstraint.create(BadCount.class, \"bad2()\", 2,  2, \"Store\"),\n+                 BadCountsConstraint.create(BadCount.class, \"bad3()\", 1,  2, \"Load\"),\n+                 BadCountsConstraint.create(BadCount.class, \"bad3()\", 2,  2, \"Store\")\n@@ -631,4 +631,0 @@\n-                  IRNode.STORE, \"!= 0\",\n-                  IRNode.STORE, \"!=0\",\n-                  IRNode.STORE, \" != 0\",\n-                  IRNode.STORE, \"  !=  0\",\n@@ -825,0 +821,1 @@\n+    int iFld2;\n@@ -826,0 +823,1 @@\n+    int result2;\n@@ -827,1 +825,1 @@\n-    @IR(counts = {IRNode.LOAD, \"!= 1\"}) \/\/ fail\n+    @IR(counts = {IRNode.LOAD, \"> 1000\"}) \/\/ fail\n@@ -831,0 +829,1 @@\n+        result2 = iFld2;\n@@ -834,2 +833,2 @@\n-    @IR(counts = {IRNode.LOAD, \"1\"}) \/\/ fail\n-    @IR(counts = {IRNode.STORE, \"< 1\"})\n+    @IR(counts = {IRNode.LOAD, \"2\"}) \/\/ fail\n+    @IR(counts = {IRNode.STORE, \"< 2\"})\n@@ -838,0 +837,1 @@\n+        result2 = iFld2;\n@@ -843,1 +843,1 @@\n-    @IR(counts = {IRNode.STORE, \" <= 0\"}) \/\/ fail\n+    @IR(counts = {IRNode.STORE, \" <= 1\"}) \/\/ fail\n@@ -846,0 +846,1 @@\n+        result2 = iFld2;\n","filename":"test\/hotspot\/jtreg\/testlibrary_tests\/ir_framework\/tests\/TestIRMatching.java","additions":13,"deletions":12,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -46,0 +46,1 @@\n+import java.util.function.Predicate;\n@@ -104,0 +105,1 @@\n+        map.put(\"vm.jvmci.enabled\", this::vmJvmciEnabled);\n@@ -124,0 +126,4 @@\n+        \/\/ jdk.hasLibgraal is true if the libgraal shared library file is present\n+        map.put(\"jdk.hasLibgraal\", this::hasLibgraal);\n+        \/\/ vm.libgraal.enabled is true if libgraal is used as JIT\n+        map.put(\"vm.libgraal.enabled\", this::isLibgraalEnabled);\n@@ -268,0 +274,14 @@\n+\n+    \/**\n+     * @return true if JVMCI is enabled\n+     *\/\n+    protected String vmJvmciEnabled() {\n+        \/\/ builds with jvmci have this flag\n+        if (\"false\".equals(vmJvmci())) {\n+            return \"false\";\n+        }\n+\n+        return \"\" + Compiler.isJVMCIEnabled();\n+    }\n+\n+\n@@ -297,0 +317,3 @@\n+        Predicate<GC> vmGCProperty = (GC gc) -> (gc.isSupported()\n+                                        && (!isJVMCIEnabled || gc.isSupportedByJVMCICompiler())\n+                                        && (gc.isSelected() || GC.isSelectedErgonomically()));\n@@ -298,4 +321,1 @@\n-            map.put(\"vm.gc.\" + gc.name(),\n-                    () -> \"\" + (gc.isSupported()\n-                            && (!isJVMCIEnabled || gc.isSupportedByJVMCICompiler())\n-                            && (gc.isSelected() || GC.isSelectedErgonomically())));\n+            map.put(\"vm.gc.\" + gc.name(), () -> \"\" + vmGCProperty.test(gc));\n@@ -303,0 +323,11 @@\n+\n+        \/\/ Special handling for ZGC modes\n+        var vmGCZ = vmGCProperty.test(GC.Z);\n+        var genZ = WB.getBooleanVMFlag(\"ZGenerational\");\n+        var genZIsDefault = WB.isDefaultVMFlag(\"ZGenerational\");\n+        \/\/ vm.gc.ZGenerational=true means:\n+        \/\/    vm.gc.Z is true and ZGenerational is either explicitly true, or default\n+        map.put(\"vm.gc.ZGenerational\", () -> \"\" + (vmGCZ && (genZ || genZIsDefault)));\n+        \/\/ vm.gc.ZSinglegen=true means:\n+        \/\/    vm.gc.Z is true and ZGenerational is either explicitly false, or default\n+        map.put(\"vm.gc.ZSinglegen\", () -> \"\" + (vmGCZ && (!genZ || genZIsDefault)));\n@@ -462,0 +493,18 @@\n+    \/**\n+     * Check if the libgraal shared library file is present.\n+     *\n+     * @return true if the libgraal shared library file is present.\n+     *\/\n+    protected String hasLibgraal() {\n+        return \"\" + WB.hasLibgraal();\n+    }\n+\n+    \/**\n+     * Check if libgraal is used as JIT compiler.\n+     *\n+     * @return true if libgraal is used as JIT compiler.\n+     *\/\n+    protected String isLibgraalEnabled() {\n+        return \"\" + Compiler.isLibgraalEnabled();\n+    }\n+\n@@ -689,0 +738,1 @@\n+        Collections.sort(lines);\n","filename":"test\/jtreg-ext\/requires\/VMProps.java","additions":54,"deletions":4,"binary":false,"changes":58,"status":"modified"},{"patch":"@@ -26,0 +26,17 @@\n+#############################################################################\n+#\n+# List of quarantined tests -- tests that should not be run by default, because\n+# they may fail due to known reason. The reason (CR#) must be mandatory specified.\n+#\n+# List items are testnames followed by labels, all MUST BE commented\n+#   as to why they are here and use a label:\n+#     generic-all   Problems on all platforms\n+#     generic-ARCH  Where ARCH is one of: x64, i586, ppc64, ppc64le, s390x etc.\n+#     OSNAME-all    Where OSNAME is one of: linux, windows, macosx, aix\n+#     OSNAME-ARCH   Specific on to one OSNAME and ARCH, e.g. macosx-x64\n+#     OSNAME-REV    Specific on to one OSNAME and REV, e.g. macosx-10.7.4\n+#\n+# More than one label is allowed but must be on the same line.\n+#\n+#############################################################################\n+\n@@ -52,0 +69,2 @@\n+tools\/javac\/lambda\/bytecode\/TestLambdaBytecodeTargetRelease14.java              8312534    linux-i586     fails with assert \"g1ConcurrentMark.cpp: Overflow during reference processing\"\n+tools\/javac\/varargs\/warning\/Warn5.java                                          8312534    linux-i586     fails with assert \"g1ConcurrentMark.cpp: Overflow during reference processing\"\n","filename":"test\/langtools\/ProblemList.txt","additions":19,"deletions":0,"binary":false,"changes":19,"status":"modified"}]}