{"files":[{"patch":"@@ -41,0 +41,1 @@\n+#include \"oops\/klass.hpp\"\n@@ -63,2 +64,0 @@\n-  const int klass_alignment_words = KlassAlignmentInBytes \/ BytesPerWord;\n-\n@@ -69,1 +68,0 @@\n-      metaspace::MetaspaceMinAlignmentWords,\n@@ -82,1 +80,0 @@\n-        klass_alignment_words,\n@@ -107,0 +104,6 @@\n+  UL2(trace, \"Allocate \" SIZE_FORMAT \" words (type %s)...\",\n+      word_size, mdType == Metaspace::ClassType ? \"class\" : \"nonclass\");\n+  MetaWord* p = nullptr;\n+\n+#ifdef ASSERT\n+  \/\/ Sanity checks\n@@ -108,1 +111,26 @@\n-    return class_space_arena()->allocate(word_size);\n+    assert(Metaspace::using_class_space(), \"Sanity\");\n+    assert(word_size >= sizeof(Klass) \/ BytesPerWord, \"odd size for class space allocation (\" SIZE_FORMAT \")\", word_size);\n+  }\n+#endif\n+\n+  if (UseCompactObjectHeaders) {\n+    \/\/ New header mode\n+    if (Metaspace::is_class_space_allocation(mdType)) {\n+      \/\/ Allocate Klass at a location suitable for placing a Klass\n+      p = class_space_arena()->allocate_for_klass(word_size);\n+    } else {\n+      \/\/ Try to steal from class space first. Non-class allocations are typically\n+      \/\/ fine grained, so we may satisfy the allocation from the salvaged alignment\n+      \/\/ gaps in class space.\n+      if (Metaspace::using_class_space()) {\n+        p = class_space_arena()->allocate_from_freeblocks_only(word_size);\n+        if (p != nullptr) {\n+          UL2(trace, \"Stole \" SIZE_FORMAT \" words from class space.\", word_size);\n+          DEBUG_ONLY(InternalStats::inc_num_allocs_stolen_from_class_space();)\n+        }\n+      }\n+      \/\/ Failing that, just use the normal metaspace\n+      if (p == nullptr) {\n+        p = non_class_space_arena()->allocate(word_size);\n+      }\n+    }\n@@ -110,1 +138,6 @@\n-    return non_class_space_arena()->allocate(word_size);\n+    \/\/ Legacy header mode. No special placement logic needed for Klass.\n+    if (Metaspace::is_class_space_allocation(mdType)) {\n+      return class_space_arena()->allocate(word_size);\n+    } else {\n+      return non_class_space_arena()->allocate(word_size);\n+    }\n@@ -112,0 +145,2 @@\n+  UL2(trace, \"Returning \" PTR_FORMAT \".\", p2i(p));\n+  return p;\n@@ -148,2 +183,16 @@\n-  if (Metaspace::using_class_space() && is_class) {\n-    class_space_arena()->deallocate(ptr, word_size);\n+  \/\/ Sanity checks\n+  assert(is_aligned(ptr, BytesPerWord), \"misaligned pointer\");\n+  assert(word_size != 0, \"Invalid size\");\n+\n+  if (UseCompactObjectHeaders) {\n+    \/\/ New header mode\n+    if (Metaspace::using_class_space() && is_class) {\n+      class_space_arena()->deallocate(ptr, word_size);\n+    } else {\n+      \/\/ If we stole this allocation from class space, return it to class space.\n+      if (Metaspace::class_space_contains(ptr)) {\n+        class_space_arena()->deallocate(ptr, word_size);\n+      } else {\n+        non_class_space_arena()->deallocate(ptr, word_size);\n+      }\n+    }\n@@ -151,1 +200,6 @@\n-    non_class_space_arena()->deallocate(ptr, word_size);\n+    \/\/ Legacy header mode\n+    if (Metaspace::using_class_space() && is_class) {\n+      class_space_arena()->deallocate(ptr, word_size);\n+    } else {\n+      non_class_space_arena()->deallocate(ptr, word_size);\n+    }\n","filename":"src\/hotspot\/share\/memory\/classLoaderMetaspace.cpp","additions":63,"deletions":9,"binary":false,"changes":72,"status":"modified"},{"patch":"@@ -876,1 +876,1 @@\n-  return metaspace::chunklevel::MAX_CHUNK_WORD_SIZE;\n+  return metaspace::chunklevel::MAX_CHUNK_WORD_SIZE - KlassAlignmentInWords;\n@@ -1055,4 +1055,3 @@\n-bool Metaspace::contains_non_shared(const void* ptr) {\n-  if (using_class_space() && VirtualSpaceList::vslist_class()->contains((MetaWord*)ptr)) {\n-     return true;\n-  }\n+bool Metaspace::class_space_contains(const void* ptr) {\n+  return using_class_space() && VirtualSpaceList::vslist_class()->contains((MetaWord*)ptr);\n+}\n@@ -1060,1 +1059,3 @@\n-  return VirtualSpaceList::vslist_nonclass()->contains((MetaWord*)ptr);\n+bool Metaspace::contains_non_shared(const void* ptr) {\n+  return class_space_contains(ptr) ||\n+         VirtualSpaceList::vslist_nonclass()->contains((MetaWord*)ptr);\n","filename":"src\/hotspot\/share\/memory\/metaspace.cpp","additions":7,"deletions":6,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -124,0 +124,1 @@\n+  static bool class_space_contains(const void* ptr);\n","filename":"src\/hotspot\/share\/memory\/metaspace.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -365,1 +365,1 @@\n-    assert(word_size >= MinWordSize, \"invalid block size \" SIZE_FORMAT, word_size);\n+    assert(word_size > 0, \"invalid block size \" SIZE_FORMAT, word_size);\n","filename":"src\/hotspot\/share\/memory\/metaspace\/blockTree.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -34,1 +34,2 @@\n-  assert(word_size >= MinWordSize, \"sanity (\" SIZE_FORMAT \")\", word_size);\n+  assert(is_aligned(p, BytesPerWord), \"Bad alignment\");\n+  assert(word_size >= MinWordSize, \"invalid block size (\" SIZE_FORMAT \")\", word_size);\n@@ -50,0 +51,2 @@\n+    \/\/ Small blocks: prefer bin lists; if nothing fitting found, take a bigger block\n+    \/\/ and reuse that.\n@@ -51,0 +54,3 @@\n+    if (p == nullptr) {\n+      p = _tree.remove_block(requested_word_size, &real_size);\n+    }\n","filename":"src\/hotspot\/share\/memory\/metaspace\/freeBlocks.cpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -58,0 +58,9 @@\n+  \/* Number of times an non-class allocation was *\/ \\\n+  \/*  satisfied via deallocated blocks from      *\/ \\\n+  \/*  class space. *\/                               \\\n+  DEBUG_ONLY(x_atomic(num_allocs_stolen_from_class_space)) \\\n+                                                   \\\n+  \/* Number of splinter blocks added from        *\/ \\\n+  \/*  Klass alignment gaps                       *\/ \\\n+  DEBUG_ONLY(x_atomic(num_klass_alignment_splinters_added)) \\\n+                                                    \\\n","filename":"src\/hotspot\/share\/memory\/metaspace\/internalStats.hpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -48,4 +48,4 @@\n-\/\/ Given a net allocation word size and an alignment value, return the raw word size we actually\n-\/\/ allocate internally.\n-inline size_t get_raw_word_size_for_requested_word_size(size_t net_word_size,\n-                                                        size_t alignment_words) {\n+\/\/ Given a net allocation word size, return the raw word size we actually allocate.\n+\/\/ Note: externally visible for gtests.\n+\/\/static\n+inline size_t get_raw_word_size_for_requested_word_size(size_t word_size) {\n@@ -53,5 +53,3 @@\n-  \/\/ The alignment should be between the minimum alignment but cannot be larger than the smallest chunk size\n-  assert(is_power_of_2(alignment_words), \"invalid alignment\");\n-  assert(alignment_words >= MetaspaceMinAlignmentWords &&\n-         alignment_words <= MetaspaceMaxAlignmentWords,\n-         \"invalid alignment (\" SIZE_FORMAT \")\", alignment_words);\n+  \/\/ Deallocated metablocks are kept in a binlist which limits their minimal\n+  \/\/  size to at least the size of a binlist item.\n+  size_t raw_word_size = MAX2(word_size, FreeBlocks::MinWordSize);\n@@ -59,5 +57,3 @@\n-  \/\/ Deallocated metablocks are kept in a binlist which means blocks need to have\n-  \/\/ a minimal size\n-  size_t raw_word_size = MAX2(net_word_size, FreeBlocks::MinWordSize);\n-\n-  raw_word_size = align_up(raw_word_size, alignment_words);\n+  \/\/ Metaspace allocations are aligned to the minimum metaspace alignment\n+  \/\/ (1 word on 64-bit, 2 words on 32-bit)\n+  raw_word_size = align_up(raw_word_size, metaspace::MetaspaceMinAlignmentWords);\n","filename":"src\/hotspot\/share\/memory\/metaspace\/metaspaceAlignment.hpp","additions":10,"deletions":14,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"oops\/klass.hpp\"    \/\/ just for sizeof(Klass)\n@@ -54,0 +55,2 @@\n+constexpr size_t min_reasonable_klass_size = align_up(sizeof(Klass), BytesPerWord) \/ BytesPerWord;\n+\n@@ -111,1 +114,1 @@\n-MetaspaceArena::MetaspaceArena(ChunkManager* chunk_manager, const ArenaGrowthPolicy* growth_policy, int alignment_words,\n+MetaspaceArena::MetaspaceArena(ChunkManager* chunk_manager, const ArenaGrowthPolicy* growth_policy,\n@@ -118,1 +121,0 @@\n-  _alignment_words(alignment_words),\n@@ -219,0 +221,82 @@\n+MetaWord* MetaspaceArena::allocate_from_freeblocks_only(size_t word_size) {\n+  MutexLocker cl(lock(), Mutex::_no_safepoint_check_flag);\n+  const size_t raw_word_size = get_raw_word_size_for_requested_word_size(word_size);\n+  return allocate_from_freeblocks_locked(raw_word_size);\n+}\n+\n+MetaWord* MetaspaceArena::allocate_for_klass(size_t word_size) {\n+  MutexLocker cl(lock(), Mutex::_no_safepoint_check_flag);\n+  assert(word_size >= min_reasonable_klass_size,\n+         \"Suspiciously small for Klass: \" SIZE_FORMAT, word_size);\n+\n+  UL2(trace, \"Allocate for Klass: \" SIZE_FORMAT \" words.\", word_size);\n+\n+  \/\/ Klass alignment must be <= smallest chunk size to guarantee that a newly created chunk\n+  \/\/ will always allocate at a Klass-suitable address.\n+  assert((size_t)KlassAlignmentInWords <= chunklevel::MIN_CHUNK_WORD_SIZE, \"Must be\");\n+\n+  size_t alignment_padding_needed = 0;\n+  if (current_chunk() != nullptr) {\n+    const size_t used_words = current_chunk()->used_words();\n+    const size_t next_klass_offset = align_up(used_words, KlassAlignmentInWords);\n+    alignment_padding_needed = next_klass_offset - used_words;\n+  }\n+  const size_t total_word_size = alignment_padding_needed + word_size;\n+\n+  MetaWord* k = nullptr;\n+  MetaWord* all = allocate_inner(total_word_size);\n+\n+  if (all != nullptr) {\n+\n+    MetaWord* salvage = nullptr;\n+    size_t salvage_words = alignment_padding_needed;\n+\n+    if (is_aligned(all, KlassAlignmentInBytes)) {\n+      \/\/ The new allocation is already properly aligned. Happens if the allocation caused\n+      \/\/ a new chunk to be created. Salvage the trailing alignment gap.\n+      k = all;\n+      salvage = k + word_size;\n+    } else {\n+      \/\/ The new allocation is not aligned; we expanded the current chunk. Salvage the leading\n+      \/\/ alignment padding.\n+      k = all + alignment_padding_needed;\n+      salvage = all;\n+    }\n+\n+    \/\/ If alignment gap is large enough to be reused, squirrel it away\n+    if (alignment_padding_needed > FreeBlocks::MinWordSize) {\n+      UL2(trace, \"Save off alignment gap \" PTR_FORMAT \", \" SIZE_FORMAT \" words.\",\n+          p2i(salvage), alignment_padding_needed);\n+      DEBUG_ONLY(InternalStats::inc_num_klass_alignment_splinters_added();)\n+      \/\/ Note: Don't use deallocate here; it is only for external use since it does size\n+      \/\/ adjustment\n+      add_allocation_to_fbl(salvage, alignment_padding_needed);\n+    }\n+\n+  }\n+\n+  assert_is_aligned(k, KlassAlignmentInWords);\n+  UL2(trace, \"Returning \" PTR_FORMAT \".\", p2i(k));\n+  return k;\n+}\n+\n+\/\/ Allocate from freeblocks\n+MetaWord* MetaspaceArena::allocate_from_freeblocks_locked(size_t word_size) {\n+  DEBUG_ONLY(verify_locked();)\n+  MetaWord* p = nullptr;\n+  assert_lock_strong(lock());\n+  UL2(trace, \"Attempt to take requested \" SIZE_FORMAT \" words from fbl.\", word_size);\n+  if (_fbl != nullptr && !_fbl->is_empty()) {\n+    p = _fbl->remove_block(word_size);\n+    if (p != nullptr) {\n+      DEBUG_ONLY(InternalStats::inc_num_allocs_from_deallocated_blocks();)\n+      UL2(trace, \"taken from fbl (now: %d, \" SIZE_FORMAT \").\",\n+          _fbl->count(), _fbl->total_size());\n+      \/\/ Note: free blocks in freeblock dictionary still count as \"used\" as far as statistics go;\n+      \/\/ therefore we have no need to adjust any usage counters (see epilogue of allocate_inner())\n+      \/\/ and can just return here.\n+    }\n+  }\n+  return p;\n+}\n+\n@@ -230,1 +314,1 @@\n-  const size_t raw_word_size = get_raw_word_size_for_requested_word_size(requested_word_size, _alignment_words);\n+  const size_t raw_word_size = get_raw_word_size_for_requested_word_size(requested_word_size);\n@@ -233,11 +317,3 @@\n-  if (_fbl != nullptr && !_fbl->is_empty()) {\n-    p = _fbl->remove_block(raw_word_size);\n-    if (p != nullptr) {\n-      DEBUG_ONLY(InternalStats::inc_num_allocs_from_deallocated_blocks();)\n-      UL2(trace, \"taken from fbl (now: %d, \" SIZE_FORMAT \").\",\n-          _fbl->count(), _fbl->total_size());\n-      \/\/ Note: free blocks in freeblock dictionary still count as \"used\" as far as statistics go;\n-      \/\/ therefore we have no need to adjust any usage counters (see epilogue of allocate_inner())\n-      \/\/ and can just return here.\n-      return p;\n-    }\n+  p = allocate_from_freeblocks_locked(raw_word_size);\n+  if (p != nullptr) {\n+    return p;\n@@ -274,1 +350,1 @@\n-  const size_t raw_word_size = get_raw_word_size_for_requested_word_size(requested_word_size, _alignment_words);\n+  const size_t raw_word_size = get_raw_word_size_for_requested_word_size(requested_word_size);\n@@ -371,1 +447,1 @@\n-  size_t raw_word_size = get_raw_word_size_for_requested_word_size(word_size, _alignment_words);\n+  size_t raw_word_size = get_raw_word_size_for_requested_word_size(word_size);\n@@ -485,2 +561,2 @@\n-  st->print_cr(\"growth-policy \" PTR_FORMAT \", alignment %d, lock \" PTR_FORMAT \", cm \" PTR_FORMAT \", fbl \" PTR_FORMAT,\n-                p2i(_growth_policy), _alignment_words * BytesPerWord, p2i(_lock), p2i(_chunk_manager), p2i(_fbl));\n+  st->print_cr(\"growth-policy \" PTR_FORMAT \", lock \" PTR_FORMAT \", cm \" PTR_FORMAT \", fbl \" PTR_FORMAT,\n+                p2i(_growth_policy), p2i(_lock), p2i(_chunk_manager), p2i(_fbl));\n","filename":"src\/hotspot\/share\/memory\/metaspace\/metaspaceArena.cpp","additions":94,"deletions":18,"binary":false,"changes":112,"status":"modified"},{"patch":"@@ -97,3 +97,0 @@\n-  \/\/ Alignment alignment, in words.\n-  const int _alignment_words;\n-\n@@ -168,0 +165,3 @@\n+  \/\/ Allocate from freeblocks\n+  MetaWord* allocate_from_freeblocks_locked(size_t wordsize);\n+\n@@ -170,1 +170,1 @@\n-  MetaspaceArena(ChunkManager* chunk_manager, const ArenaGrowthPolicy* growth_policy, int alignment_words,\n+  MetaspaceArena(ChunkManager* chunk_manager, const ArenaGrowthPolicy* growth_policy,\n@@ -184,0 +184,6 @@\n+  \/\/ Allocate a block suitable for placing a Klass\n+  MetaWord* allocate_for_klass(size_t word_size);\n+\n+  \/\/ Allocate from the heap of salvaged (prematurely deallocated) blocks\n+  MetaWord* allocate_from_freeblocks_only(size_t word_size);\n+\n","filename":"src\/hotspot\/share\/memory\/metaspace\/metaspaceArena.hpp","additions":10,"deletions":4,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -56,0 +56,8 @@\n+MetaWord* MetaspaceTestArena::allocate_for_klass(size_t word_size) {\n+  return _arena->allocate_for_klass(word_size);\n+}\n+\n+MetaWord* MetaspaceTestArena::allocate_from_freeblocks_only(size_t word_size) {\n+  return _arena->allocate_from_freeblocks_only(word_size);\n+}\n+\n@@ -60,0 +68,5 @@\n+\/\/ Update statistics. This walks all in-use chunks.\n+void MetaspaceTestArena::add_to_statistics(ArenaStats* out) const {\n+  return _arena->add_to_statistics(out);\n+}\n+\n@@ -101,1 +114,1 @@\n-    arena = new MetaspaceArena(_context->cm(), growth_policy, MetaspaceMinAlignmentWords,\n+    arena = new MetaspaceArena(_context->cm(), growth_policy,\n","filename":"src\/hotspot\/share\/memory\/metaspace\/testHelpers.cpp","additions":14,"deletions":1,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -46,0 +46,1 @@\n+struct ArenaStats;\n@@ -65,0 +66,2 @@\n+  MetaWord* allocate_for_klass(size_t word_size);\n+  MetaWord* allocate_from_freeblocks_only(size_t word_size);\n@@ -66,1 +69,1 @@\n-\n+  void add_to_statistics(ArenaStats* out) const;\n","filename":"src\/hotspot\/share\/memory\/metaspace\/testHelpers.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -232,1 +232,2 @@\n-  UL2(debug, \"born (word_size \" SIZE_FORMAT \").\", _word_size);\n+  UL2(debug, \"born: [\" PTR_FORMAT \"..\" PTR_FORMAT \"), (word_size \" SIZE_FORMAT \").\",\n+      p2i(_rs.base()), p2i(_rs.end()), _word_size);\n","filename":"src\/hotspot\/share\/memory\/metaspace\/virtualSpaceNode.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+int KlassAlignmentInWords    = -1;\n","filename":"src\/hotspot\/share\/oops\/compressedKlass.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+extern int KlassAlignmentInWords;\n@@ -55,0 +56,1 @@\n+const int KlassAlignmentInWords = KlassAlignmentInBytes \/ BytesPerWord;\n","filename":"src\/hotspot\/share\/oops\/compressedKlass.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -198,1 +198,3 @@\n-  assert(is_aligned(p, KlassAlignmentInBytes), \"metaspace returned badly aligned memory.\");\n+  assert(is_aligned(p, KlassAlignmentInBytes),\n+         \"metaspace returned badly aligned memory (\" PTR_FORMAT \"), alignment required: %u\",\n+         p2i(p), (unsigned)KlassAlignmentInBytes);\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1503,0 +1503,2 @@\n+  assert(is_aligned(KlassAlignmentInBytes, BytesPerWord), \"Must be at least word-sized\");\n+  KlassAlignmentInWords = KlassAlignmentInBytes \/ BytesPerWord;\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -30,0 +30,2 @@\n+#include \"memory\/metaspace\/freeBlocks.hpp\"\n+#include \"memory\/metaspace\/internalStats.hpp\"\n@@ -35,0 +37,1 @@\n+#include \"oops\/klass.hpp\" \/\/ sizeof\n@@ -45,0 +48,2 @@\n+#include \"unittest.hpp\"\n+\n@@ -47,0 +52,1 @@\n+using metaspace::FreeBlocks;\n@@ -50,0 +56,1 @@\n+using metaspace::MetaspaceTestArena;\n@@ -61,1 +68,0 @@\n-  int _alignment_words;\n@@ -64,2 +70,1 @@\n-  void initialize(const ArenaGrowthPolicy* growth_policy, int alignment_words,\n-                  const char* name = \"gtest-MetaspaceArena\") {\n+  void initialize(const ArenaGrowthPolicy* growth_policy, const char* name = \"gtest-MetaspaceArena\") {\n@@ -72,1 +77,1 @@\n-      _arena = new MetaspaceArena(&_context.cm(), _growth_policy, alignment_words, _lock, &_used_words_counter, name);\n+      _arena = new MetaspaceArena(&_context.cm(), _growth_policy, _lock, &_used_words_counter, name);\n@@ -86,1 +91,1 @@\n-    initialize(ArenaGrowthPolicy::policy_for_space_type(space_type, is_class), metaspace::MetaspaceMinAlignmentWords, name);\n+    initialize(ArenaGrowthPolicy::policy_for_space_type(space_type, is_class), name);\n@@ -94,1 +99,1 @@\n-    initialize(growth_policy, metaspace::MetaspaceMinAlignmentWords, name);\n+    initialize(growth_policy, name);\n@@ -273,1 +278,1 @@\n-    allocated += metaspace::get_raw_word_size_for_requested_word_size(s, metaspace::MetaspaceMinAlignmentWords);\n+    allocated += metaspace::get_raw_word_size_for_requested_word_size(s);\n@@ -330,1 +335,1 @@\n-    allocated += metaspace::get_raw_word_size_for_requested_word_size(s, metaspace::MetaspaceMinAlignmentWords);\n+    allocated += metaspace::get_raw_word_size_for_requested_word_size(s);\n@@ -594,1 +599,1 @@\n-    words_allocated += metaspace::get_raw_word_size_for_requested_word_size(alloc_words, metaspace::MetaspaceMinAlignmentWords);\n+    words_allocated += metaspace::get_raw_word_size_for_requested_word_size(alloc_words);\n@@ -783,0 +788,112 @@\n+\n+template <class T>\n+static T calc_envelope_min(T value, float width) {\n+  return (T)((double)value * (1.0f - width));\n+}\n+\n+template <class T>\n+static T calc_envelope_max(T value, float width) {\n+  return (T)((double)value * (1.0f + width));\n+}\n+\n+\/\/ test_with_chunk_turnover: if true, we test allocation for an arena that allocates multiple chunks.\n+\/\/ if false, we test allocation for one gigantic chunk (the easier, better predictable scenario).\n+static void test_allocate_for_klass(bool test_with_chunk_turnover) {\n+\n+  if (!Metaspace::using_class_space()) {\n+    return; \/\/ Skip for compressed class pointers off\n+  }\n+\n+  if (!UseCompactObjectHeaders) {\n+    return; \/\/ Skip for traditional headers.\n+  }\n+\n+  \/\/ Repeatedly call allocate_for_klass with random but legit looking Klass sizes.\n+  \/\/ Test that each all allocations succeed, are aligned correctly, and that the remainder\n+  \/\/ alignment gaps are salvaged.\n+\n+  \/\/ The different MetaspaceType will cause us to exercise different chunk allocation policies and\n+  \/\/ exercise allocate_for_klass with chunk turnover\n+  MetaspaceGtestContext context;\n+  MetaspaceTestArena* arena = context.create_arena(\n+      test_with_chunk_turnover ?\n+          Metaspace::ReflectionMetaspaceType : \/\/ starts with a small chunk, working itself up\n+          Metaspace::BootMetaspaceType         \/\/ starts with a big chunk\n+          );\n+\n+  const size_t klass_slot_size_words = KlassAlignmentInWords;\n+  const size_t klass_size_words = align_up(sizeof(Klass), BytesPerWord) \/ BytesPerWord;\n+  assert(klass_size_words < klass_slot_size_words, \"Sanity\");\n+\n+  \/\/ We allocate blocks that are always < 1 klass_slot_size_words in size. Therefore we can exactly determine\n+  \/\/ the remainder size regardless of the underlying chunk geometry (since one slot is always smaller than\n+  \/\/ the smallest chunk)\n+  RandSizeGenerator rgen(1 + sizeof(Klass) \/ BytesPerWord, klass_slot_size_words);\n+\n+  size_t allocated_words = 0;\n+  size_t last_word_size = 0;\n+  size_t expected_used_words = 0;\n+  size_t expected_freeblock_words = 0;\n+  uintx expected_freeblock_num = 0;\n+  for (int i = 0; i < 1000; i++) {\n+    size_t word_size = rgen.get();\n+    assert(word_size <= klass_slot_size_words && word_size >= klass_size_words, \"Sanity\");\n+\n+    MetaWord* p = arena->allocate_for_klass(word_size);\n+    ASSERT_NOT_NULL(p);\n+    ASSERT_TRUE(is_aligned(p, KlassAlignmentInBytes));\n+\n+    allocated_words += word_size;\n+    const size_t remainder_last_block = last_word_size > 0 ? klass_slot_size_words - last_word_size : 0;\n+    expected_used_words += (word_size + remainder_last_block);\n+\n+    if (remainder_last_block >= FreeBlocks::MinWordSize) {\n+      expected_freeblock_words += remainder_last_block;\n+      expected_freeblock_num++;\n+    }\n+\n+    last_word_size = word_size;\n+  }\n+\n+  \/\/ Test\n+  ArenaStats stats;\n+  arena->add_to_statistics(&stats);\n+\n+  \/\/ Check size of salvaged space.\n+  \/\/ Note: due to chunk turnover, exact numbers for used\/salvaged can be lower (because on chunk turnover, only *committed*\n+  \/\/ space of old chunk is salvaged). Predicting that gets intricate and makes the test brittle, therefore I just require that\n+  \/\/ we are within 30% of the target.\n+  const size_t expected_used_words_min = calc_envelope_min(expected_used_words, 0.3f);\n+  const size_t expected_used_words_max = calc_envelope_max(expected_used_words, 0.3f);\n+\n+  const size_t expected_freeblock_words_min = calc_envelope_min(expected_freeblock_words, 0.3f);\n+  const size_t expected_freeblock_words_max = calc_envelope_max(expected_freeblock_words, 0.3f);\n+\n+  const uintx expected_freeblock_num_min = calc_envelope_min(expected_freeblock_num, 0.3f);\n+  const uintx expected_freeblock_num_max = calc_envelope_max(expected_freeblock_num, 0.3f);\n+\n+  EXPECT_GE(stats.totals()._used_words, expected_used_words_min);\n+  EXPECT_LE(stats.totals()._used_words, expected_used_words_max);\n+\n+  EXPECT_GE(stats._free_blocks_word_size, expected_freeblock_words_min);\n+  EXPECT_LE(stats._free_blocks_word_size, expected_freeblock_words_max);\n+\n+  EXPECT_GE(stats._free_blocks_num, expected_freeblock_num_min);\n+  EXPECT_LE(stats._free_blocks_num, expected_freeblock_num_max);\n+\n+  \/\/ Now check that we can allocate the salvaged space via allocate_from_freeblocks_only;\n+  size_t allocated_from_fbl = 0;\n+  for (MetaWord* p = arena->allocate_from_freeblocks_only(2); p != nullptr; p = arena->allocate_from_freeblocks_only(2)) {\n+    allocated_from_fbl += 2;\n+  }\n+\n+  EXPECT_GE(allocated_from_fbl, expected_freeblock_words_min);\n+  EXPECT_LE(allocated_from_fbl, expected_freeblock_words_max);\n+\n+  delete arena;\n+\n+}\n+\n+TEST_VM(metaspace, MetaspaceArena_test_klass_allocation_1) { test_allocate_for_klass(Metaspace::StandardMetaspaceType); }\n+TEST_VM(metaspace, MetaspaceArena_test_klass_allocation_2) { test_allocate_for_klass(Metaspace::BootMetaspaceType); }\n+\n","filename":"test\/hotspot\/gtest\/metaspace\/test_metaspacearena.cpp","additions":126,"deletions":9,"binary":false,"changes":135,"status":"modified"},{"patch":"@@ -37,0 +37,2 @@\n+\n+\n@@ -42,0 +44,2 @@\n+#include \"unittest.hpp\"\n+\n@@ -60,3 +64,0 @@\n-  const SizeRange _allocation_range;\n-  const int _alignment_words;\n-\n@@ -64,0 +65,1 @@\n+\n@@ -65,0 +67,2 @@\n+\n+  const SizeRange _allocation_range;\n@@ -86,2 +90,1 @@\n-  MemRangeCounter _alloc_count_net; \/\/ net used bytes\n-  MemRangeCounter _alloc_count_raw; \/\/ net used bytes + internal overhead\n+  MemRangeCounter _alloc_count;\n@@ -99,2 +102,2 @@\n-    assert(_dealloc_count.total_size() <= _alloc_count_net.total_size() &&\n-           _dealloc_count.count() <= _alloc_count_net.count(), \"Sanity\");\n+    assert(_dealloc_count.total_size() <= _alloc_count.total_size() &&\n+           _dealloc_count.count() <= _alloc_count.count(), \"Sanity\");\n@@ -116,1 +119,1 @@\n-    const size_t at_least_allocated = _alloc_count_net.total_size() - _dealloc_count.total_size();\n+    const size_t at_least_allocated = _alloc_count.total_size() - _dealloc_count.total_size();\n@@ -119,6 +122,3 @@\n-    size_t max_word_overhead_per_alloc = align_up(4, _alignment_words);\n-    \/\/ Guard fences come as a separate, secondary block\n-    if (metaspace::Settings::use_allocation_guard()) {\n-      max_word_overhead_per_alloc *= 2;\n-    }\n-    const size_t at_most_allocated = _alloc_count_raw.total_size() + max_word_overhead_per_alloc * _alloc_count_raw.count();\n+    const size_t max_word_overhead_per_alloc =\n+        4 + (metaspace::Settings::use_allocation_guard() ? 4 : 0);\n+    const size_t at_most_allocated = _alloc_count.total_size() + max_word_overhead_per_alloc * _alloc_count.count();\n@@ -128,0 +128,1 @@\n+\n@@ -134,1 +135,1 @@\n-  MetaspaceArenaTestBed(ChunkManager* cm, const ArenaGrowthPolicy* alloc_sequence, int alignment_words,\n+  MetaspaceArenaTestBed(ChunkManager* cm, const ArenaGrowthPolicy* alloc_sequence,\n@@ -136,2 +137,0 @@\n-    _allocation_range(allocation_range),\n-    _alignment_words(alignment_words),\n@@ -140,0 +139,1 @@\n+    _allocation_range(allocation_range),\n@@ -142,1 +142,1 @@\n-    _alloc_count_net(),\n+    _alloc_count(),\n@@ -149,1 +149,1 @@\n-    _arena = new MetaspaceArena(cm, alloc_sequence, alignment_words, _lock, used_words_counter, \"gtest-MetaspaceArenaTestBed-sm\");\n+    _arena = new MetaspaceArena(cm, alloc_sequence, _lock, used_words_counter, \"gtest-MetaspaceArenaTestBed-sm\");\n@@ -172,2 +172,2 @@\n-  size_t words_allocated() const        { return _alloc_count_net.total_size(); }\n-  int num_allocations() const           { return _alloc_count_net.count(); }\n+  size_t words_allocated() const        { return _alloc_count.total_size(); }\n+  int num_allocations() const           { return _alloc_count.count(); }\n@@ -177,4 +177,0 @@\n-  size_t calc_expected_usage_for_allocated_words(size_t word_size) {\n-    return metaspace::get_raw_word_size_for_requested_word_size(word_size, _alignment_words);\n-  }\n-\n@@ -186,1 +182,1 @@\n-      EXPECT_TRUE(is_aligned(p, _alignment_words * BytesPerWord));\n+      EXPECT_TRUE(is_aligned(p, sizeof(MetaWord)));\n@@ -193,3 +189,2 @@\n-      _alloc_count_net.add(word_size);\n-      _alloc_count_raw.add(calc_expected_usage_for_allocated_words(word_size));\n-      if ((_alloc_count_net.count() % 20) == 0) {\n+      _alloc_count.add(word_size);\n+      if ((_alloc_count.count() % 20) == 0) {\n@@ -237,1 +232,1 @@\n-  void create_new_test_bed_at(int slotindex, const ArenaGrowthPolicy* growth_policy, int alignment_words, SizeRange allocation_range) {\n+  void create_new_test_bed_at(int slotindex, const ArenaGrowthPolicy* growth_policy, SizeRange allocation_range) {\n@@ -239,1 +234,1 @@\n-    MetaspaceArenaTestBed* bed = new MetaspaceArenaTestBed(&_context.cm(), growth_policy, alignment_words,\n+    MetaspaceArenaTestBed* bed = new MetaspaceArenaTestBed(&_context.cm(), growth_policy,\n@@ -250,4 +245,1 @@\n-    const int alignment_bytes =\n-        1 << IntRange(metaspace::LogMetaspaceMinimalAlignment,\n-                      metaspace::LogMetaspaceMinimalAlignment + 7).random_value(); \/\/ zw 8 byte and 1K\n-    create_new_test_bed_at(slotindex, growth_policy, alignment_bytes \/ BytesPerWord, allocation_range);\n+    create_new_test_bed_at(slotindex, growth_policy, allocation_range);\n@@ -266,0 +258,9 @@\n+  \/\/ Create test beds for all slots\n+  void create_all_test_beds() {\n+    for (int slot = 0; slot < _testbeds.size(); slot++) {\n+      if (_testbeds.slot_is_null(slot)) {\n+        create_random_test_bed_at(slot);\n+      }\n+    }\n+  }\n+\n@@ -301,1 +302,1 @@\n-                bed->calc_expected_usage_for_allocated_words(bed->size_of_last_failed_allocation()));\n+                metaspace::get_raw_word_size_for_requested_word_size(bed->size_of_last_failed_allocation()));\n","filename":"test\/hotspot\/gtest\/metaspace\/test_metaspacearena_stress.cpp","additions":37,"deletions":36,"binary":false,"changes":73,"status":"modified"},{"patch":"@@ -380,2 +380,0 @@\n- -runtime\/Metaspace\/elastic\/TestMetaspaceAllocationMT1.java \\\n- -runtime\/Metaspace\/elastic\/TestMetaspaceAllocationMT2.java \\\n","filename":"test\/hotspot\/jtreg\/TEST.groups","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"}]}