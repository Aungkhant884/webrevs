{"files":[{"patch":"@@ -2,1 +2,1 @@\n-project=jdk\n+project=lilliput\n@@ -7,1 +7,1 @@\n-error=author,committer,reviewers,merge,issues,executable,symlink,message,hg-tag,whitespace,problemlists\n+error=author,committer,reviewers,executable,symlink,message,hg-tag,whitespace,problemlists\n@@ -21,4 +21,1 @@\n-[checks \"merge\"]\n-message=Merge\n-\n-reviewers=1\n+committers=1\n@@ -31,3 +28,0 @@\n-[checks \"issues\"]\n-pattern=^([124-8][0-9]{6}): (\\S.*)$\n-\n","filename":".jcheck\/conf","additions":3,"deletions":9,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -7185,1 +7185,1 @@\n-instruct loadNKlass(iRegNNoSp dst, memory4 mem)\n+instruct loadNKlass(iRegNNoSp dst, memory4 mem, rFlagsReg cr)\n@@ -7188,0 +7188,1 @@\n+  effect(TEMP_DEF dst, KILL cr);\n@@ -7192,4 +7193,6 @@\n-\n-  ins_encode(aarch64_enc_ldrw(dst, mem));\n-\n-  ins_pipe(iload_reg_mem);\n+  ins_encode %{\n+    assert($mem$$disp == oopDesc::klass_offset_in_bytes(), \"expect correct offset\");\n+    assert($mem$$index$$Register == noreg, \"expect no index\");\n+    __ load_nklass($dst$$Register, $mem$$base$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":8,"deletions":5,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -1638,1 +1638,2 @@\n-  load_klass(obj, obj);\n+  load_klass(rscratch1, obj);\n+  mov(obj, rscratch1);\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -48,0 +48,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n@@ -4030,6 +4031,32 @@\n-void MacroAssembler::load_klass(Register dst, Register src) {\n-  if (UseCompressedClassPointers) {\n-    ldrw(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n-    decode_klass_not_null(dst);\n-  } else {\n-    ldr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+\/\/ Loads the obj's Klass* into dst.\n+\/\/ src and dst must be distinct registers\n+\/\/ Preserves all registers (incl src, rscratch1 and rscratch2), but clobbers condition flags\n+void MacroAssembler::load_nklass(Register dst, Register src) {\n+  assert(UseCompressedClassPointers, \"expects UseCompressedClassPointers\");\n+\n+  assert_different_registers(src, dst);\n+\n+  Label slow, done;\n+\n+  \/\/ Check if we can take the (common) fast path, if obj is unlocked.\n+  ldr(dst, Address(src, oopDesc::mark_offset_in_bytes()));\n+  eor(dst, dst, markWord::unlocked_value);\n+  tst(dst, markWord::lock_mask_in_place);\n+  br(Assembler::NE, slow);\n+\n+  \/\/ Fast-path: shift and decode Klass*.\n+  lsr(dst, dst, markWord::klass_shift);\n+  b(done);\n+\n+  bind(slow);\n+  RegSet saved_regs = RegSet::of(lr);\n+  \/\/ We need r0 as argument and return register for the call. Preserve it, if necessary.\n+  if (dst != r0) {\n+    saved_regs += RegSet::of(r0);\n+  }\n+  push(saved_regs, sp);\n+  mov(r0, src);\n+  assert(StubRoutines::load_nklass() != NULL, \"Must have stub\");\n+  far_call(RuntimeAddress(StubRoutines::load_nklass()));\n+  if (dst != r0) {\n+    mov(dst, r0);\n@@ -4037,0 +4064,7 @@\n+  pop(saved_regs, sp);\n+  bind(done);\n+}\n+\n+void MacroAssembler::load_klass(Register dst, Register src) {\n+  load_nklass(dst, src);\n+  decode_klass_not_null(dst);\n@@ -4070,14 +4104,10 @@\n-  if (UseCompressedClassPointers) {\n-    ldrw(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));\n-    if (CompressedKlassPointers::base() == NULL) {\n-      cmp(trial_klass, tmp, LSL, CompressedKlassPointers::shift());\n-      return;\n-    } else if (((uint64_t)CompressedKlassPointers::base() & 0xffffffff) == 0\n-               && CompressedKlassPointers::shift() == 0) {\n-      \/\/ Only the bottom 32 bits matter\n-      cmpw(trial_klass, tmp);\n-      return;\n-    }\n-    decode_klass_not_null(tmp);\n-  } else {\n-    ldr(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));\n+  assert(UseCompressedClassPointers, \"Lilliput\");\n+  load_nklass(tmp, oop);\n+  if (CompressedKlassPointers::base() == NULL) {\n+    cmp(trial_klass, tmp, LSL, CompressedKlassPointers::shift());\n+    return;\n+  } else if (((uint64_t)CompressedKlassPointers::base() & 0xffffffff) == 0\n+             && CompressedKlassPointers::shift() == 0) {\n+    \/\/ Only the bottom 32 bits matter\n+    cmpw(trial_klass, tmp);\n+    return;\n@@ -4085,0 +4115,1 @@\n+  decode_klass_not_null(tmp);\n@@ -4088,18 +4119,0 @@\n-void MacroAssembler::store_klass(Register dst, Register src) {\n-  \/\/ FIXME: Should this be a store release?  concurrent gcs assumes\n-  \/\/ klass length is valid if klass field is not null.\n-  if (UseCompressedClassPointers) {\n-    encode_klass_not_null(src);\n-    strw(src, Address(dst, oopDesc::klass_offset_in_bytes()));\n-  } else {\n-    str(src, Address(dst, oopDesc::klass_offset_in_bytes()));\n-  }\n-}\n-\n-void MacroAssembler::store_klass_gap(Register dst, Register src) {\n-  if (UseCompressedClassPointers) {\n-    \/\/ Store to klass gap in destination\n-    strw(src, Address(dst, oopDesc::klass_gap_offset_in_bytes()));\n-  }\n-}\n-\n@@ -4240,0 +4253,14 @@\n+\/\/ Returns a static string\n+const char* MacroAssembler::describe_klass_decode_mode(MacroAssembler::KlassDecodeMode mode) {\n+  switch (mode) {\n+  case KlassDecodeNone: return \"none\";\n+  case KlassDecodeZero: return \"zero\";\n+  case KlassDecodeXor:  return \"xor\";\n+  case KlassDecodeMovk: return \"movk\";\n+  default:\n+    ShouldNotReachHere();\n+  }\n+  return NULL;\n+}\n+\n+\/\/ Return the current narrow Klass pointer decode mode.\n@@ -4241,2 +4268,4 @@\n-  assert(UseCompressedClassPointers, \"not using compressed class pointers\");\n-  assert(Metaspace::initialized(), \"metaspace not initialized yet\");\n+  if (_klass_decode_mode == KlassDecodeNone) {\n+    \/\/ First time initialization\n+    assert(UseCompressedClassPointers, \"not using compressed class pointers\");\n+    assert(Metaspace::initialized(), \"metaspace not initialized yet\");\n@@ -4244,2 +4273,5 @@\n-  if (_klass_decode_mode != KlassDecodeNone) {\n-    return _klass_decode_mode;\n+    _klass_decode_mode = klass_decode_mode_for_base(CompressedKlassPointers::base());\n+    guarantee(_klass_decode_mode != KlassDecodeNone,\n+              PTR_FORMAT \" is not a valid encoding base on aarch64\",\n+              p2i(CompressedKlassPointers::base()));\n+    log_info(metaspace)(\"klass decode mode initialized: %s\", describe_klass_decode_mode(_klass_decode_mode));\n@@ -4247,0 +4279,2 @@\n+  return _klass_decode_mode;\n+}\n@@ -4248,2 +4282,4 @@\n-  assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift()\n-         || 0 == CompressedKlassPointers::shift(), \"decode alg wrong\");\n+\/\/ Given an arbitrary base address, return the KlassDecodeMode that would be used. Return KlassDecodeNone\n+\/\/ if base address is not valid for encoding.\n+MacroAssembler::KlassDecodeMode MacroAssembler::klass_decode_mode_for_base(address base) {\n+  assert(CompressedKlassPointers::shift() != 0, \"not lilliput?\");\n@@ -4251,2 +4287,4 @@\n-  if (CompressedKlassPointers::base() == NULL) {\n-    return (_klass_decode_mode = KlassDecodeZero);\n+  const uint64_t base_u64 = (uint64_t) base;\n+\n+  if (base_u64 == 0) {\n+    return KlassDecodeZero;\n@@ -4255,7 +4293,3 @@\n-  if (operand_valid_for_logical_immediate(\n-        \/*is32*\/false, (uint64_t)CompressedKlassPointers::base())) {\n-    const uint64_t range_mask =\n-      (1ULL << log2i(CompressedKlassPointers::range())) - 1;\n-    if (((uint64_t)CompressedKlassPointers::base() & range_mask) == 0) {\n-      return (_klass_decode_mode = KlassDecodeXor);\n-    }\n+  if (operand_valid_for_logical_immediate(false, base_u64) &&\n+      ((base_u64 & (KlassEncodingMetaspaceMax - 1)) == 0)) {\n+    return KlassDecodeXor;\n@@ -4264,4 +4298,4 @@\n-  const uint64_t shifted_base =\n-    (uint64_t)CompressedKlassPointers::base() >> CompressedKlassPointers::shift();\n-  guarantee((shifted_base & 0xffff0000ffffffff) == 0,\n-            \"compressed class base bad alignment\");\n+  const uint64_t shifted_base = base_u64 >> CompressedKlassPointers::shift();\n+  if ((shifted_base & 0xffff0000ffffffff) == 0) {\n+    return KlassDecodeMovk;\n+  }\n@@ -4269,1 +4303,1 @@\n-  return (_klass_decode_mode = KlassDecodeMovk);\n+  return KlassDecodeNone;\n@@ -4273,0 +4307,2 @@\n+  assert (UseCompressedClassPointers, \"should only be used for compressed headers\");\n+  assert(CompressedKlassPointers::shift() != 0, \"not lilliput?\");\n@@ -4275,5 +4311,1 @@\n-    if (CompressedKlassPointers::shift() != 0) {\n-      lsr(dst, src, LogKlassAlignmentInBytes);\n-    } else {\n-      if (dst != src) mov(dst, src);\n-    }\n+    lsr(dst, src, LogKlassAlignmentInBytes);\n@@ -4283,6 +4315,2 @@\n-    if (CompressedKlassPointers::shift() != 0) {\n-      eor(dst, src, (uint64_t)CompressedKlassPointers::base());\n-      lsr(dst, dst, LogKlassAlignmentInBytes);\n-    } else {\n-      eor(dst, src, (uint64_t)CompressedKlassPointers::base());\n-    }\n+    eor(dst, src, (uint64_t)CompressedKlassPointers::base());\n+    lsr(dst, dst, LogKlassAlignmentInBytes);\n@@ -4292,5 +4320,1 @@\n-    if (CompressedKlassPointers::shift() != 0) {\n-      ubfx(dst, src, LogKlassAlignmentInBytes, 32);\n-    } else {\n-      movw(dst, src);\n-    }\n+    ubfx(dst, src, LogKlassAlignmentInBytes, MaxNarrowKlassPointerBits);\n@@ -4312,0 +4336,2 @@\n+  assert(CompressedKlassPointers::shift() != 0, \"not lilliput?\");\n+\n@@ -4314,5 +4340,1 @@\n-    if (CompressedKlassPointers::shift() != 0) {\n-      lsl(dst, src, LogKlassAlignmentInBytes);\n-    } else {\n-      if (dst != src) mov(dst, src);\n-    }\n+    if (dst != src) mov(dst, src);\n@@ -4322,6 +4344,2 @@\n-    if (CompressedKlassPointers::shift() != 0) {\n-      lsl(dst, src, LogKlassAlignmentInBytes);\n-      eor(dst, dst, (uint64_t)CompressedKlassPointers::base());\n-    } else {\n-      eor(dst, src, (uint64_t)CompressedKlassPointers::base());\n-    }\n+    lsl(dst, src, LogKlassAlignmentInBytes);\n+    eor(dst, dst, (uint64_t)CompressedKlassPointers::base());\n@@ -4334,0 +4352,3 @@\n+    \/\/ Invalid base should have been gracefully handled via klass_decode_mode() in VM initialization.\n+    assert((shifted_base & 0xffff0000ffffffff) == 0, \"incompatible base\");\n+\n@@ -4336,5 +4357,1 @@\n-\n-    if (CompressedKlassPointers::shift() != 0) {\n-      lsl(dst, dst, LogKlassAlignmentInBytes);\n-    }\n-\n+    lsl(dst, dst, LogKlassAlignmentInBytes);\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":107,"deletions":90,"binary":false,"changes":197,"status":"modified"},{"patch":"@@ -89,0 +89,2 @@\n+ public:\n+\n@@ -96,1 +98,9 @@\n-  KlassDecodeMode klass_decode_mode();\n+  \/\/ Return the current narrow Klass pointer decode mode. Initialized on first call.\n+  static KlassDecodeMode klass_decode_mode();\n+\n+  \/\/ Given an arbitrary base address, return the KlassDecodeMode that would be used. Return KlassDecodeNone\n+  \/\/ if base address is not valid for encoding.\n+  static KlassDecodeMode klass_decode_mode_for_base(address base);\n+\n+  \/\/ Returns a static string\n+  static const char* describe_klass_decode_mode(KlassDecodeMode mode);\n@@ -99,0 +109,1 @@\n+\n@@ -834,0 +845,1 @@\n+  void load_nklass(Register dst, Register src);\n@@ -835,1 +847,0 @@\n-  void store_klass(Register dst, Register src);\n@@ -861,2 +872,0 @@\n-  void store_klass_gap(Register dst, Register src);\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":13,"deletions":4,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -325,1 +325,1 @@\n-        __ null_check(receiver_reg, oopDesc::klass_offset_in_bytes());\n+        __ null_check(receiver_reg, oopDesc::mark_offset_in_bytes());\n","filename":"src\/hotspot\/cpu\/aarch64\/methodHandles_aarch64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -592,2 +592,12 @@\n-    __ load_klass(r0, r0);  \/\/ get klass\n-    __ cbz(r0, error);      \/\/ if klass is NULL it is broken\n+    \/\/ NOTE: We used to load the Klass* here, and compare that to zero.\n+    \/\/ However, with current Lilliput implementation, that would require\n+    \/\/ checking the locking bits and calling into the runtime, which\n+    \/\/ clobbers the condition flags, which may be live around this call.\n+    \/\/ OTOH, this is a simple NULL-check, and we can simply load the upper\n+    \/\/ 32bit of the header as narrowKlass, and compare that to 0. The\n+    \/\/ worst that can happen (rarely) is that the object is locked and\n+    \/\/ we have lock pointer bits in the upper 32bits. We can't get a false\n+    \/\/ negative.\n+    assert(oopDesc::klass_offset_in_bytes() % 4 == 0, \"must be 4 byte aligned\");\n+    __ ldrw(r0, Address(r0, oopDesc::klass_offset_in_bytes()));  \/\/ get klass\n+    __ cbzw(r0, error);      \/\/ if klass is NULL it is broken\n@@ -6787,0 +6797,23 @@\n+  \/\/ Pass object argument in r0 (which has to be preserved outside this stub)\n+  \/\/ Pass back result in r0\n+  \/\/ Clobbers rscratch1\n+  address generate_load_nklass() {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", \"load_nklass\");\n+\n+    address start = __ pc();\n+\n+    __ set_last_Java_frame(sp, rfp, lr, rscratch1);\n+    __ enter();\n+    __ push(RegSet::of(rscratch1, rscratch2), sp);\n+    __ push_call_clobbered_registers_except(r0);\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, oopDesc::load_nklass_runtime), 1);\n+    __ pop_call_clobbered_registers_except(r0);\n+    __ pop(RegSet::of(rscratch1, rscratch2), sp);\n+    __ leave();\n+    __ reset_last_Java_frame(true);\n+    __ ret(lr);\n+\n+    return start;\n+  }\n+\n@@ -7954,0 +7987,2 @@\n+\n+    StubRoutines::_load_nklass = generate_load_nklass();\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":37,"deletions":2,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -3239,1 +3239,1 @@\n-  __ null_check(recv, oopDesc::klass_offset_in_bytes());\n+  __ null_check(recv, oopDesc::mark_offset_in_bytes());\n@@ -3329,1 +3329,1 @@\n-  __ null_check(r2, oopDesc::klass_offset_in_bytes());\n+  __ null_check(r2, oopDesc::mark_offset_in_bytes());\n@@ -3346,1 +3346,1 @@\n-  __ null_check(r2, oopDesc::klass_offset_in_bytes());\n+  __ null_check(r2, oopDesc::mark_offset_in_bytes());\n@@ -3534,1 +3534,1 @@\n-    __ mov(rscratch1, (intptr_t)markWord::prototype().value());\n+    __ ldr(rscratch1, Address(r4, Klass::prototype_header_offset()));\n@@ -3536,2 +3536,0 @@\n-    __ store_klass_gap(r0, zr);  \/\/ zero klass gap for compressed oops\n-    __ store_klass(r0, r4);      \/\/ store klass last\n@@ -3667,1 +3665,2 @@\n-  __ load_klass(r3, r3);\n+  __ load_klass(rscratch1, r3);\n+  __ mov(r3, rscratch1);\n","filename":"src\/hotspot\/cpu\/aarch64\/templateTable_aarch64.cpp","additions":6,"deletions":7,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -166,1 +166,1 @@\n-                                       int header_size, int element_size,\n+                                       int header_size_in_bytes, int element_size,\n@@ -169,1 +169,0 @@\n-  const int header_size_in_bytes = header_size * BytesPerWord;\n","filename":"src\/hotspot\/cpu\/arm\/c1_MacroAssembler_arm.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n","filename":"src\/hotspot\/cpu\/ppc\/macroAssembler_ppc.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n","filename":"src\/hotspot\/cpu\/s390\/macroAssembler_s390.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1638,1 +1638,1 @@\n-                      arrayOopDesc::header_size(op->type()),\n+                      arrayOopDesc::base_offset_in_bytes(op->type()),\n@@ -3069,0 +3069,1 @@\n+  Register tmp2 = LP64_ONLY(rscratch2) NOT_LP64(noreg);\n@@ -3193,0 +3194,1 @@\n+#ifndef _LP64\n@@ -3195,1 +3197,1 @@\n-\n+#endif\n@@ -3260,7 +3262,8 @@\n-      if (UseCompressedClassPointers) {\n-        __ movl(tmp, src_klass_addr);\n-        __ cmpl(tmp, dst_klass_addr);\n-      } else {\n-        __ movptr(tmp, src_klass_addr);\n-        __ cmpptr(tmp, dst_klass_addr);\n-      }\n+#ifdef _LP64\n+      __ load_nklass(tmp, src);\n+      __ load_nklass(tmp2, dst);\n+      __ cmpl(tmp, tmp2);\n+#else\n+      __ movptr(tmp, src_klass_addr);\n+      __ cmpptr(tmp, dst_klass_addr);\n+#endif\n@@ -3422,5 +3425,2 @@\n-    if (UseCompressedClassPointers) {\n-      __ encode_klass_not_null(tmp, rscratch1);\n-    }\n-#endif\n-\n+    assert(UseCompressedClassPointers, \"Lilliput\");\n+    __ encode_klass_not_null(tmp, rscratch1);\n@@ -3428,3 +3428,12 @@\n-\n-      if (UseCompressedClassPointers)          __ cmpl(tmp, dst_klass_addr);\n-      else                   __ cmpptr(tmp, dst_klass_addr);\n+      __ load_nklass(tmp2, dst);\n+      __ cmpl(tmp, tmp2);\n+      __ jcc(Assembler::notEqual, halt);\n+      __ load_nklass(tmp2, src);\n+      __ cmpl(tmp, tmp2);\n+      __ jcc(Assembler::equal, known_ok);\n+    } else {\n+      __ load_nklass(tmp2, dst);\n+      __ cmpl(tmp, tmp2);\n+#else\n+    if (basic_type != T_OBJECT) {\n+      __ cmpptr(tmp, dst_klass_addr);\n@@ -3432,2 +3441,1 @@\n-      if (UseCompressedClassPointers)          __ cmpl(tmp, src_klass_addr);\n-      else                   __ cmpptr(tmp, src_klass_addr);\n+      __ cmpptr(tmp, src_klass_addr);\n@@ -3436,2 +3444,2 @@\n-      if (UseCompressedClassPointers)          __ cmpl(tmp, dst_klass_addr);\n-      else                   __ cmpptr(tmp, dst_klass_addr);\n+      __ cmpptr(tmp, dst_klass_addr);\n+#endif\n@@ -3526,3 +3534,2 @@\n-  CodeEmitInfo* info = op->info();\n-  if (info != NULL) {\n-    add_debug_info_for_null_check_here(info);\n+  if (op->info() != NULL) {\n+    add_debug_info_for_null_check_here(op->info());\n@@ -3530,5 +3537,20 @@\n-\n-  if (UseCompressedClassPointers) {\n-    __ movl(result, Address(obj, oopDesc::klass_offset_in_bytes()));\n-    __ decode_klass_not_null(result, rscratch1);\n-  } else\n+  Register tmp = rscratch1;\n+  assert_different_registers(tmp, obj);\n+  assert_different_registers(tmp, result);\n+\n+  \/\/ Check if we can take the (common) fast path, if obj is unlocked.\n+  __ movq(tmp, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  __ xorq(tmp, markWord::unlocked_value);\n+  __ testb(tmp, markWord::lock_mask_in_place);\n+  __ jcc(Assembler::notZero, *op->stub()->entry());\n+\n+  \/\/ Fast-path: shift and decode Klass*.\n+  __ movq(result, tmp);\n+  __ shrq(result, markWord::klass_shift);\n+\n+  __ bind(*op->stub()->continuation());\n+  __ decode_klass_not_null(result, tmp);\n+#else\n+  __ movptr(result, Address(obj, oopDesc::klass_offset_in_bytes()));\n+  \/\/ Not really needed, but bind the label anyway to make compiler happy.\n+  __ bind(*op->stub()->continuation());\n@@ -3537,1 +3559,0 @@\n-    __ movptr(result, Address(obj, oopDesc::klass_offset_in_bytes()));\n@@ -3643,4 +3664,0 @@\n-#ifndef ASSERT\n-      __ jmpb(next);\n-    }\n-#else\n@@ -3649,0 +3666,1 @@\n+#ifdef ASSERT\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":52,"deletions":34,"binary":false,"changes":86,"status":"modified"},{"patch":"@@ -147,8 +147,5 @@\n-  assert_different_registers(obj, klass, len);\n-  movptr(Address(obj, oopDesc::mark_offset_in_bytes()), checked_cast<int32_t>(markWord::prototype().value()));\n-#ifdef _LP64\n-  if (UseCompressedClassPointers) { \/\/ Take care not to kill klass\n-    movptr(t1, klass);\n-    encode_klass_not_null(t1, rscratch1);\n-    movl(Address(obj, oopDesc::klass_offset_in_bytes()), t1);\n-  } else\n+  assert_different_registers(obj, klass, len, t1, t2);\n+  movptr(t1, Address(klass, Klass::prototype_header_offset()));\n+  movptr(Address(obj, oopDesc::mark_offset_in_bytes()), t1);\n+#ifndef _LP64\n+  movptr(Address(obj, oopDesc::klass_offset_in_bytes()), klass);\n@@ -156,3 +153,0 @@\n-  {\n-    movptr(Address(obj, oopDesc::klass_offset_in_bytes()), klass);\n-  }\n@@ -163,6 +157,0 @@\n-#ifdef _LP64\n-  else if (UseCompressedClassPointers) {\n-    xorptr(t1, t1);\n-    store_klass_gap(obj, t1);\n-  }\n-#endif\n@@ -245,1 +233,1 @@\n-void C1_MacroAssembler::allocate_array(Register obj, Register len, Register t1, Register t2, int header_size, Address::ScaleFactor f, Register klass, Label& slow_case) {\n+void C1_MacroAssembler::allocate_array(Register obj, Register len, Register t1, Register t2, int base_offset_in_bytes, Address::ScaleFactor f, Register klass, Label& slow_case) {\n@@ -258,1 +246,1 @@\n-  movptr(arr_size, header_size * BytesPerWord + MinObjAlignmentInBytesMask);\n+  movptr(arr_size, (int32_t)base_offset_in_bytes + MinObjAlignmentInBytesMask);\n@@ -268,1 +256,1 @@\n-  initialize_body(obj, arr_size, header_size * BytesPerWord, len_zero);\n+  initialize_body(obj, arr_size, base_offset_in_bytes, len_zero);\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":8,"deletions":20,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n@@ -4134,1 +4135,1 @@\n-  assert((offset_in_bytes & (BytesPerWord - 1)) == 0, \"offset must be a multiple of BytesPerWord\");\n+  assert((offset_in_bytes & (BytesPerInt - 1)) == 0, \"offset must be a multiple of BytesPerInt\");\n@@ -4140,0 +4141,13 @@\n+  \/\/ Emit single 32bit store to clear leading bytes, if necessary.\n+  xorptr(temp, temp);    \/\/ use _zero reg to clear memory (shorter code)\n+#ifdef _LP64\n+  if (!is_aligned(offset_in_bytes, BytesPerWord)) {\n+    movl(Address(address, offset_in_bytes), temp);\n+    offset_in_bytes += BytesPerInt;\n+    decrement(length_in_bytes, BytesPerInt);\n+  }\n+  assert((offset_in_bytes & (BytesPerWord - 1)) == 0, \"offset must be a multiple of BytesPerWord\");\n+  testptr(length_in_bytes, length_in_bytes);\n+  jcc(Assembler::zero, done);\n+#endif\n+\n@@ -4152,1 +4166,0 @@\n-  xorptr(temp, temp);    \/\/ use _zero reg to clear memory (shorter code)\n@@ -5084,9 +5097,30 @@\n-void MacroAssembler::load_klass(Register dst, Register src, Register tmp) {\n-  assert_different_registers(src, tmp);\n-  assert_different_registers(dst, tmp);\n-  if (UseCompressedClassPointers) {\n-    movl(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n-    decode_klass_not_null(dst, tmp);\n-  } else\n-#endif\n-    movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+void MacroAssembler::load_nklass(Register dst, Register src) {\n+  assert_different_registers(src, dst);\n+  assert(UseCompressedClassPointers, \"expect compressed class pointers\");\n+\n+  Label slow, done;\n+  movq(dst, Address(src, oopDesc::mark_offset_in_bytes()));\n+  \/\/ NOTE: While it would seem nice to use xorb instead (for which we don't have an encoding in our assembler),\n+  \/\/ the encoding for xorq uses the signed version (0x81\/6) of xor, which encodes as compact as xorb would,\n+  \/\/ and does't make a difference performance-wise.\n+  xorq(dst, markWord::unlocked_value);\n+  testb(dst, markWord::lock_mask_in_place);\n+  jccb(Assembler::notZero, slow);\n+\n+  shrq(dst, markWord::klass_shift);\n+  jmp(done);\n+  bind(slow);\n+\n+  if (dst != rax) {\n+    push(rax);\n+  }\n+  if (src != rax) {\n+    mov(rax, src);\n+  }\n+  call(RuntimeAddress(StubRoutines::load_nklass()));\n+  if (dst != rax) {\n+    mov(dst, rax);\n+    pop(rax);\n+  }\n+\n+  bind(done);\n@@ -5095,0 +5129,1 @@\n+#endif\n@@ -5096,1 +5131,1 @@\n-void MacroAssembler::store_klass(Register dst, Register src, Register tmp) {\n+void MacroAssembler::load_klass(Register dst, Register src, Register tmp, bool null_check_src) {\n@@ -5100,4 +5135,18 @@\n-  if (UseCompressedClassPointers) {\n-    encode_klass_not_null(src, tmp);\n-    movl(Address(dst, oopDesc::klass_offset_in_bytes()), src);\n-  } else\n+  assert(UseCompressedClassPointers, \"expect compressed class pointers\");\n+  Register d = dst;\n+  if (src == dst) {\n+    d = tmp;\n+  }\n+  if (null_check_src) {\n+    null_check(src, oopDesc::mark_offset_in_bytes());\n+  }\n+  load_nklass(d, src);\n+  if (src == dst) {\n+    mov(dst, d);\n+  }\n+  decode_klass_not_null(dst, tmp);\n+#else\n+  if (null_check_src) {\n+    null_check(src, oopDesc::klass_offset_in_bytes());\n+  }\n+  movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n@@ -5105,1 +5154,0 @@\n-    movptr(Address(dst, oopDesc::klass_offset_in_bytes()), src);\n@@ -5108,0 +5156,6 @@\n+#ifndef _LP64\n+void MacroAssembler::store_klass(Register dst, Register src) {\n+  movptr(Address(dst, oopDesc::klass_offset_in_bytes()), src);\n+}\n+#endif\n+\n@@ -5154,7 +5208,0 @@\n-void MacroAssembler::store_klass_gap(Register dst, Register src) {\n-  if (UseCompressedClassPointers) {\n-    \/\/ Store to klass gap in destination\n-    movl(Address(dst, oopDesc::klass_gap_offset_in_bytes()), src);\n-  }\n-}\n-\n@@ -5312,0 +5359,62 @@\n+MacroAssembler::KlassDecodeMode MacroAssembler::_klass_decode_mode = KlassDecodeNone;\n+\n+\/\/ Returns a static string\n+const char* MacroAssembler::describe_klass_decode_mode(MacroAssembler::KlassDecodeMode mode) {\n+  switch (mode) {\n+  case KlassDecodeNone: return \"none\";\n+  case KlassDecodeZero: return \"zero\";\n+  case KlassDecodeXor:  return \"xor\";\n+  case KlassDecodeAdd:  return \"add\";\n+  default:\n+    ShouldNotReachHere();\n+  }\n+  return NULL;\n+}\n+\n+\/\/ Return the current narrow Klass pointer decode mode.\n+MacroAssembler::KlassDecodeMode MacroAssembler::klass_decode_mode() {\n+  if (_klass_decode_mode == KlassDecodeNone) {\n+    \/\/ First time initialization\n+    assert(UseCompressedClassPointers, \"not using compressed class pointers\");\n+    assert(Metaspace::initialized(), \"metaspace not initialized yet\");\n+\n+    _klass_decode_mode = klass_decode_mode_for_base(CompressedKlassPointers::base());\n+    guarantee(_klass_decode_mode != KlassDecodeNone,\n+              PTR_FORMAT \" is not a valid encoding base on aarch64\",\n+              p2i(CompressedKlassPointers::base()));\n+    log_info(metaspace)(\"klass decode mode initialized: %s\", describe_klass_decode_mode(_klass_decode_mode));\n+  }\n+  return _klass_decode_mode;\n+}\n+\n+\/\/ Given an arbitrary base address, return the KlassDecodeMode that would be used. Return KlassDecodeNone\n+\/\/ if base address is not valid for encoding.\n+MacroAssembler::KlassDecodeMode MacroAssembler::klass_decode_mode_for_base(address base) {\n+  assert(CompressedKlassPointers::shift() != 0, \"not lilliput?\");\n+\n+  const uint64_t base_u64 = (uint64_t) base;\n+\n+  if (base_u64 == 0) {\n+    return KlassDecodeZero;\n+  }\n+\n+  if ((base_u64 & (KlassEncodingMetaspaceMax - 1)) == 0) {\n+    return KlassDecodeXor;\n+  }\n+\n+  \/\/ Note that there is no point in optimizing for shift=3 since lilliput\n+  \/\/ will use larger shifts\n+\n+  \/\/ The add+shift mode for decode_and_move_klass_not_null() requires the base to be\n+  \/\/  shiftable-without-loss. So, this is the minimum restriction on x64 for a valid\n+  \/\/  encoding base. This does not matter in reality since the shift values we use for\n+  \/\/  Lilliput, while large, won't be larger than a page size. And the encoding base\n+  \/\/  will be quite likely page aligned since it usually falls to the beginning of\n+  \/\/  either CDS or CCS.\n+  if ((base_u64 & (KlassAlignmentInBytes - 1)) == 0) {\n+    return KlassDecodeAdd;\n+  }\n+\n+  return KlassDecodeNone;\n+}\n+\n@@ -5314,1 +5423,12 @@\n-  if (CompressedKlassPointers::base() != NULL) {\n+  switch (klass_decode_mode()) {\n+  case KlassDecodeZero: {\n+    shrq(r, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  case KlassDecodeXor: {\n+    mov64(tmp, (int64_t)CompressedKlassPointers::base());\n+    xorq(r, tmp);\n+    shrq(r, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  case KlassDecodeAdd: {\n@@ -5317,0 +5437,2 @@\n+    shrq(r, CompressedKlassPointers::shift());\n+    break;\n@@ -5318,3 +5440,2 @@\n-  if (CompressedKlassPointers::shift() != 0) {\n-    assert (LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-    shrq(r, LogKlassAlignmentInBytes);\n+  default:\n+    ShouldNotReachHere();\n@@ -5326,1 +5447,13 @@\n-  if (CompressedKlassPointers::base() != NULL) {\n+  switch (klass_decode_mode()) {\n+  case KlassDecodeZero: {\n+    movptr(dst, src);\n+    shrq(dst, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  case KlassDecodeXor: {\n+    mov64(dst, (int64_t)CompressedKlassPointers::base());\n+    xorq(dst, src);\n+    shrq(dst, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  case KlassDecodeAdd: {\n@@ -5329,2 +5462,2 @@\n-  } else {\n-    movptr(dst, src);\n+    shrq(dst, CompressedKlassPointers::shift());\n+    break;\n@@ -5332,3 +5465,2 @@\n-  if (CompressedKlassPointers::shift() != 0) {\n-    assert (LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-    shrq(dst, LogKlassAlignmentInBytes);\n+  default:\n+    ShouldNotReachHere();\n@@ -5340,8 +5472,5 @@\n-  \/\/ Note: it will change flags\n-  assert(UseCompressedClassPointers, \"should only be used for compressed headers\");\n-  \/\/ Cannot assert, unverified entry point counts instructions (see .ad file)\n-  \/\/ vtableStubs also counts instructions in pd_code_size_limit.\n-  \/\/ Also do not verify_oop as this is called by verify_oop.\n-  if (CompressedKlassPointers::shift() != 0) {\n-    assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-    shlq(r, LogKlassAlignmentInBytes);\n+  const uint64_t base_u64 = (uint64_t)CompressedKlassPointers::base();\n+  switch (klass_decode_mode()) {\n+  case KlassDecodeZero: {\n+    shlq(r, CompressedKlassPointers::shift());\n+    break;\n@@ -5349,2 +5478,11 @@\n-  if (CompressedKlassPointers::base() != NULL) {\n-    mov64(tmp, (int64_t)CompressedKlassPointers::base());\n+  case KlassDecodeXor: {\n+    assert((base_u64 & (KlassEncodingMetaspaceMax - 1)) == 0,\n+           \"base \" UINT64_FORMAT_X \" invalid for xor mode\", base_u64); \/\/ should have been handled at VM init.\n+    shlq(r, CompressedKlassPointers::shift());\n+    mov64(tmp, base_u64);\n+    xorq(r, tmp);\n+    break;\n+  }\n+  case KlassDecodeAdd: {\n+    shlq(r, CompressedKlassPointers::shift());\n+    mov64(tmp, base_u64);\n@@ -5352,0 +5490,4 @@\n+    break;\n+  }\n+  default:\n+    ShouldNotReachHere();\n@@ -5357,3 +5499,1 @@\n-  \/\/ Note: it will change flags\n-  assert (UseCompressedClassPointers, \"should only be used for compressed headers\");\n-  \/\/ Cannot assert, unverified entry point counts instructions (see .ad file)\n+  \/\/ Note: Cannot assert, unverified entry point counts instructions (see .ad file)\n@@ -5363,18 +5503,28 @@\n-  if (CompressedKlassPointers::base() == NULL &&\n-      CompressedKlassPointers::shift() == 0) {\n-    \/\/ The best case scenario is that there is no base or shift. Then it is already\n-    \/\/ a pointer that needs nothing but a register rename.\n-    movl(dst, src);\n-  } else {\n-    if (CompressedKlassPointers::base() != NULL) {\n-      mov64(dst, (int64_t)CompressedKlassPointers::base());\n-    } else {\n-      xorq(dst, dst);\n-    }\n-    if (CompressedKlassPointers::shift() != 0) {\n-      assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-      assert(LogKlassAlignmentInBytes == Address::times_8, \"klass not aligned on 64bits?\");\n-      leaq(dst, Address(dst, src, Address::times_8, 0));\n-    } else {\n-      addq(dst, src);\n-    }\n+  const uint64_t base_u64 = (uint64_t)CompressedKlassPointers::base();\n+\n+  switch (klass_decode_mode()) {\n+  case KlassDecodeZero: {\n+    movq(dst, src);\n+    shlq(dst, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  case KlassDecodeXor: {\n+    assert((base_u64 & (KlassEncodingMetaspaceMax - 1)) == 0,\n+           \"base \" UINT64_FORMAT_X \" invalid for xor mode\", base_u64); \/\/ should have been handled at VM init.\n+    const uint64_t base_right_shifted = base_u64 >> CompressedKlassPointers::shift();\n+    mov64(dst, base_right_shifted);\n+    xorq(dst, src);\n+    shlq(dst, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  case KlassDecodeAdd: {\n+    assert((base_u64 & (KlassAlignmentInBytes - 1)) == 0,\n+           \"base \" UINT64_FORMAT_X \" invalid for add mode\", base_u64); \/\/ should have been handled at VM init.\n+    const uint64_t base_right_shifted = base_u64 >> CompressedKlassPointers::shift();\n+    mov64(dst, base_right_shifted);\n+    addq(dst, src);\n+    shlq(dst, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  default:\n+    ShouldNotReachHere();\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":215,"deletions":65,"binary":false,"changes":280,"status":"modified"},{"patch":"@@ -82,0 +82,23 @@\n+ public:\n+\n+  enum KlassDecodeMode {\n+    KlassDecodeNone,\n+    KlassDecodeZero,\n+    KlassDecodeXor,\n+    KlassDecodeAdd\n+  };\n+\n+  \/\/ Return the current narrow Klass pointer decode mode. Initialized on first call.\n+  static KlassDecodeMode klass_decode_mode();\n+\n+  \/\/ Given an arbitrary base address, return the KlassDecodeMode that would be used. Return KlassDecodeNone\n+  \/\/ if base address is not valid for encoding.\n+  static KlassDecodeMode klass_decode_mode_for_base(address base);\n+\n+  \/\/ Returns a static string\n+  static const char* describe_klass_decode_mode(KlassDecodeMode mode);\n+\n+ private:\n+\n+  static KlassDecodeMode _klass_decode_mode;\n+\n@@ -350,2 +373,6 @@\n-  void load_klass(Register dst, Register src, Register tmp);\n-  void store_klass(Register dst, Register src, Register tmp);\n+  void load_klass(Register dst, Register src, Register tmp, bool null_check_src = false);\n+#ifdef _LP64\n+  void load_nklass(Register dst, Register src);\n+#else\n+  void store_klass(Register dst, Register src);\n+#endif\n@@ -370,2 +397,0 @@\n-  void store_klass_gap(Register dst, Register src);\n-\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":29,"deletions":4,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -382,2 +382,1 @@\n-        __ null_check(receiver_reg, oopDesc::klass_offset_in_bytes());\n-        __ load_klass(temp1_recv_klass, receiver_reg, temp2);\n+        __ load_klass(temp1_recv_klass, receiver_reg, temp2, true);\n","filename":"src\/hotspot\/cpu\/x86\/methodHandles_x86.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -3356,0 +3356,40 @@\n+  \/\/ Call stub to call runtime oopDesc::load_nklass_runtime().\n+  \/\/ rax: call argument (object)\n+  \/\/ rax: return object's narrowKlass\n+  \/\/ Preserves all caller-saved registers, except rax\n+#ifdef _LP64\n+address StubGenerator::generate_load_nklass() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark(this, \"StubRoutines\", \"load_nklass\");\n+  address start = __ pc();\n+  __ enter(); \/\/ save rbp\n+\n+  __ andptr(rsp, -(StackAlignmentInBytes));    \/\/ Align stack\n+  __ push_FPU_state();\n+\n+  __ push(rdi);\n+  __ push(rsi);\n+  __ push(rdx);\n+  __ push(rcx);\n+  __ push(r8);\n+  __ push(r9);\n+  __ push(r10);\n+  __ push(r11);\n+  __ call_VM_leaf(CAST_FROM_FN_PTR(address, oopDesc::load_nklass_runtime), rax);\n+  __ pop(r11);\n+  __ pop(r10);\n+  __ pop(r9);\n+  __ pop(r8);\n+  __ pop(rcx);\n+  __ pop(rdx);\n+  __ pop(rsi);\n+  __ pop(rdi);\n+\n+  __ pop_FPU_state();\n+\n+  __ leave();\n+  __ ret(0);\n+  return start;\n+}\n+#endif \/\/ _LP64\n+\n@@ -3731,0 +3771,4 @@\n+#ifdef _LP64\n+  StubRoutines::_load_nklass = generate_load_nklass();\n+#endif\n+\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":44,"deletions":0,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -505,0 +505,3 @@\n+#ifdef _LP64\n+  address generate_load_nklass();\n+#endif\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -3661,2 +3661,1 @@\n-  __ null_check(recv, oopDesc::klass_offset_in_bytes());\n-  __ load_klass(rax, recv, rscratch1);\n+  __ load_klass(rax, recv, rscratch1, true);\n@@ -3753,2 +3752,1 @@\n-  __ null_check(rcx, oopDesc::klass_offset_in_bytes());\n-  __ load_klass(rlocals, rcx, rscratch1);\n+  __ load_klass(rlocals, rcx, rscratch1, true);\n@@ -3776,2 +3774,1 @@\n-  __ null_check(rcx, oopDesc::klass_offset_in_bytes());\n-  __ load_klass(rdx, rcx, rscratch1);\n+  __ load_klass(rdx, rcx, rscratch1, true);\n@@ -3999,5 +3996,4 @@\n-    __ movptr(Address(rax, oopDesc::mark_offset_in_bytes()),\n-              (intptr_t)markWord::prototype().value()); \/\/ header\n-#ifdef _LP64\n-    __ xorl(rsi, rsi); \/\/ use zero reg to clear memory (shorter code)\n-    __ store_klass_gap(rax, rsi);  \/\/ zero klass gap for compressed oops\n+    __ movptr(rbx, Address(rcx, Klass::prototype_header_offset()));\n+    __ movptr(Address(rax, oopDesc::mark_offset_in_bytes ()), rbx);\n+#ifndef _LP64\n+    __ store_klass(rax, rcx);  \/\/ klass\n@@ -4006,1 +4002,0 @@\n-    __ store_klass(rax, rcx, rscratch1);  \/\/ klass\n@@ -4158,1 +4153,1 @@\n-  __ jmpb(resolved);\n+  __ jmp(resolved);\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.cpp","additions":8,"deletions":13,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -5315,1 +5315,1 @@\n-instruct loadNKlass(rRegN dst, memory mem)\n+instruct loadNKlass(rRegN dst, indOffset8 mem, rFlagsReg cr)\n@@ -5318,1 +5318,1 @@\n-\n+  effect(TEMP_DEF dst, KILL cr);\n@@ -5322,1 +5322,3 @@\n-    __ movl($dst$$Register, $mem$$Address);\n+    assert($mem$$disp == oopDesc::klass_offset_in_bytes(), \"expect correct offset 4, but got: %d\", $mem$$disp);\n+    assert($mem$$index == 4, \"expect no index register: %d\", $mem$$index);\n+    __ load_nklass($dst$$Register, $mem$$base$$Register);\n@@ -5324,1 +5326,1 @@\n-  ins_pipe(ialu_reg_mem); \/\/ XXX\n+  ins_pipe(pipe_slow); \/\/ XXX\n@@ -12648,0 +12650,3 @@\n+\/\/ Disabled because the compressed Klass* in header cannot be safely\n+\/\/ accessed. TODO: Re-enable it as soon as synchronization does not\n+\/\/ overload the upper header bits anymore.\n@@ -12650,0 +12655,1 @@\n+  predicate(false);\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":10,"deletions":4,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -891,0 +891,1 @@\n+      do_stub(opLoadKlass->_stub);\n@@ -1071,0 +1072,1 @@\n+  masm->append_code_stub(stub());\n@@ -2039,0 +2041,1 @@\n+  out->print(\"[lbl:\" INTPTR_FORMAT \"]\", p2i(stub()->entry()));\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1244,1 +1244,2 @@\n-  __ load_klass(obj, klass, null_check_info);\n+  CodeStub* slow_path = new LoadKlassStub(obj, klass);\n+  __ load_klass(obj, klass, null_check_info, slow_path);\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -247,0 +247,1 @@\n+  case load_klass_id:\n","filename":"src\/hotspot\/share\/c1\/c1_Runtime1.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n@@ -43,0 +44,1 @@\n+#include \"oops\/klass.inline.hpp\"\n@@ -222,2 +224,4 @@\n-    \/\/ See RunTimeClassInfo::get_for()\n-    _estimated_metaspaceobj_bytes += align_up(BytesPerWord, SharedSpaceObjectAlignment);\n+    \/\/ See ArchiveBuilder::make_shallow_copies: make sure we have enough space for both maximum\n+    \/\/ Klass alignment as well as the RuntimeInfo* pointer we will embed in front of a Klass.\n+    _estimated_metaspaceobj_bytes += align_up(BytesPerWord, KlassAlignmentInBytes) +\n+        align_up(sizeof(void*), SharedSpaceObjectAlignment);\n@@ -620,4 +624,5 @@\n-    \/\/ Save a pointer immediate in front of an InstanceKlass, so\n-    \/\/ we can do a quick lookup from InstanceKlass* -> RunTimeClassInfo*\n-    \/\/ without building another hashtable. See RunTimeClassInfo::get_for()\n-    \/\/ in systemDictionaryShared.cpp.\n+    \/\/ Reserve space for a pointer immediately in front of an InstanceKlass. That space will\n+    \/\/ later be used to store the RuntimeClassInfo* pointer directly in front of the archived\n+    \/\/ InstanceKlass, in order to have a quick lookup InstanceKlass* -> RunTimeClassInfo*\n+    \/\/ without building another hashtable. See RunTimeClassInfo::get_for()\/::set_for() for\n+    \/\/ details.\n@@ -629,0 +634,3 @@\n+    dest = dump_region->allocate(bytes, KlassAlignmentInBytes);\n+  } else {\n+    dest = dump_region->allocate(bytes);\n@@ -630,1 +638,0 @@\n-  dest = dump_region->allocate(bytes);\n@@ -649,1 +656,2 @@\n-  log_trace(cds)(\"Copy: \" PTR_FORMAT \" ==> \" PTR_FORMAT \" %d\", p2i(src), p2i(dest), bytes);\n+  log_trace(cds)(\"Copy: \" PTR_FORMAT \" ==> \" PTR_FORMAT \" %d (%s)\", p2i(src), p2i(dest), bytes,\n+                 MetaspaceObj::type_name(ref->msotype()));\n@@ -653,0 +661,2 @@\n+\n+  DEBUG_ONLY(_alloc_stats.verify((int)dump_region->used(), src_info->read_only()));\n@@ -749,0 +759,7 @@\n+    Klass* requested_k = to_requested(k);\n+#ifdef _LP64\n+    narrowKlass nk = CompressedKlassPointers::encode_not_null(requested_k, _requested_static_archive_bottom);\n+    k->set_prototype_header(markWord::prototype().set_narrow_klass(nk));\n+#else\n+    k->set_prototype_header(markWord::prototype());\n+#endif\n@@ -840,1 +857,3 @@\n-  o->set_narrow_klass(nk);\n+#ifdef _LP64\n+  o->set_mark(o->mark().set_narrow_klass(nk));\n+#endif\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.cpp","additions":28,"deletions":9,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"memory\/metaspace\/metaspaceAlignment.hpp\"\n@@ -46,3 +47,13 @@\n-\/\/ Metaspace::allocate() requires that all blocks must be aligned with KlassAlignmentInBytes.\n-\/\/ We enforce the same alignment rule in blocks allocated from the shared space.\n-const int SharedSpaceObjectAlignment = KlassAlignmentInBytes;\n+\/\/ CDS has three alignments to deal with:\n+\/\/ - SharedSpaceObjectAlignment, always 8 bytes: used for placing arbitrary structures.\n+\/\/   These may contain 64-bit members (not larger, we know that much). Therefore we\n+\/\/   need to use 64-bit alignment on both 32-bit and 64-bit platforms. We reuse metaspace\n+\/\/   minimal alignment for this, which follows the same logic.\n+\/\/ - With CompressedClassPointers=1, we need to store Klass structures with a large\n+\/\/   alignment (Lilliput specific narrow Klass pointer encoding) - KlassAlignmentInBytes.\n+\/\/ - Header data and tags are squeezed in with word alignment, which happens to be 4 bytes\n+\/\/   on 32-bit. See ReadClosure::do_xxx() and DumpRegion::append_intptr().\n+const int SharedSpaceObjectAlignment = metaspace::MetaspaceMinAlignmentBytes;\n+\n+\/\/ standard alignment should be sufficient for storing 64-bit values.\n+STATIC_ASSERT(SharedSpaceObjectAlignment >= sizeof(uint64_t));\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.hpp","additions":14,"deletions":3,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -208,3 +208,13 @@\n-char* DumpRegion::allocate(size_t num_bytes) {\n-  char* p = (char*)align_up(_top, (size_t)SharedSpaceObjectAlignment);\n-  char* newtop = p + align_up(num_bytes, (size_t)SharedSpaceObjectAlignment);\n+char* DumpRegion::allocate(size_t num_bytes, size_t alignment) {\n+  \/\/ We align the starting address of each allocation.\n+  char* p = (char*)align_up(_top, alignment);\n+  char* newtop = p + num_bytes;\n+  \/\/ Leave _top always SharedSpaceObjectAlignment aligned. But not more -\n+  \/\/  if we allocate with large alignments, lets not waste the gaps.\n+  \/\/ Ideally we would not need to align _top to anything here but CDS has\n+  \/\/  a number of implicit alignment assumptions. Leaving this unaligned\n+  \/\/  here will trip of at least ReadClosure (assuming word alignment) and\n+  \/\/  DumpAllocStats (will get confused about counting bytes on 32-bit\n+  \/\/  platforms if we align to anything less than SharedSpaceObjectAlignment\n+  \/\/  here).\n+  newtop = align_up(newtop, SharedSpaceObjectAlignment);\n@@ -212,1 +222,1 @@\n-  memset(p, 0, newtop - p);\n+  memset(p, 0, newtop - p); \/\/ todo: needed? debug_only?\n@@ -216,0 +226,4 @@\n+char* DumpRegion::allocate(size_t num_bytes) {\n+  return allocate(num_bytes, SharedSpaceObjectAlignment);\n+}\n+\n@@ -312,1 +326,1 @@\n-  assert(tag == old_tag, \"old tag doesn't match\");\n+  assert(tag == old_tag, \"tag doesn't match (%d, expected %d)\", old_tag, tag);\n","filename":"src\/hotspot\/share\/cds\/archiveUtils.cpp","additions":19,"deletions":5,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -110,0 +110,12 @@\n+\n+#ifdef ASSERT\n+void DumpAllocStats::verify(int expected_byte_size, bool read_only) const {\n+  int bytes = 0;\n+  const int what = (int)(read_only ? RO : RW);\n+  for (int type = 0; type < int(_number_of_types); type ++) {\n+    bytes += _bytes[what][type];\n+  }\n+  assert(bytes == expected_byte_size, \"counter mismatch (%s: %d vs %d)\",\n+         (read_only ? \"RO\" : \"RW\"), bytes, expected_byte_size);\n+}\n+#endif \/\/ ASSERT\n","filename":"src\/hotspot\/share\/cds\/dumpAllocStats.cpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -91,0 +91,2 @@\n+  DEBUG_ONLY(void verify(int expected_byte_size, bool read_only) const;)\n+\n","filename":"src\/hotspot\/share\/cds\/dumpAllocStats.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -58,0 +58,1 @@\n+#include \"runtime\/safepoint.hpp\"\n@@ -317,1 +318,8 @@\n-    archived_oop->set_mark(markWord::prototype().copy_set_hash(hash_original));\n+\n+    assert(SafepointSynchronize::is_at_safepoint(), \"resolving displaced headers only at safepoint\");\n+    markWord mark = obj->mark();\n+    if (mark.has_displaced_mark_helper()) {\n+      mark = mark.displaced_mark_helper();\n+    }\n+    narrowKlass nklass = mark.narrow_klass();\n+    archived_oop->set_mark(markWord::prototype().copy_set_hash(hash_original) LP64_ONLY(.set_narrow_klass(nklass)));\n@@ -581,1 +589,2 @@\n-    oopDesc::set_mark(mem, markWord::prototype());\n+    oopDesc::set_mark(mem, k->prototype_header());\n+#ifndef _LP64\n@@ -583,0 +592,1 @@\n+#endif\n","filename":"src\/hotspot\/share\/cds\/heapShared.cpp","additions":12,"deletions":2,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -61,0 +61,1 @@\n+#include \"oops\/compressedKlass.hpp\"\n@@ -1314,0 +1315,5 @@\n+#ifdef ASSERT\n+    if (UseCompressedClassPointers) {\n+      CompressedKlassPointers::verify_klass_pointer(record->_klass);\n+    }\n+#endif\n@@ -1315,1 +1321,0 @@\n-    assert(check_alignment(record->_klass), \"Address not aligned\");\n","filename":"src\/hotspot\/share\/classfile\/systemDictionaryShared.cpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -90,0 +90,2 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n+#include \"gc\/shared\/suspendibleThreadSet.hpp\"\n@@ -1569,0 +1571,2 @@\n+  _forwarding = new SlidingForwarding(heap_rs.region(), HeapRegion::LogOfHRGrainBytes - LogHeapWordSize);\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -89,0 +89,1 @@\n+class SlidingForwarding;\n@@ -233,0 +234,2 @@\n+  SlidingForwarding* _forwarding;\n+\n@@ -246,0 +249,4 @@\n+  SlidingForwarding* forwarding() const {\n+    return _forwarding;\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -44,0 +44,1 @@\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n@@ -324,0 +325,2 @@\n+  _heap->forwarding()->clear();\n+\n@@ -331,3 +334,4 @@\n-  if (!has_free_compaction_targets) {\n-    phase2c_prepare_serial_compaction();\n-  }\n+  \/\/ TODO: Disabled for now because it violates sliding-forwarding assumption.\n+  \/\/ if (!has_free_compaction_targets) {\n+  \/\/   phase2c_prepare_serial_compaction();\n+  \/\/ }\n@@ -353,1 +357,2 @@\n-  GCTraceTime(Debug, gc, phases) debug(\"Phase 2: Prepare serial compaction\", scope()->timer());\n+  ShouldNotReachHere(); \/\/ Disabled in Lilliput.\n+  \/\/GCTraceTime(Debug, gc, phases) debug(\"Phase 2: Prepare serial compaction\", scope()->timer());\n@@ -359,0 +364,1 @@\n+  \/*\n@@ -365,0 +371,1 @@\n+  *\/\n@@ -368,0 +375,1 @@\n+  \/*\n@@ -383,0 +391,1 @@\n+  *\/\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullCollector.cpp","additions":13,"deletions":4,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -97,1 +97,2 @@\n-  marker->preserved_stack()->adjust_during_full_gc();\n+  const SlidingForwarding* const forwarding = G1CollectedHeap::heap()->forwarding();\n+  marker->preserved_stack()->adjust_during_full_gc(forwarding);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCAdjustTask.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -94,1 +95,1 @@\n-void G1FullGCCompactionPoint::forward(oop object, size_t size) {\n+void G1FullGCCompactionPoint::forward(SlidingForwarding* const forwarding, oop object, size_t size) {\n@@ -104,1 +105,1 @@\n-    object->forward_to(cast_to_oop(_compaction_top));\n+    forwarding->forward_to(object, cast_to_oop(_compaction_top));\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactionPoint.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"gc\/g1\/g1CollectedHeap.hpp\"\n@@ -35,0 +36,1 @@\n+class SlidingForwarding;\n@@ -78,0 +80,1 @@\n+  const SlidingForwarding* const _forwarding;\n@@ -81,1 +84,3 @@\n-  G1AdjustClosure(G1FullCollector* collector) : _collector(collector) { }\n+  G1AdjustClosure(G1FullCollector* collector) :\n+    _collector(collector),\n+    _forwarding(G1CollectedHeap::heap()->forwarding()) { }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCOopClosures.hpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -69,1 +70,1 @@\n-    oop forwardee = obj->forwardee();\n+    oop forwardee = _forwarding->forwardee(obj);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCOopClosures.inline.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -229,1 +229,1 @@\n-      forwardee = cast_to_oop(m.decode_pointer());\n+      forwardee = obj->forwardee(m);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1OopClosures.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -209,1 +209,1 @@\n-    obj = cast_to_oop(m.decode_pointer());\n+    obj = obj->forwardee(m);\n@@ -223,1 +223,0 @@\n-  assert(from_obj->is_objArray(), \"must be obj array\");\n@@ -253,1 +252,0 @@\n-  assert(from_obj->is_objArray(), \"precondition\");\n@@ -380,1 +378,1 @@\n-                                                  oop const old, size_t word_sz, uint age,\n+                                                  oop const old, Klass* klass, size_t word_sz, uint age,\n@@ -384,1 +382,1 @@\n-    _g1h->gc_tracer_stw()->report_promotion_in_new_plab_event(old->klass(), word_sz * HeapWordSize, age,\n+    _g1h->gc_tracer_stw()->report_promotion_in_new_plab_event(klass, word_sz * HeapWordSize, age,\n@@ -388,1 +386,1 @@\n-    _g1h->gc_tracer_stw()->report_promotion_outside_plab_event(old->klass(), word_sz * HeapWordSize, age,\n+    _g1h->gc_tracer_stw()->report_promotion_outside_plab_event(klass, word_sz * HeapWordSize, age,\n@@ -396,0 +394,1 @@\n+                                                   Klass* klass,\n@@ -418,1 +417,1 @@\n-      report_promotion_event(*dest_attr, old, word_sz, age, obj_ptr, node_index);\n+      report_promotion_event(*dest_attr, old, klass, word_sz, age, obj_ptr, node_index);\n@@ -453,0 +452,4 @@\n+  if (old_mark.is_marked()) {\n+    \/\/ Already forwarded by somebody else, return forwardee.\n+    return old->forwardee(old_mark);\n+  }\n@@ -455,0 +458,3 @@\n+#ifdef _LP64\n+  Klass* klass = old_mark.safe_klass();\n+#else\n@@ -456,0 +462,1 @@\n+#endif\n@@ -468,1 +475,1 @@\n-    obj_ptr = allocate_copy_slow(&dest_attr, old, word_sz, age, node_index);\n+    obj_ptr = allocate_copy_slow(&dest_attr, old, klass, word_sz, age, node_index);\n@@ -621,1 +628,1 @@\n-  oop forward_ptr = old->forward_to_atomic(old, m, memory_order_relaxed);\n+  oop forward_ptr = old->forward_to_self_atomic(m, memory_order_relaxed);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.cpp","additions":16,"deletions":9,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -164,0 +164,1 @@\n+                               Klass* klass,\n@@ -198,1 +199,1 @@\n-                              oop const old, size_t word_sz, uint age,\n+                              oop const old, Klass* klass, size_t word_sz, uint age,\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -112,1 +112,1 @@\n-              touched_words = MIN2((size_t)align_object_size(typeArrayOopDesc::header_size(T_INT)),\n+              touched_words = MIN2((size_t)align_object_size(align_up(typeArrayOopDesc::base_offset_in_bytes(T_INT), HeapWordSize) \/ HeapWordSize),\n","filename":"src\/hotspot\/share\/gc\/parallel\/mutableNUMASpace.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -679,0 +679,10 @@\n+#ifdef _LP64\n+        oop forwardee = obj->forwardee();\n+        markWord header = forwardee->mark();\n+        if (header.has_displaced_mark_helper()) {\n+          header = header.displaced_mark_helper();\n+        }\n+        assert(UseCompressedClassPointers, \"assume +UseCompressedClassPointers\");\n+        narrowKlass nklass = header.narrow_klass();\n+        obj->set_mark(markWord::prototype().set_narrow_klass(nklass));\n+#else\n@@ -680,0 +690,1 @@\n+#endif\n@@ -703,1 +714,1 @@\n-  old->forward_to(old);\n+  old->forward_to_self();\n","filename":"src\/hotspot\/share\/gc\/serial\/defNewGeneration.cpp","additions":12,"deletions":1,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -266,0 +266,2 @@\n+  AdjustPointerClosure adjust_pointer_closure(gch->forwarding());\n+  CLDToOopClosure      adjust_cld_closure(&adjust_pointer_closure, ClassLoaderData::_claim_stw_fullgc_adjust);\n","filename":"src\/hotspot\/share\/gc\/serial\/genMarkSweep.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shared\/genCollectedHeap.hpp\"\n@@ -66,1 +67,0 @@\n-CLDToOopClosure    MarkSweep::adjust_cld_closure(&adjust_pointer_closure, ClassLoaderData::_claim_stw_fullgc_adjust);\n@@ -148,2 +148,2 @@\n-void PreservedMark::adjust_pointer() {\n-  MarkSweep::adjust_pointer(&_obj);\n+void PreservedMark::adjust_pointer(const SlidingForwarding* const forwarding) {\n+  MarkSweep::adjust_pointer(forwarding, &_obj);\n@@ -183,0 +183,2 @@\n+  ContinuationGCSupport::transform_stack_chunk(obj);\n+\n@@ -186,0 +188,8 @@\n+#ifdef _LP64\n+  markWord real_mark = mark;\n+  if (real_mark.has_displaced_mark_helper()) {\n+    real_mark = real_mark.displaced_mark_helper();\n+  }\n+  Klass* klass = real_mark.klass();\n+  obj->set_mark(klass->prototype_header().set_marked());\n+#else\n@@ -187,2 +197,1 @@\n-\n-  ContinuationGCSupport::transform_stack_chunk(obj);\n+#endif\n@@ -211,2 +220,2 @@\n-AdjustPointerClosure MarkSweep::adjust_pointer_closure;\n-\n+  const SlidingForwarding* const forwarding = GenCollectedHeap::heap()->forwarding();\n+\n@@ -216,1 +225,1 @@\n-    _preserved_marks[i].adjust_pointer();\n+    _preserved_marks[i].adjust_pointer(forwarding);\n@@ -223,1 +232,1 @@\n-    p->adjust_pointer();\n+    p->adjust_pointer(forwarding);\n","filename":"src\/hotspot\/share\/gc\/serial\/markSweep.cpp","additions":18,"deletions":9,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+class SlidingForwarding;\n@@ -129,2 +130,0 @@\n-  static AdjustPointerClosure adjust_pointer_closure;\n-  static CLDToOopClosure      adjust_cld_closure;\n@@ -147,1 +146,1 @@\n-  static size_t adjust_pointers(oop obj);\n+  static size_t adjust_pointers(const SlidingForwarding* const forwarding, oop obj);\n@@ -151,1 +150,1 @@\n-  template <class T> static inline void adjust_pointer(T* p);\n+  template <class T> static inline void adjust_pointer(const SlidingForwarding* const forwarding, T* p);\n@@ -185,0 +184,2 @@\n+private:\n+  const SlidingForwarding* const _forwarding;\n@@ -186,0 +187,1 @@\n+  AdjustPointerClosure(const SlidingForwarding* forwarding) : _forwarding(forwarding) {}\n@@ -199,1 +201,1 @@\n-  void adjust_pointer();\n+  void adjust_pointer(const SlidingForwarding* const forwarding);\n","filename":"src\/hotspot\/share\/gc\/serial\/markSweep.hpp","additions":7,"deletions":5,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -42,1 +43,1 @@\n-template <class T> inline void MarkSweep::adjust_pointer(T* p) {\n+template <class T> inline void MarkSweep::adjust_pointer(const SlidingForwarding* const forwarding, T* p) {\n@@ -48,2 +49,4 @@\n-    if (obj->is_forwarded()) {\n-      oop new_obj = obj->forwardee();\n+    markWord header = obj->mark();\n+    if (header.is_marked()) {\n+      oop new_obj = forwarding->forwardee(obj);\n+      assert(new_obj != NULL, \"must be forwarded\");\n@@ -57,1 +60,1 @@\n-void AdjustPointerClosure::do_oop_work(T* p)           { MarkSweep::adjust_pointer(p); }\n+void AdjustPointerClosure::do_oop_work(T* p)           { MarkSweep::adjust_pointer(_forwarding, p); }\n@@ -61,2 +64,3 @@\n-inline size_t MarkSweep::adjust_pointers(oop obj) {\n-  return obj->oop_iterate_size(&MarkSweep::adjust_pointer_closure);\n+inline size_t MarkSweep::adjust_pointers(const SlidingForwarding* const forwarding, oop obj) {\n+  AdjustPointerClosure cl(forwarding);\n+  return obj->oop_iterate_size(&cl);\n","filename":"src\/hotspot\/share\/gc\/serial\/markSweep.inline.hpp","additions":10,"deletions":6,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -226,4 +226,0 @@\n-  if (is_in(object->klass_or_null())) {\n-    return false;\n-  }\n-\n@@ -256,2 +252,4 @@\n-  _filler_array_max_size = align_object_size(filler_array_hdr_size() +\n-                                             max_len \/ elements_per_word);\n+  int header_size_in_bytes = arrayOopDesc::base_offset_in_bytes(T_INT);\n+  assert(header_size_in_bytes % sizeof(jint) == 0, \"must be aligned to int\");\n+  int header_size_in_ints = header_size_in_bytes \/ sizeof(jint);\n+  _filler_array_max_size = align_object_size((header_size_in_ints + max_len) \/ elements_per_word);\n@@ -419,1 +417,3 @@\n-  size_t max_int_size = typeArrayOopDesc::header_size(T_INT) +\n+  int header_size_in_bytes = typeArrayOopDesc::base_offset_in_bytes(T_INT);\n+  assert(header_size_in_bytes % sizeof(jint) == 0, \"header size must align to int\");\n+  size_t max_int_size = header_size_in_bytes \/ HeapWordSize +\n@@ -425,5 +425,2 @@\n-size_t CollectedHeap::filler_array_hdr_size() {\n-  return align_object_offset(arrayOopDesc::header_size(T_INT)); \/\/ align to Long\n-}\n-\n-  return align_object_size(filler_array_hdr_size()); \/\/ align to MinObjAlignment\n+  int aligned_header_size_words = align_up(arrayOopDesc::base_offset_in_bytes(T_INT), HeapWordSize) \/ HeapWordSize;\n+  return align_object_size(aligned_header_size_words); \/\/ align to MinObjAlignment\n@@ -434,2 +431,3 @@\n-  Copy::fill_to_words(start + filler_array_hdr_size(),\n-                      words - filler_array_hdr_size(), value);\n+  int payload_start = align_up(arrayOopDesc::base_offset_in_bytes(T_INT), HeapWordSize) \/ HeapWordSize;\n+  Copy::fill_to_words(start + payload_start,\n+                      words - payload_start, value);\n@@ -459,2 +457,3 @@\n-  const size_t payload_size = words - filler_array_hdr_size();\n-  const size_t len = payload_size * HeapWordSize \/ sizeof(jint);\n+  const size_t payload_size_bytes = words * HeapWordSize - arrayOopDesc::base_offset_in_bytes(T_INT);\n+  assert(payload_size_bytes % sizeof(jint) == 0, \"must be int aligned\");\n+  const size_t len = payload_size_bytes \/ sizeof(jint);\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.cpp","additions":15,"deletions":16,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -166,1 +166,0 @@\n-  static inline size_t filler_array_hdr_size();\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -58,0 +58,1 @@\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n@@ -127,0 +128,1 @@\n+  _forwarding = new SlidingForwarding(_reserved);\n@@ -1046,0 +1048,1 @@\n+  _forwarding->clear();\n","filename":"src\/hotspot\/share\/gc\/shared\/genCollectedHeap.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+class SlidingForwarding;\n@@ -90,0 +91,2 @@\n+  SlidingForwarding* _forwarding;\n+\n@@ -319,0 +322,4 @@\n+  SlidingForwarding* forwarding() const {\n+    return _forwarding;\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/genCollectedHeap.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -402,1 +402,0 @@\n-  oopDesc::set_klass_gap(mem, 0);\n@@ -408,2 +407,0 @@\n-  \/\/ May be bootstrapping\n-  oopDesc::set_mark(mem, markWord::prototype());\n@@ -413,0 +410,4 @@\n+#ifdef _LP64\n+  oopDesc::release_set_mark(mem, _klass->prototype_header());\n+#else\n+  oopDesc::set_mark(mem, _klass->prototype_header());\n@@ -414,0 +415,1 @@\n+#endif\n@@ -427,1 +429,1 @@\n-  const size_t hs = arrayOopDesc::header_size(array_klass->element_type());\n+  const size_t hs = align_up(arrayOopDesc::base_offset_in_bytes(array_klass->element_type()), HeapWordSize) \/ HeapWordSize;\n","filename":"src\/hotspot\/share\/gc\/shared\/memAllocator.cpp","additions":6,"deletions":4,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"runtime\/objectMonitor.hpp\"\n@@ -266,0 +267,1 @@\n+        ObjectMonitor::maybe_deflate_dead(ptr);\n","filename":"src\/hotspot\/share\/gc\/shared\/oopStorage.inline.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -346,1 +347,1 @@\n-                                    CompactPoint* cp, HeapWord* compact_top) {\n+                                    CompactPoint* cp, HeapWord* compact_top, SlidingForwarding* const forwarding) {\n@@ -369,1 +370,1 @@\n-    q->forward_to(cast_to_oop(compact_top));\n+    forwarding->forward_to(q, cast_to_oop(compact_top));\n@@ -417,0 +418,1 @@\n+  SlidingForwarding* const forwarding = GenCollectedHeap::heap()->forwarding();\n@@ -422,1 +424,1 @@\n-      compact_top = cp->space->forward(cast_to_oop(cur_obj), size, cp, compact_top);\n+      compact_top = cp->space->forward(cast_to_oop(cur_obj), size, cp, compact_top, forwarding);\n@@ -438,1 +440,1 @@\n-        compact_top = cp->space->forward(obj, obj->size(), cp, compact_top);\n+        compact_top = cp->space->forward(obj, obj->size(), cp, compact_top, forwarding);\n@@ -481,0 +483,1 @@\n+  const SlidingForwarding* const forwarding = GenCollectedHeap::heap()->forwarding();\n@@ -492,1 +495,1 @@\n-      size_t size = MarkSweep::adjust_pointers(cast_to_oop(cur_obj));\n+      size_t size = MarkSweep::adjust_pointers(forwarding, cast_to_oop(cur_obj));\n@@ -533,0 +536,2 @@\n+  const SlidingForwarding* const forwarding = GenCollectedHeap::heap()->forwarding();\n+\n@@ -546,1 +551,1 @@\n-      HeapWord* compaction_top = cast_from_oop<HeapWord*>(cast_to_oop(cur_obj)->forwardee());\n+      HeapWord* compaction_top = cast_from_oop<HeapWord*>(forwarding->forwardee(cast_to_oop(cur_obj)));\n","filename":"src\/hotspot\/share\/gc\/shared\/space.cpp","additions":11,"deletions":6,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -59,0 +59,1 @@\n+class SlidingForwarding;\n@@ -380,1 +381,1 @@\n-                    HeapWord* compact_top);\n+                    HeapWord* compact_top, SlidingForwarding* const forwarding);\n","filename":"src\/hotspot\/share\/gc\/shared\/space.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/shared\/genCollectedHeap.hpp\"\n","filename":"src\/hotspot\/share\/gc\/shared\/space.inline.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -190,0 +191,1 @@\n+    heap->forwarding()->clear();\n@@ -300,2 +302,3 @@\n-  PreservedMarks*          const _preserved_marks;\n-  ShenandoahHeap*          const _heap;\n+  PreservedMarks*    const _preserved_marks;\n+  SlidingForwarding* const _forwarding;\n+  ShenandoahHeap*    const _heap;\n@@ -313,0 +316,1 @@\n+    _forwarding(ShenandoahHeap::heap()->forwarding()),\n@@ -366,1 +370,1 @@\n-    p->forward_to(cast_to_oop(_compact_point));\n+    _forwarding->forward_to(p, cast_to_oop(_compact_point));\n@@ -440,0 +444,1 @@\n+  SlidingForwarding* forwarding = heap->forwarding();\n@@ -474,1 +479,1 @@\n-        old_obj->forward_to(cast_to_oop(heap->get_region(start)->bottom()));\n+        forwarding->forward_to(old_obj, cast_to_oop(heap->get_region(start)->bottom()));\n@@ -725,1 +730,2 @@\n-  ShenandoahHeap* const _heap;\n+  ShenandoahHeap*           const _heap;\n+  const SlidingForwarding*  const _forwarding;\n@@ -735,1 +741,1 @@\n-        oop forw = obj->forwardee();\n+        oop forw = _forwarding->forwardee(obj);\n@@ -744,0 +750,1 @@\n+    _forwarding(_heap->forwarding()),\n@@ -805,1 +812,2 @@\n-    _preserved_marks->get(worker_id)->adjust_during_full_gc();\n+    const SlidingForwarding* const forwarding = ShenandoahHeap::heap()->forwarding();\n+    _preserved_marks->get(worker_id)->adjust_during_full_gc(forwarding);\n@@ -835,2 +843,3 @@\n-  ShenandoahHeap* const _heap;\n-  uint            const _worker_id;\n+  ShenandoahHeap*          const _heap;\n+  const SlidingForwarding* const _forwarding;\n+  uint                     const _worker_id;\n@@ -840,1 +849,1 @@\n-    _heap(ShenandoahHeap::heap()), _worker_id(worker_id) {}\n+    _heap(ShenandoahHeap::heap()), _forwarding(_heap->forwarding()), _worker_id(worker_id) {}\n@@ -847,1 +856,1 @@\n-      HeapWord* compact_to = cast_from_oop<HeapWord*>(p->forwardee());\n+      HeapWord* compact_to = cast_from_oop<HeapWord*>(_forwarding->forwardee(p));\n@@ -944,0 +953,1 @@\n+  const SlidingForwarding* const forwarding = heap->forwarding();\n@@ -958,1 +968,1 @@\n-      size_t new_start = heap->heap_region_index_containing(old_obj->forwardee());\n+      size_t new_start = heap->heap_region_index_containing(forwarding->forwardee(old_obj));\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":22,"deletions":12,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n@@ -196,0 +197,2 @@\n+  _forwarding = new SlidingForwarding(_heap_region, ShenandoahHeapRegion::region_size_words_shift());\n+\n@@ -952,1 +955,1 @@\n-    if (!p->is_forwarded()) {\n+    if (!ShenandoahForwarding::is_forwarded(p)) {\n@@ -1297,0 +1300,1 @@\n+    shenandoah_assert_not_in_cset_except(NULL, obj, cancelled_gc());\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -301,1 +301,0 @@\n-        assert(!UseCompressedClassPointers, \"should only happen without compressed class pointers\");\n","filename":"src\/hotspot\/share\/gc\/z\/c2\/zBarrierSetC2.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -53,2 +53,9 @@\n-  const size_t header = arrayOopDesc::header_size(element_type);\n-  const size_t payload_size = _word_size - header;\n+  int base_offset = arrayOopDesc::base_offset_in_bytes(element_type);\n+\n+  \/\/ Clear leading 32 bit, if necessary.\n+  if (!is_aligned(base_offset, HeapWordSize)) {\n+    assert(is_aligned(base_offset, BytesPerInt), \"array base must be 32 bit aligned\");\n+    *reinterpret_cast<jint*>(reinterpret_cast<char*>(mem) + base_offset) = 0;\n+    base_offset += BytesPerInt;\n+  }\n+  assert(is_aligned(base_offset, HeapWordSize), \"remaining array base must be 64 bit aligned\");\n@@ -56,0 +63,2 @@\n+  const size_t header = heap_word_size(base_offset);\n+  const size_t payload_size = _word_size - header;\n@@ -66,2 +75,1 @@\n-  arrayOopDesc::set_mark(mem, markWord::prototype());\n-  arrayOopDesc::release_set_klass(mem, _klass);\n+  oopDesc::release_set_mark(mem, _klass->prototype_header());\n","filename":"src\/hotspot\/share\/gc\/z\/zObjArrayAllocator.cpp","additions":12,"deletions":4,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -2002,0 +2002,3 @@\n+#ifdef _LP64\n+              oopDesc::release_set_mark(result, ik->prototype_header());\n+#else\n@@ -2003,2 +2006,1 @@\n-              oopDesc::set_klass_gap(result, 0);\n-\n+#endif\n","filename":"src\/hotspot\/share\/interpreter\/zero\/bytecodeInterpreter.cpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2207,1 +2207,1 @@\n-  return arrayOopDesc::header_size(type) * HeapWordSize;\n+  return arrayOopDesc::base_offset_in_bytes(type);\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -271,1 +271,0 @@\n-  volatile_nonstatic_field(oopDesc,            _metadata._klass,                              Klass*)                                \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -550,0 +550,4 @@\n+    \/\/ Note Lilliput: the advantages of this strategy were questionable before\n+    \/\/  (since CDS=off + Compressed oops + heap large enough to suffocate us out of lower 32g\n+    \/\/  is rare) and with Lilliput the encoding range drastically shrank. We may just do away\n+    \/\/  with this altogether.\n","filename":"src\/hotspot\/share\/memory\/virtualspace.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -53,0 +53,1 @@\n+#include \"runtime\/globals.hpp\"\n@@ -54,0 +55,1 @@\n+#include \"utilities\/align.hpp\"\n@@ -195,1 +197,3 @@\n-  return Metaspace::allocate(loader_data, word_size, MetaspaceObj::ClassType, THREAD);\n+  MetaWord* p = Metaspace::allocate(loader_data, word_size, MetaspaceObj::ClassType, THREAD);\n+  assert(is_aligned(p, KlassAlignmentInBytes), \"metaspace returned badly aligned memory.\");\n+  return p;\n@@ -203,0 +207,1 @@\n+                           _prototype_header(markWord::prototype() LP64_ONLY(.set_klass(this))),\n@@ -747,0 +752,2 @@\n+     st->print(BULLET\"prototype_header: \" INTPTR_FORMAT, _prototype_header.value());\n+     st->cr();\n@@ -770,0 +777,4 @@\n+  if (UseCompressedClassPointers) {\n+    assert(is_aligned(this, KlassAlignmentInBytes), \"misaligned Klass structure\");\n+  }\n+\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":12,"deletions":1,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -168,0 +168,2 @@\n+  markWord _prototype_header;   \/\/ Used to initialize objects' header\n+\n@@ -671,0 +673,4 @@\n+  markWord prototype_header() const      { return _prototype_header; }\n+  inline void set_prototype_header(markWord header);\n+  static ByteSize prototype_header_offset() { return in_ByteSize(offset_of(Klass, _prototype_header)); }\n+\n","filename":"src\/hotspot\/share\/oops\/klass.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n@@ -37,0 +38,1 @@\n+#include \"runtime\/interfaceSupport.inline.hpp\"\n@@ -147,20 +149,8 @@\n-bool oopDesc::has_klass_gap() {\n-  \/\/ Only has a klass gap when compressed class pointers are used.\n-  return UseCompressedClassPointers;\n-}\n-\n-#if INCLUDE_CDS_JAVA_HEAP\n-void oopDesc::set_narrow_klass(narrowKlass nk) {\n-  assert(DumpSharedSpaces, \"Used by CDS only. Do not abuse!\");\n-  assert(UseCompressedClassPointers, \"must be\");\n-  _metadata._compressed_klass = nk;\n-}\n-#endif\n-\n-  if (UseCompressedClassPointers) {\n-    narrowKlass narrow_klass = obj->_metadata._compressed_klass;\n-    if (narrow_klass == 0) return NULL;\n-    return (void*)CompressedKlassPointers::decode_raw(narrow_klass);\n-  } else {\n-    return obj->_metadata._klass;\n-  }\n+  \/\/ TODO: Remove method altogether and replace with calls to obj->klass() ?\n+  \/\/ OTOH, we may eventually get rid of locking in header, and then no\n+  \/\/ longer have to deal with that anymore.\n+#ifdef _LP64\n+  return obj->klass();\n+#else\n+  return obj->_klass;\n+#endif\n@@ -181,0 +171,15 @@\n+#ifdef _LP64\n+JRT_LEAF(narrowKlass, oopDesc::load_nklass_runtime(oopDesc* o))\n+  assert(o != NULL, \"null-check\");\n+  oop obj = oop(o);\n+  assert(oopDesc::is_oop(obj), \"need a valid oop here: \" PTR_FORMAT, p2i(o));\n+  markWord header = obj->mark();\n+  if (!header.is_neutral()) {\n+    header = ObjectSynchronizer::stable_mark(obj);\n+  }\n+  assert(header.is_neutral(), \"expect neutral header here\");\n+  narrowKlass nklass = header.narrow_klass();\n+  return nklass;\n+JRT_END\n+#endif\n+\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":25,"deletions":20,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"oops\/compressedKlass.hpp\"\n@@ -57,4 +58,3 @@\n-  union _metadata {\n-    Klass*      _klass;\n-    narrowKlass _compressed_klass;\n-  } _metadata;\n+#ifndef _LP64\n+  Klass*            _klass;\n+#endif\n@@ -78,0 +78,1 @@\n+  static inline void release_set_mark(HeapWord* mem, markWord m);\n@@ -89,1 +90,1 @@\n-  void set_narrow_klass(narrowKlass nk) NOT_CDS_JAVA_HEAP_RETURN;\n+#ifndef _LP64\n@@ -92,3 +93,1 @@\n-\n-  \/\/ For klass field compression\n-  static inline void set_klass_gap(HeapWord* mem, int z);\n+#endif\n@@ -260,0 +259,1 @@\n+  inline void forward_to_self();\n@@ -266,0 +266,1 @@\n+  inline oop forward_to_self_atomic(markWord compare, atomic_memory_order order = memory_order_conservative);\n@@ -268,0 +269,1 @@\n+  inline oop forwardee(markWord header) const;\n@@ -307,2 +309,0 @@\n-  static bool has_klass_gap();\n-\n@@ -311,4 +311,7 @@\n-  static int klass_offset_in_bytes()     { return offset_of(oopDesc, _metadata._klass); }\n-  static int klass_gap_offset_in_bytes() {\n-    assert(has_klass_gap(), \"only applicable to compressed klass pointers\");\n-    return klass_offset_in_bytes() + sizeof(narrowKlass);\n+  static int klass_offset_in_bytes()     {\n+#ifdef _LP64\n+    STATIC_ASSERT(markWord::klass_shift % 8 == 0);\n+    return mark_offset_in_bytes() + markWord::klass_shift \/ 8;\n+#else\n+    return offset_of(oopDesc, _klass);\n+#endif\n@@ -321,0 +324,5 @@\n+  \/\/ Runtime entry\n+#ifdef _LP64\n+  static narrowKlass load_nklass_runtime(oopDesc* o);\n+#endif\n+\n","filename":"src\/hotspot\/share\/oops\/oop.hpp","additions":22,"deletions":14,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n@@ -36,1 +37,1 @@\n-#include \"oops\/markWord.hpp\"\n+#include \"oops\/markWord.inline.hpp\"\n@@ -40,0 +41,2 @@\n+#include \"runtime\/safepoint.hpp\"\n+#include \"runtime\/synchronizer.hpp\"\n@@ -72,0 +75,4 @@\n+void oopDesc::release_set_mark(HeapWord* mem, markWord m) {\n+  Atomic::release_store((markWord*)(((char*)mem) + mark_offset_in_bytes()), m);\n+}\n+\n@@ -81,1 +88,8 @@\n-  set_mark(markWord::prototype());\n+#ifdef _LP64\n+  markWord header = ObjectSynchronizer::stable_mark(cast_to_oop(this));\n+  assert(UseCompressedClassPointers, \"expect compressed klass pointers\");\n+  header = markWord((header.value() & markWord::klass_mask_in_place) | markWord::prototype().value());\n+#else\n+  markWord header = markWord::prototype();\n+#endif\n+  set_mark(header);\n@@ -85,4 +99,5 @@\n-  if (UseCompressedClassPointers) {\n-    return CompressedKlassPointers::decode_not_null(_metadata._compressed_klass);\n-  } else {\n-    return _metadata._klass;\n+#ifdef _LP64\n+  assert(UseCompressedClassPointers, \"only with compressed class pointers\");\n+  markWord header = mark();\n+  if (!header.is_neutral()) {\n+    header = ObjectSynchronizer::stable_mark(cast_to_oop(this));\n@@ -90,0 +105,4 @@\n+  return header.klass();\n+#else\n+  return _klass;\n+#endif\n@@ -93,4 +112,5 @@\n-  if (UseCompressedClassPointers) {\n-    return CompressedKlassPointers::decode(_metadata._compressed_klass);\n-  } else {\n-    return _metadata._klass;\n+#ifdef _LP64\n+  assert(UseCompressedClassPointers, \"only with compressed class pointers\");\n+  markWord header = mark();\n+  if (!header.is_neutral()) {\n+    header = ObjectSynchronizer::stable_mark(cast_to_oop(this));\n@@ -98,0 +118,4 @@\n+  return header.klass_or_null();\n+#else\n+  return _klass;\n+#endif\n@@ -101,5 +125,5 @@\n-  if (UseCompressedClassPointers) {\n-    narrowKlass nklass = Atomic::load_acquire(&_metadata._compressed_klass);\n-    return CompressedKlassPointers::decode(nklass);\n-  } else {\n-    return Atomic::load_acquire(&_metadata._klass);\n+#ifdef _LP64\n+  assert(UseCompressedClassPointers, \"only with compressed class pointers\");\n+  markWord header = mark_acquire();\n+  if (!header.is_neutral()) {\n+    header = ObjectSynchronizer::stable_mark(cast_to_oop(this));\n@@ -107,0 +131,4 @@\n+  return header.klass_or_null();\n+#else\n+  return Atomic::load_acquire(&_klass);\n+#endif\n@@ -109,0 +137,1 @@\n+#ifndef _LP64\n@@ -111,5 +140,1 @@\n-  if (UseCompressedClassPointers) {\n-    _metadata._compressed_klass = CompressedKlassPointers::encode_not_null(k);\n-  } else {\n-    _metadata._klass = k;\n-  }\n+  _klass = k;\n@@ -128,6 +153,1 @@\n-\n-void oopDesc::set_klass_gap(HeapWord* mem, int v) {\n-  if (UseCompressedClassPointers) {\n-    *(int*)(((char*)mem) + klass_gap_offset_in_bytes()) = v;\n-  }\n-}\n+#endif\n@@ -262,1 +282,1 @@\n-  assert(m.decode_pointer() == p, \"encoding must be reversible\");\n+  assert(forwardee(m) == p, \"encoding must be reversable\");\n@@ -266,0 +286,17 @@\n+void oopDesc::forward_to_self() {\n+#ifdef _LP64\n+  verify_forwardee(this);\n+  markWord m = mark();\n+  \/\/ If mark is displaced, we need to preserve the Klass* from real header.\n+  assert(SafepointSynchronize::is_at_safepoint(), \"we can only safely fetch the displaced header at safepoint\");\n+  if (m.has_displaced_mark_helper()) {\n+    m = m.displaced_mark_helper();\n+  }\n+  m = m.set_self_forwarded();\n+  assert(forwardee(m) == cast_to_oop(this), \"encoding must be reversable\");\n+  set_mark(m);\n+#else\n+  forward_to(oop(this));\n+#endif\n+}\n+\n@@ -269,1 +306,20 @@\n-  assert(m.decode_pointer() == p, \"encoding must be reversible\");\n+  assert(forwardee(m) == p, \"encoding must be reversable\");\n+  markWord old_mark = cas_set_mark(m, compare, order);\n+  if (old_mark == compare) {\n+    return NULL;\n+  } else {\n+    return forwardee(old_mark);\n+  }\n+}\n+\n+oop oopDesc::forward_to_self_atomic(markWord compare, atomic_memory_order order) {\n+#ifdef _LP64\n+  verify_forwardee(this);\n+  markWord m = compare;\n+  \/\/ If mark is displaced, we need to preserve the Klass* from real header.\n+  assert(SafepointSynchronize::is_at_safepoint(), \"we can only safely fetch the displaced header at safepoint\");\n+  if (m.has_displaced_mark_helper()) {\n+    m = m.displaced_mark_helper();\n+  }\n+  m = m.set_self_forwarded();\n+  assert(forwardee(m) == cast_to_oop(this), \"encoding must be reversable\");\n@@ -274,1 +330,2 @@\n-    return cast_to_oop(old_mark.decode_pointer());\n+    assert(old_mark.is_marked(), \"must be marked here\");\n+    return forwardee(old_mark);\n@@ -276,0 +333,3 @@\n+#else\n+  return forward_to_atomic(oop(this), compare, order);\n+#endif\n@@ -282,2 +342,14 @@\n-  assert(is_forwarded(), \"only decode when actually forwarded\");\n-  return cast_to_oop(mark().decode_pointer());\n+  return forwardee(mark());\n+}\n+\n+oop oopDesc::forwardee(markWord header) const {\n+  assert(header.is_marked(), \"must be forwarded\");\n+#ifdef _LP64\n+  if (header.self_forwarded()) {\n+    return cast_to_oop(this);\n+  } else\n+#endif\n+  {\n+    assert(header.is_marked(), \"only decode when actually forwarded\");\n+    return cast_to_oop(header.decode_pointer());\n+  }\n@@ -338,1 +410,0 @@\n-  assert(k == klass(), \"wrong klass\");\n","filename":"src\/hotspot\/share\/oops\/oop.inline.hpp","additions":103,"deletions":32,"binary":false,"changes":135,"status":"modified"},{"patch":"@@ -1575,3 +1575,3 @@\n-  Node* mark_node = NULL;\n-  \/\/ For now only enable fast locking for non-array types\n-  mark_node = phase->MakeConX(markWord::prototype().value());\n+  Node* klass_node = in(AllocateNode::KlassNode);\n+  Node* proto_adr = phase->transform(new AddPNode(klass_node, klass_node, phase->MakeConX(in_bytes(Klass::prototype_header_offset()))));\n+  Node* mark_node = LoadNode::make(*phase, control, mem, proto_adr, TypeRawPtr::BOTTOM, TypeX_X, TypeX_X->basic_type(), MemNode::unordered);\n","filename":"src\/hotspot\/share\/opto\/callnode.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1664,0 +1664,1 @@\n+#ifndef _LP64\n@@ -1665,0 +1666,1 @@\n+#endif\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -311,1 +311,1 @@\n-    const size_t hs = arrayOopDesc::header_size(elem_type);\n+    const size_t hs_bytes = arrayOopDesc::base_offset_in_bytes(elem_type);\n@@ -313,1 +313,1 @@\n-    const size_t aligned_hs = align_object_offset(hs);\n+    const size_t aligned_hs_bytes = align_up(hs_bytes, BytesPerLong);\n@@ -315,2 +315,2 @@\n-    if (aligned_hs > hs) {\n-      Copy::zero_to_words(obj+hs, aligned_hs-hs);\n+    if (aligned_hs_bytes > hs_bytes) {\n+      Copy::zero_to_bytes(obj + hs_bytes, aligned_hs_bytes - hs_bytes);\n@@ -319,0 +319,1 @@\n+    const size_t aligned_hs = aligned_hs_bytes \/ HeapWordSize;\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -70,1 +70,1 @@\n-  ( arrayOopDesc::header_size(T_DOUBLE) * HeapWordSize \\\n+  ( arrayOopDesc::base_offset_in_bytes(T_DOUBLE) \\\n","filename":"src\/hotspot\/share\/prims\/unsafe.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1504,3 +1504,0 @@\n-      if (COMPRESSED_CLASS_POINTERS_DEPENDS_ON_COMPRESSED_OOPS) {\n-        FLAG_SET_DEFAULT(UseCompressedClassPointers, false);\n-      }\n@@ -1512,1 +1509,0 @@\n-\n@@ -1517,25 +1513,14 @@\n-  \/\/ On some architectures, the use of UseCompressedClassPointers implies the use of\n-  \/\/ UseCompressedOops. The reason is that the rheap_base register of said platforms\n-  \/\/ is reused to perform some optimized spilling, in order to use rheap_base as a\n-  \/\/ temp register. But by treating it as any other temp register, spilling can typically\n-  \/\/ be completely avoided instead. So it is better not to perform this trick. And by\n-  \/\/ not having that reliance, large heaps, or heaps not supporting compressed oops,\n-  \/\/ can still use compressed class pointers.\n-  if (COMPRESSED_CLASS_POINTERS_DEPENDS_ON_COMPRESSED_OOPS && !UseCompressedOops) {\n-    if (UseCompressedClassPointers) {\n-      warning(\"UseCompressedClassPointers requires UseCompressedOops\");\n-    }\n-    FLAG_SET_DEFAULT(UseCompressedClassPointers, false);\n-  } else {\n-    \/\/ Turn on UseCompressedClassPointers too\n-    if (FLAG_IS_DEFAULT(UseCompressedClassPointers)) {\n-      FLAG_SET_ERGO(UseCompressedClassPointers, true);\n-    }\n-    \/\/ Check the CompressedClassSpaceSize to make sure we use compressed klass ptrs.\n-    if (UseCompressedClassPointers) {\n-      if (CompressedClassSpaceSize > KlassEncodingMetaspaceMax) {\n-        warning(\"CompressedClassSpaceSize is too large for UseCompressedClassPointers\");\n-        FLAG_SET_DEFAULT(UseCompressedClassPointers, false);\n-      }\n-    }\n-  }\n+  if (!UseCompressedClassPointers) {\n+    \/\/ Lilliput requires compressed class pointers. Default shall reflect that.\n+    \/\/ If user specifies -UseCompressedClassPointers, it should be reverted with\n+    \/\/ a warning.\n+    assert(!FLAG_IS_DEFAULT(UseCompressedClassPointers), \"Wrong default for UseCompressedClassPointers\");\n+    warning(\"Lilliput reqires compressed class pointers.\");\n+    FLAG_SET_ERGO(UseCompressedClassPointers, true);\n+  }\n+  \/\/ Assert validity of compressed class space size. User arg should have been checked at this point\n+  \/\/ (see CompressedClassSpaceSizeConstraintFunc()), so no need to be nice about it, this fires in\n+  \/\/ case the default is wrong.\n+  assert(CompressedClassSpaceSize <= Metaspace::max_class_space_size(),\n+         \"CompressedClassSpaceSize \" SIZE_FORMAT \" too large (max: \" SIZE_FORMAT \")\",\n+         CompressedClassSpaceSize, Metaspace::max_class_space_size());\n@@ -1704,3 +1689,0 @@\n-          if (COMPRESSED_CLASS_POINTERS_DEPENDS_ON_COMPRESSED_OOPS) {\n-            FLAG_SET_ERGO(UseCompressedClassPointers, false);\n-          }\n@@ -4096,0 +4078,1 @@\n+\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":15,"deletions":32,"binary":false,"changes":47,"status":"modified"},{"patch":"@@ -128,1 +128,1 @@\n-  product(bool, UseCompressedClassPointers, false,                          \\\n+  product(bool, UseCompressedClassPointers, true,                           \\\n@@ -1059,1 +1059,1 @@\n-  develop(bool, UseHeavyMonitors, false,                                    \\\n+  product(bool, UseHeavyMonitors, false, DIAGNOSTIC,                        \\\n@@ -1420,1 +1420,1 @@\n-          range(1*M, 3*G)                                                   \\\n+          constraint(CompressedClassSpaceSizeConstraintFunc,AtParse)        \\\n@@ -1422,1 +1422,1 @@\n-  develop(size_t, CompressedClassSpaceBaseAddress, 0,                       \\\n+  product(size_t, CompressedClassSpaceBaseAddress, 0, DIAGNOSTIC,           \\\n@@ -1985,0 +1985,6 @@\n+                                                                            \\\n+  product(bool, HeapObjectStats, false, DIAGNOSTIC,                         \\\n+             \"Enable gathering of heap object statistics\")                  \\\n+                                                                            \\\n+  product(size_t, HeapObjectStatsSamplingInterval, 500, DIAGNOSTIC,         \\\n+             \"Heap object statistics sampling interval (ms)\")               \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":10,"deletions":4,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -603,0 +603,16 @@\n+\/\/ We might access the dead object headers for parsable heap walk, make sure\n+\/\/ headers are in correct shape, e.g. monitors deflated.\n+void ObjectMonitor::maybe_deflate_dead(oop* p) {\n+  oop obj = *p;\n+  assert(obj != NULL, \"must not yet been cleared\");\n+  markWord mark = obj->mark();\n+  if (mark.has_monitor()) {\n+    ObjectMonitor* monitor = mark.monitor();\n+    if (p == monitor->_object.ptr_raw()) {\n+      assert(monitor->object_peek() == obj, \"lock object must match\");\n+      markWord dmw = monitor->header();\n+      obj->set_mark(dmw);\n+    }\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.cpp","additions":16,"deletions":0,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -326,0 +326,2 @@\n+  static void maybe_deflate_dead(oop* p);\n+\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+#include \"oops\/compressedKlass.hpp\"\n","filename":"src\/hotspot\/share\/runtime\/os.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -174,0 +174,2 @@\n+address StubRoutines::_load_nklass = NULL;\n+\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -263,1 +263,3 @@\n- public:\n+  static address _load_nklass;\n+\n+public:\n@@ -427,0 +429,2 @@\n+  static address load_nklass()         { return _load_nklass; }\n+\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.hpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"gc\/shared\/suspendibleThreadSet.hpp\"\n@@ -742,1 +743,1 @@\n-static markWord read_stable_mark(oop obj) {\n+markWord ObjectSynchronizer::read_stable_mark(oop obj) {\n@@ -802,0 +803,67 @@\n+\/\/ Safely load a mark word from an object, even with racing stack-locking or monitor inflation.\n+\/\/ The protocol is a partial inflation-protocol: it installs INFLATING into the object's mark\n+\/\/ word in order to prevent an stack-locks or inflations from interferring (or detect such\n+\/\/ interference and retry), but then, instead of creating and installing a monitor, simply\n+\/\/ read and return the real mark word.\n+markWord ObjectSynchronizer::stable_mark(oop object) {\n+  for (;;) {\n+    const markWord mark = read_stable_mark(object);\n+    assert(!mark.is_being_inflated(), \"read_stable_mark must prevent inflating mark\");\n+\n+    \/\/ The mark can be in one of the following states:\n+    \/\/ *  Inflated     - just return mark from inflated monitor\n+    \/\/ *  Stack-locked - coerce it to inflating, and then return displaced mark\n+    \/\/ *  Neutral      - return mark\n+    \/\/ *  Marked       - return mark\n+\n+    \/\/ CASE: inflated\n+    if (mark.has_monitor()) {\n+      ObjectMonitor* inf = mark.monitor();\n+      markWord dmw = inf->header();\n+      assert(dmw.is_neutral(), \"invariant: header=\" INTPTR_FORMAT, dmw.value());\n+      return dmw;\n+    }\n+\n+    \/\/ CASE: stack-locked\n+    \/\/ Could be stack-locked either by this thread or by some other thread.\n+    if (mark.has_locker()) {\n+      BasicLock* lock = mark.locker();\n+      if (Thread::current()->is_lock_owned((address)lock)) {\n+        \/\/ If locked by this thread, it is safe to access the displaced header.\n+        markWord dmw = lock->displaced_header();\n+        assert(dmw.is_neutral(), \"invariant: header=\" INTPTR_FORMAT, dmw.value());\n+        return dmw;\n+      }\n+\n+      \/\/ Otherwise, attempt to temporarily install INFLATING into the mark-word,\n+      \/\/ to prevent inflation or unlocking by competing thread.\n+      markWord cmp = object->cas_set_mark(markWord::INFLATING(), mark);\n+      if (cmp != mark) {\n+        continue;       \/\/ Interference -- just retry\n+      }\n+\n+      \/\/ fetch the displaced mark from the owner's stack.\n+      \/\/ The owner can't die or unwind past the lock while our INFLATING\n+      \/\/ object is in the mark.  Furthermore the owner can't complete\n+      \/\/ an unlock on the object, either.\n+      markWord dmw = mark.displaced_mark_helper();\n+      \/\/ Catch if the object's header is not neutral (not locked and\n+      \/\/ not marked is what we care about here).\n+      assert(dmw.is_neutral(), \"invariant: header=\" INTPTR_FORMAT, dmw.value());\n+\n+      \/\/ Must preserve store ordering. The monitor state must\n+      \/\/ be stable at the time of publishing the monitor address.\n+      assert(object->mark() == markWord::INFLATING(), \"invariant\");\n+      \/\/ Release semantics so that above set_object() is seen first.\n+      object->release_set_mark(mark);\n+\n+      return dmw;\n+    }\n+\n+    \/\/ CASE: neutral or marked (for GC)\n+    \/\/ Catch if the object's header is not neutral or marked (it must not be locked).\n+    assert(mark.is_neutral() || mark.is_marked(), \"invariant: header=\" INTPTR_FORMAT, mark.value());\n+    return mark;\n+  }\n+}\n+\n@@ -1461,0 +1529,10 @@\n+class VM_RendezvousGCThreads : public VM_Operation {\n+public:\n+  bool evaluate_at_safepoint() const override { return false; }\n+  VMOp_Type type() const override { return VMOp_RendezvousGCThreads; }\n+  void doit() override {\n+    SuspendibleThreadSet::synchronize();\n+    SuspendibleThreadSet::desynchronize();\n+  };\n+};\n+\n@@ -1514,0 +1592,3 @@\n+      \/\/ Also, we sync and desync GC threads around the handshake, so that they can\n+      \/\/ safely read the mark-word and look-through to the object-monitor, without\n+      \/\/ being afraid that the object-monitor is going away.\n@@ -1516,0 +1597,2 @@\n+      VM_RendezvousGCThreads sync_gc;\n+      VMThread::execute(&sync_gc);\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":84,"deletions":1,"binary":false,"changes":85,"status":"modified"},{"patch":"@@ -173,0 +173,5 @@\n+  \/\/ Read mark-word and spin-wait as long as INFLATING is observed.\n+  static markWord read_stable_mark(oop obj);\n+\n+  static markWord stable_mark(oop obj);\n+\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -201,2 +201,1 @@\n-  volatile_nonstatic_field(oopDesc,            _metadata._klass,                              Klass*)                                \\\n-  volatile_nonstatic_field(oopDesc,            _metadata._compressed_klass,                   narrowKlass)                           \\\n+  NOT_LP64(volatile_nonstatic_field(oopDesc,   _klass,                                        Klass*))                               \\\n@@ -380,2 +379,2 @@\n-     static_field(CompressedKlassPointers,     _narrow_klass._base,                           address)                               \\\n-     static_field(CompressedKlassPointers,     _narrow_klass._shift,                          int)                                   \\\n+     static_field(CompressedKlassPointers,     _base,                           address)                                             \\\n+     static_field(CompressedKlassPointers,     _shift_copy,                          int)                                            \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":3,"deletions":4,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -588,5 +588,0 @@\n-const int LogKlassAlignmentInBytes = 3;\n-const int LogKlassAlignment        = LogKlassAlignmentInBytes - LogHeapWordSize;\n-const int KlassAlignmentInBytes    = 1 << LogKlassAlignmentInBytes;\n-const int KlassAlignment           = KlassAlignmentInBytes \/ HeapWordSize;\n-\n@@ -600,5 +595,0 @@\n-\/\/ Maximal size of compressed class space. Above this limit compression is not possible.\n-\/\/ Also upper bound for placement of zero based class space. (Class space is further limited\n-\/\/ to be < 3G, see arguments.cpp.)\n-const  uint64_t KlassEncodingMetaspaceMax = (uint64_t(max_juint) + 1) << LogKlassAlignmentInBytes;\n-\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":0,"deletions":10,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"oops\/compressedKlass.hpp\"\n","filename":"src\/hotspot\/share\/utilities\/vmError.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -76,1 +76,2 @@\n-    final int hubOffset = getFieldOffset(\"oopDesc::_metadata._klass\", Integer.class, \"Klass*\");\n+    \/\/ TODO: Lilliput. Probably ok.\n+    final int hubOffset = 4; \/\/ getFieldOffset(\"oopDesc::_metadata._klass\", Integer.class, \"Klass*\");\n","filename":"src\/jdk.internal.vm.ci\/share\/classes\/jdk.vm.ci.hotspot\/src\/jdk\/vm\/ci\/hotspot\/HotSpotVMConfig.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -126,0 +126,34 @@\n+# Missing Lilliput support to load Klass*\n+serviceability\/sa\/CDSJMapClstats.java 1234567 generic-all\n+serviceability\/sa\/ClhsdbCDSCore.java 1234567 generic-all\n+serviceability\/sa\/ClhsdbCDSJstackPrintAll.java 1234567 generic-all\n+serviceability\/sa\/ClhsdbDumpheap.java 1234567 generic-all\n+serviceability\/sa\/ClhsdbFindPC.java#no-xcomp-core\n+serviceability\/sa\/ClhsdbFindPC.java#no-xcomp-process\n+serviceability\/sa\/ClhsdbFindPC.java#xcomp-core\n+serviceability\/sa\/ClhsdbFindPC.java#xcomp-process\n+serviceability\/sa\/ClhsdbInspect.java 1234567 generic-all\n+serviceability\/sa\/ClhsdbJdis.java 1234567 generic-all\n+serviceability\/sa\/ClhsdbJhisto.java 1234567 generic-all\n+serviceability\/sa\/ClhsdbJstack.java#id0 1234567 generic-all\n+serviceability\/sa\/ClhsdbJstack.java#id1 1234567 generic-all\n+serviceability\/sa\/ClhsdbPrintAs.java 1234567 generic-all\n+serviceability\/sa\/ClhsdbPstack.java#core 1234567 generic-all\n+serviceability\/sa\/ClhsdbPstack.java#process 1234567 generic-all\n+serviceability\/sa\/ClhsdbSource.java 1234567 generic-all\n+serviceability\/sa\/ClhsdbThread.java 1234567 generic-all\n+serviceability\/sa\/ClhsdbThreadContext.java 1234567 generic-all\n+serviceability\/sa\/ClhsdbWhere.java 1234567 generic-all\n+serviceability\/sa\/DeadlockDetectionTest.java 1234567 generic-all\n+serviceability\/sa\/JhsdbThreadInfoTest.java 1234567 generic-all\n+serviceability\/sa\/TestClhsdbJstackLock.java 1234567 generic-all\n+serviceability\/sa\/TestHeapDumpForInvokeDynamic.java 1234567 generic-all\n+serviceability\/sa\/TestJhsdbJstackLineNumbers.java 1234567 generic-all\n+serviceability\/sa\/TestJhsdbJstackLock.java 1234567 generic-all\n+serviceability\/sa\/TestJhsdbJstackMixed.java 1234567 generic-all\n+serviceability\/sa\/TestObjectMonitorIterate.java 1234567 generic-all\n+serviceability\/sa\/TestSysProps.java 1234567 generic-all\n+serviceability\/sa\/jmap-hprof\/JMapHProfLargeHeapTest.java 1234567 generic-all\n+serviceability\/sa\/sadebugd\/DebugdConnectTest.java 1234567 generic-all\n+serviceability\/sa\/sadebugd\/DisableRegistryTest.java 1234567 generic-all\n+\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":34,"deletions":0,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -52,1 +52,1 @@\n- *   -Xmx1G -XX:G1HeapRegionSize=8m -XX:MaxGCPauseMillis=1000 gc.stress.TestMultiThreadStressRSet 60 16\n+ *   -Xmx1100M -XX:G1HeapRegionSize=8m -XX:MaxGCPauseMillis=1000 gc.stress.TestMultiThreadStressRSet 60 16\n","filename":"test\/hotspot\/jtreg\/gc\/stress\/TestMultiThreadStressRSet.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -57,0 +57,3 @@\n+    \/* Lilliput: cannot work due to drastically reduced narrow klass pointer range (atm 2g and that may get\n+       smaller still). There is an argument for improving CDS\/CCS reservation and make it more likely to run\n+       zero-based, but that logic has to be rethought.\n@@ -71,0 +74,1 @@\n+     *\/\n@@ -74,0 +78,1 @@\n+    \/* Lilliput: See comment above.\n@@ -88,0 +93,1 @@\n+    *\/\n@@ -92,0 +98,2 @@\n+    \/* Lilliput: I am not sure what the point of this test CCS reservation is independent from\n+       heap. See below the desparate attempts to predict heap reservation on PPC. Why do we even care?\n@@ -112,0 +120,1 @@\n+     *\/\n@@ -117,0 +126,1 @@\n+    \/* Lilliput: narrow klass pointer range drastically reduced. See comments under smallHeapTest().\n@@ -136,0 +146,1 @@\n+    *\/\n@@ -138,0 +149,4 @@\n+    \/* Lilliput: not sure what the point of this test is. The ability to have a class space if heap uses\n+       large pages? Why would that be a problem? Kept alive for now since it makes no problems even with\n+       smaller class pointers.\n+     *\/\n@@ -198,0 +213,1 @@\n+    \/* Lilliput: narrow klass pointer range drastically reduced. See comments under smallHeapTest().\n@@ -213,0 +229,1 @@\n+    *\/\n@@ -214,0 +231,1 @@\n+    \/* Lilliput: narrow klass pointer range drastically reduced. See comments under smallHeapTest().\n@@ -233,0 +251,1 @@\n+    *\/\n@@ -234,0 +253,1 @@\n+    \/* Lilliput: narrow klass pointer range drastically reduced. See comments under smallHeapTest().\n@@ -253,0 +273,1 @@\n+    *\/\n@@ -321,4 +342,4 @@\n-        smallHeapTest();\n-        smallHeapTestWith1G();\n-        largeHeapTest();\n-        largeHeapAbove32GTest();\n+        \/\/ smallHeapTest();\n+        \/\/ smallHeapTestWith1G();\n+        \/\/ largeHeapTest();\n+        \/\/ largeHeapAbove32GTest();\n@@ -336,3 +357,3 @@\n-            smallHeapTestNoCoop();\n-            smallHeapTestWith1GNoCoop();\n-            largeHeapTestNoCoop();\n+            \/\/ smallHeapTestNoCoop();\n+            \/\/ smallHeapTestWith1GNoCoop();\n+            \/\/ largeHeapTestNoCoop();\n","filename":"test\/hotspot\/jtreg\/runtime\/CompressedOops\/CompressedClassPointers.java","additions":28,"deletions":7,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -134,2 +134,1 @@\n-        runCheck(new String[] {\"-XX:+IgnoreUnrecognizedVMOptions\", \"-XX:-UseCompressedClassPointers\"},\n-                 BadFailOnConstraint.create(Loads.class, \"load()\", 1, 1, \"Load\"),\n+        runCheck(BadFailOnConstraint.create(Loads.class, \"load()\", 1, 1, \"Load\"),\n","filename":"test\/hotspot\/jtreg\/testlibrary_tests\/ir_framework\/tests\/TestIRMatching.java","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -375,1 +375,1 @@\n-        long expected = roundUp(Platform.is64bit() ? 16 : 8, OBJ_ALIGN);\n+        long expected = roundUp(8, OBJ_ALIGN);\n@@ -382,1 +382,1 @@\n-        long expected = roundUp(Platform.is64bit() ? 16 : 8, OBJ_ALIGN);\n+        long expected = roundUp(8, OBJ_ALIGN);\n@@ -392,1 +392,1 @@\n-        long expected = roundUp(Platform.is64bit() ? 16 : 8, OBJ_ALIGN);\n+        long expected = roundUp(8, OBJ_ALIGN);\n","filename":"test\/jdk\/java\/lang\/instrument\/GetObjectSizeIntrinsicsTest.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -62,0 +62,3 @@\n+\n+\n+jdk\/jshell\/ToolTabSnippetTest.java 1234567 generic-all\n","filename":"test\/langtools\/ProblemList.txt","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"}]}