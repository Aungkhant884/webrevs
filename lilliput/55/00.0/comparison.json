{"files":[{"patch":"@@ -182,4 +182,0 @@\n-  reg_def V0_L ( SOC, SOC, Op_RegF, 0, v0->as_VMReg()->next(4) );\n-  reg_def V0_M ( SOC, SOC, Op_RegF, 0, v0->as_VMReg()->next(5) );\n-  reg_def V0_N ( SOC, SOC, Op_RegF, 0, v0->as_VMReg()->next(6) );\n-  reg_def V0_O ( SOC, SOC, Op_RegF, 0, v0->as_VMReg()->next(7) );\n@@ -191,4 +187,0 @@\n-  reg_def V1_L ( SOC, SOC, Op_RegF, 1, v1->as_VMReg()->next(4) );\n-  reg_def V1_M ( SOC, SOC, Op_RegF, 1, v1->as_VMReg()->next(5) );\n-  reg_def V1_N ( SOC, SOC, Op_RegF, 1, v1->as_VMReg()->next(6) );\n-  reg_def V1_O ( SOC, SOC, Op_RegF, 1, v1->as_VMReg()->next(7) );\n@@ -200,4 +192,0 @@\n-  reg_def V2_L ( SOC, SOC, Op_RegF, 2, v2->as_VMReg()->next(4) );\n-  reg_def V2_M ( SOC, SOC, Op_RegF, 2, v2->as_VMReg()->next(5) );\n-  reg_def V2_N ( SOC, SOC, Op_RegF, 2, v2->as_VMReg()->next(6) );\n-  reg_def V2_O ( SOC, SOC, Op_RegF, 2, v2->as_VMReg()->next(7) );\n@@ -209,4 +197,0 @@\n-  reg_def V3_L ( SOC, SOC, Op_RegF, 3, v3->as_VMReg()->next(4) );\n-  reg_def V3_M ( SOC, SOC, Op_RegF, 3, v3->as_VMReg()->next(5) );\n-  reg_def V3_N ( SOC, SOC, Op_RegF, 3, v3->as_VMReg()->next(6) );\n-  reg_def V3_O ( SOC, SOC, Op_RegF, 3, v3->as_VMReg()->next(7) );\n@@ -218,4 +202,0 @@\n-  reg_def V4_L ( SOC, SOC, Op_RegF, 4, v4->as_VMReg()->next(4) );\n-  reg_def V4_M ( SOC, SOC, Op_RegF, 4, v4->as_VMReg()->next(5) );\n-  reg_def V4_N ( SOC, SOC, Op_RegF, 4, v4->as_VMReg()->next(6) );\n-  reg_def V4_O ( SOC, SOC, Op_RegF, 4, v4->as_VMReg()->next(7) );\n@@ -227,4 +207,0 @@\n-  reg_def V5_L ( SOC, SOC, Op_RegF, 5, v5->as_VMReg()->next(4) );\n-  reg_def V5_M ( SOC, SOC, Op_RegF, 5, v5->as_VMReg()->next(5) );\n-  reg_def V5_N ( SOC, SOC, Op_RegF, 5, v5->as_VMReg()->next(6) );\n-  reg_def V5_O ( SOC, SOC, Op_RegF, 5, v5->as_VMReg()->next(7) );\n@@ -236,4 +212,0 @@\n-  reg_def V6_L ( SOC, SOC, Op_RegF, 6, v6->as_VMReg()->next(4) );\n-  reg_def V6_M ( SOC, SOC, Op_RegF, 6, v6->as_VMReg()->next(5) );\n-  reg_def V6_N ( SOC, SOC, Op_RegF, 6, v6->as_VMReg()->next(6) );\n-  reg_def V6_O ( SOC, SOC, Op_RegF, 6, v6->as_VMReg()->next(7) );\n@@ -245,4 +217,0 @@\n-  reg_def V7_L ( SOC, SOC, Op_RegF, 7, v7->as_VMReg()->next(4) );\n-  reg_def V7_M ( SOC, SOC, Op_RegF, 7, v7->as_VMReg()->next(5) );\n-  reg_def V7_N ( SOC, SOC, Op_RegF, 7, v7->as_VMReg()->next(6) );\n-  reg_def V7_O ( SOC, SOC, Op_RegF, 7, v7->as_VMReg()->next(7) );\n@@ -254,4 +222,0 @@\n-  reg_def V8_L ( SOC, SOC, Op_RegF, 8, v8->as_VMReg()->next(4) );\n-  reg_def V8_M ( SOC, SOC, Op_RegF, 8, v8->as_VMReg()->next(5) );\n-  reg_def V8_N ( SOC, SOC, Op_RegF, 8, v8->as_VMReg()->next(6) );\n-  reg_def V8_O ( SOC, SOC, Op_RegF, 8, v8->as_VMReg()->next(7) );\n@@ -263,4 +227,0 @@\n-  reg_def V9_L ( SOC, SOC, Op_RegF, 9, v9->as_VMReg()->next(4) );\n-  reg_def V9_M ( SOC, SOC, Op_RegF, 9, v9->as_VMReg()->next(5) );\n-  reg_def V9_N ( SOC, SOC, Op_RegF, 9, v9->as_VMReg()->next(6) );\n-  reg_def V9_O ( SOC, SOC, Op_RegF, 9, v9->as_VMReg()->next(7) );\n@@ -272,4 +232,0 @@\n-  reg_def V10_L ( SOC, SOC, Op_RegF, 10, v10->as_VMReg()->next(4) );\n-  reg_def V10_M ( SOC, SOC, Op_RegF, 10, v10->as_VMReg()->next(5) );\n-  reg_def V10_N ( SOC, SOC, Op_RegF, 10, v10->as_VMReg()->next(6) );\n-  reg_def V10_O ( SOC, SOC, Op_RegF, 10, v10->as_VMReg()->next(7) );\n@@ -281,4 +237,0 @@\n-  reg_def V11_L ( SOC, SOC, Op_RegF, 11, v11->as_VMReg()->next(4) );\n-  reg_def V11_M ( SOC, SOC, Op_RegF, 11, v11->as_VMReg()->next(5) );\n-  reg_def V11_N ( SOC, SOC, Op_RegF, 11, v11->as_VMReg()->next(6) );\n-  reg_def V11_O ( SOC, SOC, Op_RegF, 11, v11->as_VMReg()->next(7) );\n@@ -290,4 +242,0 @@\n-  reg_def V12_L ( SOC, SOC, Op_RegF, 12, v12->as_VMReg()->next(4) );\n-  reg_def V12_M ( SOC, SOC, Op_RegF, 12, v12->as_VMReg()->next(5) );\n-  reg_def V12_N ( SOC, SOC, Op_RegF, 12, v12->as_VMReg()->next(6) );\n-  reg_def V12_O ( SOC, SOC, Op_RegF, 12, v12->as_VMReg()->next(7) );\n@@ -299,4 +247,0 @@\n-  reg_def V13_L ( SOC, SOC, Op_RegF, 13, v13->as_VMReg()->next(4) );\n-  reg_def V13_M ( SOC, SOC, Op_RegF, 13, v13->as_VMReg()->next(5) );\n-  reg_def V13_N ( SOC, SOC, Op_RegF, 13, v13->as_VMReg()->next(6) );\n-  reg_def V13_O ( SOC, SOC, Op_RegF, 13, v13->as_VMReg()->next(7) );\n@@ -308,4 +252,0 @@\n-  reg_def V14_L ( SOC, SOC, Op_RegF, 14, v14->as_VMReg()->next(4) );\n-  reg_def V14_M ( SOC, SOC, Op_RegF, 14, v14->as_VMReg()->next(5) );\n-  reg_def V14_N ( SOC, SOC, Op_RegF, 14, v14->as_VMReg()->next(6) );\n-  reg_def V14_O ( SOC, SOC, Op_RegF, 14, v14->as_VMReg()->next(7) );\n@@ -317,4 +257,0 @@\n-  reg_def V15_L ( SOC, SOC, Op_RegF, 15, v15->as_VMReg()->next(4) );\n-  reg_def V15_M ( SOC, SOC, Op_RegF, 15, v15->as_VMReg()->next(5) );\n-  reg_def V15_N ( SOC, SOC, Op_RegF, 15, v15->as_VMReg()->next(6) );\n-  reg_def V15_O ( SOC, SOC, Op_RegF, 15, v15->as_VMReg()->next(7) );\n@@ -326,4 +262,0 @@\n-  reg_def V16_L ( SOC, SOC, Op_RegF, 16, v16->as_VMReg()->next(4) );\n-  reg_def V16_M ( SOC, SOC, Op_RegF, 16, v16->as_VMReg()->next(5) );\n-  reg_def V16_N ( SOC, SOC, Op_RegF, 16, v16->as_VMReg()->next(6) );\n-  reg_def V16_O ( SOC, SOC, Op_RegF, 16, v16->as_VMReg()->next(7) );\n@@ -335,4 +267,0 @@\n-  reg_def V17_L ( SOC, SOC, Op_RegF, 17, v17->as_VMReg()->next(4) );\n-  reg_def V17_M ( SOC, SOC, Op_RegF, 17, v17->as_VMReg()->next(5) );\n-  reg_def V17_N ( SOC, SOC, Op_RegF, 17, v17->as_VMReg()->next(6) );\n-  reg_def V17_O ( SOC, SOC, Op_RegF, 17, v17->as_VMReg()->next(7) );\n@@ -344,4 +272,0 @@\n-  reg_def V18_L ( SOC, SOC, Op_RegF, 18, v18->as_VMReg()->next(4) );\n-  reg_def V18_M ( SOC, SOC, Op_RegF, 18, v18->as_VMReg()->next(5) );\n-  reg_def V18_N ( SOC, SOC, Op_RegF, 18, v18->as_VMReg()->next(6) );\n-  reg_def V18_O ( SOC, SOC, Op_RegF, 18, v18->as_VMReg()->next(7) );\n@@ -353,4 +277,0 @@\n-  reg_def V19_L ( SOC, SOC, Op_RegF, 19, v19->as_VMReg()->next(4) );\n-  reg_def V19_M ( SOC, SOC, Op_RegF, 19, v19->as_VMReg()->next(5) );\n-  reg_def V19_N ( SOC, SOC, Op_RegF, 19, v19->as_VMReg()->next(6) );\n-  reg_def V19_O ( SOC, SOC, Op_RegF, 19, v19->as_VMReg()->next(7) );\n@@ -362,4 +282,0 @@\n-  reg_def V20_L ( SOC, SOC, Op_RegF, 20, v20->as_VMReg()->next(4) );\n-  reg_def V20_M ( SOC, SOC, Op_RegF, 20, v20->as_VMReg()->next(5) );\n-  reg_def V20_N ( SOC, SOC, Op_RegF, 20, v20->as_VMReg()->next(6) );\n-  reg_def V20_O ( SOC, SOC, Op_RegF, 20, v20->as_VMReg()->next(7) );\n@@ -371,4 +287,0 @@\n-  reg_def V21_L ( SOC, SOC, Op_RegF, 21, v21->as_VMReg()->next(4) );\n-  reg_def V21_M ( SOC, SOC, Op_RegF, 21, v21->as_VMReg()->next(5) );\n-  reg_def V21_N ( SOC, SOC, Op_RegF, 21, v21->as_VMReg()->next(6) );\n-  reg_def V21_O ( SOC, SOC, Op_RegF, 21, v21->as_VMReg()->next(7) );\n@@ -380,4 +292,0 @@\n-  reg_def V22_L ( SOC, SOC, Op_RegF, 22, v22->as_VMReg()->next(4) );\n-  reg_def V22_M ( SOC, SOC, Op_RegF, 22, v22->as_VMReg()->next(5) );\n-  reg_def V22_N ( SOC, SOC, Op_RegF, 22, v22->as_VMReg()->next(6) );\n-  reg_def V22_O ( SOC, SOC, Op_RegF, 22, v22->as_VMReg()->next(7) );\n@@ -389,4 +297,0 @@\n-  reg_def V23_L ( SOC, SOC, Op_RegF, 23, v23->as_VMReg()->next(4) );\n-  reg_def V23_M ( SOC, SOC, Op_RegF, 23, v23->as_VMReg()->next(5) );\n-  reg_def V23_N ( SOC, SOC, Op_RegF, 23, v23->as_VMReg()->next(6) );\n-  reg_def V23_O ( SOC, SOC, Op_RegF, 23, v23->as_VMReg()->next(7) );\n@@ -398,4 +302,0 @@\n-  reg_def V24_L ( SOC, SOC, Op_RegF, 24, v24->as_VMReg()->next(4) );\n-  reg_def V24_M ( SOC, SOC, Op_RegF, 24, v24->as_VMReg()->next(5) );\n-  reg_def V24_N ( SOC, SOC, Op_RegF, 24, v24->as_VMReg()->next(6) );\n-  reg_def V24_O ( SOC, SOC, Op_RegF, 24, v24->as_VMReg()->next(7) );\n@@ -407,4 +307,0 @@\n-  reg_def V25_L ( SOC, SOC, Op_RegF, 25, v25->as_VMReg()->next(4) );\n-  reg_def V25_M ( SOC, SOC, Op_RegF, 25, v25->as_VMReg()->next(5) );\n-  reg_def V25_N ( SOC, SOC, Op_RegF, 25, v25->as_VMReg()->next(6) );\n-  reg_def V25_O ( SOC, SOC, Op_RegF, 25, v25->as_VMReg()->next(7) );\n@@ -416,4 +312,0 @@\n-  reg_def V26_L ( SOC, SOC, Op_RegF, 26, v26->as_VMReg()->next(4) );\n-  reg_def V26_M ( SOC, SOC, Op_RegF, 26, v26->as_VMReg()->next(5) );\n-  reg_def V26_N ( SOC, SOC, Op_RegF, 26, v26->as_VMReg()->next(6) );\n-  reg_def V26_O ( SOC, SOC, Op_RegF, 26, v26->as_VMReg()->next(7) );\n@@ -425,4 +317,0 @@\n-  reg_def V27_L ( SOC, SOC, Op_RegF, 27, v27->as_VMReg()->next(4) );\n-  reg_def V27_M ( SOC, SOC, Op_RegF, 27, v27->as_VMReg()->next(5) );\n-  reg_def V27_N ( SOC, SOC, Op_RegF, 27, v27->as_VMReg()->next(6) );\n-  reg_def V27_O ( SOC, SOC, Op_RegF, 27, v27->as_VMReg()->next(7) );\n@@ -434,4 +322,0 @@\n-  reg_def V28_L ( SOC, SOC, Op_RegF, 28, v28->as_VMReg()->next(4) );\n-  reg_def V28_M ( SOC, SOC, Op_RegF, 28, v28->as_VMReg()->next(5) );\n-  reg_def V28_N ( SOC, SOC, Op_RegF, 28, v28->as_VMReg()->next(6) );\n-  reg_def V28_O ( SOC, SOC, Op_RegF, 28, v28->as_VMReg()->next(7) );\n@@ -443,4 +327,0 @@\n-  reg_def V29_L ( SOC, SOC, Op_RegF, 29, v29->as_VMReg()->next(4) );\n-  reg_def V29_M ( SOC, SOC, Op_RegF, 29, v29->as_VMReg()->next(5) );\n-  reg_def V29_N ( SOC, SOC, Op_RegF, 29, v29->as_VMReg()->next(6) );\n-  reg_def V29_O ( SOC, SOC, Op_RegF, 29, v29->as_VMReg()->next(7) );\n@@ -452,4 +332,0 @@\n-  reg_def V30_L ( SOC, SOC, Op_RegF, 30, v30->as_VMReg()->next(4) );\n-  reg_def V30_M ( SOC, SOC, Op_RegF, 30, v30->as_VMReg()->next(5) );\n-  reg_def V30_N ( SOC, SOC, Op_RegF, 30, v30->as_VMReg()->next(6) );\n-  reg_def V30_O ( SOC, SOC, Op_RegF, 30, v30->as_VMReg()->next(7) );\n@@ -461,5 +337,0 @@\n-  reg_def V31_L ( SOC, SOC, Op_RegF, 31, v31->as_VMReg()->next(4) );\n-  reg_def V31_M ( SOC, SOC, Op_RegF, 31, v31->as_VMReg()->next(5) );\n-  reg_def V31_N ( SOC, SOC, Op_RegF, 31, v31->as_VMReg()->next(6) );\n-  reg_def V31_O ( SOC, SOC, Op_RegF, 31, v31->as_VMReg()->next(7) );\n-\n@@ -553,16 +424,16 @@\n-    V16, V16_H, V16_J, V16_K, V16_L, V16_M, V16_N, V16_O,\n-    V17, V17_H, V17_J, V17_K, V17_L, V17_M, V17_N, V17_O,\n-    V18, V18_H, V18_J, V18_K, V18_L, V18_M, V18_N, V18_O,\n-    V19, V19_H, V19_J, V19_K, V19_L, V19_M, V19_N, V19_O,\n-    V20, V20_H, V20_J, V20_K, V20_L, V20_M, V20_N, V20_O,\n-    V21, V21_H, V21_J, V21_K, V21_L, V21_M, V21_N, V21_O,\n-    V22, V22_H, V22_J, V22_K, V22_L, V22_M, V22_N, V22_O,\n-    V23, V23_H, V23_J, V23_K, V23_L, V23_M, V23_N, V23_O,\n-    V24, V24_H, V24_J, V24_K, V24_L, V24_M, V24_N, V24_O,\n-    V25, V25_H, V25_J, V25_K, V25_L, V25_M, V25_N, V25_O,\n-    V26, V26_H, V26_J, V26_K, V26_L, V26_M, V26_N, V26_O,\n-    V27, V27_H, V27_J, V27_K, V27_L, V27_M, V27_N, V27_O,\n-    V28, V28_H, V28_J, V28_K, V28_L, V28_M, V28_N, V28_O,\n-    V29, V29_H, V29_J, V29_K, V29_L, V29_M, V29_N, V29_O,\n-    V30, V30_H, V30_J, V30_K, V30_L, V30_M, V30_N, V30_O,\n-    V31, V31_H, V31_J, V31_K, V31_L, V31_M, V31_N, V31_O,\n+    V16, V16_H, V16_J, V16_K,\n+    V17, V17_H, V17_J, V17_K,\n+    V18, V18_H, V18_J, V18_K,\n+    V19, V19_H, V19_J, V19_K,\n+    V20, V20_H, V20_J, V20_K,\n+    V21, V21_H, V21_J, V21_K,\n+    V22, V22_H, V22_J, V22_K,\n+    V23, V23_H, V23_J, V23_K,\n+    V24, V24_H, V24_J, V24_K,\n+    V25, V25_H, V25_J, V25_K,\n+    V26, V26_H, V26_J, V26_K,\n+    V27, V27_H, V27_J, V27_K,\n+    V28, V28_H, V28_J, V28_K,\n+    V29, V29_H, V29_J, V29_K,\n+    V30, V30_H, V30_J, V30_K,\n+    V31, V31_H, V31_J, V31_K,\n@@ -571,8 +442,8 @@\n-    V0, V0_H, V0_J, V0_K, V0_L, V0_M, V0_N, V0_O,\n-    V1, V1_H, V1_J, V1_K, V1_L, V1_M, V1_N, V1_O,\n-    V2, V2_H, V2_J, V2_K, V2_L, V2_M, V2_N, V2_O,\n-    V3, V3_H, V3_J, V3_K, V3_L, V3_M, V3_N, V3_O,\n-    V4, V4_H, V4_J, V4_K, V4_L, V4_M, V4_N, V4_O,\n-    V5, V5_H, V5_J, V5_K, V5_L, V5_M, V5_N, V5_O,\n-    V6, V6_H, V6_J, V6_K, V6_L, V6_M, V6_N, V6_O,\n-    V7, V7_H, V7_J, V7_K, V7_L, V7_M, V7_N, V7_O,\n+    V0, V0_H, V0_J, V0_K,\n+    V1, V1_H, V1_J, V1_K,\n+    V2, V2_H, V2_J, V2_K,\n+    V3, V3_H, V3_J, V3_K,\n+    V4, V4_H, V4_J, V4_K,\n+    V5, V5_H, V5_J, V5_K,\n+    V6, V6_H, V6_J, V6_K,\n+    V7, V7_H, V7_J, V7_K,\n@@ -581,8 +452,8 @@\n-    V8, V8_H, V8_J, V8_K, V8_L, V8_M, V8_N, V8_O,\n-    V9, V9_H, V9_J, V9_K, V9_L, V9_M, V9_N, V9_O,\n-    V10, V10_H, V10_J, V10_K, V10_L, V10_M, V10_N, V10_O,\n-    V11, V11_H, V11_J, V11_K, V11_L, V11_M, V11_N, V11_O,\n-    V12, V12_H, V12_J, V12_K, V12_L, V12_M, V12_N, V12_O,\n-    V13, V13_H, V13_J, V13_K, V13_L, V13_M, V13_N, V13_O,\n-    V14, V14_H, V14_J, V14_K, V14_L, V14_M, V14_N, V14_O,\n-    V15, V15_H, V15_J, V15_K, V15_L, V15_M, V15_N, V15_O,\n+    V8, V8_H, V8_J, V8_K,\n+    V9, V9_H, V9_J, V9_K,\n+    V10, V10_H, V10_J, V10_K,\n+    V11, V11_H, V11_J, V11_K,\n+    V12, V12_H, V12_J, V12_K,\n+    V13, V13_H, V13_J, V13_K,\n+    V14, V14_H, V14_J, V14_K,\n+    V15, V15_H, V15_J, V15_K,\n@@ -903,32 +774,32 @@\n-    V0, V0_H, V0_J, V0_K, V0_L, V0_M, V0_N, V0_O,\n-    V1, V1_H, V1_J, V1_K, V1_L, V1_M, V1_N, V1_O,\n-    V2, V2_H, V2_J, V2_K, V2_L, V2_M, V2_N, V2_O,\n-    V3, V3_H, V3_J, V3_K, V3_L, V3_M, V3_N, V3_O,\n-    V4, V4_H, V4_J, V4_K, V4_L, V4_M, V4_N, V4_O,\n-    V5, V5_H, V5_J, V5_K, V5_L, V5_M, V5_N, V5_O,\n-    V6, V6_H, V6_J, V6_K, V6_L, V6_M, V6_N, V6_O,\n-    V7, V7_H, V7_J, V7_K, V7_L, V7_M, V7_N, V7_O,\n-    V8, V8_H, V8_J, V8_K, V8_L, V8_M, V8_N, V8_O,\n-    V9, V9_H, V9_J, V9_K, V9_L, V9_M, V9_N, V9_O,\n-    V10, V10_H, V10_J, V10_K, V10_L, V10_M, V10_N, V10_O,\n-    V11, V11_H, V11_J, V11_K, V11_L, V11_M, V11_N, V11_O,\n-    V12, V12_H, V12_J, V12_K, V12_L, V12_M, V12_N, V12_O,\n-    V13, V13_H, V13_J, V13_K, V13_L, V13_M, V13_N, V13_O,\n-    V14, V14_H, V14_J, V14_K, V14_L, V14_M, V14_N, V14_O,\n-    V15, V15_H, V15_J, V15_K, V15_L, V15_M, V15_N, V15_O,\n-    V16, V16_H, V16_J, V16_K, V16_L, V16_M, V16_N, V16_O,\n-    V17, V17_H, V17_J, V17_K, V17_L, V17_M, V17_N, V17_O,\n-    V18, V18_H, V18_J, V18_K, V18_L, V18_M, V18_N, V18_O,\n-    V19, V19_H, V19_J, V19_K, V19_L, V19_M, V19_N, V19_O,\n-    V20, V20_H, V20_J, V20_K, V20_L, V20_M, V20_N, V20_O,\n-    V21, V21_H, V21_J, V21_K, V21_L, V21_M, V21_N, V21_O,\n-    V22, V22_H, V22_J, V22_K, V22_L, V22_M, V22_N, V22_O,\n-    V23, V23_H, V23_J, V23_K, V23_L, V23_M, V23_N, V23_O,\n-    V24, V24_H, V24_J, V24_K, V24_L, V24_M, V24_N, V24_O,\n-    V25, V25_H, V25_J, V25_K, V25_L, V25_M, V25_N, V25_O,\n-    V26, V26_H, V26_J, V26_K, V26_L, V26_M, V26_N, V26_O,\n-    V27, V27_H, V27_J, V27_K, V27_L, V27_M, V27_N, V27_O,\n-    V28, V28_H, V28_J, V28_K, V28_L, V28_M, V28_N, V28_O,\n-    V29, V29_H, V29_J, V29_K, V29_L, V29_M, V29_N, V29_O,\n-    V30, V30_H, V30_J, V30_K, V30_L, V30_M, V30_N, V30_O,\n-    V31, V31_H, V31_J, V31_K, V31_L, V31_M, V31_N, V31_O,\n+    V0, V0_H, V0_J, V0_K,\n+    V1, V1_H, V1_J, V1_K,\n+    V2, V2_H, V2_J, V2_K,\n+    V3, V3_H, V3_J, V3_K,\n+    V4, V4_H, V4_J, V4_K,\n+    V5, V5_H, V5_J, V5_K,\n+    V6, V6_H, V6_J, V6_K,\n+    V7, V7_H, V7_J, V7_K,\n+    V8, V8_H, V8_J, V8_K,\n+    V9, V9_H, V9_J, V9_K,\n+    V10, V10_H, V10_J, V10_K,\n+    V11, V11_H, V11_J, V11_K,\n+    V12, V12_H, V12_J, V12_K,\n+    V13, V13_H, V13_J, V13_K,\n+    V14, V14_H, V14_J, V14_K,\n+    V15, V15_H, V15_J, V15_K,\n+    V16, V16_H, V16_J, V16_K,\n+    V17, V17_H, V17_J, V17_K,\n+    V18, V18_H, V18_J, V18_K,\n+    V19, V19_H, V19_J, V19_K,\n+    V20, V20_H, V20_J, V20_K,\n+    V21, V21_H, V21_J, V21_K,\n+    V22, V22_H, V22_J, V22_K,\n+    V23, V23_H, V23_J, V23_K,\n+    V24, V24_H, V24_J, V24_K,\n+    V25, V25_H, V25_J, V25_K,\n+    V26, V26_H, V26_J, V26_K,\n+    V27, V27_H, V27_J, V27_K,\n+    V28, V28_H, V28_J, V28_K,\n+    V29, V29_H, V29_J, V29_K,\n+    V30, V30_H, V30_J, V30_K,\n+    V31, V31_H, V31_J, V31_K,\n@@ -1317,3 +1188,0 @@\n-  \/\/ Assert that the given node is not a variable shift.\n-  bool assert_not_var_shift(const Node* n);\n-\n@@ -1734,6 +1602,0 @@\n-\/\/ Assert that the given node is not a variable shift.\n-bool assert_not_var_shift(const Node* n) {\n-  assert(!n->as_ShiftV()->is_var_shift(), \"illegal variable shift\");\n-  return true;\n-}\n-\n@@ -2050,1 +1912,1 @@\n-  int slots_of_int_registers = RegisterImpl::max_slots_per_register * RegisterImpl::number_of_registers;\n+  int slots_of_int_registers = Register::number_of_registers * Register::max_slots_per_register;\n@@ -2057,1 +1919,1 @@\n-  int slots_of_float_registers = FloatRegisterImpl::max_slots_per_register * FloatRegisterImpl::number_of_registers;\n+  int slots_of_float_registers = FloatRegister::number_of_registers * FloatRegister::max_slots_per_register;\n@@ -2062,1 +1924,1 @@\n-  int slots_of_predicate_registers = PRegisterImpl::max_slots_per_register * PRegisterImpl::number_of_registers;\n+  int slots_of_predicate_registers = PRegister::number_of_registers * PRegister::max_slots_per_register;\n@@ -2435,12 +2297,0 @@\n-    case Op_LoadVectorMasked:\n-    case Op_StoreVectorMasked:\n-    case Op_LoadVectorGatherMasked:\n-    case Op_StoreVectorScatterMasked:\n-    case Op_MaskAll:\n-    case Op_AndVMask:\n-    case Op_OrVMask:\n-    case Op_XorVMask:\n-      if (UseSVE == 0) {\n-        ret_value = false;\n-      }\n-      break;\n@@ -2452,76 +2302,0 @@\n-const bool Matcher::match_rule_supported_superword(int opcode, int vlen, BasicType bt) {\n-  if (UseSVE == 0) {\n-    \/\/ ConvD2I and ConvL2F are not profitable to be vectorized on NEON, because no direct\n-    \/\/ NEON instructions support them. But the match rule support for them is profitable for\n-    \/\/ Vector API intrinsics.\n-    if ((opcode == Op_VectorCastD2X && bt == T_INT) ||\n-        (opcode == Op_VectorCastL2X && bt == T_FLOAT)) {\n-      return false;\n-    }\n-  }\n-  return match_rule_supported_vector(opcode, vlen, bt);\n-}\n-\n-\/\/ Identify extra cases that we might want to provide match rules for vector nodes and\n-\/\/ other intrinsics guarded with vector length (vlen) and element type (bt).\n-const bool Matcher::match_rule_supported_vector(int opcode, int vlen, BasicType bt) {\n-  if (!match_rule_supported(opcode)) {\n-    return false;\n-  }\n-  int bit_size = vlen * type2aelembytes(bt) * 8;\n-  if (UseSVE == 0 && bit_size > 128) {\n-    return false;\n-  }\n-  if (UseSVE > 0) {\n-    return op_sve_supported(opcode, vlen, bt);\n-  } else { \/\/ NEON\n-    \/\/ Special cases\n-    switch (opcode) {\n-    case Op_VectorMaskCmp:\n-      if (vlen < 2 || bit_size < 64) {\n-        return false;\n-      }\n-      break;\n-    case Op_MulAddVS2VI:\n-      if (bit_size < 128) {\n-        return false;\n-      }\n-      break;\n-    case Op_MulVL:\n-    case Op_PopulateIndex:\n-      return false;\n-    case Op_VectorLoadShuffle:\n-    case Op_VectorRearrange:\n-      if (vlen < 4) {\n-        return false;\n-      }\n-      break;\n-    case Op_VectorMaskGen:\n-    case Op_LoadVectorGather:\n-    case Op_StoreVectorScatter:\n-    case Op_CompressV:\n-    case Op_CompressM:\n-    case Op_ExpandV:\n-    case Op_VectorLongToMask:\n-      return false;\n-    default:\n-      break;\n-    }\n-  }\n-  return vector_size_supported(bt, vlen);\n-}\n-\n-const bool Matcher::match_rule_supported_vector_masked(int opcode, int vlen, BasicType bt) {\n-  \/\/ Only SVE supports masked operations.\n-  if (UseSVE == 0) {\n-    return false;\n-  }\n-  return match_rule_supported(opcode) &&\n-         masked_op_sve_supported(opcode, vlen, bt);\n-}\n-\n-const bool Matcher::vector_needs_partial_operations(Node* node, const TypeVect* vt) {\n-  \/\/ Only SVE has partial vector operations\n-  return (UseSVE > 0) && partial_op_sve_needed(node, vt);\n-}\n-\n@@ -2594,1 +2368,1 @@\n-  if (UseSVE > 0 && 2 <= len && len <= 256) {\n+  if (UseSVE > 0 && 16 < len && len <= 256) {\n@@ -2608,2 +2382,8 @@\n-MachOper* Matcher::pd_specialize_generic_vector_operand(MachOper* original_opnd, uint ideal_reg, bool is_temp) {\n-  ShouldNotReachHere(); \/\/ generic vector operands not supported\n+MachOper* Matcher::pd_specialize_generic_vector_operand(MachOper* generic_opnd, uint ideal_reg, bool is_temp) {\n+  assert(Matcher::is_generic_vector(generic_opnd), \"not generic\");\n+  switch (ideal_reg) {\n+    case Op_VecA: return new vecAOper();\n+    case Op_VecD: return new vecDOper();\n+    case Op_VecX: return new vecXOper();\n+  }\n+  ShouldNotReachHere();\n@@ -2614,1 +2394,0 @@\n-  ShouldNotReachHere();  \/\/ generic vector operands not supported\n@@ -2619,2 +2398,1 @@\n-  ShouldNotReachHere();  \/\/ generic vector operands not supported\n-  return false;\n+  return opnd->opcode() == VREG;\n@@ -3208,1 +2986,1 @@\n-  enc_class aarch64_enc_ldrvH(vecD dst, memory mem) %{\n+  enc_class aarch64_enc_ldrvH(vReg dst, memory mem) %{\n@@ -3214,1 +2992,1 @@\n-  enc_class aarch64_enc_ldrvS(vecD dst, memory mem) %{\n+  enc_class aarch64_enc_ldrvS(vReg dst, memory mem) %{\n@@ -3220,1 +2998,1 @@\n-  enc_class aarch64_enc_ldrvD(vecD dst, memory mem) %{\n+  enc_class aarch64_enc_ldrvD(vReg dst, memory mem) %{\n@@ -3226,1 +3004,1 @@\n-  enc_class aarch64_enc_ldrvQ(vecX dst, memory mem) %{\n+  enc_class aarch64_enc_ldrvQ(vReg dst, memory mem) %{\n@@ -3232,1 +3010,1 @@\n-  enc_class aarch64_enc_strvH(vecD src, memory mem) %{\n+  enc_class aarch64_enc_strvH(vReg src, memory mem) %{\n@@ -3238,1 +3016,1 @@\n-  enc_class aarch64_enc_strvS(vecD src, memory mem) %{\n+  enc_class aarch64_enc_strvS(vReg src, memory mem) %{\n@@ -3244,1 +3022,1 @@\n-  enc_class aarch64_enc_strvD(vecD src, memory mem) %{\n+  enc_class aarch64_enc_strvD(vReg src, memory mem) %{\n@@ -3250,1 +3028,1 @@\n-  enc_class aarch64_enc_strvQ(vecX src, memory mem) %{\n+  enc_class aarch64_enc_strvQ(vReg src, memory mem) %{\n@@ -3593,1 +3371,1 @@\n-        __ movoop(dst_reg, (jobject)con, \/*immediate*\/true);\n+        __ movoop(dst_reg, (jobject)con);\n@@ -3847,1 +3625,1 @@\n-      call = __ trampoline_call(Address(addr, relocInfo::runtime_call_type), &cbuf);\n+      call = __ trampoline_call(Address(addr, relocInfo::runtime_call_type));\n@@ -3856,1 +3634,1 @@\n-      call = __ trampoline_call(Address(addr, rspec), &cbuf);\n+      call = __ trampoline_call(Address(addr, rspec));\n@@ -3864,1 +3642,1 @@\n-        cbuf.shared_stub_to_interp_for(_method, cbuf.insts()->mark_off());\n+        cbuf.shared_stub_to_interp_for(_method, call - cbuf.insts_begin());\n@@ -3867,1 +3645,1 @@\n-        address stub = CompiledStaticCall::emit_to_interp_stub(cbuf);\n+        address stub = CompiledStaticCall::emit_to_interp_stub(cbuf, call);\n@@ -3875,1 +3653,0 @@\n-    _masm.clear_inst_mark();\n@@ -3922,1 +3699,0 @@\n-      _masm.clear_inst_mark();\n@@ -4372,40 +4148,0 @@\n-operand immI_31()\n-%{\n-  predicate(n->get_int() == 31);\n-  match(ConI);\n-\n-  op_cost(0);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-operand immI_2()\n-%{\n-  predicate(n->get_int() == 2);\n-  match(ConI);\n-\n-  op_cost(0);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-operand immI_4()\n-%{\n-  predicate(n->get_int() == 4);\n-  match(ConI);\n-\n-  op_cost(0);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-operand immI_8()\n-%{\n-  predicate(n->get_int() == 8);\n-  match(ConI);\n-\n-  op_cost(0);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n@@ -5460,2 +5196,1 @@\n-\/\/ all vector operands, including NEON and SVE,\n-\/\/ but currently only used for SVE VecA.\n+\/\/ all vector operands, including NEON and SVE.\n@@ -5463,0 +5198,12 @@\n+%{\n+  constraint(ALLOC_IN_RC(dynamic));\n+  match(VecA);\n+  match(VecD);\n+  match(VecX);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand vecA()\n@@ -5466,0 +5213,1 @@\n+\n@@ -6575,285 +6323,1 @@\n-  NEON_FP : S5;\n-%}\n-\n-pipe_class fp_dop_reg_reg_d(vRegD dst, vRegD src1, vRegD src2)\n-%{\n-  single_instruction;\n-  src1   : S1(read);\n-  src2   : S2(read);\n-  dst    : S5(write);\n-  INS01  : ISS;\n-  NEON_FP : S5;\n-%}\n-\n-pipe_class fp_uop_s(vRegF dst, vRegF src)\n-%{\n-  single_instruction;\n-  src    : S1(read);\n-  dst    : S5(write);\n-  INS01  : ISS;\n-  NEON_FP : S5;\n-%}\n-\n-pipe_class fp_uop_d(vRegD dst, vRegD src)\n-%{\n-  single_instruction;\n-  src    : S1(read);\n-  dst    : S5(write);\n-  INS01  : ISS;\n-  NEON_FP : S5;\n-%}\n-\n-pipe_class fp_d2f(vRegF dst, vRegD src)\n-%{\n-  single_instruction;\n-  src    : S1(read);\n-  dst    : S5(write);\n-  INS01  : ISS;\n-  NEON_FP : S5;\n-%}\n-\n-pipe_class fp_f2d(vRegD dst, vRegF src)\n-%{\n-  single_instruction;\n-  src    : S1(read);\n-  dst    : S5(write);\n-  INS01  : ISS;\n-  NEON_FP : S5;\n-%}\n-\n-pipe_class fp_f2i(iRegINoSp dst, vRegF src)\n-%{\n-  single_instruction;\n-  src    : S1(read);\n-  dst    : S5(write);\n-  INS01  : ISS;\n-  NEON_FP : S5;\n-%}\n-\n-pipe_class fp_f2l(iRegLNoSp dst, vRegF src)\n-%{\n-  single_instruction;\n-  src    : S1(read);\n-  dst    : S5(write);\n-  INS01  : ISS;\n-  NEON_FP : S5;\n-%}\n-\n-pipe_class fp_i2f(vRegF dst, iRegIorL2I src)\n-%{\n-  single_instruction;\n-  src    : S1(read);\n-  dst    : S5(write);\n-  INS01  : ISS;\n-  NEON_FP : S5;\n-%}\n-\n-pipe_class fp_l2f(vRegF dst, iRegL src)\n-%{\n-  single_instruction;\n-  src    : S1(read);\n-  dst    : S5(write);\n-  INS01  : ISS;\n-  NEON_FP : S5;\n-%}\n-\n-pipe_class fp_d2i(iRegINoSp dst, vRegD src)\n-%{\n-  single_instruction;\n-  src    : S1(read);\n-  dst    : S5(write);\n-  INS01  : ISS;\n-  NEON_FP : S5;\n-%}\n-\n-pipe_class fp_d2l(iRegLNoSp dst, vRegD src)\n-%{\n-  single_instruction;\n-  src    : S1(read);\n-  dst    : S5(write);\n-  INS01  : ISS;\n-  NEON_FP : S5;\n-%}\n-\n-pipe_class fp_i2d(vRegD dst, iRegIorL2I src)\n-%{\n-  single_instruction;\n-  src    : S1(read);\n-  dst    : S5(write);\n-  INS01  : ISS;\n-  NEON_FP : S5;\n-%}\n-\n-pipe_class fp_l2d(vRegD dst, iRegIorL2I src)\n-%{\n-  single_instruction;\n-  src    : S1(read);\n-  dst    : S5(write);\n-  INS01  : ISS;\n-  NEON_FP : S5;\n-%}\n-\n-pipe_class fp_div_s(vRegF dst, vRegF src1, vRegF src2)\n-%{\n-  single_instruction;\n-  src1   : S1(read);\n-  src2   : S2(read);\n-  dst    : S5(write);\n-  INS0   : ISS;\n-  NEON_FP : S5;\n-%}\n-\n-pipe_class fp_div_d(vRegD dst, vRegD src1, vRegD src2)\n-%{\n-  single_instruction;\n-  src1   : S1(read);\n-  src2   : S2(read);\n-  dst    : S5(write);\n-  INS0   : ISS;\n-  NEON_FP : S5;\n-%}\n-\n-pipe_class fp_cond_reg_reg_s(vRegF dst, vRegF src1, vRegF src2, rFlagsReg cr)\n-%{\n-  single_instruction;\n-  cr     : S1(read);\n-  src1   : S1(read);\n-  src2   : S1(read);\n-  dst    : S3(write);\n-  INS01  : ISS;\n-  NEON_FP : S3;\n-%}\n-\n-pipe_class fp_cond_reg_reg_d(vRegD dst, vRegD src1, vRegD src2, rFlagsReg cr)\n-%{\n-  single_instruction;\n-  cr     : S1(read);\n-  src1   : S1(read);\n-  src2   : S1(read);\n-  dst    : S3(write);\n-  INS01  : ISS;\n-  NEON_FP : S3;\n-%}\n-\n-pipe_class fp_imm_s(vRegF dst)\n-%{\n-  single_instruction;\n-  dst    : S3(write);\n-  INS01  : ISS;\n-  NEON_FP : S3;\n-%}\n-\n-pipe_class fp_imm_d(vRegD dst)\n-%{\n-  single_instruction;\n-  dst    : S3(write);\n-  INS01  : ISS;\n-  NEON_FP : S3;\n-%}\n-\n-pipe_class fp_load_constant_s(vRegF dst)\n-%{\n-  single_instruction;\n-  dst    : S4(write);\n-  INS01  : ISS;\n-  NEON_FP : S4;\n-%}\n-\n-pipe_class fp_load_constant_d(vRegD dst)\n-%{\n-  single_instruction;\n-  dst    : S4(write);\n-  INS01  : ISS;\n-  NEON_FP : S4;\n-%}\n-\n-pipe_class vmul64(vecD dst, vecD src1, vecD src2)\n-%{\n-  single_instruction;\n-  dst    : S5(write);\n-  src1   : S1(read);\n-  src2   : S1(read);\n-  INS01  : ISS;\n-  NEON_FP : S5;\n-%}\n-\n-pipe_class vmul128(vecX dst, vecX src1, vecX src2)\n-%{\n-  single_instruction;\n-  dst    : S5(write);\n-  src1   : S1(read);\n-  src2   : S1(read);\n-  INS0   : ISS;\n-  NEON_FP : S5;\n-%}\n-\n-pipe_class vmla64(vecD dst, vecD src1, vecD src2)\n-%{\n-  single_instruction;\n-  dst    : S5(write);\n-  src1   : S1(read);\n-  src2   : S1(read);\n-  dst    : S1(read);\n-  INS01  : ISS;\n-  NEON_FP : S5;\n-%}\n-\n-pipe_class vmla128(vecX dst, vecX src1, vecX src2)\n-%{\n-  single_instruction;\n-  dst    : S5(write);\n-  src1   : S1(read);\n-  src2   : S1(read);\n-  dst    : S1(read);\n-  INS0   : ISS;\n-  NEON_FP : S5;\n-%}\n-\n-pipe_class vdop64(vecD dst, vecD src1, vecD src2)\n-%{\n-  single_instruction;\n-  dst    : S4(write);\n-  src1   : S2(read);\n-  src2   : S2(read);\n-  INS01  : ISS;\n-  NEON_FP : S4;\n-%}\n-\n-pipe_class vdop128(vecX dst, vecX src1, vecX src2)\n-%{\n-  single_instruction;\n-  dst    : S4(write);\n-  src1   : S2(read);\n-  src2   : S2(read);\n-  INS0   : ISS;\n-  NEON_FP : S4;\n-%}\n-\n-pipe_class vlogical64(vecD dst, vecD src1, vecD src2)\n-%{\n-  single_instruction;\n-  dst    : S3(write);\n-  src1   : S2(read);\n-  src2   : S2(read);\n-  INS01  : ISS;\n-  NEON_FP : S3;\n-%}\n-\n-pipe_class vlogical128(vecX dst, vecX src1, vecX src2)\n-%{\n-  single_instruction;\n-  dst    : S3(write);\n-  src1   : S2(read);\n-  src2   : S2(read);\n-  INS0   : ISS;\n-  NEON_FP : S3;\n-%}\n-\n-pipe_class vshift64(vecD dst, vecD src, vecX shift)\n-%{\n-  single_instruction;\n-  dst    : S3(write);\n-  src    : S1(read);\n-  shift  : S1(read);\n-  INS01  : ISS;\n-  NEON_FP : S3;\n+  NEON_FP : S5;\n@@ -6862,1 +6326,1 @@\n-pipe_class vshift128(vecX dst, vecX src, vecX shift)\n+pipe_class fp_dop_reg_reg_d(vRegD dst, vRegD src1, vRegD src2)\n@@ -6865,5 +6329,5 @@\n-  dst    : S3(write);\n-  src    : S1(read);\n-  shift  : S1(read);\n-  INS0   : ISS;\n-  NEON_FP : S3;\n+  src1   : S1(read);\n+  src2   : S2(read);\n+  dst    : S5(write);\n+  INS01  : ISS;\n+  NEON_FP : S5;\n@@ -6872,1 +6336,1 @@\n-pipe_class vshift64_imm(vecD dst, vecD src, immI shift)\n+pipe_class fp_uop_s(vRegF dst, vRegF src)\n@@ -6875,1 +6339,1 @@\n-  dst    : S3(write);\n+  dst    : S5(write);\n@@ -6878,1 +6342,1 @@\n-  NEON_FP : S3;\n+  NEON_FP : S5;\n@@ -6881,1 +6345,1 @@\n-pipe_class vshift128_imm(vecX dst, vecX src, immI shift)\n+pipe_class fp_uop_d(vRegD dst, vRegD src)\n@@ -6884,3 +6348,3 @@\n-  dst    : S3(write);\n-  INS0   : ISS;\n-  NEON_FP : S3;\n+  dst    : S5(write);\n+  INS01  : ISS;\n+  NEON_FP : S5;\n@@ -6890,1 +6354,1 @@\n-pipe_class vdop_fp64(vecD dst, vecD src1, vecD src2)\n+pipe_class fp_d2f(vRegF dst, vRegD src)\n@@ -6893,0 +6357,1 @@\n+  src    : S1(read);\n@@ -6894,2 +6359,0 @@\n-  src1   : S1(read);\n-  src2   : S1(read);\n@@ -6900,1 +6363,1 @@\n-pipe_class vdop_fp128(vecX dst, vecX src1, vecX src2)\n+pipe_class fp_f2d(vRegD dst, vRegF src)\n@@ -6903,0 +6366,1 @@\n+  src    : S1(read);\n@@ -6904,3 +6368,1 @@\n-  src1   : S1(read);\n-  src2   : S1(read);\n-  INS0   : ISS;\n+  INS01  : ISS;\n@@ -6910,1 +6372,1 @@\n-pipe_class vmuldiv_fp64(vecD dst, vecD src1, vecD src2)\n+pipe_class fp_f2i(iRegINoSp dst, vRegF src)\n@@ -6913,0 +6375,1 @@\n+  src    : S1(read);\n@@ -6914,3 +6377,1 @@\n-  src1   : S1(read);\n-  src2   : S1(read);\n-  INS0   : ISS;\n+  INS01  : ISS;\n@@ -6920,1 +6381,1 @@\n-pipe_class vmuldiv_fp128(vecX dst, vecX src1, vecX src2)\n+pipe_class fp_f2l(iRegLNoSp dst, vRegF src)\n@@ -6923,0 +6384,1 @@\n+  src    : S1(read);\n@@ -6924,3 +6386,1 @@\n-  src1   : S1(read);\n-  src2   : S1(read);\n-  INS0   : ISS;\n+  INS01  : ISS;\n@@ -6930,1 +6390,1 @@\n-pipe_class vsqrt_fp128(vecX dst, vecX src)\n+pipe_class fp_i2f(vRegF dst, iRegIorL2I src)\n@@ -6933,2 +6393,2 @@\n-  dst    : S5(write);\n-  INS0   : ISS;\n+  dst    : S5(write);\n+  INS01  : ISS;\n@@ -6939,1 +6399,1 @@\n-pipe_class vunop_fp64(vecD dst, vecD src)\n+pipe_class fp_l2f(vRegF dst, iRegL src)\n@@ -6942,1 +6402,1 @@\n-  dst    : S5(write);\n+  dst    : S5(write);\n@@ -6948,1 +6408,1 @@\n-pipe_class vunop_fp128(vecX dst, vecX src)\n+pipe_class fp_d2i(iRegINoSp dst, vRegD src)\n@@ -6951,2 +6411,2 @@\n-  dst    : S5(write);\n-  INS0   : ISS;\n+  dst    : S5(write);\n+  INS01  : ISS;\n@@ -6957,1 +6417,1 @@\n-pipe_class vdup_reg_reg64(vecD dst, iRegI src)\n+pipe_class fp_d2l(iRegLNoSp dst, vRegD src)\n@@ -6960,1 +6420,1 @@\n-  dst    : S3(write);\n+  dst    : S5(write);\n@@ -6963,1 +6423,1 @@\n-  NEON_FP : S3;\n+  NEON_FP : S5;\n@@ -6966,1 +6426,1 @@\n-pipe_class vdup_reg_reg128(vecX dst, iRegI src)\n+pipe_class fp_i2d(vRegD dst, iRegIorL2I src)\n@@ -6969,1 +6429,1 @@\n-  dst    : S3(write);\n+  dst    : S5(write);\n@@ -6972,1 +6432,1 @@\n-  NEON_FP : S3;\n+  NEON_FP : S5;\n@@ -6975,1 +6435,1 @@\n-pipe_class vdup_reg_freg64(vecD dst, vRegF src)\n+pipe_class fp_l2d(vRegD dst, iRegIorL2I src)\n@@ -6978,1 +6438,1 @@\n-  dst    : S3(write);\n+  dst    : S5(write);\n@@ -6981,1 +6441,1 @@\n-  NEON_FP : S3;\n+  NEON_FP : S5;\n@@ -6984,1 +6444,1 @@\n-pipe_class vdup_reg_freg128(vecX dst, vRegF src)\n+pipe_class fp_div_s(vRegF dst, vRegF src1, vRegF src2)\n@@ -6987,4 +6447,5 @@\n-  dst    : S3(write);\n-  src    : S1(read);\n-  INS01  : ISS;\n-  NEON_FP : S3;\n+  src1   : S1(read);\n+  src2   : S2(read);\n+  dst    : S5(write);\n+  INS0   : ISS;\n+  NEON_FP : S5;\n@@ -6993,1 +6454,1 @@\n-pipe_class vdup_reg_dreg128(vecX dst, vRegD src)\n+pipe_class fp_div_d(vRegD dst, vRegD src1, vRegD src2)\n@@ -6996,4 +6457,5 @@\n-  dst    : S3(write);\n-  src    : S1(read);\n-  INS01  : ISS;\n-  NEON_FP : S3;\n+  src1   : S1(read);\n+  src2   : S2(read);\n+  dst    : S5(write);\n+  INS0   : ISS;\n+  NEON_FP : S5;\n@@ -7002,1 +6464,1 @@\n-pipe_class vmovi_reg_imm64(vecD dst)\n+pipe_class fp_cond_reg_reg_s(vRegF dst, vRegF src1, vRegF src2, rFlagsReg cr)\n@@ -7005,0 +6467,3 @@\n+  cr     : S1(read);\n+  src1   : S1(read);\n+  src2   : S1(read);\n@@ -7010,1 +6475,1 @@\n-pipe_class vmovi_reg_imm128(vecX dst)\n+pipe_class fp_cond_reg_reg_d(vRegD dst, vRegD src1, vRegD src2, rFlagsReg cr)\n@@ -7013,0 +6478,3 @@\n+  cr     : S1(read);\n+  src1   : S1(read);\n+  src2   : S1(read);\n@@ -7014,1 +6482,1 @@\n-  INS0   : ISS;\n+  INS01  : ISS;\n@@ -7018,1 +6486,1 @@\n-pipe_class vload_reg_mem64(vecD dst, vmem8 mem)\n+pipe_class fp_imm_s(vRegF dst)\n@@ -7021,2 +6489,1 @@\n-  dst    : S5(write);\n-  mem    : ISS(read);\n+  dst    : S3(write);\n@@ -7027,1 +6494,1 @@\n-pipe_class vload_reg_mem128(vecX dst, vmem16 mem)\n+pipe_class fp_imm_d(vRegD dst)\n@@ -7030,2 +6497,1 @@\n-  dst    : S5(write);\n-  mem    : ISS(read);\n+  dst    : S3(write);\n@@ -7036,1 +6502,1 @@\n-pipe_class vstore_reg_mem64(vecD src, vmem8 mem)\n+pipe_class fp_load_constant_s(vRegF dst)\n@@ -7039,2 +6505,1 @@\n-  mem    : ISS(read);\n-  src    : S2(read);\n+  dst    : S4(write);\n@@ -7042,1 +6507,1 @@\n-  NEON_FP : S3;\n+  NEON_FP : S4;\n@@ -7045,1 +6510,1 @@\n-pipe_class vstore_reg_mem128(vecD src, vmem16 mem)\n+pipe_class fp_load_constant_d(vRegD dst)\n@@ -7048,2 +6513,1 @@\n-  mem    : ISS(read);\n-  src    : S2(read);\n+  dst    : S4(write);\n@@ -7051,1 +6515,1 @@\n-  NEON_FP : S3;\n+  NEON_FP : S4;\n@@ -9189,22 +8653,0 @@\n-instruct castVVD(vecD dst)\n-%{\n-  match(Set dst (CastVV dst));\n-\n-  size(0);\n-  format %{ \"# castVV of $dst\" %}\n-  ins_encode(\/* empty encoding *\/);\n-  ins_cost(0);\n-  ins_pipe(pipe_class_empty);\n-%}\n-\n-instruct castVVX(vecX dst)\n-%{\n-  match(Set dst (CastVV dst));\n-\n-  size(0);\n-  format %{ \"# castVV of $dst\" %}\n-  ins_encode(\/* empty encoding *\/);\n-  ins_cost(0);\n-  ins_pipe(pipe_class_empty);\n-%}\n-\n@@ -9236,97 +8678,0 @@\n-\/\/ Intel and SPARC both implement Ideal Node LoadPLocked and\n-\/\/ Store{PIL}Conditional instructions using a normal load for the\n-\/\/ LoadPLocked and a CAS for the Store{PIL}Conditional.\n-\/\/\n-\/\/ The ideal code appears only to use LoadPLocked\/StorePLocked as a\n-\/\/ pair to lock object allocations from Eden space when not using\n-\/\/ TLABs.\n-\/\/\n-\/\/ There does not appear to be a Load{IL}Locked Ideal Node and the\n-\/\/ Ideal code appears to use Store{IL}Conditional as an alias for CAS\n-\/\/ and to use StoreIConditional only for 32-bit and StoreLConditional\n-\/\/ only for 64-bit.\n-\/\/\n-\/\/ We implement LoadPLocked and StorePLocked instructions using,\n-\/\/ respectively the AArch64 hw load-exclusive and store-conditional\n-\/\/ instructions. Whereas we must implement each of\n-\/\/ Store{IL}Conditional using a CAS which employs a pair of\n-\/\/ instructions comprising a load-exclusive followed by a\n-\/\/ store-conditional.\n-\n-\n-\/\/ Locked-load (linked load) of the current heap-top\n-\/\/ used when updating the eden heap top\n-\/\/ implemented using ldaxr on AArch64\n-\n-instruct loadPLocked(iRegPNoSp dst, indirect mem)\n-%{\n-  match(Set dst (LoadPLocked mem));\n-\n-  ins_cost(VOLATILE_REF_COST);\n-\n-  format %{ \"ldaxr $dst, $mem\\t# ptr linked acquire\" %}\n-\n-  ins_encode(aarch64_enc_ldaxr(dst, mem));\n-\n-  ins_pipe(pipe_serial);\n-%}\n-\n-\/\/ Conditional-store of the updated heap-top.\n-\/\/ Used during allocation of the shared heap.\n-\/\/ Sets flag (EQ) on success.\n-\/\/ implemented using stlxr on AArch64.\n-\n-instruct storePConditional(memory8 heap_top_ptr, iRegP oldval, iRegP newval, rFlagsReg cr)\n-%{\n-  match(Set cr (StorePConditional heap_top_ptr (Binary oldval newval)));\n-\n-  ins_cost(VOLATILE_REF_COST);\n-\n- \/\/ TODO\n- \/\/ do we need to do a store-conditional release or can we just use a\n- \/\/ plain store-conditional?\n-\n-  format %{\n-    \"stlxr rscratch1, $newval, $heap_top_ptr\\t# ptr cond release\"\n-    \"cmpw rscratch1, zr\\t# EQ on successful write\"\n-  %}\n-\n-  ins_encode(aarch64_enc_stlxr(newval, heap_top_ptr));\n-\n-  ins_pipe(pipe_serial);\n-%}\n-\n-instruct storeLConditional(indirect mem, iRegLNoSp oldval, iRegLNoSp newval, rFlagsReg cr)\n-%{\n-  match(Set cr (StoreLConditional mem (Binary oldval newval)));\n-\n-  ins_cost(VOLATILE_REF_COST);\n-\n-  format %{\n-    \"cmpxchg rscratch1, $mem, $oldval, $newval, $mem\\t# if $mem == $oldval then $mem <-- $newval\"\n-    \"cmpw rscratch1, zr\\t# EQ on successful write\"\n-  %}\n-\n-  ins_encode(aarch64_enc_cmpxchg_acq(mem, oldval, newval));\n-\n-  ins_pipe(pipe_slow);\n-%}\n-\n-\/\/ storeIConditional also has acquire semantics, for no better reason\n-\/\/ than matching storeLConditional.  At the time of writing this\n-\/\/ comment storeIConditional was not used anywhere by AArch64.\n-instruct storeIConditional(indirect mem, iRegINoSp oldval, iRegINoSp newval, rFlagsReg cr)\n-%{\n-  match(Set cr (StoreIConditional mem (Binary oldval newval)));\n-\n-  ins_cost(VOLATILE_REF_COST);\n-\n-  format %{\n-    \"cmpxchgw rscratch1, $mem, $oldval, $newval, $mem\\t# if $mem == $oldval then $mem <-- $newval\"\n-    \"cmpw rscratch1, zr\\t# EQ on successful write\"\n-  %}\n-\n-  ins_encode(aarch64_enc_cmpxchgw_acq(mem, oldval, newval));\n-\n-  ins_pipe(pipe_slow);\n-%}\n@@ -17057,0 +16402,2 @@\n+\/\/ Intrisics for String.compareTo()\n+\n@@ -17132,0 +16479,96 @@\n+\/\/ Note that Z registers alias the corresponding NEON registers, we declare the vector operands of\n+\/\/ these string_compare variants as NEON register type for convenience so that the prototype of\n+\/\/ string_compare can be shared with all variants.\n+\n+instruct string_compareLL_sve(iRegP_R1 str1, iRegI_R2 cnt1, iRegP_R3 str2, iRegI_R4 cnt2,\n+                              iRegI_R0 result, iRegP_R10 tmp1, iRegL_R11 tmp2,\n+                              vRegD_V0 vtmp1, vRegD_V1 vtmp2, pRegGov_P0 pgtmp1,\n+                              pRegGov_P1 pgtmp2, rFlagsReg cr)\n+%{\n+  predicate((UseSVE > 0) && (((StrCompNode*)n)->encoding() == StrIntrinsicNode::LL));\n+  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP vtmp1, TEMP vtmp2, TEMP pgtmp1, TEMP pgtmp2,\n+         USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n+\n+  format %{ \"String Compare $str1,$cnt1,$str2,$cnt2 -> $result   # USE sve\" %}\n+  ins_encode %{\n+    \/\/ Count is in 8-bit bytes; non-Compact chars are 16 bits.\n+    __ string_compare($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n+                      $tmp1$$Register, $tmp2$$Register,\n+                      $vtmp1$$FloatRegister, $vtmp2$$FloatRegister, fnoreg,\n+                      as_PRegister($pgtmp1$$reg), as_PRegister($pgtmp2$$reg),\n+                      StrIntrinsicNode::LL);\n+  %}\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n+instruct string_compareLU_sve(iRegP_R1 str1, iRegI_R2 cnt1, iRegP_R3 str2, iRegI_R4 cnt2,\n+                              iRegI_R0 result, iRegP_R10 tmp1, iRegL_R11 tmp2,\n+                              vRegD_V0 vtmp1, vRegD_V1 vtmp2, pRegGov_P0 pgtmp1,\n+                              pRegGov_P1 pgtmp2, rFlagsReg cr)\n+%{\n+  predicate((UseSVE > 0) && (((StrCompNode*)n)->encoding() == StrIntrinsicNode::LU));\n+  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP vtmp1, TEMP vtmp2, TEMP pgtmp1, TEMP pgtmp2,\n+         USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n+\n+  format %{ \"String Compare $str1,$cnt1,$str2,$cnt2 -> $result   # USE sve\" %}\n+  ins_encode %{\n+    \/\/ Count is in 8-bit bytes; non-Compact chars are 16 bits.\n+    __ string_compare($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n+                      $tmp1$$Register, $tmp2$$Register,\n+                      $vtmp1$$FloatRegister, $vtmp2$$FloatRegister, fnoreg,\n+                      as_PRegister($pgtmp1$$reg), as_PRegister($pgtmp2$$reg),\n+                      StrIntrinsicNode::LU);\n+  %}\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n+instruct string_compareUL_sve(iRegP_R1 str1, iRegI_R2 cnt1, iRegP_R3 str2, iRegI_R4 cnt2,\n+                              iRegI_R0 result, iRegP_R10 tmp1, iRegL_R11 tmp2,\n+                              vRegD_V0 vtmp1, vRegD_V1 vtmp2, pRegGov_P0 pgtmp1,\n+                              pRegGov_P1 pgtmp2, rFlagsReg cr)\n+%{\n+  predicate((UseSVE > 0) && (((StrCompNode*)n)->encoding() == StrIntrinsicNode::UL));\n+  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP vtmp1, TEMP vtmp2, TEMP pgtmp1, TEMP pgtmp2,\n+         USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n+\n+  format %{ \"String Compare $str1,$cnt1,$str2,$cnt2 -> $result   # USE sve\" %}\n+  ins_encode %{\n+    \/\/ Count is in 8-bit bytes; non-Compact chars are 16 bits.\n+    __ string_compare($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n+                      $tmp1$$Register, $tmp2$$Register,\n+                      $vtmp1$$FloatRegister, $vtmp2$$FloatRegister, fnoreg,\n+                      as_PRegister($pgtmp1$$reg), as_PRegister($pgtmp2$$reg),\n+                      StrIntrinsicNode::UL);\n+  %}\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n+instruct string_compareUU_sve(iRegP_R1 str1, iRegI_R2 cnt1, iRegP_R3 str2, iRegI_R4 cnt2,\n+                              iRegI_R0 result, iRegP_R10 tmp1, iRegL_R11 tmp2,\n+                              vRegD_V0 vtmp1, vRegD_V1 vtmp2, pRegGov_P0 pgtmp1,\n+                              pRegGov_P1 pgtmp2, rFlagsReg cr)\n+%{\n+  predicate((UseSVE > 0) && (((StrCompNode*)n)->encoding() == StrIntrinsicNode::UU));\n+  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP vtmp1, TEMP vtmp2, TEMP pgtmp1, TEMP pgtmp2,\n+         USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n+\n+  format %{ \"String Compare $str1,$cnt1,$str2,$cnt2 -> $result   # USE sve\" %}\n+  ins_encode %{\n+    \/\/ Count is in 8-bit bytes; non-Compact chars are 16 bits.\n+    __ string_compare($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n+                      $tmp1$$Register, $tmp2$$Register,\n+                      $vtmp1$$FloatRegister, $vtmp2$$FloatRegister, fnoreg,\n+                      as_PRegister($pgtmp1$$reg), as_PRegister($pgtmp2$$reg),\n+                      StrIntrinsicNode::UU);\n+  %}\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n@@ -17296,0 +16739,32 @@\n+instruct stringL_indexof_char_sve(iRegP_R1 str1, iRegI_R2 cnt1, iRegI_R3 ch,\n+                                  iRegI_R0 result, vecA ztmp1, vecA ztmp2,\n+                                  pRegGov pgtmp, pReg ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && ((StrIndexOfCharNode*)n)->encoding() == StrIntrinsicNode::L);\n+  match(Set result (StrIndexOfChar (Binary str1 cnt1) ch));\n+  effect(TEMP ztmp1, TEMP ztmp2, TEMP pgtmp, TEMP ptmp, KILL cr);\n+  format %{ \"StringLatin1 IndexOf char[] $str1,$cnt1,$ch -> $result # use sve\" %}\n+  ins_encode %{\n+    __ string_indexof_char_sve($str1$$Register, $cnt1$$Register, $ch$$Register,\n+                               $result$$Register, $ztmp1$$FloatRegister,\n+                               $ztmp2$$FloatRegister, $pgtmp$$PRegister,\n+                               $ptmp$$PRegister, true \/* isL *\/);\n+  %}\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n+instruct stringU_indexof_char_sve(iRegP_R1 str1, iRegI_R2 cnt1, iRegI_R3 ch,\n+                                  iRegI_R0 result, vecA ztmp1, vecA ztmp2,\n+                                  pRegGov pgtmp, pReg ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && ((StrIndexOfCharNode*)n)->encoding() == StrIntrinsicNode::U);\n+  match(Set result (StrIndexOfChar (Binary str1 cnt1) ch));\n+  effect(TEMP ztmp1, TEMP ztmp2, TEMP pgtmp, TEMP ptmp, KILL cr);\n+  format %{ \"StringUTF16 IndexOf char[] $str1,$cnt1,$ch -> $result # use sve\" %}\n+  ins_encode %{\n+    __ string_indexof_char_sve($str1$$Register, $cnt1$$Register, $ch$$Register,\n+                               $result$$Register, $ztmp1$$FloatRegister,\n+                               $ztmp2$$FloatRegister, $pgtmp$$PRegister,\n+                               $ptmp$$PRegister, false \/* isL *\/);\n+  %}\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":307,"deletions":832,"binary":false,"changes":1139,"status":"modified"},{"patch":"@@ -336,1 +336,1 @@\n-  __ far_call(RuntimeAddress(Runtime1::entry_for(_stub)), NULL, rscratch2);\n+  __ far_call(RuntimeAddress(Runtime1::entry_for(_stub)), rscratch2);\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_CodeStubs_aarch64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -334,1 +334,1 @@\n-    __ movoop(reg, o, \/*immediate*\/true);\n+    __ movoop(reg, o);\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -281,1 +281,1 @@\n-  resolve_oop_handle(result, tmp);\n+  resolve_oop_handle(result, tmp, rscratch2);\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -689,1 +689,1 @@\n-void MacroAssembler::far_call(Address entry, CodeBuffer *cbuf, Register tmp) {\n+void MacroAssembler::far_call(Address entry, Register tmp) {\n@@ -702,1 +702,0 @@\n-    if (cbuf) cbuf->set_insts_mark();\n@@ -705,1 +704,0 @@\n-    if (cbuf) cbuf->set_insts_mark();\n@@ -710,1 +708,1 @@\n-int MacroAssembler::far_jump(Address entry, CodeBuffer *cbuf, Register tmp) {\n+int MacroAssembler::far_jump(Address entry, Register tmp) {\n@@ -724,1 +722,0 @@\n-    if (cbuf) cbuf->set_insts_mark();\n@@ -727,1 +724,0 @@\n-    if (cbuf) cbuf->set_insts_mark();\n@@ -854,1 +850,32 @@\n-\/\/ Maybe emit a call via a trampoline.  If the code cache is small\n+\/\/ Check the entry target is always reachable from any branch.\n+static bool is_always_within_branch_range(Address entry) {\n+  const address target = entry.target();\n+\n+  if (!CodeCache::contains(target)) {\n+    \/\/ We always use trampolines for callees outside CodeCache.\n+    assert(entry.rspec().type() == relocInfo::runtime_call_type, \"non-runtime call of an external target\");\n+    return false;\n+  }\n+\n+  if (!MacroAssembler::far_branches()) {\n+    return true;\n+  }\n+\n+  if (entry.rspec().type() == relocInfo::runtime_call_type) {\n+    \/\/ Runtime calls are calls of a non-compiled method (stubs, adapters).\n+    \/\/ Non-compiled methods stay forever in CodeCache.\n+    \/\/ We check whether the longest possible branch is within the branch range.\n+    assert(CodeCache::find_blob(target) != NULL &&\n+          !CodeCache::find_blob(target)->is_compiled(),\n+          \"runtime call of compiled method\");\n+    const address right_longest_branch_start = CodeCache::high_bound() - NativeInstruction::instruction_size;\n+    const address left_longest_branch_start = CodeCache::low_bound();\n+    const bool is_reachable = Assembler::reachable_from_branch_at(left_longest_branch_start, target) &&\n+                              Assembler::reachable_from_branch_at(right_longest_branch_start, target);\n+    return is_reachable;\n+  }\n+\n+  return false;\n+}\n+\n+\/\/ Maybe emit a call via a trampoline. If the code cache is small\n@@ -856,1 +883,1 @@\n-address MacroAssembler::trampoline_call1(Address entry, CodeBuffer* cbuf, bool check_emit_size) {\n+address MacroAssembler::trampoline_call(Address entry) {\n@@ -862,20 +889,4 @@\n-  bool need_trampoline = far_branches();\n-  if (!need_trampoline && entry.rspec().type() == relocInfo::runtime_call_type && !CodeCache::contains(entry.target())) {\n-    \/\/ If it is a runtime call of an address outside small CodeCache,\n-    \/\/ we need to check whether it is in range.\n-    address target = entry.target();\n-    assert(target < CodeCache::low_bound() || target >= CodeCache::high_bound(), \"target is inside CodeCache\");\n-    \/\/ Case 1: -------T-------L====CodeCache====H-------\n-    \/\/                ^-------longest branch---|\n-    \/\/ Case 2: -------L====CodeCache====H-------T-------\n-    \/\/                |-------longest branch ---^\n-    address longest_branch_start = (target < CodeCache::low_bound()) ? CodeCache::high_bound() - NativeInstruction::instruction_size\n-                                                                     : CodeCache::low_bound();\n-    need_trampoline = !reachable_from_branch_at(longest_branch_start, target);\n-  }\n-\n-  \/\/ We need a trampoline if branches are far.\n-  if (need_trampoline) {\n-    bool in_scratch_emit_size = false;\n-#ifdef COMPILER2\n-    if (check_emit_size) {\n+  address target = entry.target();\n+\n+  if (!is_always_within_branch_range(entry)) {\n+    if (!in_scratch_emit_size()) {\n@@ -884,11 +895,9 @@\n-      CompileTask* task = ciEnv::current()->task();\n-      in_scratch_emit_size =\n-        (task != NULL && is_c2_compile(task->comp_level()) &&\n-         Compile::current()->output()->in_scratch_emit_size());\n-    }\n-#endif\n-    if (!in_scratch_emit_size) {\n-      address stub = emit_trampoline_stub(offset(), entry.target());\n-      if (stub == NULL) {\n-        postcond(pc() == badAddress);\n-        return NULL; \/\/ CodeCache is full\n+      if (entry.rspec().type() == relocInfo::runtime_call_type) {\n+        assert(CodeBuffer::supports_shared_stubs(), \"must support shared stubs\");\n+        code()->share_trampoline_for(entry.target(), offset());\n+      } else {\n+        address stub = emit_trampoline_stub(offset(), target);\n+        if (stub == NULL) {\n+          postcond(pc() == badAddress);\n+          return NULL; \/\/ CodeCache is full\n+        }\n@@ -897,0 +906,1 @@\n+    target = pc();\n@@ -899,1 +909,1 @@\n-  if (cbuf) cbuf->set_insts_mark();\n+  address call_pc = pc();\n@@ -901,6 +911,2 @@\n-  if (!need_trampoline) {\n-    bl(entry.target());\n-  } else {\n-    bl(pc());\n-  }\n-  \/\/ just need to return a non-null address\n+  bl(target);\n+\n@@ -908,1 +914,1 @@\n-  return pc();\n+  return call_pc;\n@@ -911,1 +917,0 @@\n-\n@@ -2209,1 +2214,1 @@\n-  regs[count++] = zr->encoding_nocheck();\n+  regs[count++] = zr->raw_encoding();\n@@ -2239,1 +2244,1 @@\n-  regs[count++] = zr->encoding_nocheck();\n+  regs[count++] = zr->raw_encoding();\n@@ -2394,1 +2399,1 @@\n-  unsigned char regs[PRegisterImpl::number_of_saved_registers];\n+  unsigned char regs[PRegister::number_of_saved_registers];\n@@ -2396,1 +2401,1 @@\n-  for (int reg = 0; reg < PRegisterImpl::number_of_saved_registers; reg++) {\n+  for (int reg = 0; reg < PRegister::number_of_saved_registers; reg++) {\n@@ -2431,1 +2436,1 @@\n-  unsigned char regs[PRegisterImpl::number_of_saved_registers];\n+  unsigned char regs[PRegister::number_of_saved_registers];\n@@ -2433,1 +2438,1 @@\n-  for (int reg = 0; reg < PRegisterImpl::number_of_saved_registers; reg++) {\n+  for (int reg = 0; reg < PRegister::number_of_saved_registers; reg++) {\n@@ -2464,1 +2469,1 @@\n-    cmpptr(rheapbase, ExternalAddress((address)CompressedOops::ptrs_base_addr()));\n+    cmpptr(rheapbase, ExternalAddress(CompressedOops::ptrs_base_addr()));\n@@ -2474,1 +2479,1 @@\n-void MacroAssembler::resolve_jobject(Register value, Register thread, Register tmp) {\n+void MacroAssembler::resolve_jobject(Register value, Register tmp1, Register tmp2) {\n@@ -2483,1 +2488,1 @@\n-                 Address(value, -JNIHandles::weak_tag_value), tmp, thread);\n+                 Address(value, -JNIHandles::weak_tag_value), tmp1, tmp2);\n@@ -2489,1 +2494,1 @@\n-  access_load_at(T_OBJECT, IN_NATIVE, value, Address(value, 0), tmp, thread);\n+  access_load_at(T_OBJECT, IN_NATIVE, value, Address(value, 0), tmp1, tmp2);\n@@ -2598,1 +2603,1 @@\n-      lea(rheapbase, ExternalAddress((address)CompressedOops::ptrs_base_addr()));\n+      lea(rheapbase, ExternalAddress(CompressedOops::ptrs_base_addr()));\n@@ -2921,2 +2926,2 @@\n-    sub(sp, sp, sve_vector_size_in_bytes * FloatRegisterImpl::number_of_registers);\n-    for (int i = 0; i < FloatRegisterImpl::number_of_registers; i++) {\n+    sub(sp, sp, sve_vector_size_in_bytes * FloatRegister::number_of_registers);\n+    for (int i = 0; i < FloatRegister::number_of_registers; i++) {\n@@ -2937,1 +2942,1 @@\n-    for (int i = 0; i < PRegisterImpl::number_of_saved_registers; i++) {\n+    for (int i = 0; i < PRegister::number_of_saved_registers; i++) {\n@@ -2946,1 +2951,1 @@\n-    for (int i = PRegisterImpl::number_of_saved_registers - 1; i >= 0; i--) {\n+    for (int i = PRegister::number_of_saved_registers - 1; i >= 0; i--) {\n@@ -2952,1 +2957,1 @@\n-    for (int i = FloatRegisterImpl::number_of_registers - 1; i >= 0; i--) {\n+    for (int i = FloatRegister::number_of_registers - 1; i >= 0; i--) {\n@@ -2955,1 +2960,1 @@\n-    add(sp, sp, sve_vector_size_in_bytes * FloatRegisterImpl::number_of_registers);\n+    add(sp, sp, sve_vector_size_in_bytes * FloatRegister::number_of_registers);\n@@ -4084,1 +4089,1 @@\n-void MacroAssembler::resolve_oop_handle(Register result, Register tmp) {\n+void MacroAssembler::resolve_oop_handle(Register result, Register tmp1, Register tmp2) {\n@@ -4086,1 +4091,1 @@\n-  access_load_at(T_OBJECT, IN_NATIVE, result, Address(result, 0), tmp, noreg);\n+  access_load_at(T_OBJECT, IN_NATIVE, result, Address(result, 0), tmp1, tmp2);\n@@ -4090,2 +4095,2 @@\n-void MacroAssembler::resolve_weak_handle(Register rresult, Register rtmp) {\n-  assert_different_registers(rresult, rtmp);\n+void MacroAssembler::resolve_weak_handle(Register result, Register tmp1, Register tmp2) {\n+  assert_different_registers(result, tmp1, tmp2);\n@@ -4095,1 +4100,1 @@\n-  cbz(rresult, resolved);\n+  cbz(result, resolved);\n@@ -4098,1 +4103,0 @@\n-  \/\/ Only IN_HEAP loads require a thread_tmp register\n@@ -4101,1 +4105,1 @@\n-                 rresult, Address(rresult), rtmp, \/*tmp_thread*\/noreg);\n+                 result, Address(result), tmp1, tmp2);\n@@ -4105,1 +4109,1 @@\n-void MacroAssembler::load_mirror(Register dst, Register method, Register tmp) {\n+void MacroAssembler::load_mirror(Register dst, Register method, Register tmp1, Register tmp2) {\n@@ -4111,1 +4115,1 @@\n-  resolve_oop_handle(dst, tmp);\n+  resolve_oop_handle(dst, tmp1, tmp2);\n@@ -4416,1 +4420,1 @@\n-                                    Register tmp1, Register thread_tmp) {\n+                                    Register tmp1, Register tmp2) {\n@@ -4421,1 +4425,1 @@\n-    bs->BarrierSetAssembler::load_at(this, decorators, type, dst, src, tmp1, thread_tmp);\n+    bs->BarrierSetAssembler::load_at(this, decorators, type, dst, src, tmp1, tmp2);\n@@ -4423,1 +4427,1 @@\n-    bs->load_at(this, decorators, type, dst, src, tmp1, thread_tmp);\n+    bs->load_at(this, decorators, type, dst, src, tmp1, tmp2);\n@@ -4429,1 +4433,1 @@\n-                                     Register tmp1, Register thread_tmp) {\n+                                     Register tmp1, Register tmp2, Register tmp3) {\n@@ -4434,1 +4438,1 @@\n-    bs->BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, thread_tmp);\n+    bs->BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, tmp2, tmp3);\n@@ -4436,1 +4440,1 @@\n-    bs->store_at(this, decorators, type, dst, src, tmp1, thread_tmp);\n+    bs->store_at(this, decorators, type, dst, src, tmp1, tmp2, tmp3);\n@@ -4441,2 +4445,2 @@\n-                                   Register thread_tmp, DecoratorSet decorators) {\n-  access_load_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, thread_tmp);\n+                                   Register tmp2, DecoratorSet decorators) {\n+  access_load_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, tmp2);\n@@ -4446,2 +4450,2 @@\n-                                            Register thread_tmp, DecoratorSet decorators) {\n-  access_load_at(T_OBJECT, IN_HEAP | IS_NOT_NULL | decorators, dst, src, tmp1, thread_tmp);\n+                                            Register tmp2, DecoratorSet decorators) {\n+  access_load_at(T_OBJECT, IN_HEAP | IS_NOT_NULL | decorators, dst, src, tmp1, tmp2);\n@@ -4451,2 +4455,2 @@\n-                                    Register thread_tmp, DecoratorSet decorators) {\n-  access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, thread_tmp);\n+                                    Register tmp2, Register tmp3, DecoratorSet decorators) {\n+  access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, tmp2, tmp3);\n@@ -4457,1 +4461,1 @@\n-  access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg);\n+  access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg, noreg);\n@@ -4467,5 +4471,2 @@\n-\/\/ Move an oop into a register.  immediate is true if we want\n-\/\/ immediate instructions and nmethod entry barriers are not enabled.\n-\/\/ i.e. we are not going to patch this instruction while the code is being\n-\/\/ executed by another thread.\n-void MacroAssembler::movoop(Register dst, jobject obj, bool immediate) {\n+\/\/ Move an oop into a register.\n+void MacroAssembler::movoop(Register dst, jobject obj) {\n@@ -4486,5 +4487,3 @@\n-  \/\/ nmethod entry barrier necessitate using the constant pool. They have to be\n-  \/\/ ordered with respected to oop accesses.\n-  \/\/ Using immediate literals would necessitate ISBs.\n-  BarrierSet* bs = BarrierSet::barrier_set();\n-  if ((bs->barrier_set_nmethod() != NULL && bs->barrier_set_assembler()->nmethod_patching_type() == NMethodPatchingType::conc_data_patch) || !immediate) {\n+  if (BarrierSet::barrier_set()->barrier_set_assembler()->supports_instruction_patching()) {\n+    mov(dst, Address((address)obj, rspec));\n+  } else {\n@@ -4493,2 +4492,1 @@\n-  } else\n-    mov(dst, Address((address)obj, rspec));\n+  }\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":97,"deletions":99,"binary":false,"changes":196,"status":"modified"},{"patch":"@@ -647,0 +647,4 @@\n+  \/\/ Return whether code is emitted to a scratch blob.\n+  virtual bool in_scratch_emit_size() {\n+    return false;\n+  }\n@@ -840,1 +844,1 @@\n-  void resolve_jobject(Register value, Register thread, Register tmp);\n+  void resolve_jobject(Register value, Register tmp1, Register tmp2);\n@@ -853,3 +857,3 @@\n-  void resolve_weak_handle(Register result, Register tmp);\n-  void resolve_oop_handle(Register result, Register tmp = r5);\n-  void load_mirror(Register dst, Register method, Register tmp = r5);\n+  void resolve_weak_handle(Register result, Register tmp1, Register tmp2);\n+  void resolve_oop_handle(Register result, Register tmp1, Register tmp2);\n+  void load_mirror(Register dst, Register method, Register tmp1, Register tmp2);\n@@ -858,1 +862,1 @@\n-                      Register tmp1, Register tmp_thread);\n+                      Register tmp1, Register tmp2);\n@@ -861,1 +865,1 @@\n-                       Register tmp1, Register tmp_thread);\n+                       Register tmp1, Register tmp2, Register tmp3);\n@@ -864,1 +868,1 @@\n-                     Register thread_tmp = noreg, DecoratorSet decorators = 0);\n+                     Register tmp2 = noreg, DecoratorSet decorators = 0);\n@@ -867,1 +871,1 @@\n-                              Register thread_tmp = noreg, DecoratorSet decorators = 0);\n+                              Register tmp2 = noreg, DecoratorSet decorators = 0);\n@@ -869,1 +873,1 @@\n-                      Register tmp_thread = noreg, DecoratorSet decorators = 0);\n+                      Register tmp2 = noreg, Register tmp3 = noreg, DecoratorSet decorators = 0);\n@@ -1186,3 +1190,2 @@\n-  \/\/ Return: NULL if CodeCache is full.\n-  address trampoline_call(Address entry, CodeBuffer* cbuf = NULL) { return trampoline_call1(entry, cbuf, true); }\n-  address trampoline_call1(Address entry, CodeBuffer* cbuf, bool check_emit_size = true);\n+  \/\/ Return: the call PC or NULL if CodeCache is full.\n+  address trampoline_call(Address entry);\n@@ -1210,2 +1213,2 @@\n-  void far_call(Address entry, CodeBuffer *cbuf = NULL, Register tmp = rscratch1);\n-  int far_jump(Address entry, CodeBuffer *cbuf = NULL, Register tmp = rscratch1);\n+  void far_call(Address entry, Register tmp = rscratch1);\n+  int far_jump(Address entry, Register tmp = rscratch1);\n@@ -1232,1 +1235,1 @@\n-  void movoop(Register dst, jobject obj, bool immediate = false);\n+  void movoop(Register dst, jobject obj);\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":18,"deletions":15,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -1866,1 +1866,1 @@\n-    __ store_heap_oop(__ post(to, UseCompressedOops ? 4 : 8), copied_oop, noreg, noreg, AS_RAW);  \/\/ store the oop\n+    __ store_heap_oop(__ post(to, UseCompressedOops ? 4 : 8), copied_oop, noreg, noreg, noreg, AS_RAW);  \/\/ store the oop\n@@ -3058,2 +3058,3 @@\n-      for (FloatRegister f = v0; f < v0 + bulk_width; f++) {\n-        __ rev32(f, __ T16B, v16);\n+      for (int i = 0; i < bulk_width; i++) {\n+        FloatRegister v0_ofs = as_FloatRegister(v0->encoding() + i);\n+        __ rev32(v0_ofs, __ T16B, v16);\n@@ -3074,1 +3075,3 @@\n-        __ eor(v0 + i, __ T16B, v0 + i, v8 + i);\n+        FloatRegister v0_ofs = as_FloatRegister(v0->encoding() + i);\n+        FloatRegister v8_ofs = as_FloatRegister(v8->encoding() + i);\n+        __ eor(v0_ofs, __ T16B, v0_ofs, v8_ofs);\n@@ -3175,1 +3178,4 @@\n-      for (FloatRegister f = v0; f < v8; f++) {\n+\n+      assert(v0->encoding() < v8->encoding(), \"\");\n+      for (int i = v0->encoding(); i < v8->encoding(); i++) {\n+        FloatRegister f = as_FloatRegister(i);\n@@ -3189,1 +3195,3 @@\n-        __ eor(v0 + i, __ T16B, v0 + i, v8 + i);\n+        FloatRegister v0_ofs = as_FloatRegister(v0->encoding() + i);\n+        FloatRegister v8_ofs = as_FloatRegister(v8->encoding() + i);\n+        __ eor(v0_ofs, __ T16B, v0_ofs, v8_ofs);\n@@ -3229,0 +3237,68 @@\n+  \/\/ Utility routines for md5.\n+  \/\/ Clobbers r10 and r11.\n+  void md5_FF(Register buf, Register r1, Register r2, Register r3, Register r4,\n+              int k, int s, int t) {\n+    Register rscratch3 = r10;\n+    Register rscratch4 = r11;\n+\n+    __ eorw(rscratch3, r3, r4);\n+    __ movw(rscratch2, t);\n+    __ andw(rscratch3, rscratch3, r2);\n+    __ addw(rscratch4, r1, rscratch2);\n+    __ ldrw(rscratch1, Address(buf, k*4));\n+    __ eorw(rscratch3, rscratch3, r4);\n+    __ addw(rscratch3, rscratch3, rscratch1);\n+    __ addw(rscratch3, rscratch3, rscratch4);\n+    __ rorw(rscratch2, rscratch3, 32 - s);\n+    __ addw(r1, rscratch2, r2);\n+  }\n+\n+  void md5_GG(Register buf, Register r1, Register r2, Register r3, Register r4,\n+              int k, int s, int t) {\n+    Register rscratch3 = r10;\n+    Register rscratch4 = r11;\n+\n+    __ eorw(rscratch2, r2, r3);\n+    __ ldrw(rscratch1, Address(buf, k*4));\n+    __ andw(rscratch3, rscratch2, r4);\n+    __ movw(rscratch2, t);\n+    __ eorw(rscratch3, rscratch3, r3);\n+    __ addw(rscratch4, r1, rscratch2);\n+    __ addw(rscratch3, rscratch3, rscratch1);\n+    __ addw(rscratch3, rscratch3, rscratch4);\n+    __ rorw(rscratch2, rscratch3, 32 - s);\n+    __ addw(r1, rscratch2, r2);\n+  }\n+\n+  void md5_HH(Register buf, Register r1, Register r2, Register r3, Register r4,\n+              int k, int s, int t) {\n+    Register rscratch3 = r10;\n+    Register rscratch4 = r11;\n+\n+    __ eorw(rscratch3, r3, r4);\n+    __ movw(rscratch2, t);\n+    __ addw(rscratch4, r1, rscratch2);\n+    __ ldrw(rscratch1, Address(buf, k*4));\n+    __ eorw(rscratch3, rscratch3, r2);\n+    __ addw(rscratch3, rscratch3, rscratch1);\n+    __ addw(rscratch3, rscratch3, rscratch4);\n+    __ rorw(rscratch2, rscratch3, 32 - s);\n+    __ addw(r1, rscratch2, r2);\n+  }\n+\n+  void md5_II(Register buf, Register r1, Register r2, Register r3, Register r4,\n+              int k, int s, int t) {\n+    Register rscratch3 = r10;\n+    Register rscratch4 = r11;\n+\n+    __ movw(rscratch3, t);\n+    __ ornw(rscratch2, r2, r4);\n+    __ addw(rscratch4, r1, rscratch3);\n+    __ ldrw(rscratch1, Address(buf, k*4));\n+    __ eorw(rscratch3, rscratch2, r3);\n+    __ addw(rscratch3, rscratch3, rscratch1);\n+    __ addw(rscratch3, rscratch3, rscratch4);\n+    __ rorw(rscratch2, rscratch3, 32 - s);\n+    __ addw(r1, rscratch2, r2);\n+  }\n+\n@@ -3253,2 +3329,0 @@\n-    Label keys;\n-\n@@ -3264,62 +3338,16 @@\n-#define FF(r1, r2, r3, r4, k, s, t)              \\\n-    __ eorw(rscratch3, r3, r4);                  \\\n-    __ movw(rscratch2, t);                       \\\n-    __ andw(rscratch3, rscratch3, r2);           \\\n-    __ addw(rscratch4, r1, rscratch2);           \\\n-    __ ldrw(rscratch1, Address(buf, k*4));       \\\n-    __ eorw(rscratch3, rscratch3, r4);           \\\n-    __ addw(rscratch3, rscratch3, rscratch1);    \\\n-    __ addw(rscratch3, rscratch3, rscratch4);    \\\n-    __ rorw(rscratch2, rscratch3, 32 - s);       \\\n-    __ addw(r1, rscratch2, r2);\n-\n-#define GG(r1, r2, r3, r4, k, s, t)              \\\n-    __ eorw(rscratch2, r2, r3);                  \\\n-    __ ldrw(rscratch1, Address(buf, k*4));       \\\n-    __ andw(rscratch3, rscratch2, r4);           \\\n-    __ movw(rscratch2, t);                       \\\n-    __ eorw(rscratch3, rscratch3, r3);           \\\n-    __ addw(rscratch4, r1, rscratch2);           \\\n-    __ addw(rscratch3, rscratch3, rscratch1);    \\\n-    __ addw(rscratch3, rscratch3, rscratch4);    \\\n-    __ rorw(rscratch2, rscratch3, 32 - s);       \\\n-    __ addw(r1, rscratch2, r2);\n-\n-#define HH(r1, r2, r3, r4, k, s, t)              \\\n-    __ eorw(rscratch3, r3, r4);                  \\\n-    __ movw(rscratch2, t);                       \\\n-    __ addw(rscratch4, r1, rscratch2);           \\\n-    __ ldrw(rscratch1, Address(buf, k*4));       \\\n-    __ eorw(rscratch3, rscratch3, r2);           \\\n-    __ addw(rscratch3, rscratch3, rscratch1);    \\\n-    __ addw(rscratch3, rscratch3, rscratch4);    \\\n-    __ rorw(rscratch2, rscratch3, 32 - s);       \\\n-    __ addw(r1, rscratch2, r2);\n-\n-#define II(r1, r2, r3, r4, k, s, t)              \\\n-    __ movw(rscratch3, t);                       \\\n-    __ ornw(rscratch2, r2, r4);                  \\\n-    __ addw(rscratch4, r1, rscratch3);           \\\n-    __ ldrw(rscratch1, Address(buf, k*4));       \\\n-    __ eorw(rscratch3, rscratch2, r3);           \\\n-    __ addw(rscratch3, rscratch3, rscratch1);    \\\n-    __ addw(rscratch3, rscratch3, rscratch4);    \\\n-    __ rorw(rscratch2, rscratch3, 32 - s);       \\\n-    __ addw(r1, rscratch2, r2);\n-\n-    FF(a, b, c, d,  0,  7, 0xd76aa478)\n-    FF(d, a, b, c,  1, 12, 0xe8c7b756)\n-    FF(c, d, a, b,  2, 17, 0x242070db)\n-    FF(b, c, d, a,  3, 22, 0xc1bdceee)\n-    FF(a, b, c, d,  4,  7, 0xf57c0faf)\n-    FF(d, a, b, c,  5, 12, 0x4787c62a)\n-    FF(c, d, a, b,  6, 17, 0xa8304613)\n-    FF(b, c, d, a,  7, 22, 0xfd469501)\n-    FF(a, b, c, d,  8,  7, 0x698098d8)\n-    FF(d, a, b, c,  9, 12, 0x8b44f7af)\n-    FF(c, d, a, b, 10, 17, 0xffff5bb1)\n-    FF(b, c, d, a, 11, 22, 0x895cd7be)\n-    FF(a, b, c, d, 12,  7, 0x6b901122)\n-    FF(d, a, b, c, 13, 12, 0xfd987193)\n-    FF(c, d, a, b, 14, 17, 0xa679438e)\n-    FF(b, c, d, a, 15, 22, 0x49b40821)\n+    md5_FF(buf, a, b, c, d,  0,  7, 0xd76aa478);\n+    md5_FF(buf, d, a, b, c,  1, 12, 0xe8c7b756);\n+    md5_FF(buf, c, d, a, b,  2, 17, 0x242070db);\n+    md5_FF(buf, b, c, d, a,  3, 22, 0xc1bdceee);\n+    md5_FF(buf, a, b, c, d,  4,  7, 0xf57c0faf);\n+    md5_FF(buf, d, a, b, c,  5, 12, 0x4787c62a);\n+    md5_FF(buf, c, d, a, b,  6, 17, 0xa8304613);\n+    md5_FF(buf, b, c, d, a,  7, 22, 0xfd469501);\n+    md5_FF(buf, a, b, c, d,  8,  7, 0x698098d8);\n+    md5_FF(buf, d, a, b, c,  9, 12, 0x8b44f7af);\n+    md5_FF(buf, c, d, a, b, 10, 17, 0xffff5bb1);\n+    md5_FF(buf, b, c, d, a, 11, 22, 0x895cd7be);\n+    md5_FF(buf, a, b, c, d, 12,  7, 0x6b901122);\n+    md5_FF(buf, d, a, b, c, 13, 12, 0xfd987193);\n+    md5_FF(buf, c, d, a, b, 14, 17, 0xa679438e);\n+    md5_FF(buf, b, c, d, a, 15, 22, 0x49b40821);\n@@ -3329,16 +3357,16 @@\n-    GG(a, b, c, d,  1,  5, 0xf61e2562)\n-    GG(d, a, b, c,  6,  9, 0xc040b340)\n-    GG(c, d, a, b, 11, 14, 0x265e5a51)\n-    GG(b, c, d, a,  0, 20, 0xe9b6c7aa)\n-    GG(a, b, c, d,  5,  5, 0xd62f105d)\n-    GG(d, a, b, c, 10,  9, 0x02441453)\n-    GG(c, d, a, b, 15, 14, 0xd8a1e681)\n-    GG(b, c, d, a,  4, 20, 0xe7d3fbc8)\n-    GG(a, b, c, d,  9,  5, 0x21e1cde6)\n-    GG(d, a, b, c, 14,  9, 0xc33707d6)\n-    GG(c, d, a, b,  3, 14, 0xf4d50d87)\n-    GG(b, c, d, a,  8, 20, 0x455a14ed)\n-    GG(a, b, c, d, 13,  5, 0xa9e3e905)\n-    GG(d, a, b, c,  2,  9, 0xfcefa3f8)\n-    GG(c, d, a, b,  7, 14, 0x676f02d9)\n-    GG(b, c, d, a, 12, 20, 0x8d2a4c8a)\n+    md5_GG(buf, a, b, c, d,  1,  5, 0xf61e2562);\n+    md5_GG(buf, d, a, b, c,  6,  9, 0xc040b340);\n+    md5_GG(buf, c, d, a, b, 11, 14, 0x265e5a51);\n+    md5_GG(buf, b, c, d, a,  0, 20, 0xe9b6c7aa);\n+    md5_GG(buf, a, b, c, d,  5,  5, 0xd62f105d);\n+    md5_GG(buf, d, a, b, c, 10,  9, 0x02441453);\n+    md5_GG(buf, c, d, a, b, 15, 14, 0xd8a1e681);\n+    md5_GG(buf, b, c, d, a,  4, 20, 0xe7d3fbc8);\n+    md5_GG(buf, a, b, c, d,  9,  5, 0x21e1cde6);\n+    md5_GG(buf, d, a, b, c, 14,  9, 0xc33707d6);\n+    md5_GG(buf, c, d, a, b,  3, 14, 0xf4d50d87);\n+    md5_GG(buf, b, c, d, a,  8, 20, 0x455a14ed);\n+    md5_GG(buf, a, b, c, d, 13,  5, 0xa9e3e905);\n+    md5_GG(buf, d, a, b, c,  2,  9, 0xfcefa3f8);\n+    md5_GG(buf, c, d, a, b,  7, 14, 0x676f02d9);\n+    md5_GG(buf, b, c, d, a, 12, 20, 0x8d2a4c8a);\n@@ -3347,16 +3375,16 @@\n-    HH(a, b, c, d,  5,  4, 0xfffa3942)\n-    HH(d, a, b, c,  8, 11, 0x8771f681)\n-    HH(c, d, a, b, 11, 16, 0x6d9d6122)\n-    HH(b, c, d, a, 14, 23, 0xfde5380c)\n-    HH(a, b, c, d,  1,  4, 0xa4beea44)\n-    HH(d, a, b, c,  4, 11, 0x4bdecfa9)\n-    HH(c, d, a, b,  7, 16, 0xf6bb4b60)\n-    HH(b, c, d, a, 10, 23, 0xbebfbc70)\n-    HH(a, b, c, d, 13,  4, 0x289b7ec6)\n-    HH(d, a, b, c,  0, 11, 0xeaa127fa)\n-    HH(c, d, a, b,  3, 16, 0xd4ef3085)\n-    HH(b, c, d, a,  6, 23, 0x04881d05)\n-    HH(a, b, c, d,  9,  4, 0xd9d4d039)\n-    HH(d, a, b, c, 12, 11, 0xe6db99e5)\n-    HH(c, d, a, b, 15, 16, 0x1fa27cf8)\n-    HH(b, c, d, a,  2, 23, 0xc4ac5665)\n+    md5_HH(buf, a, b, c, d,  5,  4, 0xfffa3942);\n+    md5_HH(buf, d, a, b, c,  8, 11, 0x8771f681);\n+    md5_HH(buf, c, d, a, b, 11, 16, 0x6d9d6122);\n+    md5_HH(buf, b, c, d, a, 14, 23, 0xfde5380c);\n+    md5_HH(buf, a, b, c, d,  1,  4, 0xa4beea44);\n+    md5_HH(buf, d, a, b, c,  4, 11, 0x4bdecfa9);\n+    md5_HH(buf, c, d, a, b,  7, 16, 0xf6bb4b60);\n+    md5_HH(buf, b, c, d, a, 10, 23, 0xbebfbc70);\n+    md5_HH(buf, a, b, c, d, 13,  4, 0x289b7ec6);\n+    md5_HH(buf, d, a, b, c,  0, 11, 0xeaa127fa);\n+    md5_HH(buf, c, d, a, b,  3, 16, 0xd4ef3085);\n+    md5_HH(buf, b, c, d, a,  6, 23, 0x04881d05);\n+    md5_HH(buf, a, b, c, d,  9,  4, 0xd9d4d039);\n+    md5_HH(buf, d, a, b, c, 12, 11, 0xe6db99e5);\n+    md5_HH(buf, c, d, a, b, 15, 16, 0x1fa27cf8);\n+    md5_HH(buf, b, c, d, a,  2, 23, 0xc4ac5665);\n@@ -3365,21 +3393,16 @@\n-    II(a, b, c, d,  0,  6, 0xf4292244)\n-    II(d, a, b, c,  7, 10, 0x432aff97)\n-    II(c, d, a, b, 14, 15, 0xab9423a7)\n-    II(b, c, d, a,  5, 21, 0xfc93a039)\n-    II(a, b, c, d, 12,  6, 0x655b59c3)\n-    II(d, a, b, c,  3, 10, 0x8f0ccc92)\n-    II(c, d, a, b, 10, 15, 0xffeff47d)\n-    II(b, c, d, a,  1, 21, 0x85845dd1)\n-    II(a, b, c, d,  8,  6, 0x6fa87e4f)\n-    II(d, a, b, c, 15, 10, 0xfe2ce6e0)\n-    II(c, d, a, b,  6, 15, 0xa3014314)\n-    II(b, c, d, a, 13, 21, 0x4e0811a1)\n-    II(a, b, c, d,  4,  6, 0xf7537e82)\n-    II(d, a, b, c, 11, 10, 0xbd3af235)\n-    II(c, d, a, b,  2, 15, 0x2ad7d2bb)\n-    II(b, c, d, a,  9, 21, 0xeb86d391)\n-\n-#undef FF\n-#undef GG\n-#undef HH\n-#undef II\n+    md5_II(buf, a, b, c, d,  0,  6, 0xf4292244);\n+    md5_II(buf, d, a, b, c,  7, 10, 0x432aff97);\n+    md5_II(buf, c, d, a, b, 14, 15, 0xab9423a7);\n+    md5_II(buf, b, c, d, a,  5, 21, 0xfc93a039);\n+    md5_II(buf, a, b, c, d, 12,  6, 0x655b59c3);\n+    md5_II(buf, d, a, b, c,  3, 10, 0x8f0ccc92);\n+    md5_II(buf, c, d, a, b, 10, 15, 0xffeff47d);\n+    md5_II(buf, b, c, d, a,  1, 21, 0x85845dd1);\n+    md5_II(buf, a, b, c, d,  8,  6, 0x6fa87e4f);\n+    md5_II(buf, d, a, b, c, 15, 10, 0xfe2ce6e0);\n+    md5_II(buf, c, d, a, b,  6, 15, 0xa3014314);\n+    md5_II(buf, b, c, d, a, 13, 21, 0x4e0811a1);\n+    md5_II(buf, a, b, c, d,  4,  6, 0xf7537e82);\n+    md5_II(buf, d, a, b, c, 11, 10, 0xbd3af235);\n+    md5_II(buf, c, d, a, b,  2, 15, 0x2ad7d2bb);\n+    md5_II(buf, b, c, d, a,  9, 21, 0xeb86d391);\n@@ -3623,0 +3646,28 @@\n+  \/\/ Double rounds for sha512.\n+  void sha512_dround(int dr,\n+                     FloatRegister vi0, FloatRegister vi1,\n+                     FloatRegister vi2, FloatRegister vi3,\n+                     FloatRegister vi4, FloatRegister vrc0,\n+                     FloatRegister vrc1, FloatRegister vin0,\n+                     FloatRegister vin1, FloatRegister vin2,\n+                     FloatRegister vin3, FloatRegister vin4) {\n+      if (dr < 36) {\n+        __ ld1(vrc1, __ T2D, __ post(rscratch2, 16));\n+      }\n+      __ addv(v5, __ T2D, vrc0, vin0);\n+      __ ext(v6, __ T16B, vi2, vi3, 8);\n+      __ ext(v5, __ T16B, v5, v5, 8);\n+      __ ext(v7, __ T16B, vi1, vi2, 8);\n+      __ addv(vi3, __ T2D, vi3, v5);\n+      if (dr < 32) {\n+        __ ext(v5, __ T16B, vin3, vin4, 8);\n+        __ sha512su0(vin0, __ T2D, vin1);\n+      }\n+      __ sha512h(vi3, __ T2D, v6, v7);\n+      if (dr < 32) {\n+        __ sha512su1(vin0, __ T2D, vin2, v5);\n+      }\n+      __ addv(vi4, __ T2D, vi1, vi3);\n+      __ sha512h2(vi3, __ T2D, vi1, vi0);\n+  }\n+\n@@ -3662,19 +3713,0 @@\n-    \/\/ Double rounds for sha512.\n-    #define sha512_dround(dr, i0, i1, i2, i3, i4, rc0, rc1, in0, in1, in2, in3, in4) \\\n-      if (dr < 36)                                                                   \\\n-        __ ld1(v##rc1, __ T2D, __ post(rscratch2, 16));                              \\\n-      __ addv(v5, __ T2D, v##rc0, v##in0);                                           \\\n-      __ ext(v6, __ T16B, v##i2, v##i3, 8);                                          \\\n-      __ ext(v5, __ T16B, v5, v5, 8);                                                \\\n-      __ ext(v7, __ T16B, v##i1, v##i2, 8);                                          \\\n-      __ addv(v##i3, __ T2D, v##i3, v5);                                             \\\n-      if (dr < 32) {                                                                 \\\n-        __ ext(v5, __ T16B, v##in3, v##in4, 8);                                      \\\n-        __ sha512su0(v##in0, __ T2D, v##in1);                                        \\\n-      }                                                                              \\\n-      __ sha512h(v##i3, __ T2D, v6, v7);                                             \\\n-      if (dr < 32)                                                                   \\\n-        __ sha512su1(v##in0, __ T2D, v##in2, v5);                                    \\\n-      __ addv(v##i4, __ T2D, v##i1, v##i3);                                          \\\n-      __ sha512h2(v##i3, __ T2D, v##i1, v##i0);                                      \\\n-\n@@ -3724,40 +3756,40 @@\n-    sha512_dround( 0, 0, 1, 2, 3, 4, 20, 24, 12, 13, 19, 16, 17);\n-    sha512_dround( 1, 3, 0, 4, 2, 1, 21, 25, 13, 14, 12, 17, 18);\n-    sha512_dround( 2, 2, 3, 1, 4, 0, 22, 26, 14, 15, 13, 18, 19);\n-    sha512_dround( 3, 4, 2, 0, 1, 3, 23, 27, 15, 16, 14, 19, 12);\n-    sha512_dround( 4, 1, 4, 3, 0, 2, 24, 28, 16, 17, 15, 12, 13);\n-    sha512_dround( 5, 0, 1, 2, 3, 4, 25, 29, 17, 18, 16, 13, 14);\n-    sha512_dround( 6, 3, 0, 4, 2, 1, 26, 30, 18, 19, 17, 14, 15);\n-    sha512_dround( 7, 2, 3, 1, 4, 0, 27, 31, 19, 12, 18, 15, 16);\n-    sha512_dround( 8, 4, 2, 0, 1, 3, 28, 24, 12, 13, 19, 16, 17);\n-    sha512_dround( 9, 1, 4, 3, 0, 2, 29, 25, 13, 14, 12, 17, 18);\n-    sha512_dround(10, 0, 1, 2, 3, 4, 30, 26, 14, 15, 13, 18, 19);\n-    sha512_dround(11, 3, 0, 4, 2, 1, 31, 27, 15, 16, 14, 19, 12);\n-    sha512_dround(12, 2, 3, 1, 4, 0, 24, 28, 16, 17, 15, 12, 13);\n-    sha512_dround(13, 4, 2, 0, 1, 3, 25, 29, 17, 18, 16, 13, 14);\n-    sha512_dround(14, 1, 4, 3, 0, 2, 26, 30, 18, 19, 17, 14, 15);\n-    sha512_dround(15, 0, 1, 2, 3, 4, 27, 31, 19, 12, 18, 15, 16);\n-    sha512_dround(16, 3, 0, 4, 2, 1, 28, 24, 12, 13, 19, 16, 17);\n-    sha512_dround(17, 2, 3, 1, 4, 0, 29, 25, 13, 14, 12, 17, 18);\n-    sha512_dround(18, 4, 2, 0, 1, 3, 30, 26, 14, 15, 13, 18, 19);\n-    sha512_dround(19, 1, 4, 3, 0, 2, 31, 27, 15, 16, 14, 19, 12);\n-    sha512_dround(20, 0, 1, 2, 3, 4, 24, 28, 16, 17, 15, 12, 13);\n-    sha512_dround(21, 3, 0, 4, 2, 1, 25, 29, 17, 18, 16, 13, 14);\n-    sha512_dround(22, 2, 3, 1, 4, 0, 26, 30, 18, 19, 17, 14, 15);\n-    sha512_dround(23, 4, 2, 0, 1, 3, 27, 31, 19, 12, 18, 15, 16);\n-    sha512_dround(24, 1, 4, 3, 0, 2, 28, 24, 12, 13, 19, 16, 17);\n-    sha512_dround(25, 0, 1, 2, 3, 4, 29, 25, 13, 14, 12, 17, 18);\n-    sha512_dround(26, 3, 0, 4, 2, 1, 30, 26, 14, 15, 13, 18, 19);\n-    sha512_dround(27, 2, 3, 1, 4, 0, 31, 27, 15, 16, 14, 19, 12);\n-    sha512_dround(28, 4, 2, 0, 1, 3, 24, 28, 16, 17, 15, 12, 13);\n-    sha512_dround(29, 1, 4, 3, 0, 2, 25, 29, 17, 18, 16, 13, 14);\n-    sha512_dround(30, 0, 1, 2, 3, 4, 26, 30, 18, 19, 17, 14, 15);\n-    sha512_dround(31, 3, 0, 4, 2, 1, 27, 31, 19, 12, 18, 15, 16);\n-    sha512_dround(32, 2, 3, 1, 4, 0, 28, 24, 12,  0,  0,  0,  0);\n-    sha512_dround(33, 4, 2, 0, 1, 3, 29, 25, 13,  0,  0,  0,  0);\n-    sha512_dround(34, 1, 4, 3, 0, 2, 30, 26, 14,  0,  0,  0,  0);\n-    sha512_dround(35, 0, 1, 2, 3, 4, 31, 27, 15,  0,  0,  0,  0);\n-    sha512_dround(36, 3, 0, 4, 2, 1, 24,  0, 16,  0,  0,  0,  0);\n-    sha512_dround(37, 2, 3, 1, 4, 0, 25,  0, 17,  0,  0,  0,  0);\n-    sha512_dround(38, 4, 2, 0, 1, 3, 26,  0, 18,  0,  0,  0,  0);\n-    sha512_dround(39, 1, 4, 3, 0, 2, 27,  0, 19,  0,  0,  0,  0);\n+    sha512_dround( 0, v0, v1, v2, v3, v4, v20, v24, v12, v13, v19, v16, v17);\n+    sha512_dround( 1, v3, v0, v4, v2, v1, v21, v25, v13, v14, v12, v17, v18);\n+    sha512_dround( 2, v2, v3, v1, v4, v0, v22, v26, v14, v15, v13, v18, v19);\n+    sha512_dround( 3, v4, v2, v0, v1, v3, v23, v27, v15, v16, v14, v19, v12);\n+    sha512_dround( 4, v1, v4, v3, v0, v2, v24, v28, v16, v17, v15, v12, v13);\n+    sha512_dround( 5, v0, v1, v2, v3, v4, v25, v29, v17, v18, v16, v13, v14);\n+    sha512_dround( 6, v3, v0, v4, v2, v1, v26, v30, v18, v19, v17, v14, v15);\n+    sha512_dround( 7, v2, v3, v1, v4, v0, v27, v31, v19, v12, v18, v15, v16);\n+    sha512_dround( 8, v4, v2, v0, v1, v3, v28, v24, v12, v13, v19, v16, v17);\n+    sha512_dround( 9, v1, v4, v3, v0, v2, v29, v25, v13, v14, v12, v17, v18);\n+    sha512_dround(10, v0, v1, v2, v3, v4, v30, v26, v14, v15, v13, v18, v19);\n+    sha512_dround(11, v3, v0, v4, v2, v1, v31, v27, v15, v16, v14, v19, v12);\n+    sha512_dround(12, v2, v3, v1, v4, v0, v24, v28, v16, v17, v15, v12, v13);\n+    sha512_dround(13, v4, v2, v0, v1, v3, v25, v29, v17, v18, v16, v13, v14);\n+    sha512_dround(14, v1, v4, v3, v0, v2, v26, v30, v18, v19, v17, v14, v15);\n+    sha512_dround(15, v0, v1, v2, v3, v4, v27, v31, v19, v12, v18, v15, v16);\n+    sha512_dround(16, v3, v0, v4, v2, v1, v28, v24, v12, v13, v19, v16, v17);\n+    sha512_dround(17, v2, v3, v1, v4, v0, v29, v25, v13, v14, v12, v17, v18);\n+    sha512_dround(18, v4, v2, v0, v1, v3, v30, v26, v14, v15, v13, v18, v19);\n+    sha512_dround(19, v1, v4, v3, v0, v2, v31, v27, v15, v16, v14, v19, v12);\n+    sha512_dround(20, v0, v1, v2, v3, v4, v24, v28, v16, v17, v15, v12, v13);\n+    sha512_dround(21, v3, v0, v4, v2, v1, v25, v29, v17, v18, v16, v13, v14);\n+    sha512_dround(22, v2, v3, v1, v4, v0, v26, v30, v18, v19, v17, v14, v15);\n+    sha512_dround(23, v4, v2, v0, v1, v3, v27, v31, v19, v12, v18, v15, v16);\n+    sha512_dround(24, v1, v4, v3, v0, v2, v28, v24, v12, v13, v19, v16, v17);\n+    sha512_dround(25, v0, v1, v2, v3, v4, v29, v25, v13, v14, v12, v17, v18);\n+    sha512_dround(26, v3, v0, v4, v2, v1, v30, v26, v14, v15, v13, v18, v19);\n+    sha512_dround(27, v2, v3, v1, v4, v0, v31, v27, v15, v16, v14, v19, v12);\n+    sha512_dround(28, v4, v2, v0, v1, v3, v24, v28, v16, v17, v15, v12, v13);\n+    sha512_dround(29, v1, v4, v3, v0, v2, v25, v29, v17, v18, v16, v13, v14);\n+    sha512_dround(30, v0, v1, v2, v3, v4, v26, v30, v18, v19, v17, v14, v15);\n+    sha512_dround(31, v3, v0, v4, v2, v1, v27, v31, v19, v12, v18, v15, v16);\n+    sha512_dround(32, v2, v3, v1, v4, v0, v28, v24, v12,  v0,  v0,  v0,  v0);\n+    sha512_dround(33, v4, v2, v0, v1, v3, v29, v25, v13,  v0,  v0,  v0,  v0);\n+    sha512_dround(34, v1, v4, v3, v0, v2, v30, v26, v14,  v0,  v0,  v0,  v0);\n+    sha512_dround(35, v0, v1, v2, v3, v4, v31, v27, v15,  v0,  v0,  v0,  v0);\n+    sha512_dround(36, v3, v0, v4, v2, v1, v24,  v0, v16,  v0,  v0,  v0,  v0);\n+    sha512_dround(37, v2, v3, v1, v4, v0, v25,  v0, v17,  v0,  v0,  v0,  v0);\n+    sha512_dround(38, v4, v2, v0, v1, v3, v26,  v0, v18,  v0,  v0,  v0,  v0);\n+    sha512_dround(39, v1, v4, v3, v0, v2, v27,  v0, v19,  v0,  v0,  v0,  v0);\n@@ -6648,63 +6680,0 @@\n-  RuntimeStub* generate_cont_doYield() {\n-    if (!Continuations::enabled()) return nullptr;\n-\n-    const char *name = \"cont_doYield\";\n-\n-    enum layout {\n-      rfp_off1,\n-      rfp_off2,\n-      lr_off,\n-      lr_off2,\n-      framesize \/\/ inclusive of return address\n-    };\n-    \/\/ assert(is_even(framesize\/2), \"sp not 16-byte aligned\");\n-\n-    int insts_size = 512;\n-    int locs_size  = 64;\n-    CodeBuffer code(name, insts_size, locs_size);\n-    OopMapSet* oop_maps  = new OopMapSet();\n-    MacroAssembler* masm = new MacroAssembler(&code);\n-    MacroAssembler* _masm = masm;\n-\n-    address start = __ pc();\n-\n-    __ enter();\n-\n-    __ mov(c_rarg1, sp);\n-\n-    int frame_complete = __ pc() - start;\n-    address the_pc = __ pc();\n-\n-    __ post_call_nop(); \/\/ this must be exactly after the pc value that is pushed into the frame info, we use this nop for fast CodeBlob lookup\n-\n-    __ mov(c_rarg0, rthread);\n-    __ set_last_Java_frame(sp, rfp, the_pc, rscratch1);\n-    __ call_VM_leaf(Continuation::freeze_entry(), 2);\n-    __ reset_last_Java_frame(true);\n-\n-    Label pinned;\n-\n-    __ cbnz(r0, pinned);\n-\n-    \/\/ We've succeeded, set sp to the ContinuationEntry\n-    __ ldr(rscratch1, Address(rthread, JavaThread::cont_entry_offset()));\n-    __ mov(sp, rscratch1);\n-    continuation_enter_cleanup(masm);\n-\n-    __ bind(pinned); \/\/ pinned -- return to caller\n-\n-    __ leave();\n-    __ ret(lr);\n-\n-    OopMap* map = new OopMap(framesize, 1);\n-    oop_maps->add_gc_map(the_pc - start, map);\n-\n-    RuntimeStub* stub = \/\/ codeBlob framesize is in words (not VMRegImpl::slot_size)\n-    RuntimeStub::new_runtime_stub(name,\n-                                  &code,\n-                                  frame_complete,\n-                                  (framesize >> (LogBytesPerWord - LogBytesPerInt)),\n-                                  oop_maps, false);\n-    return stub;\n-  }\n-\n@@ -6845,1 +6814,1 @@\n-  static void jfr_epilogue(MacroAssembler* _masm, Register thread) {\n+  static void jfr_epilogue(MacroAssembler* _masm) {\n@@ -6851,1 +6820,1 @@\n-    bs->load_at(_masm, decorators, T_OBJECT, r0, Address(r0, 0), c_rarg0, thread);\n+    bs->load_at(_masm, decorators, T_OBJECT, r0, Address(r0, 0), rscratch1, rscratch2);\n@@ -6880,1 +6849,1 @@\n-    jfr_epilogue(_masm, rthread);\n+    jfr_epilogue(_masm);\n@@ -7282,1 +7251,2 @@\n-      assert(tmp1 < r19 && tmp2 < r19, \"register corruption\");\n+      assert(tmp1->encoding() < r19->encoding(), \"register corruption\");\n+      assert(tmp2->encoding() < r19->encoding(), \"register corruption\");\n@@ -7886,3 +7856,0 @@\n-    StubRoutines::_cont_doYield_stub = generate_cont_doYield();\n-    StubRoutines::_cont_doYield      = StubRoutines::_cont_doYield_stub == nullptr ? nullptr\n-                                        : StubRoutines::_cont_doYield_stub->entry_point();\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":219,"deletions":252,"binary":false,"changes":471,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"compiler\/compilerDefinitions.inline.hpp\"\n@@ -148,1 +149,1 @@\n-  __ store_heap_oop(dst, val, r10, r1, decorators);\n+  __ store_heap_oop(dst, val, r10, r11, r3, decorators);\n@@ -155,1 +156,1 @@\n-  __ load_heap_oop(dst, src, r10, r1, decorators);\n+  __ load_heap_oop(dst, src, r10, r11, decorators);\n@@ -413,1 +414,1 @@\n-    __ resolve_oop_handle(tmp);\n+    __ resolve_oop_handle(tmp, r5, rscratch2);\n@@ -1061,1 +1062,1 @@\n-  __ access_store_at(T_INT, IN_HEAP | IS_ARRAY, Address(r3, r1, Address::uxtw(2)), r0, noreg, noreg);\n+  __ access_store_at(T_INT, IN_HEAP | IS_ARRAY, Address(r3, r1, Address::uxtw(2)), r0, noreg, noreg, noreg);\n@@ -1073,1 +1074,1 @@\n-  __ access_store_at(T_LONG, IN_HEAP | IS_ARRAY, Address(r3, r1, Address::uxtw(3)), r0, noreg, noreg);\n+  __ access_store_at(T_LONG, IN_HEAP | IS_ARRAY, Address(r3, r1, Address::uxtw(3)), r0, noreg, noreg, noreg);\n@@ -1085,1 +1086,1 @@\n-  __ access_store_at(T_FLOAT, IN_HEAP | IS_ARRAY, Address(r3, r1, Address::uxtw(2)), noreg \/* ftos *\/, noreg, noreg);\n+  __ access_store_at(T_FLOAT, IN_HEAP | IS_ARRAY, Address(r3, r1, Address::uxtw(2)), noreg \/* ftos *\/, noreg, noreg, noreg);\n@@ -1097,1 +1098,1 @@\n-  __ access_store_at(T_DOUBLE, IN_HEAP | IS_ARRAY, Address(r3, r1, Address::uxtw(3)), noreg \/* dtos *\/, noreg, noreg);\n+  __ access_store_at(T_DOUBLE, IN_HEAP | IS_ARRAY, Address(r3, r1, Address::uxtw(3)), noreg \/* dtos *\/, noreg, noreg, noreg);\n@@ -1174,1 +1175,1 @@\n-  __ access_store_at(T_BYTE, IN_HEAP | IS_ARRAY, Address(r3, r1, Address::uxtw(0)), r0, noreg, noreg);\n+  __ access_store_at(T_BYTE, IN_HEAP | IS_ARRAY, Address(r3, r1, Address::uxtw(0)), r0, noreg, noreg, noreg);\n@@ -1187,1 +1188,1 @@\n-  __ access_store_at(T_CHAR, IN_HEAP | IS_ARRAY, Address(r3, r1, Address::uxtw(1)), r0, noreg, noreg);\n+  __ access_store_at(T_CHAR, IN_HEAP | IS_ARRAY, Address(r3, r1, Address::uxtw(1)), r0, noreg, noreg, noreg);\n@@ -2318,1 +2319,1 @@\n-    __ resolve_oop_handle(obj);\n+    __ resolve_oop_handle(obj, r5, rscratch2);\n@@ -2689,1 +2690,1 @@\n-    __ access_store_at(T_BYTE, IN_HEAP, field, r0, noreg, noreg);\n+    __ access_store_at(T_BYTE, IN_HEAP, field, r0, noreg, noreg, noreg);\n@@ -2704,1 +2705,1 @@\n-    __ access_store_at(T_BOOLEAN, IN_HEAP, field, r0, noreg, noreg);\n+    __ access_store_at(T_BOOLEAN, IN_HEAP, field, r0, noreg, noreg, noreg);\n@@ -2735,1 +2736,1 @@\n-    __ access_store_at(T_INT, IN_HEAP, field, r0, noreg, noreg);\n+    __ access_store_at(T_INT, IN_HEAP, field, r0, noreg, noreg, noreg);\n@@ -2750,1 +2751,1 @@\n-    __ access_store_at(T_CHAR, IN_HEAP, field, r0, noreg, noreg);\n+    __ access_store_at(T_CHAR, IN_HEAP, field, r0, noreg, noreg, noreg);\n@@ -2765,1 +2766,1 @@\n-    __ access_store_at(T_SHORT, IN_HEAP, field, r0, noreg, noreg);\n+    __ access_store_at(T_SHORT, IN_HEAP, field, r0, noreg, noreg, noreg);\n@@ -2780,1 +2781,1 @@\n-    __ access_store_at(T_LONG, IN_HEAP, field, r0, noreg, noreg);\n+    __ access_store_at(T_LONG, IN_HEAP, field, r0, noreg, noreg, noreg);\n@@ -2795,1 +2796,1 @@\n-    __ access_store_at(T_FLOAT, IN_HEAP, field, noreg \/* ftos *\/, noreg, noreg);\n+    __ access_store_at(T_FLOAT, IN_HEAP, field, noreg \/* ftos *\/, noreg, noreg, noreg);\n@@ -2812,1 +2813,1 @@\n-    __ access_store_at(T_DOUBLE, IN_HEAP, field, noreg \/* dtos *\/, noreg, noreg);\n+    __ access_store_at(T_DOUBLE, IN_HEAP, field, noreg \/* dtos *\/, noreg, noreg, noreg);\n@@ -2947,1 +2948,1 @@\n-    __ access_store_at(T_LONG, IN_HEAP, field, r0, noreg, noreg);\n+    __ access_store_at(T_LONG, IN_HEAP, field, r0, noreg, noreg, noreg);\n@@ -2950,1 +2951,1 @@\n-    __ access_store_at(T_INT, IN_HEAP, field, r0, noreg, noreg);\n+    __ access_store_at(T_INT, IN_HEAP, field, r0, noreg, noreg, noreg);\n@@ -2953,1 +2954,1 @@\n-    __ access_store_at(T_BOOLEAN, IN_HEAP, field, r0, noreg, noreg);\n+    __ access_store_at(T_BOOLEAN, IN_HEAP, field, r0, noreg, noreg, noreg);\n@@ -2956,1 +2957,1 @@\n-    __ access_store_at(T_BYTE, IN_HEAP, field, r0, noreg, noreg);\n+    __ access_store_at(T_BYTE, IN_HEAP, field, r0, noreg, noreg, noreg);\n@@ -2959,1 +2960,1 @@\n-    __ access_store_at(T_SHORT, IN_HEAP, field, r0, noreg, noreg);\n+    __ access_store_at(T_SHORT, IN_HEAP, field, r0, noreg, noreg, noreg);\n@@ -2962,1 +2963,1 @@\n-    __ access_store_at(T_CHAR, IN_HEAP, field, r0, noreg, noreg);\n+    __ access_store_at(T_CHAR, IN_HEAP, field, r0, noreg, noreg, noreg);\n@@ -2965,1 +2966,1 @@\n-    __ access_store_at(T_FLOAT, IN_HEAP, field, noreg \/* ftos *\/, noreg, noreg);\n+    __ access_store_at(T_FLOAT, IN_HEAP, field, noreg \/* ftos *\/, noreg, noreg, noreg);\n@@ -2968,1 +2969,1 @@\n-    __ access_store_at(T_DOUBLE, IN_HEAP, field, noreg \/* dtos *\/, noreg, noreg);\n+    __ access_store_at(T_DOUBLE, IN_HEAP, field, noreg \/* dtos *\/, noreg, noreg, noreg);\n","filename":"src\/hotspot\/cpu\/aarch64\/templateTable_aarch64.cpp","additions":27,"deletions":26,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -4488,1 +4488,1 @@\n-    CodeBlob* cb = CodeCache::find_blob_unsafe(pc);   \/\/ Else we get assertion if nmethod is zombie.\n+    CodeBlob* cb = CodeCache::find_blob(pc);\n","filename":"src\/hotspot\/cpu\/s390\/macroAssembler_s390.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -567,1 +567,1 @@\n-    __ incrementl(ExternalAddress((address)&Runtime1::_arraycopy_slowcase_cnt));\n+    __ incrementl(ExternalAddress((address)&Runtime1::_arraycopy_slowcase_cnt), rscratch1);\n","filename":"src\/hotspot\/cpu\/x86\/c1_CodeStubs_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -199,1 +199,1 @@\n-      __ push_oop(const_opr->as_jobject());\n+      __ push_oop(const_opr->as_jobject(), rscratch1);\n@@ -322,1 +322,1 @@\n-        __ cmpptr(Address(OSR_buf, slot_offset + 1*BytesPerWord), (int32_t)NULL_WORD);\n+        __ cmpptr(Address(OSR_buf, slot_offset + 1*BytesPerWord), NULL_WORD);\n@@ -443,2 +443,2 @@\n-  __ movptr(Address(thread, JavaThread::exception_oop_offset()), (intptr_t)NULL_WORD);\n-  __ movptr(Address(thread, JavaThread::exception_pc_offset()), (intptr_t)NULL_WORD);\n+  __ movptr(Address(thread, JavaThread::exception_oop_offset()), NULL_WORD);\n+  __ movptr(Address(thread, JavaThread::exception_pc_offset()), NULL_WORD);\n@@ -472,1 +472,1 @@\n-    __ mov_metadata(Address(rsp, sizeof(void*)), method()->constant_encoding());\n+    __ mov_metadata(Address(rsp, sizeof(void*)), method()->constant_encoding(), noreg);\n@@ -506,1 +506,1 @@\n-  __ pushptr(here.addr());\n+  __ pushptr(here.addr(), rscratch1);\n@@ -694,1 +694,1 @@\n-      __ movoop(frame_map()->address_for_slot(dest->single_stack_ix()), c->as_jobject());\n+      __ movoop(frame_map()->address_for_slot(dest->single_stack_ix()), c->as_jobject(), rscratch1);\n@@ -701,1 +701,3 @@\n-                                            lo_word_offset_in_bytes), (intptr_t)c->as_jlong_bits());\n+                                              lo_word_offset_in_bytes),\n+                (intptr_t)c->as_jlong_bits(),\n+                rscratch1);\n@@ -736,1 +738,1 @@\n-          __ movl(as_Address(addr), (int32_t)NULL_WORD);\n+          __ movl(as_Address(addr), NULL_WORD);\n@@ -749,1 +751,1 @@\n-          __ movoop(as_Address(addr, noreg), c->as_jobject());\n+          __ movoop(as_Address(addr, noreg), c->as_jobject(), rscratch1);\n@@ -762,1 +764,1 @@\n-          __ movoop(as_Address(addr), c->as_jobject());\n+          __ movoop(as_Address(addr), c->as_jobject(), noreg);\n@@ -1662,1 +1664,1 @@\n-    __ cmpptr(recv_addr, (intptr_t)NULL_WORD);\n+    __ cmpptr(recv_addr, NULL_WORD);\n@@ -1714,1 +1716,1 @@\n-  __ cmpptr(obj, (int32_t)NULL_WORD);\n+  __ cmpptr(obj, NULL_WORD);\n@@ -1787,1 +1789,1 @@\n-        __ pushklass(k->constant_encoding());\n+        __ pushklass(k->constant_encoding(), noreg);\n@@ -1860,1 +1862,1 @@\n-    __ cmpptr(value, (int32_t)NULL_WORD);\n+    __ cmpptr(value, NULL_WORD);\n@@ -1956,1 +1958,1 @@\n-    assert(newval != NULL, \"new val must be register\");\n+    assert(newval != noreg, \"new val must be register\");\n@@ -1987,1 +1989,1 @@\n-    assert(newval != NULL, \"new val must be register\");\n+    assert(newval != noreg, \"new val must be register\");\n@@ -2435,1 +2437,2 @@\n-                     ExternalAddress((address)double_signmask_pool));\n+                     ExternalAddress((address)double_signmask_pool),\n+                     rscratch1);\n@@ -2666,1 +2669,1 @@\n-          __ cmpptr(reg1, (int32_t)0);\n+          __ cmpptr(reg1, NULL_WORD);\n@@ -2674,1 +2677,1 @@\n-          __ cmpptr(reg1, (int32_t)NULL_WORD);\n+          __ cmpptr(reg1, NULL_WORD);\n@@ -2676,1 +2679,1 @@\n-          __ cmpoop(reg1, o);\n+          __ cmpoop(reg1, o, rscratch1);\n@@ -3038,1 +3041,1 @@\n-void LIR_Assembler::store_parameter(jobject o,  int offset_from_rsp_in_words) {\n+void LIR_Assembler::store_parameter(jobject o, int offset_from_rsp_in_words) {\n@@ -3042,1 +3045,1 @@\n-  __ movoop (Address(rsp, offset_from_rsp_in_bytes), o);\n+  __ movoop(Address(rsp, offset_from_rsp_in_bytes), o, rscratch1);\n@@ -3046,1 +3049,1 @@\n-void LIR_Assembler::store_parameter(Metadata* m,  int offset_from_rsp_in_words) {\n+void LIR_Assembler::store_parameter(Metadata* m, int offset_from_rsp_in_words) {\n@@ -3050,1 +3053,1 @@\n-  __ mov_metadata(Address(rsp, offset_from_rsp_in_bytes), m);\n+  __ mov_metadata(Address(rsp, offset_from_rsp_in_bytes), m, rscratch1);\n@@ -3113,1 +3116,1 @@\n-      __ incrementl(ExternalAddress((address)&Runtime1::_generic_arraycopystub_cnt));\n+      __ incrementl(ExternalAddress((address)&Runtime1::_generic_arraycopystub_cnt), rscratch1);\n@@ -3122,1 +3125,1 @@\n-      __ incrementl(ExternalAddress((address)&Runtime1::_generic_arraycopystub_cnt));\n+      __ incrementl(ExternalAddress((address)&Runtime1::_generic_arraycopystub_cnt), rscratch1);\n@@ -3136,1 +3139,1 @@\n-      __ incrementl(ExternalAddress((address)&Runtime1::_generic_arraycopystub_cnt));\n+      __ incrementl(ExternalAddress((address)&Runtime1::_generic_arraycopystub_cnt), rscratch1);\n@@ -3371,1 +3374,1 @@\n-          __ incrementl(ExternalAddress((address)&Runtime1::_arraycopy_checkcast_cnt));\n+          __ incrementl(ExternalAddress((address)&Runtime1::_arraycopy_checkcast_cnt), rscratch1);\n@@ -3381,1 +3384,1 @@\n-          __ incrementl(ExternalAddress((address)&Runtime1::_arraycopy_checkcast_attempt_cnt));\n+          __ incrementl(ExternalAddress((address)&Runtime1::_arraycopy_checkcast_attempt_cnt), rscratch1);\n@@ -3608,1 +3611,1 @@\n-          __ mov_metadata(recv_addr, known_klass->constant_encoding());\n+          __ mov_metadata(recv_addr, known_klass->constant_encoding(), rscratch1);\n@@ -3837,1 +3840,2 @@\n-               ExternalAddress((address)float_signflip_pool));\n+               ExternalAddress((address)float_signflip_pool),\n+               rscratch1);\n@@ -3854,1 +3858,2 @@\n-               ExternalAddress((address)double_signflip_pool));\n+               ExternalAddress((address)double_signflip_pool),\n+               rscratch1);\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":38,"deletions":33,"binary":false,"changes":71,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,0 +28,1 @@\n+#include \"compiler\/compilerDefinitions.inline.hpp\"\n@@ -41,1 +42,0 @@\n-  const Register rklass_decode_tmp = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n@@ -57,1 +57,1 @@\n-    load_klass(hdr, obj, rklass_decode_tmp);\n+    load_klass(hdr, obj, rscratch1);\n@@ -147,1 +147,0 @@\n-  Register tmp_encode_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n@@ -243,1 +242,1 @@\n-  cmpptr(len, (int32_t)max_array_allocation_length);\n+  cmpptr(len, checked_cast<int32_t>(max_array_allocation_length));\n@@ -276,1 +275,0 @@\n-  Register tmp_load_klass = LP64_ONLY(rscratch2) NOT_LP64(noreg);\n@@ -279,1 +277,1 @@\n-    load_klass(rscratch1, receiver, tmp_load_klass);\n+    load_klass(rscratch1, receiver, rscratch2);\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":5,"deletions":7,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"compiler\/compilerDefinitions.inline.hpp\"\n@@ -79,1 +80,1 @@\n-    set_last_Java_frame(thread, noreg, rbp, NULL);\n+    set_last_Java_frame(thread, noreg, rbp, NULL, rscratch1);\n@@ -83,1 +84,1 @@\n-    set_last_Java_frame(thread, noreg, rbp, the_pc);\n+    set_last_Java_frame(thread, noreg, rbp, the_pc, rscratch1);\n@@ -113,1 +114,1 @@\n-    cmpptr(Address(thread, Thread::pending_exception_offset()), (int32_t)NULL_WORD);\n+    cmpptr(Address(thread, Thread::pending_exception_offset()), NULL_WORD);\n@@ -323,4 +324,0 @@\n-\/\/ Register is a class, but it would be assigned numerical value.\n-\/\/ \"0\" is assigned for rax. Thus we need to ignore -Wnonnull.\n-PRAGMA_DIAG_PUSH\n-PRAGMA_NONNULL_IGNORED\n@@ -421,1 +418,0 @@\n-PRAGMA_DIAG_POP\n@@ -737,1 +733,1 @@\n-  __ cmpptr(Address(thread, JavaThread::exception_oop_offset()), (int32_t) NULL_WORD);\n+  __ cmpptr(Address(thread, JavaThread::exception_oop_offset()), NULL_WORD);\n@@ -893,1 +889,1 @@\n-  __ set_last_Java_frame(thread, noreg, rbp, NULL);\n+  __ set_last_Java_frame(thread, noreg, rbp, NULL, rscratch1);\n@@ -919,1 +915,1 @@\n-    __ cmpptr(Address(thread, Thread::pending_exception_offset()), (int32_t)NULL_WORD);\n+    __ cmpptr(Address(thread, Thread::pending_exception_offset()), NULL_WORD);\n@@ -943,1 +939,1 @@\n-    __ cmpptr(Address(thread, JavaThread::exception_oop_offset()), (int32_t)NULL_WORD);\n+    __ cmpptr(Address(thread, JavaThread::exception_oop_offset()), NULL_WORD);\n@@ -949,1 +945,1 @@\n-    __ cmpptr(Address(thread, JavaThread::exception_pc_offset()), (int32_t)NULL_WORD);\n+    __ cmpptr(Address(thread, JavaThread::exception_pc_offset()), NULL_WORD);\n@@ -1164,2 +1160,1 @@\n-        Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n-        __ load_klass(t, rax, tmp_load_klass);\n+        __ load_klass(t, rax, rscratch1);\n","filename":"src\/hotspot\/cpu\/x86\/c1_Runtime1_x86.cpp","additions":10,"deletions":15,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -62,2 +62,1 @@\n-  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n-  load_klass(obj, obj, tmp_load_klass);\n+  load_klass(obj, obj, rscratch1);\n@@ -269,1 +268,1 @@\n-    cmpptr(Address(rbp, frame::interpreter_frame_last_sp_offset * wordSize), (int32_t)NULL_WORD);\n+    cmpptr(Address(rbp, frame::interpreter_frame_last_sp_offset * wordSize), NULL_WORD);\n@@ -301,1 +300,1 @@\n-    cmpptr(Address(rbp, frame::interpreter_frame_last_sp_offset * wordSize), (int32_t)NULL_WORD);\n+    cmpptr(Address(rbp, frame::interpreter_frame_last_sp_offset * wordSize), NULL_WORD);\n@@ -352,1 +351,1 @@\n-               movptr(oop_addr, (int32_t)NULL_WORD);\n+               movptr(oop_addr, NULL_WORD);\n@@ -366,2 +365,2 @@\n-  movl(tos_addr,  (int) ilgl);\n-  movl(val_addr,  (int32_t) NULL_WORD);\n+  movl(tos_addr, ilgl);\n+  movl(val_addr, NULL_WORD);\n@@ -389,2 +388,2 @@\n-  movl(tos_addr,  (int32_t) ilgl);\n-  movptr(val_addr,  NULL_WORD);\n+  movl(tos_addr, ilgl);\n+  movptr(val_addr, NULL_WORD);\n@@ -846,1 +845,1 @@\n-    cmpptr(rcx, (int32_t)min_frame_size);\n+    cmpptr(rcx, min_frame_size);\n@@ -883,1 +882,1 @@\n-    jump(dispatch_addr);\n+    jump(dispatch_addr, noreg);\n@@ -889,1 +888,1 @@\n-    jump(dispatch_addr);\n+    jump(dispatch_addr, noreg);\n@@ -1006,1 +1005,1 @@\n-  set_last_Java_frame(rthread, noreg, rbp, (address)pc());\n+  set_last_Java_frame(rthread, noreg, rbp, (address)pc(), rscratch1);\n@@ -1125,1 +1124,1 @@\n-    cmpptr(Address(rmon, BasicObjectLock::obj_offset_in_bytes()), (int32_t) NULL);\n+    cmpptr(Address(rmon, BasicObjectLock::obj_offset_in_bytes()), NULL_WORD);\n@@ -1210,1 +1209,1 @@\n-    const Register rklass_decode_tmp = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n+    const Register rklass_decode_tmp = rscratch1;\n@@ -1228,1 +1227,1 @@\n-    movl(swap_reg, (int32_t)1);\n+    movl(swap_reg, 1);\n@@ -1330,1 +1329,1 @@\n-    movptr(Address(lock_reg, BasicObjectLock::obj_offset_in_bytes()), (int32_t)NULL_WORD);\n+    movptr(Address(lock_reg, BasicObjectLock::obj_offset_in_bytes()), NULL_WORD);\n@@ -1460,1 +1459,1 @@\n-    addptr(data, (int32_t) -DataLayout::counter_increment);\n+    addptr(data, -DataLayout::counter_increment);\n@@ -1464,1 +1463,1 @@\n-    addptr(data, (int32_t) DataLayout::counter_increment);\n+    addptr(data, DataLayout::counter_increment);\n@@ -1472,1 +1471,1 @@\n-    sbbptr(data, (int32_t)0);\n+    sbbptr(data, 0);\n@@ -2015,1 +2014,1 @@\n-    SkipIfEqual skip(this, &DTraceMethodProbes, false);\n+    SkipIfEqual skip(this, &DTraceMethodProbes, false, rscratch1);\n@@ -2060,1 +2059,1 @@\n-    SkipIfEqual skip(this, &DTraceMethodProbes, false);\n+    SkipIfEqual skip(this, &DTraceMethodProbes, false, rscratch1);\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":21,"deletions":22,"binary":false,"changes":43,"status":"modified"},{"patch":"@@ -107,1 +107,2 @@\n-Address MacroAssembler::as_Address(ArrayAddress adr) {\n+Address MacroAssembler::as_Address(ArrayAddress adr, Register rscratch) {\n+  assert(rscratch == noreg, \"\");\n@@ -130,1 +131,2 @@\n-void MacroAssembler::cmpoop(Register src1, jobject obj) {\n+void MacroAssembler::cmpoop(Register src1, jobject obj, Register rscratch) {\n+  assert(rscratch == noreg, \"redundant\");\n@@ -166,2 +168,3 @@\n-void MacroAssembler::jump(ArrayAddress entry) {\n-  jmp(as_Address(entry));\n+void MacroAssembler::jump(ArrayAddress entry, Register rscratch) {\n+  assert(rscratch == noreg, \"not needed\");\n+  jmp(as_Address(entry, noreg));\n@@ -197,1 +200,1 @@\n-    mov_literal32(dst, (int32_t)src.target(), src.rspec());\n+  mov_literal32(dst, (int32_t)src.target(), src.rspec());\n@@ -200,1 +203,3 @@\n-void MacroAssembler::lea(Address dst, AddressLiteral adr) {\n+void MacroAssembler::lea(Address dst, AddressLiteral adr, Register rscratch) {\n+  assert(rscratch == noreg, \"not needed\");\n+\n@@ -203,1 +208,1 @@\n-  mov_literal32(dst, (int32_t) adr.target(), adr.rspec());\n+  mov_literal32(dst, (int32_t)adr.target(), adr.rspec());\n@@ -303,1 +308,2 @@\n-void MacroAssembler::movoop(Address dst, jobject obj) {\n+void MacroAssembler::movoop(Address dst, jobject obj, Register rscratch) {\n+  assert(rscratch == noreg, \"redundant\");\n@@ -311,1 +317,2 @@\n-void MacroAssembler::mov_metadata(Address dst, Metadata* obj) {\n+void MacroAssembler::mov_metadata(Address dst, Metadata* obj, Register rscratch) {\n+  assert(rscratch == noreg, \"redundant\");\n@@ -315,3 +322,1 @@\n-void MacroAssembler::movptr(Register dst, AddressLiteral src, Register scratch) {\n-  \/\/ scratch register is not used,\n-  \/\/ it is defined to match parameters of 64-bit version of this method.\n+void MacroAssembler::movptr(Register dst, AddressLiteral src) {\n@@ -325,2 +330,3 @@\n-void MacroAssembler::movptr(ArrayAddress dst, Register src) {\n-  movl(as_Address(dst), src);\n+void MacroAssembler::movptr(ArrayAddress dst, Register src, Register rscratch) {\n+  assert(rscratch == noreg, \"redundant\");\n+  movl(as_Address(dst, noreg), src);\n@@ -330,1 +336,1 @@\n-  movl(dst, as_Address(src));\n+  movl(dst, as_Address(src, noreg));\n@@ -333,2 +339,2 @@\n-\/\/ src should NEVER be a real pointer. Use AddressLiteral for true pointers\n-void MacroAssembler::movptr(Address dst, intptr_t src) {\n+void MacroAssembler::movptr(Address dst, intptr_t src, Register rscratch) {\n+  assert(rscratch == noreg, \"redundant\");\n@@ -338,1 +344,2 @@\n-void MacroAssembler::pushoop(jobject obj) {\n+void MacroAssembler::pushoop(jobject obj, Register rscratch) {\n+  assert(rscratch == noreg, \"redundant\");\n@@ -342,1 +349,2 @@\n-void MacroAssembler::pushklass(Metadata* obj) {\n+void MacroAssembler::pushklass(Metadata* obj, Register rscratch) {\n+  assert(rscratch == noreg, \"redundant\");\n@@ -346,1 +354,2 @@\n-void MacroAssembler::pushptr(AddressLiteral src) {\n+void MacroAssembler::pushptr(AddressLiteral src, Register rscratch) {\n+  assert(rscratch == noreg, \"redundant\");\n@@ -439,2 +448,2 @@\n-  ExternalAddress message((address)msg);\n-  pushptr(message.addr());\n+  ExternalAddress message((address)msg);\n+  pushptr(message.addr(), noreg);\n@@ -451,2 +460,2 @@\n-  ExternalAddress message((address) msg);\n-  pushptr(message.addr());\n+  ExternalAddress message((address)msg);\n+  pushptr(message.addr(), noreg);\n@@ -482,1 +491,1 @@\n-  return Address((int32_t)(intptr_t)(adr.target() - pc()), adr.target(), adr.reloc());\n+  return Address(checked_cast<int32_t>(adr.target() - pc()), adr.target(), adr.reloc());\n@@ -486,1 +495,1 @@\n-Address MacroAssembler::as_Address(ArrayAddress adr) {\n+Address MacroAssembler::as_Address(ArrayAddress adr, Register rscratch) {\n@@ -488,1 +497,1 @@\n-  lea(rscratch1, base);\n+  lea(rscratch, base);\n@@ -491,1 +500,1 @@\n-  Address array(rscratch1, index._index, index._scale, index._disp);\n+  Address array(rscratch, index._index, index._scale, index._disp);\n@@ -525,1 +534,1 @@\n-void MacroAssembler::cmp64(Register src1, AddressLiteral src2) {\n+void MacroAssembler::cmp64(Register src1, AddressLiteral src2, Register rscratch) {\n@@ -527,0 +536,1 @@\n+  assert(rscratch != noreg || always_reachable(src2), \"missing\");\n@@ -531,2 +541,2 @@\n-    lea(rscratch1, src2);\n-    Assembler::cmpq(src1, Address(rscratch1, 0));\n+    lea(rscratch, src2);\n+    Assembler::cmpq(src1, Address(rscratch, 0));\n@@ -554,1 +564,1 @@\n-  cmp64(rax, ExternalAddress((address) &min_long));\n+  cmp64(rax, ExternalAddress((address) &min_long), rdx \/*rscratch*\/);\n@@ -589,1 +599,3 @@\n-void MacroAssembler::incrementq(AddressLiteral dst) {\n+void MacroAssembler::incrementq(AddressLiteral dst, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(dst), \"missing\");\n+\n@@ -593,2 +605,2 @@\n-    lea(rscratch1, dst);\n-    incrementq(Address(rscratch1, 0));\n+    lea(rscratch, dst);\n+    incrementq(Address(rscratch, 0));\n@@ -616,2 +628,2 @@\n-void MacroAssembler::jump(ArrayAddress entry) {\n-  lea(rscratch1, entry.base());\n+void MacroAssembler::jump(ArrayAddress entry, Register rscratch) {\n+  lea(rscratch, entry.base());\n@@ -620,1 +632,1 @@\n-  dispatch._base = rscratch1;\n+  dispatch._base = rscratch;\n@@ -630,1 +642,1 @@\n-    mov_literal64(dst, (intptr_t)src.target(), src.rspec());\n+  mov_literal64(dst, (intptr_t)src.target(), src.rspec());\n@@ -633,3 +645,3 @@\n-void MacroAssembler::lea(Address dst, AddressLiteral adr) {\n-  mov_literal64(rscratch1, (intptr_t)adr.target(), adr.rspec());\n-  movptr(dst, rscratch1);\n+void MacroAssembler::lea(Address dst, AddressLiteral adr, Register rscratch) {\n+  lea(rscratch, adr);\n+  movptr(dst, rscratch);\n@@ -652,3 +664,3 @@\n-void MacroAssembler::movoop(Address dst, jobject obj) {\n-  mov_literal64(rscratch1, (intptr_t)obj, oop_Relocation::spec_for_immediate());\n-  movq(dst, rscratch1);\n+void MacroAssembler::movoop(Address dst, jobject obj, Register rscratch) {\n+  mov_literal64(rscratch, (intptr_t)obj, oop_Relocation::spec_for_immediate());\n+  movq(dst, rscratch);\n@@ -661,3 +673,3 @@\n-void MacroAssembler::mov_metadata(Address dst, Metadata* obj) {\n-  mov_literal64(rscratch1, (intptr_t)obj, metadata_Relocation::spec_for_immediate());\n-  movq(dst, rscratch1);\n+void MacroAssembler::mov_metadata(Address dst, Metadata* obj, Register rscratch) {\n+  mov_literal64(rscratch, (intptr_t)obj, metadata_Relocation::spec_for_immediate());\n+  movq(dst, rscratch);\n@@ -666,1 +678,1 @@\n-void MacroAssembler::movptr(Register dst, AddressLiteral src, Register scratch) {\n+void MacroAssembler::movptr(Register dst, AddressLiteral src) {\n@@ -673,2 +685,2 @@\n-      lea(scratch, src);\n-      movq(dst, Address(scratch, 0));\n+      lea(dst, src);\n+      movq(dst, Address(dst, 0));\n@@ -679,2 +691,2 @@\n-void MacroAssembler::movptr(ArrayAddress dst, Register src) {\n-  movq(as_Address(dst), src);\n+void MacroAssembler::movptr(ArrayAddress dst, Register src, Register rscratch) {\n+  movq(as_Address(dst, rscratch), src);\n@@ -684,1 +696,1 @@\n-  movq(dst, as_Address(src));\n+  movq(dst, as_Address(src, dst \/*rscratch*\/));\n@@ -688,1 +700,1 @@\n-void MacroAssembler::movptr(Address dst, intptr_t src) {\n+void MacroAssembler::movptr(Address dst, intptr_t src, Register rscratch) {\n@@ -692,2 +704,2 @@\n-    mov64(rscratch1, src);\n-    movq(dst, rscratch1);\n+    mov64(rscratch, src);\n+    movq(dst, rscratch);\n@@ -697,3 +709,3 @@\n-\/\/ These are mostly for initializing NULL\n-void MacroAssembler::movptr(Address dst, int32_t src) {\n-  movslq(dst, src);\n+void MacroAssembler::pushoop(jobject obj, Register rscratch) {\n+  movoop(rscratch, obj);\n+  push(rscratch);\n@@ -702,2 +714,3 @@\n-void MacroAssembler::movptr(Register dst, int32_t src) {\n-  mov64(dst, (intptr_t)src);\n+void MacroAssembler::pushklass(Metadata* obj, Register rscratch) {\n+  mov_metadata(rscratch, obj);\n+  push(rscratch);\n@@ -706,12 +719,2 @@\n-void MacroAssembler::pushoop(jobject obj) {\n-  movoop(rscratch1, obj);\n-  push(rscratch1);\n-}\n-\n-void MacroAssembler::pushklass(Metadata* obj) {\n-  mov_metadata(rscratch1, obj);\n-  push(rscratch1);\n-}\n-\n-void MacroAssembler::pushptr(AddressLiteral src) {\n-  lea(rscratch1, src);\n+void MacroAssembler::pushptr(AddressLiteral src, Register rscratch) {\n+  lea(rscratch, src);\n@@ -719,1 +722,1 @@\n-    push(rscratch1);\n+    push(rscratch);\n@@ -721,1 +724,1 @@\n-    pushq(Address(rscratch1, 0));\n+    pushq(Address(rscratch, 0));\n@@ -731,22 +734,3 @@\n-                                         address  last_java_pc) {\n-  vzeroupper();\n-  \/\/ determine last_java_sp register\n-  if (!last_java_sp->is_valid()) {\n-    last_java_sp = rsp;\n-  }\n-\n-  \/\/ last_java_fp is optional\n-  if (last_java_fp->is_valid()) {\n-    movptr(Address(r15_thread, JavaThread::last_Java_fp_offset()),\n-           last_java_fp);\n-  }\n-\n-  \/\/ last_java_pc is optional\n-  if (last_java_pc != NULL) {\n-    Address java_pc(r15_thread,\n-                    JavaThread::frame_anchor_offset() + JavaFrameAnchor::last_Java_pc_offset());\n-    lea(rscratch1, InternalAddress(last_java_pc));\n-    movptr(java_pc, rscratch1);\n-  }\n-\n-  movptr(Address(r15_thread, JavaThread::last_Java_sp_offset()), last_java_sp);\n+                                         address  last_java_pc,\n+                                         Register rscratch) {\n+  set_last_Java_frame(r15_thread, last_java_sp, last_java_fp, last_java_pc, rscratch);\n@@ -797,0 +781,1 @@\n+\n@@ -798,2 +783,2 @@\n-  lea(rax, ExternalAddress(CAST_FROM_FN_PTR(address, warning)));\n-  call(rax);\n+  call(RuntimeAddress(CAST_FROM_FN_PTR(address, warning)));\n+\n@@ -1067,1 +1052,1 @@\n-    cmpptr(Address(rbp, reg2offset_in(src.first())), (int32_t)NULL_WORD);\n+    cmpptr(Address(rbp, reg2offset_in(src.first())), NULL_WORD);\n@@ -1103,1 +1088,1 @@\n-    cmpptr(rOop, (int32_t)NULL_WORD);\n+    cmpptr(rOop, NULL_WORD);\n@@ -1131,1 +1116,3 @@\n-void MacroAssembler::addsd(XMMRegister dst, AddressLiteral src) {\n+void MacroAssembler::addsd(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -1135,2 +1122,2 @@\n-    lea(rscratch1, src);\n-    Assembler::addsd(dst, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    Assembler::addsd(dst, Address(rscratch, 0));\n@@ -1140,1 +1127,3 @@\n-void MacroAssembler::addss(XMMRegister dst, AddressLiteral src) {\n+void MacroAssembler::addss(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -1144,2 +1133,2 @@\n-    lea(rscratch1, src);\n-    addss(dst, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    addss(dst, Address(rscratch, 0));\n@@ -1149,1 +1138,3 @@\n-void MacroAssembler::addpd(XMMRegister dst, AddressLiteral src) {\n+void MacroAssembler::addpd(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -1153,2 +1144,2 @@\n-    lea(rscratch1, src);\n-    Assembler::addpd(dst, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    Assembler::addpd(dst, Address(rscratch, 0));\n@@ -1201,1 +1192,1 @@\n-void MacroAssembler::andpd(XMMRegister dst, AddressLiteral src, Register scratch_reg) {\n+void MacroAssembler::andpd(XMMRegister dst, AddressLiteral src, Register rscratch) {\n@@ -1204,0 +1195,2 @@\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -1207,2 +1200,2 @@\n-    lea(scratch_reg, src);\n-    Assembler::andpd(dst, Address(scratch_reg, 0));\n+    lea(rscratch, src);\n+    Assembler::andpd(dst, Address(rscratch, 0));\n@@ -1212,1 +1205,1 @@\n-void MacroAssembler::andps(XMMRegister dst, AddressLiteral src, Register scratch_reg) {\n+void MacroAssembler::andps(XMMRegister dst, AddressLiteral src, Register rscratch) {\n@@ -1215,0 +1208,2 @@\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -1218,2 +1213,2 @@\n-    lea(scratch_reg, src);\n-    Assembler::andps(dst, Address(scratch_reg, 0));\n+    lea(rscratch, src);\n+    Assembler::andps(dst, Address(rscratch, 0));\n@@ -1232,1 +1227,3 @@\n-void MacroAssembler::atomic_incl(AddressLiteral counter_addr, Register scr) {\n+void MacroAssembler::atomic_incl(AddressLiteral counter_addr, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(counter_addr), \"missing\");\n+\n@@ -1236,2 +1233,2 @@\n-    lea(scr, counter_addr);\n-    atomic_incl(Address(scr, 0));\n+    lea(rscratch, counter_addr);\n+    atomic_incl(Address(rscratch, 0));\n@@ -1247,1 +1244,3 @@\n-void MacroAssembler::atomic_incq(AddressLiteral counter_addr, Register scr) {\n+void MacroAssembler::atomic_incq(AddressLiteral counter_addr, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(counter_addr), \"missing\");\n+\n@@ -1251,2 +1250,2 @@\n-    lea(scr, counter_addr);\n-    atomic_incq(Address(scr, 0));\n+    lea(rscratch, counter_addr);\n+    atomic_incq(Address(rscratch, 0));\n@@ -1285,4 +1284,4 @@\n-    \/\/ testing if reserved zone needs to be enabled\n-    Label no_reserved_zone_enabling;\n-    Register thread = NOT_LP64(rsi) LP64_ONLY(r15_thread);\n-    NOT_LP64(get_thread(rsi);)\n+  \/\/ testing if reserved zone needs to be enabled\n+  Label no_reserved_zone_enabling;\n+  Register thread = NOT_LP64(rsi) LP64_ONLY(r15_thread);\n+  NOT_LP64(get_thread(rsi);)\n@@ -1290,2 +1289,2 @@\n-    cmpptr(rsp, Address(thread, JavaThread::reserved_stack_activation_offset()));\n-    jcc(Assembler::below, no_reserved_zone_enabling);\n+  cmpptr(rsp, Address(thread, JavaThread::reserved_stack_activation_offset()));\n+  jcc(Assembler::below, no_reserved_zone_enabling);\n@@ -1293,3 +1292,3 @@\n-    call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::enable_stack_reserved_zone), thread);\n-    jump(RuntimeAddress(StubRoutines::throw_delayed_StackOverflowError_entry()));\n-    should_not_reach_here();\n+  call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::enable_stack_reserved_zone), thread);\n+  jump(RuntimeAddress(StubRoutines::throw_delayed_StackOverflowError_entry()));\n+  should_not_reach_here();\n@@ -1297,1 +1296,1 @@\n-    bind(no_reserved_zone_enabling);\n+  bind(no_reserved_zone_enabling);\n@@ -1318,1 +1317,3 @@\n-void MacroAssembler::call(AddressLiteral entry) {\n+void MacroAssembler::call(AddressLiteral entry, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(entry), \"missing\");\n+\n@@ -1322,2 +1323,2 @@\n-    lea(rscratch1, entry);\n-    Assembler::call(rscratch1);\n+    lea(rscratch, entry);\n+    Assembler::call(rscratch);\n@@ -1553,1 +1554,1 @@\n-  set_last_Java_frame(java_thread, last_java_sp, rbp, NULL);\n+  set_last_Java_frame(java_thread, last_java_sp, rbp, NULL, rscratch1);\n@@ -1588,1 +1589,1 @@\n-    cmpptr(Address(java_thread, Thread::pending_exception_offset()), (int32_t) NULL_WORD);\n+    cmpptr(Address(java_thread, Thread::pending_exception_offset()), NULL_WORD);\n@@ -1733,1 +1734,3 @@\n-void MacroAssembler::cmp32(AddressLiteral src1, int32_t imm) {\n+void MacroAssembler::cmp32(AddressLiteral src1, int32_t imm, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src1), \"missing\");\n+\n@@ -1737,2 +1740,2 @@\n-    lea(rscratch1, src1);\n-    cmpl(Address(rscratch1, 0), imm);\n+    lea(rscratch, src1);\n+    cmpl(Address(rscratch, 0), imm);\n@@ -1742,1 +1745,1 @@\n-void MacroAssembler::cmp32(Register src1, AddressLiteral src2) {\n+void MacroAssembler::cmp32(Register src1, AddressLiteral src2, Register rscratch) {\n@@ -1744,0 +1747,2 @@\n+  assert(rscratch != noreg || always_reachable(src2), \"missing\");\n+\n@@ -1747,2 +1752,2 @@\n-    lea(rscratch1, src2);\n-    cmpl(src1, Address(rscratch1, 0));\n+    lea(rscratch, src2);\n+    cmpl(src1, Address(rscratch, 0));\n@@ -1805,1 +1810,3 @@\n-void MacroAssembler::cmp8(AddressLiteral src1, int imm) {\n+void MacroAssembler::cmp8(AddressLiteral src1, int imm, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src1), \"missing\");\n+\n@@ -1809,2 +1816,2 @@\n-    lea(rscratch1, src1);\n-    cmpb(Address(rscratch1, 0), imm);\n+    lea(rscratch, src1);\n+    cmpb(Address(rscratch, 0), imm);\n@@ -1814,1 +1821,1 @@\n-void MacroAssembler::cmpptr(Register src1, AddressLiteral src2) {\n+void MacroAssembler::cmpptr(Register src1, AddressLiteral src2, Register rscratch) {\n@@ -1816,0 +1823,2 @@\n+  assert(rscratch != noreg || always_reachable(src2), \"missing\");\n+\n@@ -1817,2 +1826,2 @@\n-    movptr(rscratch1, src2);\n-    Assembler::cmpq(src1, rscratch1);\n+    movptr(rscratch, src2);\n+    Assembler::cmpq(src1, rscratch);\n@@ -1822,2 +1831,2 @@\n-    lea(rscratch1, src2);\n-    Assembler::cmpq(src1, Address(rscratch1, 0));\n+    lea(rscratch, src2);\n+    Assembler::cmpq(src1, Address(rscratch, 0));\n@@ -1826,0 +1835,1 @@\n+  assert(rscratch == noreg, \"not needed\");\n@@ -1827,1 +1837,1 @@\n-    cmp_literal32(src1, (int32_t) src2.target(), src2.rspec());\n+    cmp_literal32(src1, (int32_t)src2.target(), src2.rspec());\n@@ -1834,1 +1844,1 @@\n-void MacroAssembler::cmpptr(Address src1, AddressLiteral src2) {\n+void MacroAssembler::cmpptr(Address src1, AddressLiteral src2, Register rscratch) {\n@@ -1838,2 +1848,2 @@\n-  movptr(rscratch1, src2);\n-  Assembler::cmpq(src1, rscratch1);\n+  movptr(rscratch, src2);\n+  Assembler::cmpq(src1, rscratch);\n@@ -1841,1 +1851,2 @@\n-  cmp_literal32(src1, (int32_t) src2.target(), src2.rspec());\n+  assert(rscratch == noreg, \"not needed\");\n+  cmp_literal32(src1, (int32_t)src2.target(), src2.rspec());\n@@ -1854,3 +1865,3 @@\n-void MacroAssembler::cmpoop(Register src1, jobject src2) {\n-  movoop(rscratch1, src2);\n-  cmpptr(src1, rscratch1);\n+void MacroAssembler::cmpoop(Register src1, jobject src2, Register rscratch) {\n+  movoop(rscratch, src2);\n+  cmpptr(src1, rscratch);\n@@ -1860,1 +1871,3 @@\n-void MacroAssembler::locked_cmpxchgptr(Register reg, AddressLiteral adr) {\n+void MacroAssembler::locked_cmpxchgptr(Register reg, AddressLiteral adr, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(adr), \"missing\");\n+\n@@ -1865,1 +1878,1 @@\n-    lea(rscratch1, adr);\n+    lea(rscratch, adr);\n@@ -1867,1 +1880,1 @@\n-    cmpxchgptr(reg, Address(rscratch1, 0));\n+    cmpxchgptr(reg, Address(rscratch, 0));\n@@ -1875,1 +1888,3 @@\n-void MacroAssembler::comisd(XMMRegister dst, AddressLiteral src) {\n+void MacroAssembler::comisd(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -1879,2 +1894,2 @@\n-    lea(rscratch1, src);\n-    Assembler::comisd(dst, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    Assembler::comisd(dst, Address(rscratch, 0));\n@@ -1884,1 +1899,3 @@\n-void MacroAssembler::comiss(XMMRegister dst, AddressLiteral src) {\n+void MacroAssembler::comiss(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -1888,2 +1905,2 @@\n-    lea(rscratch1, src);\n-    Assembler::comiss(dst, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    Assembler::comiss(dst, Address(rscratch, 0));\n@@ -1894,1 +1911,3 @@\n-void MacroAssembler::cond_inc32(Condition cond, AddressLiteral counter_addr) {\n+void MacroAssembler::cond_inc32(Condition cond, AddressLiteral counter_addr, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(counter_addr), \"missing\");\n+\n@@ -1899,1 +1918,1 @@\n-  atomic_incl(counter_addr);\n+  atomic_incl(counter_addr, rscratch);\n@@ -1975,1 +1994,3 @@\n-void MacroAssembler::divsd(XMMRegister dst, AddressLiteral src) {\n+void MacroAssembler::divsd(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -1979,2 +2000,2 @@\n-    lea(rscratch1, src);\n-    Assembler::divsd(dst, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    Assembler::divsd(dst, Address(rscratch, 0));\n@@ -1984,1 +2005,3 @@\n-void MacroAssembler::divss(XMMRegister dst, AddressLiteral src) {\n+void MacroAssembler::divss(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -1988,2 +2011,2 @@\n-    lea(rscratch1, src);\n-    Assembler::divss(dst, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    Assembler::divss(dst, Address(rscratch, 0));\n@@ -2099,1 +2122,1 @@\n-  Assembler::fldcw(as_Address(src));\n+  fldcw(as_Address(src));\n@@ -2133,1 +2156,2 @@\n-void MacroAssembler::mulpd(XMMRegister dst, AddressLiteral src) {\n+void MacroAssembler::mulpd(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n@@ -2137,2 +2161,2 @@\n-    lea(rscratch1, src);\n-    Assembler::mulpd(dst, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    Assembler::mulpd(dst, Address(rscratch, 0));\n@@ -2238,1 +2262,3 @@\n-void MacroAssembler::incrementl(AddressLiteral dst) {\n+void MacroAssembler::incrementl(AddressLiteral dst, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(dst), \"missing\");\n+\n@@ -2242,2 +2268,2 @@\n-    lea(rscratch1, dst);\n-    incrementl(Address(rscratch1, 0));\n+    lea(rscratch, dst);\n+    incrementl(Address(rscratch, 0));\n@@ -2247,2 +2273,2 @@\n-void MacroAssembler::incrementl(ArrayAddress dst) {\n-  incrementl(as_Address(dst));\n+void MacroAssembler::incrementl(ArrayAddress dst, Register rscratch) {\n+  incrementl(as_Address(dst, rscratch));\n@@ -2267,1 +2293,3 @@\n-void MacroAssembler::jump(AddressLiteral dst) {\n+void MacroAssembler::jump(AddressLiteral dst, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(dst), \"missing\");\n+\n@@ -2271,2 +2299,2 @@\n-    lea(rscratch1, dst);\n-    jmp(rscratch1);\n+    lea(rscratch, dst);\n+    jmp(rscratch);\n@@ -2276,1 +2304,3 @@\n-void MacroAssembler::jump_cc(Condition cc, AddressLiteral dst) {\n+void MacroAssembler::jump_cc(Condition cc, AddressLiteral dst, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(dst), \"missing\");\n+\n@@ -2299,2 +2329,2 @@\n-    lea(rscratch1, dst);\n-    Assembler::jmp(rscratch1);\n+    lea(rscratch, dst);\n+    Assembler::jmp(rscratch);\n@@ -2305,3 +2335,2 @@\n-void MacroAssembler::fld_x(AddressLiteral src) {\n-  Assembler::fld_x(as_Address(src));\n-}\n+void MacroAssembler::ldmxcsr(AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n@@ -2309,1 +2338,0 @@\n-void MacroAssembler::ldmxcsr(AddressLiteral src, Register scratchReg) {\n@@ -2313,2 +2341,2 @@\n-    lea(scratchReg, src);\n-    Assembler::ldmxcsr(Address(scratchReg, 0));\n+    lea(rscratch, src);\n+    Assembler::ldmxcsr(Address(rscratch, 0));\n@@ -2418,1 +2446,3 @@\n-void MacroAssembler::mov32(AddressLiteral dst, Register src) {\n+void MacroAssembler::mov32(AddressLiteral dst, Register src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(dst), \"missing\");\n+\n@@ -2422,2 +2452,2 @@\n-    lea(rscratch1, dst);\n-    movl(Address(rscratch1, 0), src);\n+    lea(rscratch, dst);\n+    movl(Address(rscratch, 0), src);\n@@ -2431,2 +2461,2 @@\n-    lea(rscratch1, src);\n-    movl(dst, Address(rscratch1, 0));\n+    lea(dst, src);\n+    movl(dst, Address(dst, 0));\n@@ -2474,3 +2504,2 @@\n-void MacroAssembler::movbyte(ArrayAddress dst, int src) {\n-  movb(as_Address(dst), src);\n-}\n+void MacroAssembler::movdl(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n@@ -2478,1 +2507,0 @@\n-void MacroAssembler::movdl(XMMRegister dst, AddressLiteral src) {\n@@ -2482,2 +2510,2 @@\n-    lea(rscratch1, src);\n-    movdl(dst, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    movdl(dst, Address(rscratch, 0));\n@@ -2487,1 +2515,3 @@\n-void MacroAssembler::movq(XMMRegister dst, AddressLiteral src) {\n+void MacroAssembler::movq(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -2491,2 +2521,2 @@\n-    lea(rscratch1, src);\n-    movq(dst, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    movq(dst, Address(rscratch, 0));\n@@ -2496,1 +2526,3 @@\n-void MacroAssembler::movdbl(XMMRegister dst, AddressLiteral src) {\n+void MacroAssembler::movdbl(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -2504,1 +2536,1 @@\n-    lea(rscratch1, src);\n+    lea(rscratch, src);\n@@ -2506,1 +2538,1 @@\n-      movsd (dst, Address(rscratch1, 0));\n+      movsd (dst, Address(rscratch, 0));\n@@ -2508,1 +2540,1 @@\n-      movlpd(dst, Address(rscratch1, 0));\n+      movlpd(dst, Address(rscratch, 0));\n@@ -2513,1 +2545,3 @@\n-void MacroAssembler::movflt(XMMRegister dst, AddressLiteral src) {\n+void MacroAssembler::movflt(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -2517,2 +2551,2 @@\n-    lea(rscratch1, src);\n-    movss(dst, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    movss(dst, Address(rscratch, 0));\n@@ -2539,0 +2573,4 @@\n+void MacroAssembler::movptr(Address dst, int32_t src) {\n+  LP64_ONLY(movslq(dst, src)) NOT_LP64(movl(dst, src));\n+}\n+\n@@ -2540,2 +2578,2 @@\n-    assert(((src->encoding() < 16) || VM_Version::supports_avx512vl()),\"XMM register should be 0-15\");\n-    Assembler::movdqu(dst, src);\n+  assert(((src->encoding() < 16) || VM_Version::supports_avx512vl()),\"XMM register should be 0-15\");\n+  Assembler::movdqu(dst, src);\n@@ -2545,2 +2583,2 @@\n-    assert(((dst->encoding() < 16) || VM_Version::supports_avx512vl()),\"XMM register should be 0-15\");\n-    Assembler::movdqu(dst, src);\n+  assert(((dst->encoding() < 16) || VM_Version::supports_avx512vl()),\"XMM register should be 0-15\");\n+  Assembler::movdqu(dst, src);\n@@ -2550,2 +2588,2 @@\n-    assert(((dst->encoding() < 16  && src->encoding() < 16) || VM_Version::supports_avx512vl()),\"XMM register should be 0-15\");\n-    Assembler::movdqu(dst, src);\n+  assert(((dst->encoding() < 16  && src->encoding() < 16) || VM_Version::supports_avx512vl()),\"XMM register should be 0-15\");\n+  Assembler::movdqu(dst, src);\n@@ -2554,1 +2592,3 @@\n-void MacroAssembler::movdqu(XMMRegister dst, AddressLiteral src, Register scratchReg) {\n+void MacroAssembler::movdqu(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -2558,2 +2598,2 @@\n-    lea(scratchReg, src);\n-    movdqu(dst, Address(scratchReg, 0));\n+    lea(rscratch, src);\n+    movdqu(dst, Address(rscratch, 0));\n@@ -2564,2 +2604,2 @@\n-    assert(((src->encoding() < 16) || VM_Version::supports_avx512vl()),\"XMM register should be 0-15\");\n-    Assembler::vmovdqu(dst, src);\n+  assert(((src->encoding() < 16) || VM_Version::supports_avx512vl()),\"XMM register should be 0-15\");\n+  Assembler::vmovdqu(dst, src);\n@@ -2569,2 +2609,2 @@\n-    assert(((dst->encoding() < 16) || VM_Version::supports_avx512vl()),\"XMM register should be 0-15\");\n-    Assembler::vmovdqu(dst, src);\n+  assert(((dst->encoding() < 16) || VM_Version::supports_avx512vl()),\"XMM register should be 0-15\");\n+  Assembler::vmovdqu(dst, src);\n@@ -2574,2 +2614,2 @@\n-    assert(((dst->encoding() < 16  && src->encoding() < 16) || VM_Version::supports_avx512vl()),\"XMM register should be 0-15\");\n-    Assembler::vmovdqu(dst, src);\n+  assert(((dst->encoding() < 16  && src->encoding() < 16) || VM_Version::supports_avx512vl()),\"XMM register should be 0-15\");\n+  Assembler::vmovdqu(dst, src);\n@@ -2578,1 +2618,3 @@\n-void MacroAssembler::vmovdqu(XMMRegister dst, AddressLiteral src, Register scratch_reg) {\n+void MacroAssembler::vmovdqu(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -2583,2 +2625,2 @@\n-    lea(scratch_reg, src);\n-    vmovdqu(dst, Address(scratch_reg, 0));\n+    lea(rscratch, src);\n+    vmovdqu(dst, Address(rscratch, 0));\n@@ -2588,1 +2630,3 @@\n-void MacroAssembler::vmovdqu(XMMRegister dst, AddressLiteral src, Register scratch_reg, int vector_len) {\n+void MacroAssembler::vmovdqu(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -2590,1 +2634,1 @@\n-    evmovdquq(dst, src, AVX_512bit, scratch_reg);\n+    evmovdquq(dst, src, AVX_512bit, rscratch);\n@@ -2592,1 +2636,1 @@\n-    vmovdqu(dst, src, scratch_reg);\n+    vmovdqu(dst, src, rscratch);\n@@ -2594,1 +2638,1 @@\n-    movdqu(dst, src, scratch_reg);\n+    movdqu(dst, src, rscratch);\n@@ -2643,1 +2687,3 @@\n-void MacroAssembler::kmovql(KRegister dst, AddressLiteral src, Register scratch_reg) {\n+void MacroAssembler::kmovql(KRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -2647,2 +2693,2 @@\n-    lea(scratch_reg, src);\n-    kmovql(dst, Address(scratch_reg, 0));\n+    lea(rscratch, src);\n+    kmovql(dst, Address(rscratch, 0));\n@@ -2652,1 +2698,3 @@\n-void MacroAssembler::kmovwl(KRegister dst, AddressLiteral src, Register scratch_reg) {\n+void MacroAssembler::kmovwl(KRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -2656,2 +2704,2 @@\n-    lea(scratch_reg, src);\n-    kmovwl(dst, Address(scratch_reg, 0));\n+    lea(rscratch, src);\n+    kmovwl(dst, Address(rscratch, 0));\n@@ -2662,1 +2710,3 @@\n-                               int vector_len, Register scratch_reg) {\n+                               int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -2666,2 +2716,2 @@\n-    lea(scratch_reg, src);\n-    Assembler::evmovdqub(dst, mask, Address(scratch_reg, 0), merge, vector_len);\n+    lea(rscratch, src);\n+    Assembler::evmovdqub(dst, mask, Address(rscratch, 0), merge, vector_len);\n@@ -2672,1 +2722,3 @@\n-                               int vector_len, Register scratch_reg) {\n+                               int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -2676,2 +2728,2 @@\n-    lea(scratch_reg, src);\n-    Assembler::evmovdquw(dst, mask, Address(scratch_reg, 0), merge, vector_len);\n+    lea(rscratch, src);\n+    Assembler::evmovdquw(dst, mask, Address(rscratch, 0), merge, vector_len);\n@@ -2681,2 +2733,3 @@\n-void MacroAssembler::evmovdqul(XMMRegister dst, KRegister mask, AddressLiteral src, bool merge,\n-                               int vector_len, Register scratch_reg) {\n+void MacroAssembler::evmovdqul(XMMRegister dst, KRegister mask, AddressLiteral src, bool merge, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -2686,2 +2739,2 @@\n-    lea(scratch_reg, src);\n-    Assembler::evmovdqul(dst, mask, Address(scratch_reg, 0), merge, vector_len);\n+    lea(rscratch, src);\n+    Assembler::evmovdqul(dst, mask, Address(rscratch, 0), merge, vector_len);\n@@ -2691,2 +2744,3 @@\n-void MacroAssembler::evmovdquq(XMMRegister dst, KRegister mask, AddressLiteral src, bool merge,\n-                               int vector_len, Register scratch_reg) {\n+void MacroAssembler::evmovdquq(XMMRegister dst, KRegister mask, AddressLiteral src, bool merge, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -2696,2 +2750,2 @@\n-    lea(scratch_reg, src);\n-    Assembler::evmovdquq(dst, mask, Address(scratch_reg, 0), merge, vector_len);\n+    lea(rscratch, src);\n+    Assembler::evmovdquq(dst, mask, Address(rscratch, 0), merge, vector_len);\n@@ -2702,0 +2756,2 @@\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -2710,1 +2766,3 @@\n-void MacroAssembler::movdqa(XMMRegister dst, AddressLiteral src) {\n+void MacroAssembler::movdqa(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -2714,2 +2772,2 @@\n-    lea(rscratch1, src);\n-    Assembler::movdqa(dst, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    Assembler::movdqa(dst, Address(rscratch, 0));\n@@ -2719,1 +2777,3 @@\n-void MacroAssembler::movsd(XMMRegister dst, AddressLiteral src) {\n+void MacroAssembler::movsd(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -2723,2 +2783,2 @@\n-    lea(rscratch1, src);\n-    Assembler::movsd(dst, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    Assembler::movsd(dst, Address(rscratch, 0));\n@@ -2728,1 +2788,3 @@\n-void MacroAssembler::movss(XMMRegister dst, AddressLiteral src) {\n+void MacroAssembler::movss(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -2732,2 +2794,13 @@\n-    lea(rscratch1, src);\n-    Assembler::movss(dst, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    Assembler::movss(dst, Address(rscratch, 0));\n+  }\n+}\n+\n+void MacroAssembler::movddup(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n+  if (reachable(src)) {\n+    Assembler::movddup(dst, as_Address(src));\n+  } else {\n+    lea(rscratch, src);\n+    Assembler::movddup(dst, Address(rscratch, 0));\n@@ -2738,0 +2811,2 @@\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -2746,1 +2821,3 @@\n-void MacroAssembler::mulsd(XMMRegister dst, AddressLiteral src) {\n+void MacroAssembler::mulsd(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -2750,2 +2827,2 @@\n-    lea(rscratch1, src);\n-    Assembler::mulsd(dst, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    Assembler::mulsd(dst, Address(rscratch, 0));\n@@ -2755,1 +2832,3 @@\n-void MacroAssembler::mulss(XMMRegister dst, AddressLiteral src) {\n+void MacroAssembler::mulss(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -2759,2 +2838,2 @@\n-    lea(rscratch1, src);\n-    Assembler::mulss(dst, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    Assembler::mulss(dst, Address(rscratch, 0));\n@@ -3000,1 +3079,2 @@\n-                                         address  last_java_pc) {\n+                                         address  last_java_pc,\n+                                         Register rscratch) {\n@@ -3011,2 +3091,0 @@\n-\n-\n@@ -3017,6 +3095,3 @@\n-\n-\n-    lea(Address(java_thread,\n-                 JavaThread::frame_anchor_offset() + JavaFrameAnchor::last_Java_pc_offset()),\n-        InternalAddress(last_java_pc));\n-\n+    Address java_pc(java_thread,\n+                    JavaThread::frame_anchor_offset() + JavaFrameAnchor::last_Java_pc_offset());\n+    lea(java_pc, InternalAddress(last_java_pc), rscratch);\n@@ -3055,0 +3130,16 @@\n+void MacroAssembler::testl(Address dst, int32_t imm32) {\n+  if (imm32 >= 0 && is8bit(imm32)) {\n+    testb(dst, imm32);\n+  } else {\n+    Assembler::testl(dst, imm32);\n+  }\n+}\n+\n+void MacroAssembler::testl(Register dst, int32_t imm32) {\n+  if (imm32 >= 0 && is8bit(imm32) && dst->has_byte_register()) {\n+    testb(dst, imm32);\n+  } else {\n+    Assembler::testl(dst, imm32);\n+  }\n+}\n+\n@@ -3056,1 +3147,1 @@\n-  assert(reachable(src), \"Address should be reachable\");\n+  assert(always_reachable(src), \"Address should be reachable\");\n@@ -3060,0 +3151,20 @@\n+#ifdef _LP64\n+\n+void MacroAssembler::testq(Address dst, int32_t imm32) {\n+  if (imm32 >= 0) {\n+    testl(dst, imm32);\n+  } else {\n+    Assembler::testq(dst, imm32);\n+  }\n+}\n+\n+void MacroAssembler::testq(Register dst, int32_t imm32) {\n+  if (imm32 >= 0) {\n+    testl(dst, imm32);\n+  } else {\n+    Assembler::testq(dst, imm32);\n+  }\n+}\n+\n+#endif\n+\n@@ -3100,8 +3211,2 @@\n-void MacroAssembler::sqrtsd(XMMRegister dst, AddressLiteral src) {\n-  if (reachable(src)) {\n-    Assembler::sqrtsd(dst, as_Address(src));\n-  } else {\n-    lea(rscratch1, src);\n-    Assembler::sqrtsd(dst, Address(rscratch1, 0));\n-  }\n-}\n+void MacroAssembler::sqrtss(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n@@ -3109,1 +3214,0 @@\n-void MacroAssembler::sqrtss(XMMRegister dst, AddressLiteral src) {\n@@ -3113,2 +3217,2 @@\n-    lea(rscratch1, src);\n-    Assembler::sqrtss(dst, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    Assembler::sqrtss(dst, Address(rscratch, 0));\n@@ -3118,1 +3222,3 @@\n-void MacroAssembler::subsd(XMMRegister dst, AddressLiteral src) {\n+void MacroAssembler::subsd(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3122,2 +3228,2 @@\n-    lea(rscratch1, src);\n-    Assembler::subsd(dst, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    Assembler::subsd(dst, Address(rscratch, 0));\n@@ -3127,1 +3233,3 @@\n-void MacroAssembler::roundsd(XMMRegister dst, AddressLiteral src, int32_t rmode, Register scratch_reg) {\n+void MacroAssembler::roundsd(XMMRegister dst, AddressLiteral src, int32_t rmode, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3131,2 +3239,2 @@\n-    lea(scratch_reg, src);\n-    Assembler::roundsd(dst, Address(scratch_reg, 0), rmode);\n+    lea(rscratch, src);\n+    Assembler::roundsd(dst, Address(rscratch, 0), rmode);\n@@ -3136,1 +3244,3 @@\n-void MacroAssembler::subss(XMMRegister dst, AddressLiteral src) {\n+void MacroAssembler::subss(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3140,2 +3250,2 @@\n-    lea(rscratch1, src);\n-    Assembler::subss(dst, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    Assembler::subss(dst, Address(rscratch, 0));\n@@ -3145,1 +3255,3 @@\n-void MacroAssembler::ucomisd(XMMRegister dst, AddressLiteral src) {\n+void MacroAssembler::ucomisd(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3149,2 +3261,2 @@\n-    lea(rscratch1, src);\n-    Assembler::ucomisd(dst, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    Assembler::ucomisd(dst, Address(rscratch, 0));\n@@ -3154,1 +3266,3 @@\n-void MacroAssembler::ucomiss(XMMRegister dst, AddressLiteral src) {\n+void MacroAssembler::ucomiss(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3158,2 +3272,2 @@\n-    lea(rscratch1, src);\n-    Assembler::ucomiss(dst, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    Assembler::ucomiss(dst, Address(rscratch, 0));\n@@ -3163,1 +3277,3 @@\n-void MacroAssembler::xorpd(XMMRegister dst, AddressLiteral src, Register scratch_reg) {\n+void MacroAssembler::xorpd(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3169,2 +3285,2 @@\n-    lea(scratch_reg, src);\n-    Assembler::xorpd(dst, Address(scratch_reg, 0));\n+    lea(rscratch, src);\n+    Assembler::xorpd(dst, Address(rscratch, 0));\n@@ -3191,1 +3307,3 @@\n-void MacroAssembler::xorps(XMMRegister dst, AddressLiteral src, Register scratch_reg) {\n+void MacroAssembler::xorps(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3197,2 +3315,2 @@\n-    lea(scratch_reg, src);\n-    Assembler::xorps(dst, Address(scratch_reg, 0));\n+    lea(rscratch, src);\n+    Assembler::xorps(dst, Address(rscratch, 0));\n@@ -3202,1 +3320,3 @@\n-void MacroAssembler::pshufb(XMMRegister dst, AddressLiteral src) {\n+void MacroAssembler::pshufb(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3209,2 +3329,2 @@\n-    lea(rscratch1, src);\n-    Assembler::pshufb(dst, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    Assembler::pshufb(dst, Address(rscratch, 0));\n@@ -3216,1 +3336,3 @@\n-void MacroAssembler::vaddsd(XMMRegister dst, XMMRegister nds, AddressLiteral src) {\n+void MacroAssembler::vaddsd(XMMRegister dst, XMMRegister nds, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3220,2 +3342,2 @@\n-    lea(rscratch1, src);\n-    vaddsd(dst, nds, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    vaddsd(dst, nds, Address(rscratch, 0));\n@@ -3225,1 +3347,3 @@\n-void MacroAssembler::vaddss(XMMRegister dst, XMMRegister nds, AddressLiteral src) {\n+void MacroAssembler::vaddss(XMMRegister dst, XMMRegister nds, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3229,2 +3353,2 @@\n-    lea(rscratch1, src);\n-    vaddss(dst, nds, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    vaddss(dst, nds, Address(rscratch, 0));\n@@ -3236,0 +3360,2 @@\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3246,0 +3372,2 @@\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3254,1 +3382,1 @@\n-void MacroAssembler::vabsss(XMMRegister dst, XMMRegister nds, XMMRegister src, AddressLiteral negate_field, int vector_len) {\n+void MacroAssembler::vabsss(XMMRegister dst, XMMRegister nds, XMMRegister src, AddressLiteral negate_field, int vector_len, Register rscratch) {\n@@ -3256,1 +3384,3 @@\n-  vandps(dst, nds, negate_field, vector_len);\n+  assert(rscratch != noreg || always_reachable(negate_field), \"missing\");\n+\n+  vandps(dst, nds, negate_field, vector_len, rscratch);\n@@ -3259,1 +3389,1 @@\n-void MacroAssembler::vabssd(XMMRegister dst, XMMRegister nds, XMMRegister src, AddressLiteral negate_field, int vector_len) {\n+void MacroAssembler::vabssd(XMMRegister dst, XMMRegister nds, XMMRegister src, AddressLiteral negate_field, int vector_len, Register rscratch) {\n@@ -3261,1 +3391,3 @@\n-  vandpd(dst, nds, negate_field, vector_len);\n+  assert(rscratch != noreg || always_reachable(negate_field), \"missing\");\n+\n+  vandpd(dst, nds, negate_field, vector_len, rscratch);\n@@ -3284,1 +3416,3 @@\n-void MacroAssembler::vpand(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg) {\n+void MacroAssembler::vpand(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3288,2 +3422,2 @@\n-    lea(scratch_reg, src);\n-    Assembler::vpand(dst, nds, Address(scratch_reg, 0), vector_len);\n+    lea(rscratch, src);\n+    Assembler::vpand(dst, nds, Address(rscratch, 0), vector_len);\n@@ -3293,3 +3427,9 @@\n-void MacroAssembler::vpbroadcastw(XMMRegister dst, XMMRegister src, int vector_len) {\n-  assert(((dst->encoding() < 16 && src->encoding() < 16) || VM_Version::supports_avx512vlbw()),\"XMM register should be 0-15\");\n-  Assembler::vpbroadcastw(dst, src, vector_len);\n+void MacroAssembler::vpbroadcastd(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n+  if (reachable(src)) {\n+    Assembler::vpbroadcastd(dst, as_Address(src), vector_len);\n+  } else {\n+    lea(rscratch, src);\n+    Assembler::vpbroadcastd(dst, Address(rscratch, 0), vector_len);\n+  }\n@@ -3299,0 +3439,2 @@\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3308,0 +3450,2 @@\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3316,0 +3460,11 @@\n+void MacroAssembler::vbroadcastss(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n+  if (reachable(src)) {\n+    Assembler::vbroadcastss(dst, as_Address(src), vector_len);\n+  } else {\n+    lea(rscratch, src);\n+    Assembler::vbroadcastss(dst, Address(rscratch, 0), vector_len);\n+  }\n+}\n+\n@@ -3326,2 +3481,3 @@\n-void MacroAssembler::evpcmpeqd(KRegister kdst, KRegister mask, XMMRegister nds,\n-                               AddressLiteral src, int vector_len, Register scratch_reg) {\n+void MacroAssembler::evpcmpeqd(KRegister kdst, KRegister mask, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3331,2 +3487,2 @@\n-    lea(scratch_reg, src);\n-    Assembler::evpcmpeqd(kdst, mask, nds, Address(scratch_reg, 0), vector_len);\n+    lea(rscratch, src);\n+    Assembler::evpcmpeqd(kdst, mask, nds, Address(rscratch, 0), vector_len);\n@@ -3337,1 +3493,3 @@\n-                             int comparison, bool is_signed, int vector_len, Register scratch_reg) {\n+                             int comparison, bool is_signed, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3341,2 +3499,2 @@\n-    lea(scratch_reg, src);\n-    Assembler::evpcmpd(kdst, mask, nds, Address(scratch_reg, 0), comparison, is_signed, vector_len);\n+    lea(rscratch, src);\n+    Assembler::evpcmpd(kdst, mask, nds, Address(rscratch, 0), comparison, is_signed, vector_len);\n@@ -3347,1 +3505,3 @@\n-                             int comparison, bool is_signed, int vector_len, Register scratch_reg) {\n+                             int comparison, bool is_signed, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3351,2 +3511,2 @@\n-    lea(scratch_reg, src);\n-    Assembler::evpcmpq(kdst, mask, nds, Address(scratch_reg, 0), comparison, is_signed, vector_len);\n+    lea(rscratch, src);\n+    Assembler::evpcmpq(kdst, mask, nds, Address(rscratch, 0), comparison, is_signed, vector_len);\n@@ -3357,1 +3517,3 @@\n-                             int comparison, bool is_signed, int vector_len, Register scratch_reg) {\n+                             int comparison, bool is_signed, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3361,2 +3523,2 @@\n-    lea(scratch_reg, src);\n-    Assembler::evpcmpb(kdst, mask, nds, Address(scratch_reg, 0), comparison, is_signed, vector_len);\n+    lea(rscratch, src);\n+    Assembler::evpcmpb(kdst, mask, nds, Address(rscratch, 0), comparison, is_signed, vector_len);\n@@ -3367,1 +3529,3 @@\n-                             int comparison, bool is_signed, int vector_len, Register scratch_reg) {\n+                             int comparison, bool is_signed, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3371,2 +3535,2 @@\n-    lea(scratch_reg, src);\n-    Assembler::evpcmpw(kdst, mask, nds, Address(scratch_reg, 0), comparison, is_signed, vector_len);\n+    lea(rscratch, src);\n+    Assembler::evpcmpw(kdst, mask, nds, Address(rscratch, 0), comparison, is_signed, vector_len);\n@@ -3441,1 +3605,1 @@\n-void MacroAssembler::vpmulld(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg) {\n+void MacroAssembler::vpmulld(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch) {\n@@ -3443,0 +3607,2 @@\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3446,2 +3612,2 @@\n-    lea(scratch_reg, src);\n-    Assembler::vpmulld(dst, nds, Address(scratch_reg, 0), vector_len);\n+    lea(rscratch, src);\n+    Assembler::vpmulld(dst, nds, Address(rscratch, 0), vector_len);\n@@ -3537,1 +3703,3 @@\n-void MacroAssembler::vandpd(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg) {\n+void MacroAssembler::vandpd(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3541,2 +3709,2 @@\n-    lea(scratch_reg, src);\n-    vandpd(dst, nds, Address(scratch_reg, 0), vector_len);\n+    lea(rscratch, src);\n+    vandpd(dst, nds, Address(rscratch, 0), vector_len);\n@@ -3546,1 +3714,3 @@\n-void MacroAssembler::vandps(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg) {\n+void MacroAssembler::vandps(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3550,2 +3720,2 @@\n-    lea(scratch_reg, src);\n-    vandps(dst, nds, Address(scratch_reg, 0), vector_len);\n+    lea(rscratch, src);\n+    vandps(dst, nds, Address(rscratch, 0), vector_len);\n@@ -3556,1 +3726,3 @@\n-                            bool merge, int vector_len, Register scratch_reg) {\n+                            bool merge, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3560,2 +3732,2 @@\n-    lea(scratch_reg, src);\n-    Assembler::evpord(dst, mask, nds, Address(scratch_reg, 0), merge, vector_len);\n+    lea(rscratch, src);\n+    Assembler::evpord(dst, mask, nds, Address(rscratch, 0), merge, vector_len);\n@@ -3565,1 +3737,3 @@\n-void MacroAssembler::vdivsd(XMMRegister dst, XMMRegister nds, AddressLiteral src) {\n+void MacroAssembler::vdivsd(XMMRegister dst, XMMRegister nds, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3569,2 +3743,2 @@\n-    lea(rscratch1, src);\n-    vdivsd(dst, nds, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    vdivsd(dst, nds, Address(rscratch, 0));\n@@ -3574,1 +3748,3 @@\n-void MacroAssembler::vdivss(XMMRegister dst, XMMRegister nds, AddressLiteral src) {\n+void MacroAssembler::vdivss(XMMRegister dst, XMMRegister nds, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3578,2 +3754,2 @@\n-    lea(rscratch1, src);\n-    vdivss(dst, nds, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    vdivss(dst, nds, Address(rscratch, 0));\n@@ -3583,1 +3759,3 @@\n-void MacroAssembler::vmulsd(XMMRegister dst, XMMRegister nds, AddressLiteral src) {\n+void MacroAssembler::vmulsd(XMMRegister dst, XMMRegister nds, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3587,2 +3765,2 @@\n-    lea(rscratch1, src);\n-    vmulsd(dst, nds, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    vmulsd(dst, nds, Address(rscratch, 0));\n@@ -3592,1 +3770,3 @@\n-void MacroAssembler::vmulss(XMMRegister dst, XMMRegister nds, AddressLiteral src) {\n+void MacroAssembler::vmulss(XMMRegister dst, XMMRegister nds, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3596,2 +3776,2 @@\n-    lea(rscratch1, src);\n-    vmulss(dst, nds, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    vmulss(dst, nds, Address(rscratch, 0));\n@@ -3601,1 +3781,3 @@\n-void MacroAssembler::vsubsd(XMMRegister dst, XMMRegister nds, AddressLiteral src) {\n+void MacroAssembler::vsubsd(XMMRegister dst, XMMRegister nds, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3605,2 +3787,2 @@\n-    lea(rscratch1, src);\n-    vsubsd(dst, nds, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    vsubsd(dst, nds, Address(rscratch, 0));\n@@ -3610,1 +3792,3 @@\n-void MacroAssembler::vsubss(XMMRegister dst, XMMRegister nds, AddressLiteral src) {\n+void MacroAssembler::vsubss(XMMRegister dst, XMMRegister nds, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3614,2 +3798,2 @@\n-    lea(rscratch1, src);\n-    vsubss(dst, nds, Address(rscratch1, 0));\n+    lea(rscratch, src);\n+    vsubss(dst, nds, Address(rscratch, 0));\n@@ -3619,1 +3803,1 @@\n-void MacroAssembler::vnegatess(XMMRegister dst, XMMRegister nds, AddressLiteral src) {\n+void MacroAssembler::vnegatess(XMMRegister dst, XMMRegister nds, AddressLiteral src, Register rscratch) {\n@@ -3621,1 +3805,3 @@\n-  vxorps(dst, nds, src, Assembler::AVX_128bit);\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n+  vxorps(dst, nds, src, Assembler::AVX_128bit, rscratch);\n@@ -3624,1 +3810,1 @@\n-void MacroAssembler::vnegatesd(XMMRegister dst, XMMRegister nds, AddressLiteral src) {\n+void MacroAssembler::vnegatesd(XMMRegister dst, XMMRegister nds, AddressLiteral src, Register rscratch) {\n@@ -3626,1 +3812,3 @@\n-  vxorpd(dst, nds, src, Assembler::AVX_128bit);\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n+  vxorpd(dst, nds, src, Assembler::AVX_128bit, rscratch);\n@@ -3629,1 +3817,3 @@\n-void MacroAssembler::vxorpd(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg) {\n+void MacroAssembler::vxorpd(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3633,2 +3823,2 @@\n-    lea(scratch_reg, src);\n-    vxorpd(dst, nds, Address(scratch_reg, 0), vector_len);\n+    lea(rscratch, src);\n+    vxorpd(dst, nds, Address(rscratch, 0), vector_len);\n@@ -3638,1 +3828,3 @@\n-void MacroAssembler::vxorps(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg) {\n+void MacroAssembler::vxorps(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3642,2 +3834,2 @@\n-    lea(scratch_reg, src);\n-    vxorps(dst, nds, Address(scratch_reg, 0), vector_len);\n+    lea(rscratch, src);\n+    vxorps(dst, nds, Address(rscratch, 0), vector_len);\n@@ -3647,1 +3839,3 @@\n-void MacroAssembler::vpxor(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg) {\n+void MacroAssembler::vpxor(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3652,2 +3846,2 @@\n-      lea(scratch_reg, src);\n-      Assembler::vpxor(dst, nds, Address(scratch_reg, 0), vector_len);\n+      lea(rscratch, src);\n+      Assembler::vpxor(dst, nds, Address(rscratch, 0), vector_len);\n@@ -3655,3 +3849,2 @@\n-  }\n-  else {\n-    MacroAssembler::vxorpd(dst, nds, src, vector_len, scratch_reg);\n+  } else {\n+    MacroAssembler::vxorpd(dst, nds, src, vector_len, rscratch);\n@@ -3661,1 +3854,3 @@\n-void MacroAssembler::vpermd(XMMRegister dst,  XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg) {\n+void MacroAssembler::vpermd(XMMRegister dst,  XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n@@ -3665,2 +3860,2 @@\n-    lea(scratch_reg, src);\n-    Assembler::vpermd(dst, nds, Address(scratch_reg, 0), vector_len);\n+    lea(rscratch, src);\n+    Assembler::vpermd(dst, nds, Address(rscratch, 0), vector_len);\n@@ -3755,1 +3950,1 @@\n-  int num_xmm_registers = XMMRegisterImpl::available_xmm_registers();\n+  int num_xmm_registers = XMMRegister::available_xmm_registers();\n@@ -3796,1 +3991,1 @@\n-  gp_area_size = align_up(gp_registers.size() * RegisterImpl::max_slots_per_register * VMRegImpl::stack_slot_size,\n+  gp_area_size = align_up(gp_registers.size() * Register::max_slots_per_register * VMRegImpl::stack_slot_size,\n@@ -3889,1 +4084,1 @@\n-    int register_push_size = set.size() * RegisterImpl::max_slots_per_register * VMRegImpl::stack_slot_size;\n+    int register_push_size = set.size() * Register::max_slots_per_register * VMRegImpl::stack_slot_size;\n@@ -3899,1 +4094,1 @@\n-    spill_offset += RegisterImpl::max_slots_per_register * VMRegImpl::stack_slot_size;\n+    spill_offset += Register::max_slots_per_register * VMRegImpl::stack_slot_size;\n@@ -3905,1 +4100,1 @@\n-  int gp_reg_size = RegisterImpl::max_slots_per_register * VMRegImpl::stack_slot_size;\n+  int gp_reg_size = Register::max_slots_per_register * VMRegImpl::stack_slot_size;\n@@ -4343,0 +4538,7 @@\n+  BLOCK_COMMENT(\"verify_oop {\");\n+#ifdef _LP64\n+  push(rscratch1);\n+#endif\n+  push(rax);                          \/\/ save rax\n+  push(reg);                          \/\/ pass register argument\n+\n@@ -4351,10 +4553,2 @@\n-  BLOCK_COMMENT(\"verify_oop {\");\n-#ifdef _LP64\n-  push(rscratch1);                    \/\/ save r10, trashed by movptr()\n-#endif\n-  push(rax);                          \/\/ save rax,\n-  push(reg);                          \/\/ pass register argument\n-  \/\/ avoid using pushptr, as it modifies scratch registers\n-  \/\/ and our contract is not to modify anything\n-  movptr(rax, buffer.addr());\n-  push(rax);\n+  pushptr(buffer.addr(), rscratch1);\n+\n@@ -4371,0 +4565,2 @@\n+    \/\/ Only pcmpeq has dependency breaking treatment (i.e the execution can begin without\n+    \/\/ waiting for the previous result on dst), not vpcmpeqd, so just use vpternlog\n@@ -4372,0 +4568,2 @@\n+  } else if (VM_Version::supports_avx()) {\n+    vpcmpeqd(dst, dst, dst, vector_len);\n@@ -4373,2 +4571,2 @@\n-    assert(UseAVX > 0, \"\");\n-    vpcmpeqb(dst, dst, dst, vector_len);\n+    assert(VM_Version::supports_sse2(), \"\");\n+    pcmpeqd(dst, dst);\n@@ -4402,10 +4600,1 @@\n-  \/\/ Address adjust(addr.base(), addr.index(), addr.scale(), addr.disp() + BytesPerWord);\n-  \/\/ Pass register number to verify_oop_subroutine\n-  const char* b = NULL;\n-  {\n-    ResourceMark rm;\n-    stringStream ss;\n-    ss.print(\"verify_oop_addr: %s (%s:%d)\", s, file, line);\n-    b = code_string(ss.as_string());\n-  }\n-  push(rscratch1);                    \/\/ save r10, trashed by movptr()\n+  push(rscratch1);\n@@ -4414,1 +4603,1 @@\n-  push(rax);                          \/\/ save rax,\n+  push(rax); \/\/ save rax,\n@@ -4426,0 +4615,8 @@\n+  \/\/ Pass register number to verify_oop_subroutine\n+  const char* b = NULL;\n+  {\n+    ResourceMark rm;\n+    stringStream ss;\n+    ss.print(\"verify_oop_addr: %s (%s:%d)\", s, file, line);\n+    b = code_string(ss.as_string());\n+  }\n@@ -4427,5 +4624,1 @@\n-  \/\/ pass msg argument\n-  \/\/ avoid using pushptr, as it modifies scratch registers\n-  \/\/ and our contract is not to modify anything\n-  movptr(rax, buffer.addr());\n-  push(rax);\n+  pushptr(buffer.addr(), rscratch1);\n@@ -4808,1 +5001,1 @@\n-  pushptr(msg.addr());\n+  pushptr(msg.addr(), noreg);\n@@ -4823,1 +5016,1 @@\n-void MacroAssembler::restore_cpu_control_state_after_jni() {\n+void MacroAssembler::restore_cpu_control_state_after_jni(Register rscratch) {\n@@ -4828,1 +5021,1 @@\n-      ldmxcsr(ExternalAddress(StubRoutines::x86::addr_mxcsr_std()));\n+      ldmxcsr(ExternalAddress(StubRoutines::x86::addr_mxcsr_std()), rscratch);\n@@ -5009,2 +5202,1 @@\n-    const auto src2 = ExternalAddress((address)CompressedOops::ptrs_base_addr());\n-    assert(!src2.is_lval(), \"should not be lval\");\n+    ExternalAddress src2(CompressedOops::ptrs_base_addr());\n@@ -5015,1 +5207,1 @@\n-    cmpptr(r12_heapbase, src2);\n+    cmpptr(r12_heapbase, src2, rscratch1);\n@@ -5407,1 +5599,1 @@\n-      movptr(r12_heapbase, ExternalAddress((address)CompressedOops::ptrs_base_addr()));\n+      movptr(r12_heapbase, ExternalAddress(CompressedOops::ptrs_base_addr()));\n@@ -7365,1 +7557,1 @@\n-  movdqu(xmm0, ExternalAddress(StubRoutines::x86::crc_by128_masks_addr() + 32));\n+  movdqu(xmm0, ExternalAddress(StubRoutines::x86::crc_by128_masks_addr() + 32), rscratch1);\n@@ -7379,1 +7571,1 @@\n-  movdqu(xmm0, ExternalAddress(StubRoutines::x86::crc_by128_masks_addr() + 16));\n+  movdqu(xmm0, ExternalAddress(StubRoutines::x86::crc_by128_masks_addr() + 16), rscratch1);\n@@ -7388,1 +7580,1 @@\n-  movdqu(xmm0, ExternalAddress(StubRoutines::x86::crc_by128_masks_addr() + 16));\n+  movdqu(xmm0, ExternalAddress(StubRoutines::x86::crc_by128_masks_addr() + 16), rscratch1);\n@@ -7398,1 +7590,1 @@\n-  movdqu(xmm0, ExternalAddress(StubRoutines::x86::crc_by128_masks_addr()));\n+  movdqu(xmm0, ExternalAddress(StubRoutines::x86::crc_by128_masks_addr()), rscratch1);\n@@ -9498,1 +9690,1 @@\n-    MacroAssembler* masm, const bool* flag_addr, bool value) {\n+    MacroAssembler* masm, const bool* flag_addr, bool value, Register rscratch) {\n@@ -9500,1 +9692,1 @@\n-  _masm->cmp8(ExternalAddress((address)flag_addr), value);\n+  _masm->cmp8(ExternalAddress((address)flag_addr), value, rscratch);\n@@ -9548,0 +9740,16 @@\n+\n+void MacroAssembler::check_stack_alignment(Register sp, const char* msg, unsigned bias, Register tmp) {\n+  Label L_stack_ok;\n+  if (bias == 0) {\n+    testptr(sp, 2 * wordSize - 1);\n+  } else {\n+    \/\/ lea(tmp, Address(rsp, bias);\n+    mov(tmp, sp);\n+    addptr(tmp, bias);\n+    testptr(tmp, 2 * wordSize - 1);\n+  }\n+  jcc(Assembler::equal, L_stack_ok);\n+  block_comment(msg);\n+  stop(msg);\n+  bind(L_stack_ok);\n+}\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":656,"deletions":448,"binary":false,"changes":1104,"status":"modified"},{"patch":"@@ -115,1 +115,1 @@\n-  Address as_Address(ArrayAddress adr);\n+  Address as_Address(ArrayAddress adr, Register rscratch);\n@@ -195,1 +195,1 @@\n-  void movflt(XMMRegister dst, AddressLiteral src);\n+  void movflt(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -207,1 +207,1 @@\n-  void movdbl(XMMRegister dst, AddressLiteral src);\n+  void movdbl(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -215,2 +215,2 @@\n-  void incrementl(AddressLiteral dst);\n-  void incrementl(ArrayAddress dst);\n+  void incrementl(AddressLiteral dst, Register rscratch = noreg);\n+  void incrementl(ArrayAddress   dst, Register rscratch);\n@@ -218,1 +218,1 @@\n-  void incrementq(AddressLiteral dst);\n+  void incrementq(AddressLiteral dst, Register rscratch = noreg);\n@@ -337,1 +337,2 @@\n-                           address last_java_pc);\n+                           address  last_java_pc,\n+                           Register rscratch);\n@@ -342,1 +343,2 @@\n-                           address last_java_pc);\n+                           address  last_java_pc,\n+                           Register rscratch);\n@@ -363,1 +365,1 @@\n-  void resolve_oop_handle(Register result, Register tmp = rscratch2);\n+  void resolve_oop_handle(Register result, Register tmp);\n@@ -365,1 +367,1 @@\n-  void load_mirror(Register mirror, Register method, Register tmp = rscratch2);\n+  void load_mirror(Register mirror, Register method, Register tmp);\n@@ -688,1 +690,1 @@\n-  void restore_cpu_control_state_after_jni();\n+  void restore_cpu_control_state_after_jni(Register rscratch);\n@@ -749,2 +751,2 @@\n-    if (src.is_constant()) addptr(dst, (int) src.as_constant());\n-    else                   addptr(dst,       src.as_register());\n+    if (src.is_constant()) addptr(dst, src.as_constant());\n+    else                   addptr(dst, src.as_register());\n@@ -756,1 +758,1 @@\n-  void cmp8(AddressLiteral src1, int imm);\n+  void cmp8(AddressLiteral src1, int imm, Register rscratch = noreg);\n@@ -761,1 +763,1 @@\n-  void cmp32(AddressLiteral src1, int32_t imm);\n+  void cmp32(AddressLiteral src1, int32_t imm, Register rscratch = noreg);\n@@ -763,1 +765,1 @@\n-  void cmp32(Register src1, AddressLiteral src2);\n+  void cmp32(Register src1, AddressLiteral src2, Register rscratch = noreg);\n@@ -775,1 +777,1 @@\n-  void cmpoop(Register dst, jobject obj);\n+  void cmpoop(Register dst, jobject obj, Register rscratch);\n@@ -778,1 +780,1 @@\n-  void cmpptr(Address src1, AddressLiteral src2);\n+  void cmpptr(Address src1, AddressLiteral src2, Register rscratch);\n@@ -780,1 +782,1 @@\n-  void cmpptr(Register src1, AddressLiteral src2);\n+  void cmpptr(Register src1, AddressLiteral src2, Register rscratch = noreg);\n@@ -790,1 +792,1 @@\n-  void cmp64(Register src1, AddressLiteral src);\n+  void cmp64(Register src1, AddressLiteral src, Register rscratch = noreg);\n@@ -794,2 +796,1 @@\n-  void locked_cmpxchgptr(Register reg, AddressLiteral adr);\n-\n+  void locked_cmpxchgptr(Register reg, AddressLiteral adr, Register rscratch = noreg);\n@@ -838,1 +839,1 @@\n-  void cond_inc32(Condition cond, AddressLiteral counter_addr);\n+  void cond_inc32(Condition cond, AddressLiteral counter_addr, Register rscratch = noreg);\n@@ -841,1 +842,1 @@\n-  void atomic_incl(AddressLiteral counter_addr, Register scr = rscratch1);\n+  void atomic_incl(AddressLiteral counter_addr, Register rscratch = noreg);\n@@ -844,1 +845,1 @@\n-  void atomic_incq(AddressLiteral counter_addr, Register scr = rscratch1);\n+  void atomic_incq(AddressLiteral counter_addr, Register rscratch = noreg);\n@@ -846,1 +847,1 @@\n-  void atomic_incptr(AddressLiteral counter_addr, Register scr = rscratch1) { LP64_ONLY(atomic_incq(counter_addr, scr)) NOT_LP64(atomic_incl(counter_addr, scr)) ; }\n+  void atomic_incptr(AddressLiteral counter_addr, Register rscratch = noreg) { LP64_ONLY(atomic_incq(counter_addr, rscratch)) NOT_LP64(atomic_incl(counter_addr, rscratch)) ; }\n@@ -849,0 +850,1 @@\n+  void lea(Register dst, Address        adr) { Assembler::lea(dst, adr); }\n@@ -850,2 +852,1 @@\n-  void lea(Address dst, AddressLiteral adr);\n-  void lea(Register dst, Address adr) { Assembler::lea(dst, adr); }\n+  void lea(Address  dst, AddressLiteral adr, Register rscratch);\n@@ -858,1 +859,6 @@\n-  void testl(Register dst, AddressLiteral src);\n+  void testl(Address dst, int32_t imm32);\n+  void testl(Register dst, int32_t imm32);\n+  void testl(Register dst, AddressLiteral src); \/\/ requires reachable address\n+  using Assembler::testq;\n+  void testq(Address dst, int32_t imm32);\n+  void testq(Register dst, int32_t imm32);\n@@ -881,1 +887,1 @@\n-  void call(AddressLiteral entry);\n+  void call(AddressLiteral entry, Register rscratch = rax);\n@@ -893,2 +899,3 @@\n-  void jump(AddressLiteral dst);\n-  void jump_cc(Condition cc, AddressLiteral dst);\n+  void jump(AddressLiteral dst, Register rscratch = noreg);\n+\n+  void jump_cc(Condition cc, AddressLiteral dst, Register rscratch = noreg);\n@@ -899,1 +906,1 @@\n-  void jump(ArrayAddress entry);\n+  void jump(ArrayAddress entry, Register rscratch);\n@@ -908,3 +915,3 @@\n-  void andpd(XMMRegister dst, Address src) { Assembler::andpd(dst, src); }\n-  void andpd(XMMRegister dst, AddressLiteral src, Register scratch_reg = rscratch1);\n-  void andpd(XMMRegister dst, XMMRegister src) { Assembler::andpd(dst, src); }\n+  void andpd(XMMRegister dst, XMMRegister    src) { Assembler::andpd(dst, src); }\n+  void andpd(XMMRegister dst, Address        src) { Assembler::andpd(dst, src); }\n+  void andpd(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -912,3 +919,3 @@\n-  void andps(XMMRegister dst, XMMRegister src) { Assembler::andps(dst, src); }\n-  void andps(XMMRegister dst, Address src) { Assembler::andps(dst, src); }\n-  void andps(XMMRegister dst, AddressLiteral src, Register scratch_reg = rscratch1);\n+  void andps(XMMRegister dst, XMMRegister    src) { Assembler::andps(dst, src); }\n+  void andps(XMMRegister dst, Address        src) { Assembler::andps(dst, src); }\n+  void andps(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -916,3 +923,3 @@\n-  void comiss(XMMRegister dst, XMMRegister src) { Assembler::comiss(dst, src); }\n-  void comiss(XMMRegister dst, Address src) { Assembler::comiss(dst, src); }\n-  void comiss(XMMRegister dst, AddressLiteral src);\n+  void comiss(XMMRegister dst, XMMRegister    src) { Assembler::comiss(dst, src); }\n+  void comiss(XMMRegister dst, Address        src) { Assembler::comiss(dst, src); }\n+  void comiss(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -920,3 +927,3 @@\n-  void comisd(XMMRegister dst, XMMRegister src) { Assembler::comisd(dst, src); }\n-  void comisd(XMMRegister dst, Address src) { Assembler::comisd(dst, src); }\n-  void comisd(XMMRegister dst, AddressLiteral src);\n+  void comisd(XMMRegister dst, XMMRegister    src) { Assembler::comisd(dst, src); }\n+  void comisd(XMMRegister dst, Address        src) { Assembler::comisd(dst, src); }\n+  void comisd(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -925,1 +932,1 @@\n-  void fadd_s(Address src)        { Assembler::fadd_s(src); }\n+  void fadd_s(Address        src) { Assembler::fadd_s(src); }\n@@ -928,1 +935,1 @@\n-  void fldcw(Address src) { Assembler::fldcw(src); }\n+  void fldcw(Address        src) { Assembler::fldcw(src); }\n@@ -931,2 +938,2 @@\n-  void fld_s(int index)   { Assembler::fld_s(index); }\n-  void fld_s(Address src) { Assembler::fld_s(src); }\n+  void fld_s(int index)          { Assembler::fld_s(index); }\n+  void fld_s(Address        src) { Assembler::fld_s(src); }\n@@ -935,1 +942,1 @@\n-  void fld_d(Address src) { Assembler::fld_d(src); }\n+  void fld_d(Address        src) { Assembler::fld_d(src); }\n@@ -938,3 +945,2 @@\n-  void fmul_s(Address src)        { Assembler::fmul_s(src); }\n-  void fmul_s(AddressLiteral src) { Assembler::fmul_s(as_Address(src)); }\n-#endif \/\/ _LP64\n+  void fld_x(Address        src) { Assembler::fld_x(src); }\n+  void fld_x(AddressLiteral src) { Assembler::fld_x(as_Address(src)); }\n@@ -942,2 +948,3 @@\n-  void fld_x(Address src) { Assembler::fld_x(src); }\n-  void fld_x(AddressLiteral src);\n+  void fmul_s(Address        src) { Assembler::fmul_s(src); }\n+  void fmul_s(AddressLiteral src) { Assembler::fmul_s(as_Address(src)); }\n+#endif \/\/ !_LP64\n@@ -946,1 +953,1 @@\n-  void ldmxcsr(AddressLiteral src, Register scratchReg = rscratch1);\n+  void ldmxcsr(AddressLiteral src, Register rscratch = noreg);\n@@ -979,14 +986,0 @@\n-  void gfmul(XMMRegister tmp0, XMMRegister t);\n-  void schoolbookAAD(int i, Register subkeyH, XMMRegister data, XMMRegister tmp0,\n-                     XMMRegister tmp1, XMMRegister tmp2, XMMRegister tmp3);\n-  void generateHtbl_one_block(Register htbl);\n-  void generateHtbl_eight_blocks(Register htbl);\n- public:\n-  void sha256_AVX2(XMMRegister msg, XMMRegister state0, XMMRegister state1, XMMRegister msgtmp0,\n-                   XMMRegister msgtmp1, XMMRegister msgtmp2, XMMRegister msgtmp3, XMMRegister msgtmp4,\n-                   Register buf, Register state, Register ofs, Register limit, Register rsp,\n-                   bool multi_block, XMMRegister shuf_mask);\n-  void avx_ghash(Register state, Register htbl, Register data, Register blocks);\n-#endif\n-#ifdef _LP64\n- private:\n@@ -1003,0 +996,4 @@\n+  void sha256_AVX2(XMMRegister msg, XMMRegister state0, XMMRegister state1, XMMRegister msgtmp0,\n+                   XMMRegister msgtmp1, XMMRegister msgtmp2, XMMRegister msgtmp3, XMMRegister msgtmp4,\n+                   Register buf, Register state, Register ofs, Register limit, Register rsp,\n+                   bool multi_block, XMMRegister shuf_mask);\n@@ -1007,21 +1004,1 @@\n-private:\n-  void roundEnc(XMMRegister key, int rnum);\n-  void lastroundEnc(XMMRegister key, int rnum);\n-  void roundDec(XMMRegister key, int rnum);\n-  void lastroundDec(XMMRegister key, int rnum);\n-  void ev_load_key(XMMRegister xmmdst, Register key, int offset, XMMRegister xmm_shuf_mask);\n-  void gfmul_avx512(XMMRegister ghash, XMMRegister hkey);\n-  void generateHtbl_48_block_zmm(Register htbl, Register avx512_subkeyHtbl);\n-  void ghash16_encrypt16_parallel(Register key, Register subkeyHtbl, XMMRegister ctr_blockx,\n-                                  XMMRegister aad_hashx, Register in, Register out, Register data, Register pos, bool reduction,\n-                                  XMMRegister addmask, bool no_ghash_input, Register rounds, Register ghash_pos,\n-                                  bool final_reduction, int index, XMMRegister counter_inc_mask);\n-public:\n-  void aesecb_encrypt(Register source_addr, Register dest_addr, Register key, Register len);\n-  void aesecb_decrypt(Register source_addr, Register dest_addr, Register key, Register len);\n-  void aesctr_encrypt(Register src_addr, Register dest_addr, Register key, Register counter,\n-                      Register len_reg, Register used, Register used_addr, Register saved_encCounter_start);\n-  void aesgcm_encrypt(Register in, Register len, Register ct, Register out, Register key,\n-                      Register state, Register subkeyHtbl, Register avx512_subkeyHtbl, Register counter);\n-\n-#endif\n+#endif \/\/ _LP64\n@@ -1053,17 +1030,8 @@\n-#ifdef _LP64\n-  void fast_log(XMMRegister xmm0, XMMRegister xmm1, XMMRegister xmm2, XMMRegister xmm3,\n-                XMMRegister xmm4, XMMRegister xmm5, XMMRegister xmm6, XMMRegister xmm7,\n-                Register rax, Register rcx, Register rdx, Register tmp1, Register tmp2);\n-\n-  void fast_log10(XMMRegister xmm0, XMMRegister xmm1, XMMRegister xmm2, XMMRegister xmm3,\n-                  XMMRegister xmm4, XMMRegister xmm5, XMMRegister xmm6, XMMRegister xmm7,\n-                  Register rax, Register rcx, Register rdx, Register r11);\n-\n-  void fast_pow(XMMRegister xmm0, XMMRegister xmm1, XMMRegister xmm2, XMMRegister xmm3, XMMRegister xmm4,\n-                XMMRegister xmm5, XMMRegister xmm6, XMMRegister xmm7, Register rax, Register rcx,\n-                Register rdx, Register tmp1, Register tmp2, Register tmp3, Register tmp4);\n-\n-  void fast_sin(XMMRegister xmm0, XMMRegister xmm1, XMMRegister xmm2, XMMRegister xmm3,\n-                XMMRegister xmm4, XMMRegister xmm5, XMMRegister xmm6, XMMRegister xmm7,\n-                Register rax, Register rbx, Register rcx, Register rdx, Register tmp1, Register tmp2,\n-                Register tmp3, Register tmp4);\n+#ifndef _LP64\n+ private:\n+  \/\/ Initialized in macroAssembler_x86_constants.cpp\n+  static address ONES;\n+  static address L_2IL0FLOATPACKET_0;\n+  static address PI4_INV;\n+  static address PI4X3;\n+  static address PI4X4;\n@@ -1071,9 +1039,1 @@\n-  void fast_cos(XMMRegister xmm0, XMMRegister xmm1, XMMRegister xmm2, XMMRegister xmm3,\n-                XMMRegister xmm4, XMMRegister xmm5, XMMRegister xmm6, XMMRegister xmm7,\n-                Register rax, Register rcx, Register rdx, Register tmp1,\n-                Register tmp2, Register tmp3, Register tmp4);\n-  void fast_tan(XMMRegister xmm0, XMMRegister xmm1, XMMRegister xmm2, XMMRegister xmm3,\n-                XMMRegister xmm4, XMMRegister xmm5, XMMRegister xmm6, XMMRegister xmm7,\n-                Register rax, Register rcx, Register rdx, Register tmp1,\n-                Register tmp2, Register tmp3, Register tmp4);\n-#else\n+ public:\n@@ -1114,1 +1074,1 @@\n-#endif\n+#endif \/\/ !_LP64\n@@ -1120,4 +1080,4 @@\n-  void movss(XMMRegister dst, XMMRegister src) { Assembler::movss(dst, src); }\n-  void movss(Address dst, XMMRegister src)     { Assembler::movss(dst, src); }\n-  void movss(XMMRegister dst, Address src)     { Assembler::movss(dst, src); }\n-  void movss(XMMRegister dst, AddressLiteral src);\n+  void movss(Address     dst, XMMRegister    src) { Assembler::movss(dst, src); }\n+  void movss(XMMRegister dst, XMMRegister    src) { Assembler::movss(dst, src); }\n+  void movss(XMMRegister dst, Address        src) { Assembler::movss(dst, src); }\n+  void movss(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -1125,2 +1085,2 @@\n-  void movlpd(XMMRegister dst, Address src)    {Assembler::movlpd(dst, src); }\n-  void movlpd(XMMRegister dst, AddressLiteral src);\n+  void movlpd(XMMRegister dst, Address        src) {Assembler::movlpd(dst, src); }\n+  void movlpd(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -1130,3 +1090,11 @@\n-  void addsd(XMMRegister dst, XMMRegister src)    { Assembler::addsd(dst, src); }\n-  void addsd(XMMRegister dst, Address src)        { Assembler::addsd(dst, src); }\n-  void addsd(XMMRegister dst, AddressLiteral src);\n+  void addsd(XMMRegister dst, XMMRegister    src) { Assembler::addsd(dst, src); }\n+  void addsd(XMMRegister dst, Address        src) { Assembler::addsd(dst, src); }\n+  void addsd(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n+\n+  void addss(XMMRegister dst, XMMRegister    src) { Assembler::addss(dst, src); }\n+  void addss(XMMRegister dst, Address        src) { Assembler::addss(dst, src); }\n+  void addss(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n+\n+  void addpd(XMMRegister dst, XMMRegister    src) { Assembler::addpd(dst, src); }\n+  void addpd(XMMRegister dst, Address        src) { Assembler::addpd(dst, src); }\n+  void addpd(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -1134,3 +1102,2 @@\n-  void addss(XMMRegister dst, XMMRegister src)    { Assembler::addss(dst, src); }\n-  void addss(XMMRegister dst, Address src)        { Assembler::addss(dst, src); }\n-  void addss(XMMRegister dst, AddressLiteral src);\n+  using Assembler::vbroadcastsd;\n+  void vbroadcastsd(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch = noreg);\n@@ -1138,3 +1105,2 @@\n-  void addpd(XMMRegister dst, XMMRegister src)    { Assembler::addpd(dst, src); }\n-  void addpd(XMMRegister dst, Address src)        { Assembler::addpd(dst, src); }\n-  void addpd(XMMRegister dst, AddressLiteral src);\n+  using Assembler::vbroadcastss;\n+  void vbroadcastss(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch = noreg);\n@@ -1142,3 +1108,3 @@\n-  void divsd(XMMRegister dst, XMMRegister src)    { Assembler::divsd(dst, src); }\n-  void divsd(XMMRegister dst, Address src)        { Assembler::divsd(dst, src); }\n-  void divsd(XMMRegister dst, AddressLiteral src);\n+  void divsd(XMMRegister dst, XMMRegister    src) { Assembler::divsd(dst, src); }\n+  void divsd(XMMRegister dst, Address        src) { Assembler::divsd(dst, src); }\n+  void divsd(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -1146,3 +1112,3 @@\n-  void divss(XMMRegister dst, XMMRegister src)    { Assembler::divss(dst, src); }\n-  void divss(XMMRegister dst, Address src)        { Assembler::divss(dst, src); }\n-  void divss(XMMRegister dst, AddressLiteral src);\n+  void divss(XMMRegister dst, XMMRegister    src) { Assembler::divss(dst, src); }\n+  void divss(XMMRegister dst, Address        src) { Assembler::divss(dst, src); }\n+  void divss(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -1151,18 +1117,18 @@\n-  void movdqu(Address     dst, XMMRegister src);\n-  void movdqu(XMMRegister dst, Address src);\n-  void movdqu(XMMRegister dst, XMMRegister src);\n-  void movdqu(XMMRegister dst, AddressLiteral src, Register scratchReg = rscratch1);\n-\n-  void kmovwl(KRegister dst, Register src) { Assembler::kmovwl(dst, src); }\n-  void kmovwl(Register dst, KRegister src) { Assembler::kmovwl(dst, src); }\n-  void kmovwl(KRegister dst, Address src) { Assembler::kmovwl(dst, src); }\n-  void kmovwl(KRegister dst, AddressLiteral src, Register scratch_reg = rscratch1);\n-  void kmovwl(Address dst,  KRegister src) { Assembler::kmovwl(dst, src); }\n-  void kmovwl(KRegister dst, KRegister src) { Assembler::kmovwl(dst, src); }\n-\n-  void kmovql(KRegister dst, KRegister src) { Assembler::kmovql(dst, src); }\n-  void kmovql(KRegister dst, Register src) { Assembler::kmovql(dst, src); }\n-  void kmovql(Register dst, KRegister src) { Assembler::kmovql(dst, src); }\n-  void kmovql(KRegister dst, Address src) { Assembler::kmovql(dst, src); }\n-  void kmovql(Address  dst, KRegister src) { Assembler::kmovql(dst, src); }\n-  void kmovql(KRegister dst, AddressLiteral src, Register scratch_reg = rscratch1);\n+  void movdqu(Address     dst, XMMRegister    src);\n+  void movdqu(XMMRegister dst, XMMRegister    src);\n+  void movdqu(XMMRegister dst, Address        src);\n+  void movdqu(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n+\n+  void kmovwl(Register  dst, KRegister      src) { Assembler::kmovwl(dst, src); }\n+  void kmovwl(Address   dst, KRegister      src) { Assembler::kmovwl(dst, src); }\n+  void kmovwl(KRegister dst, KRegister      src) { Assembler::kmovwl(dst, src); }\n+  void kmovwl(KRegister dst, Register       src) { Assembler::kmovwl(dst, src); }\n+  void kmovwl(KRegister dst, Address        src) { Assembler::kmovwl(dst, src); }\n+  void kmovwl(KRegister dst, AddressLiteral src, Register rscratch = noreg);\n+\n+  void kmovql(KRegister dst, KRegister      src) { Assembler::kmovql(dst, src); }\n+  void kmovql(KRegister dst, Register       src) { Assembler::kmovql(dst, src); }\n+  void kmovql(Register  dst, KRegister      src) { Assembler::kmovql(dst, src); }\n+  void kmovql(KRegister dst, Address        src) { Assembler::kmovql(dst, src); }\n+  void kmovql(Address   dst, KRegister      src) { Assembler::kmovql(dst, src); }\n+  void kmovql(KRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -1178,6 +1144,5 @@\n-  \/\/ AVX Unaligned forms\n-  void vmovdqu(Address     dst, XMMRegister src);\n-  void vmovdqu(XMMRegister dst, Address src);\n-  void vmovdqu(XMMRegister dst, XMMRegister src);\n-  void vmovdqu(XMMRegister dst, AddressLiteral src, Register scratch_reg = rscratch1);\n-  void vmovdqu(XMMRegister dst, AddressLiteral src, Register scratch_reg, int vector_len);\n+  using Assembler::movddup;\n+  void movddup(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n+\n+  using Assembler::vmovddup;\n+  void vmovddup(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch = noreg);\n@@ -1185,0 +1150,6 @@\n+  \/\/ AVX Unaligned forms\n+  void vmovdqu(Address     dst, XMMRegister    src);\n+  void vmovdqu(XMMRegister dst, Address        src);\n+  void vmovdqu(XMMRegister dst, XMMRegister    src);\n+  void vmovdqu(XMMRegister dst, AddressLiteral src,                 Register rscratch = noreg);\n+  void vmovdqu(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch = noreg);\n@@ -1187,2 +1158,2 @@\n-  void evmovdqu(BasicType type, KRegister kmask, Address dst, XMMRegister src,  bool merge, int vector_len);\n-  void evmovdqu(BasicType type, KRegister kmask, XMMRegister dst, Address src, bool merge, int vector_len);\n+  void evmovdqu(BasicType type, KRegister kmask, Address     dst, XMMRegister src, bool merge, int vector_len);\n+  void evmovdqu(BasicType type, KRegister kmask, XMMRegister dst, Address     src, bool merge, int vector_len);\n@@ -1191,1 +1162,2 @@\n-  void evmovdqub(XMMRegister dst, Address src, int vector_len) { Assembler::evmovdqub(dst, src, vector_len); }\n+  void evmovdqub(XMMRegister dst, Address     src, int vector_len) { Assembler::evmovdqub(dst, src, vector_len); }\n+\n@@ -1197,3 +1169,6 @@\n-  void evmovdqub(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len) { Assembler::evmovdqub(dst, mask, src, merge, vector_len); }\n-  void evmovdqub(Address dst, KRegister mask, XMMRegister src, bool merge, int vector_len) { Assembler::evmovdqub(dst, mask, src, merge, vector_len); }\n-  void evmovdqub(XMMRegister dst, KRegister mask, AddressLiteral src, bool merge, int vector_len, Register scratch_reg);\n+  void evmovdqub(Address     dst, KRegister mask, XMMRegister    src, bool merge, int vector_len) { Assembler::evmovdqub(dst, mask, src, merge, vector_len); }\n+  void evmovdqub(XMMRegister dst, KRegister mask, Address        src, bool merge, int vector_len) { Assembler::evmovdqub(dst, mask, src, merge, vector_len); }\n+  void evmovdqub(XMMRegister dst, KRegister mask, AddressLiteral src, bool merge, int vector_len, Register rscratch = noreg);\n+\n+  void evmovdquw(Address     dst, XMMRegister src, int vector_len) { Assembler::evmovdquw(dst, src, vector_len); }\n+  void evmovdquw(XMMRegister dst, Address     src, int vector_len) { Assembler::evmovdquw(dst, src, vector_len); }\n@@ -1201,2 +1176,0 @@\n-  void evmovdquw(XMMRegister dst, Address src, int vector_len) { Assembler::evmovdquw(dst, src, vector_len); }\n-  void evmovdquw(Address dst, XMMRegister src, int vector_len) { Assembler::evmovdquw(dst, src, vector_len); }\n@@ -1208,3 +1181,3 @@\n-  void evmovdquw(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len) { Assembler::evmovdquw(dst, mask, src, merge, vector_len); }\n-  void evmovdquw(Address dst, KRegister mask, XMMRegister src, bool merge, int vector_len) { Assembler::evmovdquw(dst, mask, src, merge, vector_len); }\n-  void evmovdquw(XMMRegister dst, KRegister mask, AddressLiteral src, bool merge, int vector_len, Register scratch_reg);\n+  void evmovdquw(XMMRegister dst, KRegister mask, Address        src, bool merge, int vector_len) { Assembler::evmovdquw(dst, mask, src, merge, vector_len); }\n+  void evmovdquw(Address     dst, KRegister mask, XMMRegister    src, bool merge, int vector_len) { Assembler::evmovdquw(dst, mask, src, merge, vector_len); }\n+  void evmovdquw(XMMRegister dst, KRegister mask, AddressLiteral src, bool merge, int vector_len, Register rscratch = noreg);\n@@ -1217,2 +1190,2 @@\n-  void evmovdqul(Address dst, XMMRegister src, int vector_len) { Assembler::evmovdqul(dst, src, vector_len); }\n-  void evmovdqul(XMMRegister dst, Address src, int vector_len) { Assembler::evmovdqul(dst, src, vector_len); }\n+  void evmovdqul(Address     dst, XMMRegister src, int vector_len) { Assembler::evmovdqul(dst, src, vector_len); }\n+  void evmovdqul(XMMRegister dst, Address     src, int vector_len) { Assembler::evmovdqul(dst, src, vector_len); }\n@@ -1225,3 +1198,3 @@\n-  void evmovdqul(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len) { Assembler::evmovdqul(dst, mask, src, merge, vector_len); }\n-  void evmovdqul(Address dst, KRegister mask, XMMRegister src, bool merge, int vector_len) { Assembler::evmovdqul(dst, mask, src, merge, vector_len); }\n-  void evmovdqul(XMMRegister dst, KRegister mask, AddressLiteral src, bool merge, int vector_len, Register scratch_reg);\n+  void evmovdqul(Address     dst, KRegister mask, XMMRegister    src, bool merge, int vector_len) { Assembler::evmovdqul(dst, mask, src, merge, vector_len); }\n+  void evmovdqul(XMMRegister dst, KRegister mask, Address        src, bool merge, int vector_len) { Assembler::evmovdqul(dst, mask, src, merge, vector_len); }\n+  void evmovdqul(XMMRegister dst, KRegister mask, AddressLiteral src, bool merge, int vector_len, Register rscratch = noreg);\n@@ -1234,3 +1207,3 @@\n-  void evmovdquq(XMMRegister dst, Address src, int vector_len) { Assembler::evmovdquq(dst, src, vector_len); }\n-  void evmovdquq(Address dst, XMMRegister src, int vector_len) { Assembler::evmovdquq(dst, src, vector_len); }\n-  void evmovdquq(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch);\n+  void evmovdquq(XMMRegister dst, Address        src, int vector_len) { Assembler::evmovdquq(dst, src, vector_len); }\n+  void evmovdquq(Address     dst, XMMRegister    src, int vector_len) { Assembler::evmovdquq(dst, src, vector_len); }\n+  void evmovdquq(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch = noreg);\n@@ -1243,3 +1216,3 @@\n-  void evmovdquq(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len) { Assembler::evmovdquq(dst, mask, src, merge, vector_len); }\n-  void evmovdquq(Address dst, KRegister mask, XMMRegister src, bool merge, int vector_len) { Assembler::evmovdquq(dst, mask, src, merge, vector_len); }\n-  void evmovdquq(XMMRegister dst, KRegister mask, AddressLiteral src, bool merge, int vector_len, Register scratch_reg);\n+  void evmovdquq(Address     dst, KRegister mask, XMMRegister    src, bool merge, int vector_len) { Assembler::evmovdquq(dst, mask, src, merge, vector_len); }\n+  void evmovdquq(XMMRegister dst, KRegister mask, Address        src, bool merge, int vector_len) { Assembler::evmovdquq(dst, mask, src, merge, vector_len); }\n+  void evmovdquq(XMMRegister dst, KRegister mask, AddressLiteral src, bool merge, int vector_len, Register rscratch = noreg);\n@@ -1248,8 +1221,3 @@\n-  void movdqa(XMMRegister dst, Address src)       { Assembler::movdqa(dst, src); }\n-  void movdqa(XMMRegister dst, XMMRegister src)   { Assembler::movdqa(dst, src); }\n-  void movdqa(XMMRegister dst, AddressLiteral src);\n-\n-  void movsd(XMMRegister dst, XMMRegister src) { Assembler::movsd(dst, src); }\n-  void movsd(Address dst, XMMRegister src)     { Assembler::movsd(dst, src); }\n-  void movsd(XMMRegister dst, Address src)     { Assembler::movsd(dst, src); }\n-  void movsd(XMMRegister dst, AddressLiteral src);\n+  void movdqa(XMMRegister dst, XMMRegister    src) { Assembler::movdqa(dst, src); }\n+  void movdqa(XMMRegister dst, Address        src) { Assembler::movdqa(dst, src); }\n+  void movdqa(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -1257,2 +1225,4 @@\n-  using Assembler::vmovddup;\n-  void vmovddup(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch = rscratch1);\n+  void movsd(Address     dst, XMMRegister    src) { Assembler::movsd(dst, src); }\n+  void movsd(XMMRegister dst, XMMRegister    src) { Assembler::movsd(dst, src); }\n+  void movsd(XMMRegister dst, Address        src) { Assembler::movsd(dst, src); }\n+  void movsd(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -1260,3 +1230,3 @@\n-  void mulpd(XMMRegister dst, XMMRegister src)    { Assembler::mulpd(dst, src); }\n-  void mulpd(XMMRegister dst, Address src)        { Assembler::mulpd(dst, src); }\n-  void mulpd(XMMRegister dst, AddressLiteral src);\n+  void mulpd(XMMRegister dst, XMMRegister    src) { Assembler::mulpd(dst, src); }\n+  void mulpd(XMMRegister dst, Address        src) { Assembler::mulpd(dst, src); }\n+  void mulpd(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -1264,3 +1234,3 @@\n-  void mulsd(XMMRegister dst, XMMRegister src)    { Assembler::mulsd(dst, src); }\n-  void mulsd(XMMRegister dst, Address src)        { Assembler::mulsd(dst, src); }\n-  void mulsd(XMMRegister dst, AddressLiteral src);\n+  void mulsd(XMMRegister dst, XMMRegister    src) { Assembler::mulsd(dst, src); }\n+  void mulsd(XMMRegister dst, Address        src) { Assembler::mulsd(dst, src); }\n+  void mulsd(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -1268,3 +1238,3 @@\n-  void mulss(XMMRegister dst, XMMRegister src)    { Assembler::mulss(dst, src); }\n-  void mulss(XMMRegister dst, Address src)        { Assembler::mulss(dst, src); }\n-  void mulss(XMMRegister dst, AddressLiteral src);\n+  void mulss(XMMRegister dst, XMMRegister    src) { Assembler::mulss(dst, src); }\n+  void mulss(XMMRegister dst, Address        src) { Assembler::mulss(dst, src); }\n+  void mulss(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -1295,3 +1265,3 @@\n-  void sqrtsd(XMMRegister dst, XMMRegister src)    { Assembler::sqrtsd(dst, src); }\n-  void sqrtsd(XMMRegister dst, Address src)        { Assembler::sqrtsd(dst, src); }\n-  void sqrtsd(XMMRegister dst, AddressLiteral src);\n+  void roundsd(XMMRegister dst, XMMRegister    src, int32_t rmode) { Assembler::roundsd(dst, src, rmode); }\n+  void roundsd(XMMRegister dst, Address        src, int32_t rmode) { Assembler::roundsd(dst, src, rmode); }\n+  void roundsd(XMMRegister dst, AddressLiteral src, int32_t rmode, Register rscratch = noreg);\n@@ -1299,3 +1269,3 @@\n-  void roundsd(XMMRegister dst, XMMRegister src, int32_t rmode)    { Assembler::roundsd(dst, src, rmode); }\n-  void roundsd(XMMRegister dst, Address src, int32_t rmode)        { Assembler::roundsd(dst, src, rmode); }\n-  void roundsd(XMMRegister dst, AddressLiteral src, int32_t rmode, Register scratch_reg);\n+  void sqrtss(XMMRegister dst, XMMRegister     src) { Assembler::sqrtss(dst, src); }\n+  void sqrtss(XMMRegister dst, Address         src) { Assembler::sqrtss(dst, src); }\n+  void sqrtss(XMMRegister dst, AddressLiteral  src, Register rscratch = noreg);\n@@ -1303,3 +1273,3 @@\n-  void sqrtss(XMMRegister dst, XMMRegister src)    { Assembler::sqrtss(dst, src); }\n-  void sqrtss(XMMRegister dst, Address src)        { Assembler::sqrtss(dst, src); }\n-  void sqrtss(XMMRegister dst, AddressLiteral src);\n+  void subsd(XMMRegister dst, XMMRegister    src) { Assembler::subsd(dst, src); }\n+  void subsd(XMMRegister dst, Address        src) { Assembler::subsd(dst, src); }\n+  void subsd(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -1307,3 +1277,3 @@\n-  void subsd(XMMRegister dst, XMMRegister src)    { Assembler::subsd(dst, src); }\n-  void subsd(XMMRegister dst, Address src)        { Assembler::subsd(dst, src); }\n-  void subsd(XMMRegister dst, AddressLiteral src);\n+  void subss(XMMRegister dst, XMMRegister    src) { Assembler::subss(dst, src); }\n+  void subss(XMMRegister dst, Address        src) { Assembler::subss(dst, src); }\n+  void subss(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -1311,3 +1281,3 @@\n-  void subss(XMMRegister dst, XMMRegister src)    { Assembler::subss(dst, src); }\n-  void subss(XMMRegister dst, Address src)        { Assembler::subss(dst, src); }\n-  void subss(XMMRegister dst, AddressLiteral src);\n+  void ucomiss(XMMRegister dst, XMMRegister    src) { Assembler::ucomiss(dst, src); }\n+  void ucomiss(XMMRegister dst, Address        src) { Assembler::ucomiss(dst, src); }\n+  void ucomiss(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -1315,7 +1285,3 @@\n-  void ucomiss(XMMRegister dst, XMMRegister src) { Assembler::ucomiss(dst, src); }\n-  void ucomiss(XMMRegister dst, Address src)     { Assembler::ucomiss(dst, src); }\n-  void ucomiss(XMMRegister dst, AddressLiteral src);\n-\n-  void ucomisd(XMMRegister dst, XMMRegister src) { Assembler::ucomisd(dst, src); }\n-  void ucomisd(XMMRegister dst, Address src)     { Assembler::ucomisd(dst, src); }\n-  void ucomisd(XMMRegister dst, AddressLiteral src);\n+  void ucomisd(XMMRegister dst, XMMRegister    src) { Assembler::ucomisd(dst, src); }\n+  void ucomisd(XMMRegister dst, Address        src) { Assembler::ucomisd(dst, src); }\n+  void ucomisd(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -1324,3 +1290,3 @@\n-  void xorpd(XMMRegister dst, XMMRegister src);\n-  void xorpd(XMMRegister dst, Address src)     { Assembler::xorpd(dst, src); }\n-  void xorpd(XMMRegister dst, AddressLiteral src, Register scratch_reg = rscratch1);\n+  void xorpd(XMMRegister dst, XMMRegister    src);\n+  void xorpd(XMMRegister dst, Address        src) { Assembler::xorpd(dst, src); }\n+  void xorpd(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -1329,3 +1295,3 @@\n-  void xorps(XMMRegister dst, XMMRegister src);\n-  void xorps(XMMRegister dst, Address src)     { Assembler::xorps(dst, src); }\n-  void xorps(XMMRegister dst, AddressLiteral src, Register scratch_reg = rscratch1);\n+  void xorps(XMMRegister dst, XMMRegister    src);\n+  void xorps(XMMRegister dst, Address        src) { Assembler::xorps(dst, src); }\n+  void xorps(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -1334,3 +1300,3 @@\n-  void pshufb(XMMRegister dst, XMMRegister src) { Assembler::pshufb(dst, src); }\n-  void pshufb(XMMRegister dst, Address src)     { Assembler::pshufb(dst, src); }\n-  void pshufb(XMMRegister dst, AddressLiteral src);\n+  void pshufb(XMMRegister dst, XMMRegister    src) { Assembler::pshufb(dst, src); }\n+  void pshufb(XMMRegister dst, Address        src) { Assembler::pshufb(dst, src); }\n+  void pshufb(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -1339,3 +1305,3 @@\n-  void vaddsd(XMMRegister dst, XMMRegister nds, XMMRegister src) { Assembler::vaddsd(dst, nds, src); }\n-  void vaddsd(XMMRegister dst, XMMRegister nds, Address src)     { Assembler::vaddsd(dst, nds, src); }\n-  void vaddsd(XMMRegister dst, XMMRegister nds, AddressLiteral src);\n+  void vaddsd(XMMRegister dst, XMMRegister nds, XMMRegister    src) { Assembler::vaddsd(dst, nds, src); }\n+  void vaddsd(XMMRegister dst, XMMRegister nds, Address        src) { Assembler::vaddsd(dst, nds, src); }\n+  void vaddsd(XMMRegister dst, XMMRegister nds, AddressLiteral src, Register rscratch = noreg);\n@@ -1343,3 +1309,3 @@\n-  void vaddss(XMMRegister dst, XMMRegister nds, XMMRegister src) { Assembler::vaddss(dst, nds, src); }\n-  void vaddss(XMMRegister dst, XMMRegister nds, Address src)     { Assembler::vaddss(dst, nds, src); }\n-  void vaddss(XMMRegister dst, XMMRegister nds, AddressLiteral src);\n+  void vaddss(XMMRegister dst, XMMRegister nds, XMMRegister    src) { Assembler::vaddss(dst, nds, src); }\n+  void vaddss(XMMRegister dst, XMMRegister nds, Address        src) { Assembler::vaddss(dst, nds, src); }\n+  void vaddss(XMMRegister dst, XMMRegister nds, AddressLiteral src, Register rscratch = noreg);\n@@ -1347,2 +1313,2 @@\n-  void vabsss(XMMRegister dst, XMMRegister nds, XMMRegister src, AddressLiteral negate_field, int vector_len);\n-  void vabssd(XMMRegister dst, XMMRegister nds, XMMRegister src, AddressLiteral negate_field, int vector_len);\n+  void vabsss(XMMRegister dst, XMMRegister nds, XMMRegister src, AddressLiteral negate_field, int vector_len, Register rscratch = noreg);\n+  void vabssd(XMMRegister dst, XMMRegister nds, XMMRegister src, AddressLiteral negate_field, int vector_len, Register rscratch = noreg);\n@@ -1350,3 +1316,3 @@\n-  void vpaddb(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len);\n-  void vpaddb(XMMRegister dst, XMMRegister nds, Address src, int vector_len);\n-  void vpaddb(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch);\n+  void vpaddb(XMMRegister dst, XMMRegister nds, XMMRegister    src, int vector_len);\n+  void vpaddb(XMMRegister dst, XMMRegister nds, Address        src, int vector_len);\n+  void vpaddb(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch = noreg);\n@@ -1355,5 +1321,1 @@\n-  void vpaddw(XMMRegister dst, XMMRegister nds, Address src, int vector_len);\n-\n-  void vpaddd(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) { Assembler::vpaddd(dst, nds, src, vector_len); }\n-  void vpaddd(XMMRegister dst, XMMRegister nds, Address src, int vector_len) { Assembler::vpaddd(dst, nds, src, vector_len); }\n-  void vpaddd(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch);\n+  void vpaddw(XMMRegister dst, XMMRegister nds, Address     src, int vector_len);\n@@ -1361,3 +1323,3 @@\n-  void vpand(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) { Assembler::vpand(dst, nds, src, vector_len); }\n-  void vpand(XMMRegister dst, XMMRegister nds, Address src, int vector_len) { Assembler::vpand(dst, nds, src, vector_len); }\n-  void vpand(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg = rscratch1);\n+  void vpaddd(XMMRegister dst, XMMRegister nds, XMMRegister    src, int vector_len) { Assembler::vpaddd(dst, nds, src, vector_len); }\n+  void vpaddd(XMMRegister dst, XMMRegister nds, Address        src, int vector_len) { Assembler::vpaddd(dst, nds, src, vector_len); }\n+  void vpaddd(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch = noreg);\n@@ -1365,8 +1327,3 @@\n-  void vpbroadcastw(XMMRegister dst, XMMRegister src, int vector_len);\n-  void vpbroadcastw(XMMRegister dst, Address src, int vector_len) { Assembler::vpbroadcastw(dst, src, vector_len); }\n-\n-  using Assembler::vbroadcastsd;\n-  void vbroadcastsd(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch = rscratch1);\n-  void vpbroadcastq(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch = rscratch1);\n-  void vpbroadcastq(XMMRegister dst, XMMRegister src, int vector_len) { Assembler::vpbroadcastq(dst, src, vector_len); }\n-  void vpbroadcastq(XMMRegister dst, Address src, int vector_len) { Assembler::vpbroadcastq(dst, src, vector_len); }\n+  void vpand(XMMRegister dst, XMMRegister nds, XMMRegister    src, int vector_len) { Assembler::vpand(dst, nds, src, vector_len); }\n+  void vpand(XMMRegister dst, XMMRegister nds, Address        src, int vector_len) { Assembler::vpand(dst, nds, src, vector_len); }\n+  void vpand(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch = noreg);\n@@ -1374,0 +1331,2 @@\n+  using Assembler::vpbroadcastd;\n+  void vpbroadcastd(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch = noreg);\n@@ -1375,0 +1334,2 @@\n+  using Assembler::vpbroadcastq;\n+  void vpbroadcastq(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch = noreg);\n@@ -1379,1 +1340,1 @@\n-  void evpcmpeqd(KRegister kdst, KRegister mask, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg);\n+  void evpcmpeqd(KRegister kdst, KRegister mask, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch = noreg);\n@@ -1382,16 +1343,19 @@\n-  void evpcmpd(KRegister kdst, KRegister mask, XMMRegister nds, XMMRegister src,\n-               int comparison, bool is_signed, int vector_len) { Assembler::evpcmpd(kdst, mask, nds, src, comparison, is_signed, vector_len); }\n-  void evpcmpd(KRegister kdst, KRegister mask, XMMRegister nds, AddressLiteral src,\n-               int comparison, bool is_signed, int vector_len, Register scratch_reg);\n-  void evpcmpq(KRegister kdst, KRegister mask, XMMRegister nds, XMMRegister src,\n-               int comparison, bool is_signed, int vector_len) { Assembler::evpcmpq(kdst, mask, nds, src, comparison, is_signed, vector_len); }\n-  void evpcmpq(KRegister kdst, KRegister mask, XMMRegister nds, AddressLiteral src,\n-               int comparison, bool is_signed, int vector_len, Register scratch_reg);\n-  void evpcmpb(KRegister kdst, KRegister mask, XMMRegister nds, XMMRegister src,\n-               int comparison, bool is_signed, int vector_len) { Assembler::evpcmpb(kdst, mask, nds, src, comparison, is_signed, vector_len); }\n-  void evpcmpb(KRegister kdst, KRegister mask, XMMRegister nds, AddressLiteral src,\n-               int comparison, bool is_signed, int vector_len, Register scratch_reg);\n-  void evpcmpw(KRegister kdst, KRegister mask, XMMRegister nds, XMMRegister src,\n-               int comparison, bool is_signed, int vector_len) { Assembler::evpcmpw(kdst, mask, nds, src, comparison, is_signed, vector_len); }\n-  void evpcmpw(KRegister kdst, KRegister mask, XMMRegister nds, AddressLiteral src,\n-               int comparison, bool is_signed, int vector_len, Register scratch_reg);\n+  void evpcmpd(KRegister kdst, KRegister mask, XMMRegister nds, XMMRegister    src, int comparison, bool is_signed, int vector_len) {\n+    Assembler::evpcmpd(kdst, mask, nds, src, comparison, is_signed, vector_len);\n+  }\n+  void evpcmpd(KRegister kdst, KRegister mask, XMMRegister nds, AddressLiteral src, int comparison, bool is_signed, int vector_len, Register rscratch = noreg);\n+\n+  void evpcmpq(KRegister kdst, KRegister mask, XMMRegister nds, XMMRegister    src, int comparison, bool is_signed, int vector_len) {\n+    Assembler::evpcmpq(kdst, mask, nds, src, comparison, is_signed, vector_len);\n+  }\n+  void evpcmpq(KRegister kdst, KRegister mask, XMMRegister nds, AddressLiteral src, int comparison, bool is_signed, int vector_len, Register rscratch = noreg);\n+\n+  void evpcmpb(KRegister kdst, KRegister mask, XMMRegister nds, XMMRegister    src, int comparison, bool is_signed, int vector_len) {\n+    Assembler::evpcmpb(kdst, mask, nds, src, comparison, is_signed, vector_len);\n+  }\n+  void evpcmpb(KRegister kdst, KRegister mask, XMMRegister nds, AddressLiteral src, int comparison, bool is_signed, int vector_len, Register rscratch = noreg);\n+\n+  void evpcmpw(KRegister kdst, KRegister mask, XMMRegister nds, XMMRegister    src, int comparison, bool is_signed, int vector_len) {\n+    Assembler::evpcmpw(kdst, mask, nds, src, comparison, is_signed, vector_len);\n+  }\n+  void evpcmpw(KRegister kdst, KRegister mask, XMMRegister nds, AddressLiteral src, int comparison, bool is_signed, int vector_len, Register rscratch = noreg);\n@@ -1405,1 +1369,1 @@\n-  void vpmovzxbw(XMMRegister dst, Address src, int vector_len);\n+  void vpmovzxbw(XMMRegister dst, Address     src, int vector_len);\n@@ -1411,8 +1375,5 @@\n-  void vpmullw(XMMRegister dst, XMMRegister nds, Address src, int vector_len);\n-  void vpmulld(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {\n-    Assembler::vpmulld(dst, nds, src, vector_len);\n-  };\n-  void vpmulld(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n-    Assembler::vpmulld(dst, nds, src, vector_len);\n-  }\n-  void vpmulld(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg);\n+  void vpmullw(XMMRegister dst, XMMRegister nds, Address     src, int vector_len);\n+\n+  void vpmulld(XMMRegister dst, XMMRegister nds, XMMRegister    src, int vector_len) { Assembler::vpmulld(dst, nds, src, vector_len); }\n+  void vpmulld(XMMRegister dst, XMMRegister nds, Address        src, int vector_len) { Assembler::vpmulld(dst, nds, src, vector_len); }\n+  void vpmulld(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch = noreg);\n@@ -1421,1 +1382,1 @@\n-  void vpsubb(XMMRegister dst, XMMRegister nds, Address src, int vector_len);\n+  void vpsubb(XMMRegister dst, XMMRegister nds, Address     src, int vector_len);\n@@ -1424,1 +1385,1 @@\n-  void vpsubw(XMMRegister dst, XMMRegister nds, Address src, int vector_len);\n+  void vpsubw(XMMRegister dst, XMMRegister nds, Address     src, int vector_len);\n@@ -1427,1 +1388,1 @@\n-  void vpsraw(XMMRegister dst, XMMRegister nds, int shift, int vector_len);\n+  void vpsraw(XMMRegister dst, XMMRegister nds, int         shift, int vector_len);\n@@ -1430,1 +1391,1 @@\n-  void evpsraq(XMMRegister dst, XMMRegister nds, int shift, int vector_len);\n+  void evpsraq(XMMRegister dst, XMMRegister nds, int         shift, int vector_len);\n@@ -1519,3 +1480,3 @@\n-  void vandpd(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) { Assembler::vandpd(dst, nds, src, vector_len); }\n-  void vandpd(XMMRegister dst, XMMRegister nds, Address src, int vector_len)     { Assembler::vandpd(dst, nds, src, vector_len); }\n-  void vandpd(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg = rscratch1);\n+  void vandpd(XMMRegister dst, XMMRegister nds, XMMRegister    src, int vector_len) { Assembler::vandpd(dst, nds, src, vector_len); }\n+  void vandpd(XMMRegister dst, XMMRegister nds, Address        src, int vector_len) { Assembler::vandpd(dst, nds, src, vector_len); }\n+  void vandpd(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch = noreg);\n@@ -1523,3 +1484,3 @@\n-  void vandps(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) { Assembler::vandps(dst, nds, src, vector_len); }\n-  void vandps(XMMRegister dst, XMMRegister nds, Address src, int vector_len)     { Assembler::vandps(dst, nds, src, vector_len); }\n-  void vandps(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg = rscratch1);\n+  void vandps(XMMRegister dst, XMMRegister nds, XMMRegister    src, int vector_len) { Assembler::vandps(dst, nds, src, vector_len); }\n+  void vandps(XMMRegister dst, XMMRegister nds, Address        src, int vector_len) { Assembler::vandps(dst, nds, src, vector_len); }\n+  void vandps(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch = noreg);\n@@ -1527,1 +1488,1 @@\n-  void evpord(XMMRegister dst, KRegister mask, XMMRegister nds, AddressLiteral src, bool merge, int vector_len, Register scratch_reg);\n+  void evpord(XMMRegister dst, KRegister mask, XMMRegister nds, AddressLiteral src, bool merge, int vector_len, Register rscratch = noreg);\n@@ -1529,3 +1490,3 @@\n-  void vdivsd(XMMRegister dst, XMMRegister nds, XMMRegister src) { Assembler::vdivsd(dst, nds, src); }\n-  void vdivsd(XMMRegister dst, XMMRegister nds, Address src)     { Assembler::vdivsd(dst, nds, src); }\n-  void vdivsd(XMMRegister dst, XMMRegister nds, AddressLiteral src);\n+  void vdivsd(XMMRegister dst, XMMRegister nds, XMMRegister    src) { Assembler::vdivsd(dst, nds, src); }\n+  void vdivsd(XMMRegister dst, XMMRegister nds, Address        src) { Assembler::vdivsd(dst, nds, src); }\n+  void vdivsd(XMMRegister dst, XMMRegister nds, AddressLiteral src, Register rscratch = noreg);\n@@ -1533,3 +1494,3 @@\n-  void vdivss(XMMRegister dst, XMMRegister nds, XMMRegister src) { Assembler::vdivss(dst, nds, src); }\n-  void vdivss(XMMRegister dst, XMMRegister nds, Address src)     { Assembler::vdivss(dst, nds, src); }\n-  void vdivss(XMMRegister dst, XMMRegister nds, AddressLiteral src);\n+  void vdivss(XMMRegister dst, XMMRegister nds, XMMRegister    src) { Assembler::vdivss(dst, nds, src); }\n+  void vdivss(XMMRegister dst, XMMRegister nds, Address        src) { Assembler::vdivss(dst, nds, src); }\n+  void vdivss(XMMRegister dst, XMMRegister nds, AddressLiteral src, Register rscratch = noreg);\n@@ -1537,3 +1498,3 @@\n-  void vmulsd(XMMRegister dst, XMMRegister nds, XMMRegister src) { Assembler::vmulsd(dst, nds, src); }\n-  void vmulsd(XMMRegister dst, XMMRegister nds, Address src)     { Assembler::vmulsd(dst, nds, src); }\n-  void vmulsd(XMMRegister dst, XMMRegister nds, AddressLiteral src);\n+  void vmulsd(XMMRegister dst, XMMRegister nds, XMMRegister    src) { Assembler::vmulsd(dst, nds, src); }\n+  void vmulsd(XMMRegister dst, XMMRegister nds, Address        src) { Assembler::vmulsd(dst, nds, src); }\n+  void vmulsd(XMMRegister dst, XMMRegister nds, AddressLiteral src, Register rscratch = noreg);\n@@ -1541,3 +1502,3 @@\n-  void vmulss(XMMRegister dst, XMMRegister nds, XMMRegister src) { Assembler::vmulss(dst, nds, src); }\n-  void vmulss(XMMRegister dst, XMMRegister nds, Address src)     { Assembler::vmulss(dst, nds, src); }\n-  void vmulss(XMMRegister dst, XMMRegister nds, AddressLiteral src);\n+  void vmulss(XMMRegister dst, XMMRegister nds, XMMRegister    src) { Assembler::vmulss(dst, nds, src); }\n+  void vmulss(XMMRegister dst, XMMRegister nds, Address        src) { Assembler::vmulss(dst, nds, src); }\n+  void vmulss(XMMRegister dst, XMMRegister nds, AddressLiteral src, Register rscratch = noreg);\n@@ -1545,3 +1506,3 @@\n-  void vsubsd(XMMRegister dst, XMMRegister nds, XMMRegister src) { Assembler::vsubsd(dst, nds, src); }\n-  void vsubsd(XMMRegister dst, XMMRegister nds, Address src)     { Assembler::vsubsd(dst, nds, src); }\n-  void vsubsd(XMMRegister dst, XMMRegister nds, AddressLiteral src);\n+  void vsubsd(XMMRegister dst, XMMRegister nds, XMMRegister    src) { Assembler::vsubsd(dst, nds, src); }\n+  void vsubsd(XMMRegister dst, XMMRegister nds, Address        src) { Assembler::vsubsd(dst, nds, src); }\n+  void vsubsd(XMMRegister dst, XMMRegister nds, AddressLiteral src, Register rscratch = noreg);\n@@ -1549,3 +1510,3 @@\n-  void vsubss(XMMRegister dst, XMMRegister nds, XMMRegister src) { Assembler::vsubss(dst, nds, src); }\n-  void vsubss(XMMRegister dst, XMMRegister nds, Address src)     { Assembler::vsubss(dst, nds, src); }\n-  void vsubss(XMMRegister dst, XMMRegister nds, AddressLiteral src);\n+  void vsubss(XMMRegister dst, XMMRegister nds, XMMRegister    src) { Assembler::vsubss(dst, nds, src); }\n+  void vsubss(XMMRegister dst, XMMRegister nds, Address        src) { Assembler::vsubss(dst, nds, src); }\n+  void vsubss(XMMRegister dst, XMMRegister nds, AddressLiteral src, Register rscratch = noreg);\n@@ -1553,2 +1514,2 @@\n-  void vnegatess(XMMRegister dst, XMMRegister nds, AddressLiteral src);\n-  void vnegatesd(XMMRegister dst, XMMRegister nds, AddressLiteral src);\n+  void vnegatess(XMMRegister dst, XMMRegister nds, AddressLiteral src, Register rscratch = noreg);\n+  void vnegatesd(XMMRegister dst, XMMRegister nds, AddressLiteral src, Register rscratch = noreg);\n@@ -1558,3 +1519,3 @@\n-  void vxorpd(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) { Assembler::vxorpd(dst, nds, src, vector_len); }\n-  void vxorpd(XMMRegister dst, XMMRegister nds, Address src, int vector_len) { Assembler::vxorpd(dst, nds, src, vector_len); }\n-  void vxorpd(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg = rscratch1);\n+  void vxorpd(XMMRegister dst, XMMRegister nds, XMMRegister    src, int vector_len) { Assembler::vxorpd(dst, nds, src, vector_len); }\n+  void vxorpd(XMMRegister dst, XMMRegister nds, Address        src, int vector_len) { Assembler::vxorpd(dst, nds, src, vector_len); }\n+  void vxorpd(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch = noreg);\n@@ -1562,3 +1523,3 @@\n-  void vxorps(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) { Assembler::vxorps(dst, nds, src, vector_len); }\n-  void vxorps(XMMRegister dst, XMMRegister nds, Address src, int vector_len) { Assembler::vxorps(dst, nds, src, vector_len); }\n-  void vxorps(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg = rscratch1);\n+  void vxorps(XMMRegister dst, XMMRegister nds, XMMRegister    src, int vector_len) { Assembler::vxorps(dst, nds, src, vector_len); }\n+  void vxorps(XMMRegister dst, XMMRegister nds, Address        src, int vector_len) { Assembler::vxorps(dst, nds, src, vector_len); }\n+  void vxorps(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch = noreg);\n@@ -1578,1 +1539,1 @@\n-  void vpxor(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg = rscratch1);\n+  void vpxor(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch = noreg);\n@@ -1590,2 +1551,2 @@\n-  void vpermd(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) { Assembler::vpermd(dst, nds, src, vector_len); }\n-  void vpermd(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg);\n+  void vpermd(XMMRegister dst, XMMRegister nds, XMMRegister    src, int vector_len) { Assembler::vpermd(dst, nds, src, vector_len); }\n+  void vpermd(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch = noreg);\n@@ -1833,1 +1794,1 @@\n-  void movoop(Address dst, jobject obj);\n+  void movoop(Address  dst, jobject obj, Register rscratch);\n@@ -1836,19 +1797,11 @@\n-  void mov_metadata(Address dst, Metadata* obj);\n-\n-  void movptr(ArrayAddress dst, Register src);\n-  \/\/ can this do an lea?\n-  void movptr(Register dst, ArrayAddress src);\n-\n-  void movptr(Register dst, Address src);\n-\n-#ifdef _LP64\n-  void movptr(Register dst, AddressLiteral src, Register scratch=rscratch1);\n-#else\n-  void movptr(Register dst, AddressLiteral src, Register scratch=noreg); \/\/ Scratch reg is ignored in 32-bit\n-#endif\n-\n-  void movptr(Register dst, intptr_t src);\n-  void movptr(Register dst, Register src);\n-  void movptr(Address dst, intptr_t src);\n-\n-  void movptr(Address dst, Register src);\n+  void mov_metadata(Address  dst, Metadata* obj, Register rscratch);\n+\n+  void movptr(Register     dst, Register       src);\n+  void movptr(Register     dst, Address        src);\n+  void movptr(Register     dst, AddressLiteral src);\n+  void movptr(Register     dst, ArrayAddress   src);\n+  void movptr(Register     dst, intptr_t       src);\n+  void movptr(Address      dst, Register       src);\n+  void movptr(Address      dst, int32_t        imm);\n+  void movptr(Address      dst, intptr_t       src, Register rscratch);\n+  void movptr(ArrayAddress dst, Register       src, Register rscratch);\n@@ -1861,11 +1814,0 @@\n-#ifdef _LP64\n-  \/\/ Generally the next two are only used for moving NULL\n-  \/\/ Although there are situations in initializing the mark word where\n-  \/\/ they could be used. They are dangerous.\n-\n-  \/\/ They only exist on LP64 so that int32_t and intptr_t are not the same\n-  \/\/ and we have ambiguous declarations.\n-\n-  void movptr(Address dst, int32_t imm32);\n-  void movptr(Register dst, int32_t imm32);\n-#endif \/\/ _LP64\n@@ -1874,5 +1816,2 @@\n-  void mov32(AddressLiteral dst, Register src);\n-  void mov32(Register dst, AddressLiteral src);\n-\n-  \/\/ to avoid hiding movb\n-  void movbyte(ArrayAddress dst, int src);\n+  void mov32(Register       dst, AddressLiteral src);\n+  void mov32(AddressLiteral dst, Register        src, Register rscratch = noreg);\n@@ -1883,0 +1822,2 @@\n+  void movdl(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n+\n@@ -1884,2 +1825,1 @@\n-  void movdl(XMMRegister dst, AddressLiteral src);\n-  void movq(XMMRegister dst, AddressLiteral src);\n+  void movq(XMMRegister dst, AddressLiteral src, Register rscratch = noreg);\n@@ -1888,1 +1828,1 @@\n-  void pushptr(AddressLiteral src);\n+  void pushptr(AddressLiteral src, Register rscratch);\n@@ -1893,2 +1833,2 @@\n-  void pushoop(jobject obj);\n-  void pushklass(Metadata* obj);\n+  void pushoop(jobject obj, Register rscratch);\n+  void pushklass(Metadata* obj, Register rscratch);\n@@ -1974,1 +1914,0 @@\n-  void updateBytesAdler32(Register adler32, Register buf, Register length, XMMRegister shuf0, XMMRegister shuf1, ExternalAddress scale);\n@@ -2061,27 +2000,1 @@\n-#if COMPILER2_OR_JVMCI\n-  void arraycopy_avx3_special_cases(XMMRegister xmm, KRegister mask, Register from,\n-                                    Register to, Register count, int shift,\n-                                    Register index, Register temp,\n-                                    bool use64byteVector, Label& L_entry, Label& L_exit);\n-\n-  void arraycopy_avx3_special_cases_conjoint(XMMRegister xmm, KRegister mask, Register from,\n-                                             Register to, Register start_index, Register end_index,\n-                                             Register count, int shift, Register temp,\n-                                             bool use64byteVector, Label& L_entry, Label& L_exit);\n-\n-  void copy64_masked_avx(Register dst, Register src, XMMRegister xmm,\n-                         KRegister mask, Register length, Register index,\n-                         Register temp, int shift = Address::times_1, int offset = 0,\n-                         bool use64byteVector = false);\n-\n-  void copy32_masked_avx(Register dst, Register src, XMMRegister xmm,\n-                         KRegister mask, Register length, Register index,\n-                         Register temp, int shift = Address::times_1, int offset = 0);\n-\n-  void copy32_avx(Register dst, Register src, Register index, XMMRegister xmm,\n-                  int shift = Address::times_1, int offset = 0);\n-\n-  void copy64_avx(Register dst, Register src, Register index, XMMRegister xmm,\n-                  bool conjoint, int shift = Address::times_1, int offset = 0,\n-                  bool use64byteVector = false);\n-\n+#ifdef COMPILER2_OR_JVMCI\n@@ -2090,1 +2003,0 @@\n-\n@@ -2093,0 +2005,3 @@\n+  OopMap* continuation_enter_setup(int& stack_slots);\n+  void fill_continuation_entry(Register reg_cont_obj, Register reg_flags);\n+  void continuation_enter_cleanup();\n@@ -2096,0 +2011,3 @@\n+\n+  void check_stack_alignment(Register sp, const char* msg, unsigned bias = 0, Register tmp = noreg);\n+\n@@ -2112,1 +2030,1 @@\n-   SkipIfEqual(MacroAssembler*, const bool* flag_addr, bool value);\n+   SkipIfEqual(MacroAssembler*, const bool* flag_addr, bool value, Register rscratch);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":317,"deletions":399,"binary":false,"changes":716,"status":"modified"},{"patch":"@@ -81,2 +81,0 @@\n-  Register temp2 = noreg;\n-  LP64_ONLY(temp2 = rscratch1);  \/\/ used by MacroAssembler::cmpptr and load_klass\n@@ -88,4 +86,5 @@\n-  __ push(temp); if (temp2 != noreg)  __ push(temp2);\n-#define UNPUSH { if (temp2 != noreg)  __ pop(temp2);  __ pop(temp); }\n-  __ load_klass(temp, obj, temp2);\n-  __ cmpptr(temp, ExternalAddress((address) klass_addr));\n+#define PUSH { __ push(temp); LP64_ONLY(  __ push(rscratch1); )               }\n+#define POP  {                LP64_ONLY(  __ pop(rscratch1);  ) __ pop(temp); }\n+  PUSH;\n+  __ load_klass(temp, obj, rscratch1);\n+  __ cmpptr(temp, ExternalAddress((address) klass_addr), rscratch1);\n@@ -95,1 +94,1 @@\n-  __ cmpptr(temp, ExternalAddress((address) klass_addr));\n+  __ cmpptr(temp, ExternalAddress((address) klass_addr), rscratch1);\n@@ -97,1 +96,1 @@\n-  UNPUSH;\n+  POP;\n@@ -101,1 +100,1 @@\n-  UNPUSH;\n+  POP;\n@@ -103,0 +102,2 @@\n+#undef POP\n+#undef PUSH\n@@ -533,1 +534,1 @@\n-    const int saved_regs_count = RegisterImpl::number_of_registers;\n+    const int saved_regs_count = Register::number_of_registers;\n@@ -538,1 +539,1 @@\n-      assert(RegisterImpl::number_of_registers == 16, \"sanity\");\n+      assert(Register::number_of_registers == 16, \"sanity\");\n@@ -674,1 +675,1 @@\n-  __ movptr(Address(rsp, 0), (intptr_t) adaptername);\n+  __ movptr(Address(rsp, 0), (intptr_t) adaptername, rscratch1);\n","filename":"src\/hotspot\/cpu\/x86\/methodHandles_x86.cpp","additions":13,"deletions":12,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -27,2 +27,1 @@\n-#include \"asm\/macroAssembler.inline.hpp\"\n-#include \"ci\/ciUtilities.hpp\"\n+#include \"classfile\/vmIntrinsics.hpp\"\n@@ -34,11 +33,0 @@\n-#include \"interpreter\/interpreter.hpp\"\n-#include \"nativeInst_x86.hpp\"\n-#include \"oops\/instanceOop.hpp\"\n-#include \"oops\/method.hpp\"\n-#include \"oops\/objArrayKlass.hpp\"\n-#include \"oops\/oop.inline.hpp\"\n-#include \"prims\/methodHandles.hpp\"\n-#include \"runtime\/continuation.hpp\"\n-#include \"runtime\/continuationEntry.inline.hpp\"\n-#include \"runtime\/frame.inline.hpp\"\n-#include \"runtime\/handles.inline.hpp\"\n@@ -50,1 +38,1 @@\n-#include \"runtime\/stubCodeGenerator.hpp\"\n+#include \"stubGenerator_x86_64.hpp\"\n@@ -54,0 +42,1 @@\n+#include \"opto\/c2_globals.hpp\"\n@@ -65,1 +54,0 @@\n-\/\/ Declaration and definition of StubGenerator (no .hpp file).\n@@ -71,1 +59,0 @@\n-#define a__ ((Assembler*)_masm)->\n@@ -77,1 +64,1 @@\n-#endif\n+#endif \/\/ PRODUCT\n@@ -80,138 +67,70 @@\n-const int MXCSR_MASK = 0xFFC0;  \/\/ Mask out any pending exceptions\n-\n-OopMap* continuation_enter_setup(MacroAssembler* masm, int& stack_slots);\n-void fill_continuation_entry(MacroAssembler* masm);\n-void continuation_enter_cleanup(MacroAssembler* masm);\n-\n-\/\/ Stub Code definitions\n-\n-class StubGenerator: public StubCodeGenerator {\n- private:\n-\n-#ifdef PRODUCT\n-#define inc_counter_np(counter) ((void)0)\n-#else\n-  void inc_counter_np_(int& counter) {\n-    \/\/ This can destroy rscratch1 if counter is far from the code cache\n-    __ incrementl(ExternalAddress((address)&counter));\n-  }\n-#define inc_counter_np(counter) \\\n-  BLOCK_COMMENT(\"inc_counter \" #counter); \\\n-  inc_counter_np_(counter);\n-#endif\n-\n-  \/\/ Call stubs are used to call Java from C\n-  \/\/\n-  \/\/ Linux Arguments:\n-  \/\/    c_rarg0:   call wrapper address                   address\n-  \/\/    c_rarg1:   result                                 address\n-  \/\/    c_rarg2:   result type                            BasicType\n-  \/\/    c_rarg3:   method                                 Method*\n-  \/\/    c_rarg4:   (interpreter) entry point              address\n-  \/\/    c_rarg5:   parameters                             intptr_t*\n-  \/\/    16(rbp): parameter size (in words)              int\n-  \/\/    24(rbp): thread                                 Thread*\n-  \/\/\n-  \/\/     [ return_from_Java     ] <--- rsp\n-  \/\/     [ argument word n      ]\n-  \/\/      ...\n-  \/\/ -12 [ argument word 1      ]\n-  \/\/ -11 [ saved r15            ] <--- rsp_after_call\n-  \/\/ -10 [ saved r14            ]\n-  \/\/  -9 [ saved r13            ]\n-  \/\/  -8 [ saved r12            ]\n-  \/\/  -7 [ saved rbx            ]\n-  \/\/  -6 [ call wrapper         ]\n-  \/\/  -5 [ result               ]\n-  \/\/  -4 [ result type          ]\n-  \/\/  -3 [ method               ]\n-  \/\/  -2 [ entry point          ]\n-  \/\/  -1 [ parameters           ]\n-  \/\/   0 [ saved rbp            ] <--- rbp\n-  \/\/   1 [ return address       ]\n-  \/\/   2 [ parameter size       ]\n-  \/\/   3 [ thread               ]\n-  \/\/\n-  \/\/ Windows Arguments:\n-  \/\/    c_rarg0:   call wrapper address                   address\n-  \/\/    c_rarg1:   result                                 address\n-  \/\/    c_rarg2:   result type                            BasicType\n-  \/\/    c_rarg3:   method                                 Method*\n-  \/\/    48(rbp): (interpreter) entry point              address\n-  \/\/    56(rbp): parameters                             intptr_t*\n-  \/\/    64(rbp): parameter size (in words)              int\n-  \/\/    72(rbp): thread                                 Thread*\n-  \/\/\n-  \/\/     [ return_from_Java     ] <--- rsp\n-  \/\/     [ argument word n      ]\n-  \/\/      ...\n-  \/\/ -60 [ argument word 1      ]\n-  \/\/ -59 [ saved xmm31          ] <--- rsp after_call\n-  \/\/     [ saved xmm16-xmm30    ] (EVEX enabled, else the space is blank)\n-  \/\/ -27 [ saved xmm15          ]\n-  \/\/     [ saved xmm7-xmm14     ]\n-  \/\/  -9 [ saved xmm6           ] (each xmm register takes 2 slots)\n-  \/\/  -7 [ saved r15            ]\n-  \/\/  -6 [ saved r14            ]\n-  \/\/  -5 [ saved r13            ]\n-  \/\/  -4 [ saved r12            ]\n-  \/\/  -3 [ saved rdi            ]\n-  \/\/  -2 [ saved rsi            ]\n-  \/\/  -1 [ saved rbx            ]\n-  \/\/   0 [ saved rbp            ] <--- rbp\n-  \/\/   1 [ return address       ]\n-  \/\/   2 [ call wrapper         ]\n-  \/\/   3 [ result               ]\n-  \/\/   4 [ result type          ]\n-  \/\/   5 [ method               ]\n-  \/\/   6 [ entry point          ]\n-  \/\/   7 [ parameters           ]\n-  \/\/   8 [ parameter size       ]\n-  \/\/   9 [ thread               ]\n-  \/\/\n-  \/\/    Windows reserves the callers stack space for arguments 1-4.\n-  \/\/    We spill c_rarg0-c_rarg3 to this space.\n-  \/\/ Call stub stack layout word offsets from rbp\n-  enum call_stub_layout {\n-#ifdef _WIN64\n-    xmm_save_first     = 6,  \/\/ save from xmm6\n-    xmm_save_last      = 31, \/\/ to xmm31\n-    xmm_save_base      = -9,\n-    rsp_after_call_off = xmm_save_base - 2 * (xmm_save_last - xmm_save_first), \/\/ -27\n-    r15_off            = -7,\n-    r14_off            = -6,\n-    r13_off            = -5,\n-    r12_off            = -4,\n-    rdi_off            = -3,\n-    rsi_off            = -2,\n-    rbx_off            = -1,\n-    rbp_off            =  0,\n-    retaddr_off        =  1,\n-    call_wrapper_off   =  2,\n-    result_off         =  3,\n-    result_type_off    =  4,\n-    method_off         =  5,\n-    entry_point_off    =  6,\n-    parameters_off     =  7,\n-    parameter_size_off =  8,\n-    thread_off         =  9\n-#else\n-    rsp_after_call_off = -12,\n-    mxcsr_off          = rsp_after_call_off,\n-    r15_off            = -11,\n-    r14_off            = -10,\n-    r13_off            = -9,\n-    r12_off            = -8,\n-    rbx_off            = -7,\n-    call_wrapper_off   = -6,\n-    result_off         = -5,\n-    result_type_off    = -4,\n-    method_off         = -3,\n-    entry_point_off    = -2,\n-    parameters_off     = -1,\n-    rbp_off            =  0,\n-    retaddr_off        =  1,\n-    parameter_size_off =  2,\n-    thread_off         =  3\n-#endif\n-  };\n+\/\/\n+\/\/ Linux Arguments:\n+\/\/    c_rarg0:   call wrapper address                   address\n+\/\/    c_rarg1:   result                                 address\n+\/\/    c_rarg2:   result type                            BasicType\n+\/\/    c_rarg3:   method                                 Method*\n+\/\/    c_rarg4:   (interpreter) entry point              address\n+\/\/    c_rarg5:   parameters                             intptr_t*\n+\/\/    16(rbp): parameter size (in words)              int\n+\/\/    24(rbp): thread                                 Thread*\n+\/\/\n+\/\/     [ return_from_Java     ] <--- rsp\n+\/\/     [ argument word n      ]\n+\/\/      ...\n+\/\/ -12 [ argument word 1      ]\n+\/\/ -11 [ saved r15            ] <--- rsp_after_call\n+\/\/ -10 [ saved r14            ]\n+\/\/  -9 [ saved r13            ]\n+\/\/  -8 [ saved r12            ]\n+\/\/  -7 [ saved rbx            ]\n+\/\/  -6 [ call wrapper         ]\n+\/\/  -5 [ result               ]\n+\/\/  -4 [ result type          ]\n+\/\/  -3 [ method               ]\n+\/\/  -2 [ entry point          ]\n+\/\/  -1 [ parameters           ]\n+\/\/   0 [ saved rbp            ] <--- rbp\n+\/\/   1 [ return address       ]\n+\/\/   2 [ parameter size       ]\n+\/\/   3 [ thread               ]\n+\/\/\n+\/\/ Windows Arguments:\n+\/\/    c_rarg0:   call wrapper address                   address\n+\/\/    c_rarg1:   result                                 address\n+\/\/    c_rarg2:   result type                            BasicType\n+\/\/    c_rarg3:   method                                 Method*\n+\/\/    48(rbp): (interpreter) entry point              address\n+\/\/    56(rbp): parameters                             intptr_t*\n+\/\/    64(rbp): parameter size (in words)              int\n+\/\/    72(rbp): thread                                 Thread*\n+\/\/\n+\/\/     [ return_from_Java     ] <--- rsp\n+\/\/     [ argument word n      ]\n+\/\/      ...\n+\/\/ -60 [ argument word 1      ]\n+\/\/ -59 [ saved xmm31          ] <--- rsp after_call\n+\/\/     [ saved xmm16-xmm30    ] (EVEX enabled, else the space is blank)\n+\/\/ -27 [ saved xmm15          ]\n+\/\/     [ saved xmm7-xmm14     ]\n+\/\/  -9 [ saved xmm6           ] (each xmm register takes 2 slots)\n+\/\/  -7 [ saved r15            ]\n+\/\/  -6 [ saved r14            ]\n+\/\/  -5 [ saved r13            ]\n+\/\/  -4 [ saved r12            ]\n+\/\/  -3 [ saved rdi            ]\n+\/\/  -2 [ saved rsi            ]\n+\/\/  -1 [ saved rbx            ]\n+\/\/   0 [ saved rbp            ] <--- rbp\n+\/\/   1 [ return address       ]\n+\/\/   2 [ call wrapper         ]\n+\/\/   3 [ result               ]\n+\/\/   4 [ result type          ]\n+\/\/   5 [ method               ]\n+\/\/   6 [ entry point          ]\n+\/\/   7 [ parameters           ]\n+\/\/   8 [ parameter size       ]\n+\/\/   9 [ thread               ]\n+\/\/\n+\/\/    Windows reserves the callers stack space for arguments 1-4.\n+\/\/    We spill c_rarg0-c_rarg3 to this space.\n@@ -220,0 +139,1 @@\n+\/\/ Call stub stack layout word offsets from rbp\n@@ -221,38 +141,83 @@\n-  Address xmm_save(int reg) {\n-    assert(reg >= xmm_save_first && reg <= xmm_save_last, \"XMM register number out of range\");\n-    return Address(rbp, (xmm_save_base - (reg - xmm_save_first) * 2) * wordSize);\n-  }\n-#endif\n-\n-  address generate_call_stub(address& return_address) {\n-    assert((int)frame::entry_frame_after_call_words == -(int)rsp_after_call_off + 1 &&\n-           (int)frame::entry_frame_call_wrapper_offset == (int)call_wrapper_off,\n-           \"adjust this code\");\n-    StubCodeMark mark(this, \"StubRoutines\", \"call_stub\");\n-    address start = __ pc();\n-\n-    \/\/ same as in generate_catch_exception()!\n-    const Address rsp_after_call(rbp, rsp_after_call_off * wordSize);\n-\n-    const Address call_wrapper  (rbp, call_wrapper_off   * wordSize);\n-    const Address result        (rbp, result_off         * wordSize);\n-    const Address result_type   (rbp, result_type_off    * wordSize);\n-    const Address method        (rbp, method_off         * wordSize);\n-    const Address entry_point   (rbp, entry_point_off    * wordSize);\n-    const Address parameters    (rbp, parameters_off     * wordSize);\n-    const Address parameter_size(rbp, parameter_size_off * wordSize);\n-\n-    \/\/ same as in generate_catch_exception()!\n-    const Address thread        (rbp, thread_off         * wordSize);\n-\n-    const Address r15_save(rbp, r15_off * wordSize);\n-    const Address r14_save(rbp, r14_off * wordSize);\n-    const Address r13_save(rbp, r13_off * wordSize);\n-    const Address r12_save(rbp, r12_off * wordSize);\n-    const Address rbx_save(rbp, rbx_off * wordSize);\n-\n-    \/\/ stub code\n-    __ enter();\n-    __ subptr(rsp, -rsp_after_call_off * wordSize);\n-\n-    \/\/ save register parameters\n+enum call_stub_layout {\n+  xmm_save_first     = 6,  \/\/ save from xmm6\n+  xmm_save_last      = 31, \/\/ to xmm31\n+  xmm_save_base      = -9,\n+  rsp_after_call_off = xmm_save_base - 2 * (xmm_save_last - xmm_save_first), \/\/ -27\n+  r15_off            = -7,\n+  r14_off            = -6,\n+  r13_off            = -5,\n+  r12_off            = -4,\n+  rdi_off            = -3,\n+  rsi_off            = -2,\n+  rbx_off            = -1,\n+  rbp_off            =  0,\n+  retaddr_off        =  1,\n+  call_wrapper_off   =  2,\n+  result_off         =  3,\n+  result_type_off    =  4,\n+  method_off         =  5,\n+  entry_point_off    =  6,\n+  parameters_off     =  7,\n+  parameter_size_off =  8,\n+  thread_off         =  9\n+};\n+\n+static Address xmm_save(int reg) {\n+  assert(reg >= xmm_save_first && reg <= xmm_save_last, \"XMM register number out of range\");\n+  return Address(rbp, (xmm_save_base - (reg - xmm_save_first) * 2) * wordSize);\n+}\n+#else \/\/ !_WIN64\n+enum call_stub_layout {\n+  rsp_after_call_off = -12,\n+  mxcsr_off          = rsp_after_call_off,\n+  r15_off            = -11,\n+  r14_off            = -10,\n+  r13_off            = -9,\n+  r12_off            = -8,\n+  rbx_off            = -7,\n+  call_wrapper_off   = -6,\n+  result_off         = -5,\n+  result_type_off    = -4,\n+  method_off         = -3,\n+  entry_point_off    = -2,\n+  parameters_off     = -1,\n+  rbp_off            =  0,\n+  retaddr_off        =  1,\n+  parameter_size_off =  2,\n+  thread_off         =  3\n+};\n+#endif \/\/ _WIN64\n+\n+address StubGenerator::generate_call_stub(address& return_address) {\n+\n+  assert((int)frame::entry_frame_after_call_words == -(int)rsp_after_call_off + 1 &&\n+         (int)frame::entry_frame_call_wrapper_offset == (int)call_wrapper_off,\n+         \"adjust this code\");\n+  StubCodeMark mark(this, \"StubRoutines\", \"call_stub\");\n+  address start = __ pc();\n+\n+  \/\/ same as in generate_catch_exception()!\n+  const Address rsp_after_call(rbp, rsp_after_call_off * wordSize);\n+\n+  const Address call_wrapper  (rbp, call_wrapper_off   * wordSize);\n+  const Address result        (rbp, result_off         * wordSize);\n+  const Address result_type   (rbp, result_type_off    * wordSize);\n+  const Address method        (rbp, method_off         * wordSize);\n+  const Address entry_point   (rbp, entry_point_off    * wordSize);\n+  const Address parameters    (rbp, parameters_off     * wordSize);\n+  const Address parameter_size(rbp, parameter_size_off * wordSize);\n+\n+  \/\/ same as in generate_catch_exception()!\n+  const Address thread        (rbp, thread_off         * wordSize);\n+\n+  const Address r15_save(rbp, r15_off * wordSize);\n+  const Address r14_save(rbp, r14_off * wordSize);\n+  const Address r13_save(rbp, r13_off * wordSize);\n+  const Address r12_save(rbp, r12_off * wordSize);\n+  const Address rbx_save(rbp, rbx_off * wordSize);\n+\n+  \/\/ stub code\n+  __ enter();\n+  __ subptr(rsp, -rsp_after_call_off * wordSize);\n+\n+  \/\/ save register parameters\n@@ -260,2 +225,2 @@\n-    __ movptr(parameters,   c_rarg5); \/\/ parameters\n-    __ movptr(entry_point,  c_rarg4); \/\/ entry_point\n+  __ movptr(parameters,   c_rarg5); \/\/ parameters\n+  __ movptr(entry_point,  c_rarg4); \/\/ entry_point\n@@ -264,4 +229,4 @@\n-    __ movptr(method,       c_rarg3); \/\/ method\n-    __ movl(result_type,  c_rarg2);   \/\/ result type\n-    __ movptr(result,       c_rarg1); \/\/ result\n-    __ movptr(call_wrapper, c_rarg0); \/\/ call wrapper\n+  __ movptr(method,       c_rarg3); \/\/ method\n+  __ movl(result_type,  c_rarg2);   \/\/ result type\n+  __ movptr(result,       c_rarg1); \/\/ result\n+  __ movptr(call_wrapper, c_rarg0); \/\/ call wrapper\n@@ -269,6 +234,6 @@\n-    \/\/ save regs belonging to calling function\n-    __ movptr(rbx_save, rbx);\n-    __ movptr(r12_save, r12);\n-    __ movptr(r13_save, r13);\n-    __ movptr(r14_save, r14);\n-    __ movptr(r15_save, r15);\n+  \/\/ save regs belonging to calling function\n+  __ movptr(rbx_save, rbx);\n+  __ movptr(r12_save, r12);\n+  __ movptr(r13_save, r13);\n+  __ movptr(r14_save, r14);\n+  __ movptr(r15_save, r15);\n@@ -277,3 +242,7 @@\n-    int last_reg = 15;\n-    if (UseAVX > 2) {\n-      last_reg = 31;\n+  int last_reg = 15;\n+  if (UseAVX > 2) {\n+    last_reg = 31;\n+  }\n+  if (VM_Version::supports_evex()) {\n+    for (int i = xmm_save_first; i <= last_reg; i++) {\n+      __ vextractf32x4(xmm_save(i), as_XMMRegister(i), 0);\n@@ -281,8 +250,3 @@\n-    if (VM_Version::supports_evex()) {\n-      for (int i = xmm_save_first; i <= last_reg; i++) {\n-        __ vextractf32x4(xmm_save(i), as_XMMRegister(i), 0);\n-      }\n-    } else {\n-      for (int i = xmm_save_first; i <= last_reg; i++) {\n-        __ movdqu(xmm_save(i), as_XMMRegister(i));\n-      }\n+  } else {\n+    for (int i = xmm_save_first; i <= last_reg; i++) {\n+      __ movdqu(xmm_save(i), as_XMMRegister(i));\n@@ -290,0 +254,1 @@\n+  }\n@@ -291,2 +256,2 @@\n-    const Address rdi_save(rbp, rdi_off * wordSize);\n-    const Address rsi_save(rbp, rsi_off * wordSize);\n+  const Address rdi_save(rbp, rdi_off * wordSize);\n+  const Address rsi_save(rbp, rsi_off * wordSize);\n@@ -294,2 +259,2 @@\n-    __ movptr(rsi_save, rsi);\n-    __ movptr(rdi_save, rdi);\n+  __ movptr(rsi_save, rsi);\n+  __ movptr(rdi_save, rdi);\n@@ -297,12 +262,12 @@\n-    const Address mxcsr_save(rbp, mxcsr_off * wordSize);\n-    {\n-      Label skip_ldmx;\n-      __ stmxcsr(mxcsr_save);\n-      __ movl(rax, mxcsr_save);\n-      __ andl(rax, MXCSR_MASK);    \/\/ Only check control and mask bits\n-      ExternalAddress mxcsr_std(StubRoutines::x86::addr_mxcsr_std());\n-      __ cmp32(rax, mxcsr_std);\n-      __ jcc(Assembler::equal, skip_ldmx);\n-      __ ldmxcsr(mxcsr_std);\n-      __ bind(skip_ldmx);\n-    }\n+  const Address mxcsr_save(rbp, mxcsr_off * wordSize);\n+  {\n+    Label skip_ldmx;\n+    __ stmxcsr(mxcsr_save);\n+    __ movl(rax, mxcsr_save);\n+    __ andl(rax, 0xFFC0); \/\/ Mask out any pending exceptions (only check control and mask bits)\n+    ExternalAddress mxcsr_std(StubRoutines::x86::addr_mxcsr_std());\n+    __ cmp32(rax, mxcsr_std, rscratch1);\n+    __ jcc(Assembler::equal, skip_ldmx);\n+    __ ldmxcsr(mxcsr_std, rscratch1);\n+    __ bind(skip_ldmx);\n+  }\n@@ -311,3 +276,3 @@\n-    \/\/ Load up thread register\n-    __ movptr(r15_thread, thread);\n-    __ reinit_heapbase();\n+  \/\/ Load up thread register\n+  __ movptr(r15_thread, thread);\n+  __ reinit_heapbase();\n@@ -316,8 +281,8 @@\n-    \/\/ make sure we have no pending exceptions\n-    {\n-      Label L;\n-      __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), (int32_t)NULL_WORD);\n-      __ jcc(Assembler::equal, L);\n-      __ stop(\"StubRoutines::call_stub: entered with pending exception\");\n-      __ bind(L);\n-    }\n+  \/\/ make sure we have no pending exceptions\n+  {\n+    Label L;\n+    __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), NULL_WORD);\n+    __ jcc(Assembler::equal, L);\n+    __ stop(\"StubRoutines::call_stub: entered with pending exception\");\n+    __ bind(L);\n+  }\n@@ -326,49 +291,49 @@\n-    \/\/ pass parameters if any\n-    BLOCK_COMMENT(\"pass parameters if any\");\n-    Label parameters_done;\n-    __ movl(c_rarg3, parameter_size);\n-    __ testl(c_rarg3, c_rarg3);\n-    __ jcc(Assembler::zero, parameters_done);\n-\n-    Label loop;\n-    __ movptr(c_rarg2, parameters);       \/\/ parameter pointer\n-    __ movl(c_rarg1, c_rarg3);            \/\/ parameter counter is in c_rarg1\n-    __ BIND(loop);\n-    __ movptr(rax, Address(c_rarg2, 0));\/\/ get parameter\n-    __ addptr(c_rarg2, wordSize);       \/\/ advance to next parameter\n-    __ decrementl(c_rarg1);             \/\/ decrement counter\n-    __ push(rax);                       \/\/ pass parameter\n-    __ jcc(Assembler::notZero, loop);\n-\n-    \/\/ call Java function\n-    __ BIND(parameters_done);\n-    __ movptr(rbx, method);             \/\/ get Method*\n-    __ movptr(c_rarg1, entry_point);    \/\/ get entry_point\n-    __ mov(r13, rsp);                   \/\/ set sender sp\n-    BLOCK_COMMENT(\"call Java function\");\n-    __ call(c_rarg1);\n-\n-    BLOCK_COMMENT(\"call_stub_return_address:\");\n-    return_address = __ pc();\n-\n-    \/\/ store result depending on type (everything that is not\n-    \/\/ T_OBJECT, T_LONG, T_FLOAT or T_DOUBLE is treated as T_INT)\n-    __ movptr(c_rarg0, result);\n-    Label is_long, is_float, is_double, exit;\n-    __ movl(c_rarg1, result_type);\n-    __ cmpl(c_rarg1, T_OBJECT);\n-    __ jcc(Assembler::equal, is_long);\n-    __ cmpl(c_rarg1, T_LONG);\n-    __ jcc(Assembler::equal, is_long);\n-    __ cmpl(c_rarg1, T_FLOAT);\n-    __ jcc(Assembler::equal, is_float);\n-    __ cmpl(c_rarg1, T_DOUBLE);\n-    __ jcc(Assembler::equal, is_double);\n-\n-    \/\/ handle T_INT case\n-    __ movl(Address(c_rarg0, 0), rax);\n-\n-    __ BIND(exit);\n-\n-    \/\/ pop parameters\n-    __ lea(rsp, rsp_after_call);\n+  \/\/ pass parameters if any\n+  BLOCK_COMMENT(\"pass parameters if any\");\n+  Label parameters_done;\n+  __ movl(c_rarg3, parameter_size);\n+  __ testl(c_rarg3, c_rarg3);\n+  __ jcc(Assembler::zero, parameters_done);\n+\n+  Label loop;\n+  __ movptr(c_rarg2, parameters);       \/\/ parameter pointer\n+  __ movl(c_rarg1, c_rarg3);            \/\/ parameter counter is in c_rarg1\n+  __ BIND(loop);\n+  __ movptr(rax, Address(c_rarg2, 0));\/\/ get parameter\n+  __ addptr(c_rarg2, wordSize);       \/\/ advance to next parameter\n+  __ decrementl(c_rarg1);             \/\/ decrement counter\n+  __ push(rax);                       \/\/ pass parameter\n+  __ jcc(Assembler::notZero, loop);\n+\n+  \/\/ call Java function\n+  __ BIND(parameters_done);\n+  __ movptr(rbx, method);             \/\/ get Method*\n+  __ movptr(c_rarg1, entry_point);    \/\/ get entry_point\n+  __ mov(r13, rsp);                   \/\/ set sender sp\n+  BLOCK_COMMENT(\"call Java function\");\n+  __ call(c_rarg1);\n+\n+  BLOCK_COMMENT(\"call_stub_return_address:\");\n+  return_address = __ pc();\n+\n+  \/\/ store result depending on type (everything that is not\n+  \/\/ T_OBJECT, T_LONG, T_FLOAT or T_DOUBLE is treated as T_INT)\n+  __ movptr(c_rarg0, result);\n+  Label is_long, is_float, is_double, exit;\n+  __ movl(c_rarg1, result_type);\n+  __ cmpl(c_rarg1, T_OBJECT);\n+  __ jcc(Assembler::equal, is_long);\n+  __ cmpl(c_rarg1, T_LONG);\n+  __ jcc(Assembler::equal, is_long);\n+  __ cmpl(c_rarg1, T_FLOAT);\n+  __ jcc(Assembler::equal, is_float);\n+  __ cmpl(c_rarg1, T_DOUBLE);\n+  __ jcc(Assembler::equal, is_double);\n+\n+  \/\/ handle T_INT case\n+  __ movl(Address(c_rarg0, 0), rax);\n+\n+  __ BIND(exit);\n+\n+  \/\/ pop parameters\n+  __ lea(rsp, rsp_after_call);\n@@ -377,22 +342,22 @@\n-    \/\/ verify that threads correspond\n-    {\n-     Label L1, L2, L3;\n-      __ cmpptr(r15_thread, thread);\n-      __ jcc(Assembler::equal, L1);\n-      __ stop(\"StubRoutines::call_stub: r15_thread is corrupted\");\n-      __ bind(L1);\n-      __ get_thread(rbx);\n-      __ cmpptr(r15_thread, thread);\n-      __ jcc(Assembler::equal, L2);\n-      __ stop(\"StubRoutines::call_stub: r15_thread is modified by call\");\n-      __ bind(L2);\n-      __ cmpptr(r15_thread, rbx);\n-      __ jcc(Assembler::equal, L3);\n-      __ stop(\"StubRoutines::call_stub: threads must correspond\");\n-      __ bind(L3);\n-    }\n-#endif\n-\n-    __ pop_cont_fastpath();\n-\n-    \/\/ restore regs belonging to calling function\n+  \/\/ verify that threads correspond\n+  {\n+   Label L1, L2, L3;\n+    __ cmpptr(r15_thread, thread);\n+    __ jcc(Assembler::equal, L1);\n+    __ stop(\"StubRoutines::call_stub: r15_thread is corrupted\");\n+    __ bind(L1);\n+    __ get_thread(rbx);\n+    __ cmpptr(r15_thread, thread);\n+    __ jcc(Assembler::equal, L2);\n+    __ stop(\"StubRoutines::call_stub: r15_thread is modified by call\");\n+    __ bind(L2);\n+    __ cmpptr(r15_thread, rbx);\n+    __ jcc(Assembler::equal, L3);\n+    __ stop(\"StubRoutines::call_stub: threads must correspond\");\n+    __ bind(L3);\n+  }\n+#endif\n+\n+  __ pop_cont_fastpath();\n+\n+  \/\/ restore regs belonging to calling function\n@@ -400,9 +365,8 @@\n-    \/\/ emit the restores for xmm regs\n-    if (VM_Version::supports_evex()) {\n-      for (int i = xmm_save_first; i <= last_reg; i++) {\n-        __ vinsertf32x4(as_XMMRegister(i), as_XMMRegister(i), xmm_save(i), 0);\n-      }\n-    } else {\n-      for (int i = xmm_save_first; i <= last_reg; i++) {\n-        __ movdqu(as_XMMRegister(i), xmm_save(i));\n-      }\n+  \/\/ emit the restores for xmm regs\n+  if (VM_Version::supports_evex()) {\n+    for (int i = xmm_save_first; i <= last_reg; i++) {\n+      __ vinsertf32x4(as_XMMRegister(i), as_XMMRegister(i), xmm_save(i), 0);\n+    }\n+  } else {\n+    for (int i = xmm_save_first; i <= last_reg; i++) {\n+      __ movdqu(as_XMMRegister(i), xmm_save(i));\n@@ -410,0 +374,1 @@\n+  }\n@@ -411,5 +376,5 @@\n-    __ movptr(r15, r15_save);\n-    __ movptr(r14, r14_save);\n-    __ movptr(r13, r13_save);\n-    __ movptr(r12, r12_save);\n-    __ movptr(rbx, rbx_save);\n+  __ movptr(r15, r15_save);\n+  __ movptr(r14, r14_save);\n+  __ movptr(r13, r13_save);\n+  __ movptr(r12, r12_save);\n+  __ movptr(rbx, rbx_save);\n@@ -418,2 +383,2 @@\n-    __ movptr(rdi, rdi_save);\n-    __ movptr(rsi, rsi_save);\n+  __ movptr(rdi, rdi_save);\n+  __ movptr(rsi, rsi_save);\n@@ -421,1 +386,1 @@\n-    __ ldmxcsr(mxcsr_save);\n+  __ ldmxcsr(mxcsr_save);\n@@ -424,2 +389,2 @@\n-    \/\/ restore rsp\n-    __ addptr(rsp, -rsp_after_call_off * wordSize);\n+  \/\/ restore rsp\n+  __ addptr(rsp, -rsp_after_call_off * wordSize);\n@@ -427,4 +392,4 @@\n-    \/\/ return\n-    __ vzeroupper();\n-    __ pop(rbp);\n-    __ ret(0);\n+  \/\/ return\n+  __ vzeroupper();\n+  __ pop(rbp);\n+  __ ret(0);\n@@ -432,4 +397,4 @@\n-    \/\/ handle return types different from T_INT\n-    __ BIND(is_long);\n-    __ movq(Address(c_rarg0, 0), rax);\n-    __ jmp(exit);\n+  \/\/ handle return types different from T_INT\n+  __ BIND(is_long);\n+  __ movq(Address(c_rarg0, 0), rax);\n+  __ jmp(exit);\n@@ -437,3 +402,3 @@\n-    __ BIND(is_float);\n-    __ movflt(Address(c_rarg0, 0), xmm0);\n-    __ jmp(exit);\n+  __ BIND(is_float);\n+  __ movflt(Address(c_rarg0, 0), xmm0);\n+  __ jmp(exit);\n@@ -441,3 +406,3 @@\n-    __ BIND(is_double);\n-    __ movdbl(Address(c_rarg0, 0), xmm0);\n-    __ jmp(exit);\n+  __ BIND(is_double);\n+  __ movdbl(Address(c_rarg0, 0), xmm0);\n+  __ jmp(exit);\n@@ -445,2 +410,2 @@\n-    return start;\n-  }\n+  return start;\n+}\n@@ -448,11 +413,11 @@\n-  \/\/ Return point for a Java call if there's an exception thrown in\n-  \/\/ Java code.  The exception is caught and transformed into a\n-  \/\/ pending exception stored in JavaThread that can be tested from\n-  \/\/ within the VM.\n-  \/\/\n-  \/\/ Note: Usually the parameters are removed by the callee. In case\n-  \/\/ of an exception crossing an activation frame boundary, that is\n-  \/\/ not the case if the callee is compiled code => need to setup the\n-  \/\/ rsp.\n-  \/\/\n-  \/\/ rax: exception oop\n+\/\/ Return point for a Java call if there's an exception thrown in\n+\/\/ Java code.  The exception is caught and transformed into a\n+\/\/ pending exception stored in JavaThread that can be tested from\n+\/\/ within the VM.\n+\/\/\n+\/\/ Note: Usually the parameters are removed by the callee. In case\n+\/\/ of an exception crossing an activation frame boundary, that is\n+\/\/ not the case if the callee is compiled code => need to setup the\n+\/\/ rsp.\n+\/\/\n+\/\/ rax: exception oop\n@@ -460,3 +425,3 @@\n-  address generate_catch_exception() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"catch_exception\");\n-    address start = __ pc();\n+address StubGenerator::generate_catch_exception() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"catch_exception\");\n+  address start = __ pc();\n@@ -464,3 +429,3 @@\n-    \/\/ same as in generate_call_stub():\n-    const Address rsp_after_call(rbp, rsp_after_call_off * wordSize);\n-    const Address thread        (rbp, thread_off         * wordSize);\n+  \/\/ same as in generate_call_stub():\n+  const Address rsp_after_call(rbp, rsp_after_call_off * wordSize);\n+  const Address thread        (rbp, thread_off         * wordSize);\n@@ -469,21 +434,31 @@\n-    \/\/ verify that threads correspond\n-    {\n-      Label L1, L2, L3;\n-      __ cmpptr(r15_thread, thread);\n-      __ jcc(Assembler::equal, L1);\n-      __ stop(\"StubRoutines::catch_exception: r15_thread is corrupted\");\n-      __ bind(L1);\n-      __ get_thread(rbx);\n-      __ cmpptr(r15_thread, thread);\n-      __ jcc(Assembler::equal, L2);\n-      __ stop(\"StubRoutines::catch_exception: r15_thread is modified by call\");\n-      __ bind(L2);\n-      __ cmpptr(r15_thread, rbx);\n-      __ jcc(Assembler::equal, L3);\n-      __ stop(\"StubRoutines::catch_exception: threads must correspond\");\n-      __ bind(L3);\n-    }\n-#endif\n-\n-    \/\/ set pending exception\n-    __ verify_oop(rax);\n+  \/\/ verify that threads correspond\n+  {\n+    Label L1, L2, L3;\n+    __ cmpptr(r15_thread, thread);\n+    __ jcc(Assembler::equal, L1);\n+    __ stop(\"StubRoutines::catch_exception: r15_thread is corrupted\");\n+    __ bind(L1);\n+    __ get_thread(rbx);\n+    __ cmpptr(r15_thread, thread);\n+    __ jcc(Assembler::equal, L2);\n+    __ stop(\"StubRoutines::catch_exception: r15_thread is modified by call\");\n+    __ bind(L2);\n+    __ cmpptr(r15_thread, rbx);\n+    __ jcc(Assembler::equal, L3);\n+    __ stop(\"StubRoutines::catch_exception: threads must correspond\");\n+    __ bind(L3);\n+  }\n+#endif\n+\n+  \/\/ set pending exception\n+  __ verify_oop(rax);\n+\n+  __ movptr(Address(r15_thread, Thread::pending_exception_offset()), rax);\n+  __ lea(rscratch1, ExternalAddress((address)__FILE__));\n+  __ movptr(Address(r15_thread, Thread::exception_file_offset()), rscratch1);\n+  __ movl(Address(r15_thread, Thread::exception_line_offset()), (int)  __LINE__);\n+\n+  \/\/ complete return to VM\n+  assert(StubRoutines::_call_stub_return_address != NULL,\n+         \"_call_stub_return_address must have been generated before\");\n+  __ jump(RuntimeAddress(StubRoutines::_call_stub_return_address));\n@@ -491,4 +466,2 @@\n-    __ movptr(Address(r15_thread, Thread::pending_exception_offset()), rax);\n-    __ lea(rscratch1, ExternalAddress((address)__FILE__));\n-    __ movptr(Address(r15_thread, Thread::exception_file_offset()), rscratch1);\n-    __ movl(Address(r15_thread, Thread::exception_line_offset()), (int)  __LINE__);\n+  return start;\n+}\n@@ -496,4 +469,10 @@\n-    \/\/ complete return to VM\n-    assert(StubRoutines::_call_stub_return_address != NULL,\n-           \"_call_stub_return_address must have been generated before\");\n-    __ jump(RuntimeAddress(StubRoutines::_call_stub_return_address));\n+\/\/ Continuation point for runtime calls returning with a pending\n+\/\/ exception.  The pending exception check happened in the runtime\n+\/\/ or native call stub.  The pending exception in Thread is\n+\/\/ converted into a Java-level exception.\n+\/\/\n+\/\/ Contract with Java-level exception handlers:\n+\/\/ rax: exception\n+\/\/ rdx: throwing pc\n+\/\/\n+\/\/ NOTE: At entry of this stub, exception-pc must be on stack !!\n@@ -501,2 +480,3 @@\n-    return start;\n-  }\n+address StubGenerator::generate_forward_exception() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"forward exception\");\n+  address start = __ pc();\n@@ -504,8 +484,3 @@\n-  \/\/ Continuation point for runtime calls returning with a pending\n-  \/\/ exception.  The pending exception check happened in the runtime\n-  \/\/ or native call stub.  The pending exception in Thread is\n-  \/\/ converted into a Java-level exception.\n-  \/\/\n-  \/\/ Contract with Java-level exception handlers:\n-  \/\/ rax: exception\n-  \/\/ rdx: throwing pc\n+  \/\/ Upon entry, the sp points to the return address returning into\n+  \/\/ Java (interpreted or compiled) code; i.e., the return address\n+  \/\/ becomes the throwing pc.\n@@ -513,14 +488,4 @@\n-  \/\/ NOTE: At entry of this stub, exception-pc must be on stack !!\n-\n-  address generate_forward_exception() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"forward exception\");\n-    address start = __ pc();\n-\n-    \/\/ Upon entry, the sp points to the return address returning into\n-    \/\/ Java (interpreted or compiled) code; i.e., the return address\n-    \/\/ becomes the throwing pc.\n-    \/\/\n-    \/\/ Arguments pushed before the runtime call are still on the stack\n-    \/\/ but the exception handler will reset the stack pointer ->\n-    \/\/ ignore them.  A potential result in registers can be ignored as\n-    \/\/ well.\n+  \/\/ Arguments pushed before the runtime call are still on the stack\n+  \/\/ but the exception handler will reset the stack pointer ->\n+  \/\/ ignore them.  A potential result in registers can be ignored as\n+  \/\/ well.\n@@ -529,8 +494,8 @@\n-    \/\/ make sure this code is only executed if there is a pending exception\n-    {\n-      Label L;\n-      __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), (int32_t) NULL);\n-      __ jcc(Assembler::notEqual, L);\n-      __ stop(\"StubRoutines::forward exception: no pending exception (1)\");\n-      __ bind(L);\n-    }\n+  \/\/ make sure this code is only executed if there is a pending exception\n+  {\n+    Label L;\n+    __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), NULL_WORD);\n+    __ jcc(Assembler::notEqual, L);\n+    __ stop(\"StubRoutines::forward exception: no pending exception (1)\");\n+    __ bind(L);\n+  }\n@@ -539,7 +504,7 @@\n-    \/\/ compute exception handler into rbx\n-    __ movptr(c_rarg0, Address(rsp, 0));\n-    BLOCK_COMMENT(\"call exception_handler_for_return_address\");\n-    __ call_VM_leaf(CAST_FROM_FN_PTR(address,\n-                         SharedRuntime::exception_handler_for_return_address),\n-                    r15_thread, c_rarg0);\n-    __ mov(rbx, rax);\n+  \/\/ compute exception handler into rbx\n+  __ movptr(c_rarg0, Address(rsp, 0));\n+  BLOCK_COMMENT(\"call exception_handler_for_return_address\");\n+  __ call_VM_leaf(CAST_FROM_FN_PTR(address,\n+                       SharedRuntime::exception_handler_for_return_address),\n+                  r15_thread, c_rarg0);\n+  __ mov(rbx, rax);\n@@ -547,4 +512,4 @@\n-    \/\/ setup rax & rdx, remove return address & clear pending exception\n-    __ pop(rdx);\n-    __ movptr(rax, Address(r15_thread, Thread::pending_exception_offset()));\n-    __ movptr(Address(r15_thread, Thread::pending_exception_offset()), (int32_t)NULL_WORD);\n+  \/\/ setup rax & rdx, remove return address & clear pending exception\n+  __ pop(rdx);\n+  __ movptr(rax, Address(r15_thread, Thread::pending_exception_offset()));\n+  __ movptr(Address(r15_thread, Thread::pending_exception_offset()), NULL_WORD);\n@@ -553,8 +518,8 @@\n-    \/\/ make sure exception is set\n-    {\n-      Label L;\n-      __ testptr(rax, rax);\n-      __ jcc(Assembler::notEqual, L);\n-      __ stop(\"StubRoutines::forward exception: no pending exception (2)\");\n-      __ bind(L);\n-    }\n+  \/\/ make sure exception is set\n+  {\n+    Label L;\n+    __ testptr(rax, rax);\n+    __ jcc(Assembler::notEqual, L);\n+    __ stop(\"StubRoutines::forward exception: no pending exception (2)\");\n+    __ bind(L);\n+  }\n@@ -563,9 +528,6 @@\n-    \/\/ continue at exception handler (return address removed)\n-    \/\/ rax: exception\n-    \/\/ rbx: exception handler\n-    \/\/ rdx: throwing pc\n-    __ verify_oop(rax);\n-    __ jmp(rbx);\n-\n-    return start;\n-  }\n+  \/\/ continue at exception handler (return address removed)\n+  \/\/ rax: exception\n+  \/\/ rbx: exception handler\n+  \/\/ rdx: throwing pc\n+  __ verify_oop(rax);\n+  __ jmp(rbx);\n@@ -573,10 +535,2 @@\n-  \/\/ Support for intptr_t OrderAccess::fence()\n-  \/\/\n-  \/\/ Arguments :\n-  \/\/\n-  \/\/ Result:\n-  address generate_orderaccess_fence() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"orderaccess_fence\");\n-    address start = __ pc();\n-    __ membar(Assembler::StoreLoad);\n-    __ ret(0);\n+  return start;\n+}\n@@ -584,2 +538,8 @@\n-    return start;\n-  }\n+\/\/ Support for intptr_t OrderAccess::fence()\n+\/\/\n+\/\/ Arguments :\n+\/\/\n+\/\/ Result:\n+address StubGenerator::generate_orderaccess_fence() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"orderaccess_fence\");\n+  address start = __ pc();\n@@ -587,0 +547,2 @@\n+  __ membar(Assembler::StoreLoad);\n+  __ ret(0);\n@@ -588,11 +550,2 @@\n-  \/\/ Support for intptr_t get_previous_sp()\n-  \/\/\n-  \/\/ This routine is used to find the previous stack pointer for the\n-  \/\/ caller.\n-  address generate_get_previous_sp() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"get_previous_sp\");\n-    address start = __ pc();\n-\n-    __ movptr(rax, rsp);\n-    __ addptr(rax, 8); \/\/ return address is at the top of the stack.\n-    __ ret(0);\n+  return start;\n+}\n@@ -600,34 +553,7 @@\n-    return start;\n-  }\n-  \/\/----------------------------------------------------------------------------------------------------\n-  \/\/ Support for void verify_mxcsr()\n-  \/\/\n-  \/\/ This routine is used with -Xcheck:jni to verify that native\n-  \/\/ JNI code does not return to Java code without restoring the\n-  \/\/ MXCSR register to our expected state.\n-\n-  address generate_verify_mxcsr() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"verify_mxcsr\");\n-    address start = __ pc();\n-\n-    const Address mxcsr_save(rsp, 0);\n-\n-    if (CheckJNICalls) {\n-      Label ok_ret;\n-      ExternalAddress mxcsr_std(StubRoutines::x86::addr_mxcsr_std());\n-      __ push(rax);\n-      __ subptr(rsp, wordSize);      \/\/ allocate a temp location\n-      __ stmxcsr(mxcsr_save);\n-      __ movl(rax, mxcsr_save);\n-      __ andl(rax, MXCSR_MASK);    \/\/ Only check control and mask bits\n-      __ cmp32(rax, mxcsr_std);\n-      __ jcc(Assembler::equal, ok_ret);\n-\n-      __ warn(\"MXCSR changed by native JNI code, use -XX:+RestoreMXCSROnJNICall\");\n-\n-      __ ldmxcsr(mxcsr_std);\n-\n-      __ bind(ok_ret);\n-      __ addptr(rsp, wordSize);\n-      __ pop(rax);\n-    }\n+\/\/ Support for intptr_t get_previous_sp()\n+\/\/\n+\/\/ This routine is used to find the previous stack pointer for the\n+\/\/ caller.\n+address StubGenerator::generate_get_previous_sp() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"get_previous_sp\");\n+  address start = __ pc();\n@@ -636,1 +562,3 @@\n-    __ ret(0);\n+  __ movptr(rax, rsp);\n+  __ addptr(rax, 8); \/\/ return address is at the top of the stack.\n+  __ ret(0);\n@@ -638,2 +566,2 @@\n-    return start;\n-  }\n+  return start;\n+}\n@@ -641,3 +569,6 @@\n-  address generate_f2i_fixup() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"f2i_fixup\");\n-    Address inout(rsp, 5 * wordSize); \/\/ return address + 4 saves\n+\/\/----------------------------------------------------------------------------------------------------\n+\/\/ Support for void verify_mxcsr()\n+\/\/\n+\/\/ This routine is used with -Xcheck:jni to verify that native\n+\/\/ JNI code does not return to Java code without restoring the\n+\/\/ MXCSR register to our expected state.\n@@ -645,1 +576,3 @@\n-    address start = __ pc();\n+address StubGenerator::generate_verify_mxcsr() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"verify_mxcsr\");\n+  address start = __ pc();\n@@ -647,1 +580,1 @@\n-    Label L;\n+  const Address mxcsr_save(rsp, 0);\n@@ -649,0 +582,3 @@\n+  if (CheckJNICalls) {\n+    Label ok_ret;\n+    ExternalAddress mxcsr_std(StubRoutines::x86::addr_mxcsr_std());\n@@ -650,15 +586,6 @@\n-    __ push(c_rarg3);\n-    __ push(c_rarg2);\n-    __ push(c_rarg1);\n-\n-    __ movl(rax, 0x7f800000);\n-    __ xorl(c_rarg3, c_rarg3);\n-    __ movl(c_rarg2, inout);\n-    __ movl(c_rarg1, c_rarg2);\n-    __ andl(c_rarg1, 0x7fffffff);\n-    __ cmpl(rax, c_rarg1); \/\/ NaN? -> 0\n-    __ jcc(Assembler::negative, L);\n-    __ testl(c_rarg2, c_rarg2); \/\/ signed ? min_jint : max_jint\n-    __ movl(c_rarg3, 0x80000000);\n-    __ movl(rax, 0x7fffffff);\n-    __ cmovl(Assembler::positive, c_rarg3, rax);\n+    __ subptr(rsp, wordSize);      \/\/ allocate a temp location\n+    __ stmxcsr(mxcsr_save);\n+    __ movl(rax, mxcsr_save);\n+    __ andl(rax, 0xFFC0); \/\/ Mask out any pending exceptions (only check control and mask bits)\n+    __ cmp32(rax, mxcsr_std, rscratch1);\n+    __ jcc(Assembler::equal, ok_ret);\n@@ -666,2 +593,1 @@\n-    __ bind(L);\n-    __ movptr(inout, c_rarg3);\n+    __ warn(\"MXCSR changed by native JNI code, use -XX:+RestoreMXCSROnJNICall\");\n@@ -669,3 +595,4 @@\n-    __ pop(c_rarg1);\n-    __ pop(c_rarg2);\n-    __ pop(c_rarg3);\n+    __ ldmxcsr(mxcsr_std, rscratch1);\n+\n+    __ bind(ok_ret);\n+    __ addptr(rsp, wordSize);\n@@ -673,0 +600,1 @@\n+  }\n@@ -674,1 +602,1 @@\n-    __ ret(0);\n+  __ ret(0);\n@@ -676,2 +604,2 @@\n-    return start;\n-  }\n+  return start;\n+}\n@@ -679,4 +607,3 @@\n-  address generate_f2l_fixup() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"f2l_fixup\");\n-    Address inout(rsp, 5 * wordSize); \/\/ return address + 4 saves\n-    address start = __ pc();\n+address StubGenerator::generate_f2i_fixup() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"f2i_fixup\");\n+  Address inout(rsp, 5 * wordSize); \/\/ return address + 4 saves\n@@ -684,1 +611,1 @@\n-    Label L;\n+  address start = __ pc();\n@@ -686,16 +613,1 @@\n-    __ push(rax);\n-    __ push(c_rarg3);\n-    __ push(c_rarg2);\n-    __ push(c_rarg1);\n-\n-    __ movl(rax, 0x7f800000);\n-    __ xorl(c_rarg3, c_rarg3);\n-    __ movl(c_rarg2, inout);\n-    __ movl(c_rarg1, c_rarg2);\n-    __ andl(c_rarg1, 0x7fffffff);\n-    __ cmpl(rax, c_rarg1); \/\/ NaN? -> 0\n-    __ jcc(Assembler::negative, L);\n-    __ testl(c_rarg2, c_rarg2); \/\/ signed ? min_jlong : max_jlong\n-    __ mov64(c_rarg3, 0x8000000000000000);\n-    __ mov64(rax, 0x7fffffffffffffff);\n-    __ cmov(Assembler::positive, c_rarg3, rax);\n+  Label L;\n@@ -703,2 +615,4 @@\n-    __ bind(L);\n-    __ movptr(inout, c_rarg3);\n+  __ push(rax);\n+  __ push(c_rarg3);\n+  __ push(c_rarg2);\n+  __ push(c_rarg1);\n@@ -706,4 +620,11 @@\n-    __ pop(c_rarg1);\n-    __ pop(c_rarg2);\n-    __ pop(c_rarg3);\n-    __ pop(rax);\n+  __ movl(rax, 0x7f800000);\n+  __ xorl(c_rarg3, c_rarg3);\n+  __ movl(c_rarg2, inout);\n+  __ movl(c_rarg1, c_rarg2);\n+  __ andl(c_rarg1, 0x7fffffff);\n+  __ cmpl(rax, c_rarg1); \/\/ NaN? -> 0\n+  __ jcc(Assembler::negative, L);\n+  __ testl(c_rarg2, c_rarg2); \/\/ signed ? min_jint : max_jint\n+  __ movl(c_rarg3, 0x80000000);\n+  __ movl(rax, 0x7fffffff);\n+  __ cmovl(Assembler::positive, c_rarg3, rax);\n@@ -711,1 +632,2 @@\n-    __ ret(0);\n+  __ bind(L);\n+  __ movptr(inout, c_rarg3);\n@@ -713,2 +635,4 @@\n-    return start;\n-  }\n+  __ pop(c_rarg1);\n+  __ pop(c_rarg2);\n+  __ pop(c_rarg3);\n+  __ pop(rax);\n@@ -716,3 +640,1 @@\n-  address generate_d2i_fixup() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"d2i_fixup\");\n-    Address inout(rsp, 6 * wordSize); \/\/ return address + 5 saves\n+  __ ret(0);\n@@ -720,1 +642,2 @@\n-    address start = __ pc();\n+  return start;\n+}\n@@ -722,1 +645,4 @@\n-    Label L;\n+address StubGenerator::generate_f2l_fixup() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"f2l_fixup\");\n+  Address inout(rsp, 5 * wordSize); \/\/ return address + 4 saves\n+  address start = __ pc();\n@@ -724,24 +650,1 @@\n-    __ push(rax);\n-    __ push(c_rarg3);\n-    __ push(c_rarg2);\n-    __ push(c_rarg1);\n-    __ push(c_rarg0);\n-\n-    __ movl(rax, 0x7ff00000);\n-    __ movq(c_rarg2, inout);\n-    __ movl(c_rarg3, c_rarg2);\n-    __ mov(c_rarg1, c_rarg2);\n-    __ mov(c_rarg0, c_rarg2);\n-    __ negl(c_rarg3);\n-    __ shrptr(c_rarg1, 0x20);\n-    __ orl(c_rarg3, c_rarg2);\n-    __ andl(c_rarg1, 0x7fffffff);\n-    __ xorl(c_rarg2, c_rarg2);\n-    __ shrl(c_rarg3, 0x1f);\n-    __ orl(c_rarg1, c_rarg3);\n-    __ cmpl(rax, c_rarg1);\n-    __ jcc(Assembler::negative, L); \/\/ NaN -> 0\n-    __ testptr(c_rarg0, c_rarg0); \/\/ signed ? min_jint : max_jint\n-    __ movl(c_rarg2, 0x80000000);\n-    __ movl(rax, 0x7fffffff);\n-    __ cmov(Assembler::positive, c_rarg2, rax);\n+  Label L;\n@@ -749,2 +652,4 @@\n-    __ bind(L);\n-    __ movptr(inout, c_rarg2);\n+  __ push(rax);\n+  __ push(c_rarg3);\n+  __ push(c_rarg2);\n+  __ push(c_rarg1);\n@@ -752,5 +657,11 @@\n-    __ pop(c_rarg0);\n-    __ pop(c_rarg1);\n-    __ pop(c_rarg2);\n-    __ pop(c_rarg3);\n-    __ pop(rax);\n+  __ movl(rax, 0x7f800000);\n+  __ xorl(c_rarg3, c_rarg3);\n+  __ movl(c_rarg2, inout);\n+  __ movl(c_rarg1, c_rarg2);\n+  __ andl(c_rarg1, 0x7fffffff);\n+  __ cmpl(rax, c_rarg1); \/\/ NaN? -> 0\n+  __ jcc(Assembler::negative, L);\n+  __ testl(c_rarg2, c_rarg2); \/\/ signed ? min_jlong : max_jlong\n+  __ mov64(c_rarg3, 0x8000000000000000);\n+  __ mov64(rax, 0x7fffffffffffffff);\n+  __ cmov(Assembler::positive, c_rarg3, rax);\n@@ -758,1 +669,2 @@\n-    __ ret(0);\n+  __ bind(L);\n+  __ movptr(inout, c_rarg3);\n@@ -760,2 +672,4 @@\n-    return start;\n-  }\n+  __ pop(c_rarg1);\n+  __ pop(c_rarg2);\n+  __ pop(c_rarg3);\n+  __ pop(rax);\n@@ -763,3 +677,1 @@\n-  address generate_d2l_fixup() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"d2l_fixup\");\n-    Address inout(rsp, 6 * wordSize); \/\/ return address + 5 saves\n+  __ ret(0);\n@@ -767,1 +679,2 @@\n-    address start = __ pc();\n+  return start;\n+}\n@@ -769,1 +682,43 @@\n-    Label L;\n+address StubGenerator::generate_d2i_fixup() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"d2i_fixup\");\n+  Address inout(rsp, 6 * wordSize); \/\/ return address + 5 saves\n+\n+  address start = __ pc();\n+\n+  Label L;\n+\n+  __ push(rax);\n+  __ push(c_rarg3);\n+  __ push(c_rarg2);\n+  __ push(c_rarg1);\n+  __ push(c_rarg0);\n+\n+  __ movl(rax, 0x7ff00000);\n+  __ movq(c_rarg2, inout);\n+  __ movl(c_rarg3, c_rarg2);\n+  __ mov(c_rarg1, c_rarg2);\n+  __ mov(c_rarg0, c_rarg2);\n+  __ negl(c_rarg3);\n+  __ shrptr(c_rarg1, 0x20);\n+  __ orl(c_rarg3, c_rarg2);\n+  __ andl(c_rarg1, 0x7fffffff);\n+  __ xorl(c_rarg2, c_rarg2);\n+  __ shrl(c_rarg3, 0x1f);\n+  __ orl(c_rarg1, c_rarg3);\n+  __ cmpl(rax, c_rarg1);\n+  __ jcc(Assembler::negative, L); \/\/ NaN -> 0\n+  __ testptr(c_rarg0, c_rarg0); \/\/ signed ? min_jint : max_jint\n+  __ movl(c_rarg2, 0x80000000);\n+  __ movl(rax, 0x7fffffff);\n+  __ cmov(Assembler::positive, c_rarg2, rax);\n+\n+  __ bind(L);\n+  __ movptr(inout, c_rarg2);\n+\n+  __ pop(c_rarg0);\n+  __ pop(c_rarg1);\n+  __ pop(c_rarg2);\n+  __ pop(c_rarg3);\n+  __ pop(rax);\n+\n+  __ ret(0);\n@@ -771,24 +726,2 @@\n-    __ push(rax);\n-    __ push(c_rarg3);\n-    __ push(c_rarg2);\n-    __ push(c_rarg1);\n-    __ push(c_rarg0);\n-\n-    __ movl(rax, 0x7ff00000);\n-    __ movq(c_rarg2, inout);\n-    __ movl(c_rarg3, c_rarg2);\n-    __ mov(c_rarg1, c_rarg2);\n-    __ mov(c_rarg0, c_rarg2);\n-    __ negl(c_rarg3);\n-    __ shrptr(c_rarg1, 0x20);\n-    __ orl(c_rarg3, c_rarg2);\n-    __ andl(c_rarg1, 0x7fffffff);\n-    __ xorl(c_rarg2, c_rarg2);\n-    __ shrl(c_rarg3, 0x1f);\n-    __ orl(c_rarg1, c_rarg3);\n-    __ cmpl(rax, c_rarg1);\n-    __ jcc(Assembler::negative, L); \/\/ NaN -> 0\n-    __ testq(c_rarg0, c_rarg0); \/\/ signed ? min_jlong : max_jlong\n-    __ mov64(c_rarg2, 0x8000000000000000);\n-    __ mov64(rax, 0x7fffffffffffffff);\n-    __ cmovq(Assembler::positive, c_rarg2, rax);\n+  return start;\n+}\n@@ -796,2 +729,43 @@\n-    __ bind(L);\n-    __ movq(inout, c_rarg2);\n+address StubGenerator::generate_d2l_fixup() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"d2l_fixup\");\n+  Address inout(rsp, 6 * wordSize); \/\/ return address + 5 saves\n+\n+  address start = __ pc();\n+\n+  Label L;\n+\n+  __ push(rax);\n+  __ push(c_rarg3);\n+  __ push(c_rarg2);\n+  __ push(c_rarg1);\n+  __ push(c_rarg0);\n+\n+  __ movl(rax, 0x7ff00000);\n+  __ movq(c_rarg2, inout);\n+  __ movl(c_rarg3, c_rarg2);\n+  __ mov(c_rarg1, c_rarg2);\n+  __ mov(c_rarg0, c_rarg2);\n+  __ negl(c_rarg3);\n+  __ shrptr(c_rarg1, 0x20);\n+  __ orl(c_rarg3, c_rarg2);\n+  __ andl(c_rarg1, 0x7fffffff);\n+  __ xorl(c_rarg2, c_rarg2);\n+  __ shrl(c_rarg3, 0x1f);\n+  __ orl(c_rarg1, c_rarg3);\n+  __ cmpl(rax, c_rarg1);\n+  __ jcc(Assembler::negative, L); \/\/ NaN -> 0\n+  __ testq(c_rarg0, c_rarg0); \/\/ signed ? min_jlong : max_jlong\n+  __ mov64(c_rarg2, 0x8000000000000000);\n+  __ mov64(rax, 0x7fffffffffffffff);\n+  __ cmovq(Assembler::positive, c_rarg2, rax);\n+\n+  __ bind(L);\n+  __ movq(inout, c_rarg2);\n+\n+  __ pop(c_rarg0);\n+  __ pop(c_rarg1);\n+  __ pop(c_rarg2);\n+  __ pop(c_rarg3);\n+  __ pop(rax);\n+\n+  __ ret(0);\n@@ -799,5 +773,2 @@\n-    __ pop(c_rarg0);\n-    __ pop(c_rarg1);\n-    __ pop(c_rarg2);\n-    __ pop(c_rarg3);\n-    __ pop(rax);\n+  return start;\n+}\n@@ -805,1 +776,4 @@\n-    __ ret(0);\n+address StubGenerator::generate_count_leading_zeros_lut(const char *stub_name) {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n@@ -807,2 +781,8 @@\n-    return start;\n-  }\n+  __ emit_data64(0x0101010102020304, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0101010102020304, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0101010102020304, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0101010102020304, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n@@ -810,14 +790,2 @@\n-  address generate_count_leading_zeros_lut(const char *stub_name) {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-    __ emit_data64(0x0101010102020304, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0101010102020304, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0101010102020304, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0101010102020304, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    return start;\n-  }\n+  return start;\n+}\n@@ -825,14 +793,4 @@\n-  address generate_popcount_avx_lut(const char *stub_name) {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-    __ emit_data64(0x0302020102010100, relocInfo::none);\n-    __ emit_data64(0x0403030203020201, relocInfo::none);\n-    __ emit_data64(0x0302020102010100, relocInfo::none);\n-    __ emit_data64(0x0403030203020201, relocInfo::none);\n-    __ emit_data64(0x0302020102010100, relocInfo::none);\n-    __ emit_data64(0x0403030203020201, relocInfo::none);\n-    __ emit_data64(0x0302020102010100, relocInfo::none);\n-    __ emit_data64(0x0403030203020201, relocInfo::none);\n-    return start;\n-  }\n+address StubGenerator::generate_popcount_avx_lut(const char *stub_name) {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n@@ -840,14 +798,8 @@\n-  address generate_iota_indices(const char *stub_name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-    __ emit_data64(0x0706050403020100, relocInfo::none);\n-    __ emit_data64(0x0F0E0D0C0B0A0908, relocInfo::none);\n-    __ emit_data64(0x1716151413121110, relocInfo::none);\n-    __ emit_data64(0x1F1E1D1C1B1A1918, relocInfo::none);\n-    __ emit_data64(0x2726252423222120, relocInfo::none);\n-    __ emit_data64(0x2F2E2D2C2B2A2928, relocInfo::none);\n-    __ emit_data64(0x3736353433323130, relocInfo::none);\n-    __ emit_data64(0x3F3E3D3C3B3A3938, relocInfo::none);\n-    return start;\n-  }\n+  __ emit_data64(0x0302020102010100, relocInfo::none);\n+  __ emit_data64(0x0403030203020201, relocInfo::none);\n+  __ emit_data64(0x0302020102010100, relocInfo::none);\n+  __ emit_data64(0x0403030203020201, relocInfo::none);\n+  __ emit_data64(0x0302020102010100, relocInfo::none);\n+  __ emit_data64(0x0403030203020201, relocInfo::none);\n+  __ emit_data64(0x0302020102010100, relocInfo::none);\n+  __ emit_data64(0x0403030203020201, relocInfo::none);\n@@ -855,14 +807,2 @@\n-  address generate_vector_reverse_bit_lut(const char *stub_name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-    __ emit_data64(0x0E060A020C040800, relocInfo::none);\n-    __ emit_data64(0x0F070B030D050901, relocInfo::none);\n-    __ emit_data64(0x0E060A020C040800, relocInfo::none);\n-    __ emit_data64(0x0F070B030D050901, relocInfo::none);\n-    __ emit_data64(0x0E060A020C040800, relocInfo::none);\n-    __ emit_data64(0x0F070B030D050901, relocInfo::none);\n-    __ emit_data64(0x0E060A020C040800, relocInfo::none);\n-    __ emit_data64(0x0F070B030D050901, relocInfo::none);\n-    return start;\n-  }\n+  return start;\n+}\n@@ -870,14 +810,4 @@\n-  address generate_vector_reverse_byte_perm_mask_long(const char *stub_name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-    __ emit_data64(0x0001020304050607, relocInfo::none);\n-    __ emit_data64(0x08090A0B0C0D0E0F, relocInfo::none);\n-    __ emit_data64(0x0001020304050607, relocInfo::none);\n-    __ emit_data64(0x08090A0B0C0D0E0F, relocInfo::none);\n-    __ emit_data64(0x0001020304050607, relocInfo::none);\n-    __ emit_data64(0x08090A0B0C0D0E0F, relocInfo::none);\n-    __ emit_data64(0x0001020304050607, relocInfo::none);\n-    __ emit_data64(0x08090A0B0C0D0E0F, relocInfo::none);\n-    return start;\n-  }\n+address StubGenerator::generate_iota_indices(const char *stub_name) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n@@ -885,14 +815,8 @@\n-  address generate_vector_reverse_byte_perm_mask_int(const char *stub_name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-    __ emit_data64(0x0405060700010203, relocInfo::none);\n-    __ emit_data64(0x0C0D0E0F08090A0B, relocInfo::none);\n-    __ emit_data64(0x0405060700010203, relocInfo::none);\n-    __ emit_data64(0x0C0D0E0F08090A0B, relocInfo::none);\n-    __ emit_data64(0x0405060700010203, relocInfo::none);\n-    __ emit_data64(0x0C0D0E0F08090A0B, relocInfo::none);\n-    __ emit_data64(0x0405060700010203, relocInfo::none);\n-    __ emit_data64(0x0C0D0E0F08090A0B, relocInfo::none);\n-    return start;\n-  }\n+  __ emit_data64(0x0706050403020100, relocInfo::none);\n+  __ emit_data64(0x0F0E0D0C0B0A0908, relocInfo::none);\n+  __ emit_data64(0x1716151413121110, relocInfo::none);\n+  __ emit_data64(0x1F1E1D1C1B1A1918, relocInfo::none);\n+  __ emit_data64(0x2726252423222120, relocInfo::none);\n+  __ emit_data64(0x2F2E2D2C2B2A2928, relocInfo::none);\n+  __ emit_data64(0x3736353433323130, relocInfo::none);\n+  __ emit_data64(0x3F3E3D3C3B3A3938, relocInfo::none);\n@@ -900,14 +824,2 @@\n-  address generate_vector_reverse_byte_perm_mask_short(const char *stub_name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-    __ emit_data64(0x0607040502030001, relocInfo::none);\n-    __ emit_data64(0x0E0F0C0D0A0B0809, relocInfo::none);\n-    __ emit_data64(0x0607040502030001, relocInfo::none);\n-    __ emit_data64(0x0E0F0C0D0A0B0809, relocInfo::none);\n-    __ emit_data64(0x0607040502030001, relocInfo::none);\n-    __ emit_data64(0x0E0F0C0D0A0B0809, relocInfo::none);\n-    __ emit_data64(0x0607040502030001, relocInfo::none);\n-    __ emit_data64(0x0E0F0C0D0A0B0809, relocInfo::none);\n-    return start;\n-  }\n+  return start;\n+}\n@@ -915,10 +827,4 @@\n-  address generate_vector_byte_shuffle_mask(const char *stub_name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-    __ emit_data64(0x7070707070707070, relocInfo::none);\n-    __ emit_data64(0x7070707070707070, relocInfo::none);\n-    __ emit_data64(0xF0F0F0F0F0F0F0F0, relocInfo::none);\n-    __ emit_data64(0xF0F0F0F0F0F0F0F0, relocInfo::none);\n-    return start;\n-  }\n+address StubGenerator::generate_vector_reverse_bit_lut(const char *stub_name) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n@@ -926,4 +832,8 @@\n-  address generate_fp_mask(const char *stub_name, int64_t mask) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n+  __ emit_data64(0x0E060A020C040800, relocInfo::none);\n+  __ emit_data64(0x0F070B030D050901, relocInfo::none);\n+  __ emit_data64(0x0E060A020C040800, relocInfo::none);\n+  __ emit_data64(0x0F070B030D050901, relocInfo::none);\n+  __ emit_data64(0x0E060A020C040800, relocInfo::none);\n+  __ emit_data64(0x0F070B030D050901, relocInfo::none);\n+  __ emit_data64(0x0E060A020C040800, relocInfo::none);\n+  __ emit_data64(0x0F070B030D050901, relocInfo::none);\n@@ -931,2 +841,2 @@\n-    __ emit_data64( mask, relocInfo::none );\n-    __ emit_data64( mask, relocInfo::none );\n+  return start;\n+}\n@@ -934,2 +844,4 @@\n-    return start;\n-  }\n+address StubGenerator::generate_vector_reverse_byte_perm_mask_long(const char *stub_name) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n@@ -937,16 +849,8 @@\n-  address generate_vector_mask(const char *stub_name, int64_t mask) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-\n-    return start;\n-  }\n+  __ emit_data64(0x0001020304050607, relocInfo::none);\n+  __ emit_data64(0x08090A0B0C0D0E0F, relocInfo::none);\n+  __ emit_data64(0x0001020304050607, relocInfo::none);\n+  __ emit_data64(0x08090A0B0C0D0E0F, relocInfo::none);\n+  __ emit_data64(0x0001020304050607, relocInfo::none);\n+  __ emit_data64(0x08090A0B0C0D0E0F, relocInfo::none);\n+  __ emit_data64(0x0001020304050607, relocInfo::none);\n+  __ emit_data64(0x08090A0B0C0D0E0F, relocInfo::none);\n@@ -954,4 +858,2 @@\n-  address generate_vector_byte_perm_mask(const char *stub_name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n+  return start;\n+}\n@@ -959,8 +861,4 @@\n-    __ emit_data64(0x0000000000000001, relocInfo::none);\n-    __ emit_data64(0x0000000000000003, relocInfo::none);\n-    __ emit_data64(0x0000000000000005, relocInfo::none);\n-    __ emit_data64(0x0000000000000007, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000002, relocInfo::none);\n-    __ emit_data64(0x0000000000000004, relocInfo::none);\n-    __ emit_data64(0x0000000000000006, relocInfo::none);\n+address StubGenerator::generate_vector_reverse_byte_perm_mask_int(const char *stub_name) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n@@ -968,2 +866,8 @@\n-    return start;\n-  }\n+  __ emit_data64(0x0405060700010203, relocInfo::none);\n+  __ emit_data64(0x0C0D0E0F08090A0B, relocInfo::none);\n+  __ emit_data64(0x0405060700010203, relocInfo::none);\n+  __ emit_data64(0x0C0D0E0F08090A0B, relocInfo::none);\n+  __ emit_data64(0x0405060700010203, relocInfo::none);\n+  __ emit_data64(0x0C0D0E0F08090A0B, relocInfo::none);\n+  __ emit_data64(0x0405060700010203, relocInfo::none);\n+  __ emit_data64(0x0C0D0E0F08090A0B, relocInfo::none);\n@@ -971,16 +875,2 @@\n-  address generate_vector_fp_mask(const char *stub_name, int64_t mask) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-\n-    return start;\n-  }\n+  return start;\n+}\n@@ -988,30 +878,4 @@\n-  address generate_vector_custom_i32(const char *stub_name, Assembler::AvxVectorLen len,\n-                                     int32_t val0, int32_t val1, int32_t val2, int32_t val3,\n-                                     int32_t val4 = 0, int32_t val5 = 0, int32_t val6 = 0, int32_t val7 = 0,\n-                                     int32_t val8 = 0, int32_t val9 = 0, int32_t val10 = 0, int32_t val11 = 0,\n-                                     int32_t val12 = 0, int32_t val13 = 0, int32_t val14 = 0, int32_t val15 = 0) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-\n-    assert(len != Assembler::AVX_NoVec, \"vector len must be specified\");\n-    __ emit_data(val0, relocInfo::none, 0);\n-    __ emit_data(val1, relocInfo::none, 0);\n-    __ emit_data(val2, relocInfo::none, 0);\n-    __ emit_data(val3, relocInfo::none, 0);\n-    if (len >= Assembler::AVX_256bit) {\n-      __ emit_data(val4, relocInfo::none, 0);\n-      __ emit_data(val5, relocInfo::none, 0);\n-      __ emit_data(val6, relocInfo::none, 0);\n-      __ emit_data(val7, relocInfo::none, 0);\n-      if (len >= Assembler::AVX_512bit) {\n-        __ emit_data(val8, relocInfo::none, 0);\n-        __ emit_data(val9, relocInfo::none, 0);\n-        __ emit_data(val10, relocInfo::none, 0);\n-        __ emit_data(val11, relocInfo::none, 0);\n-        __ emit_data(val12, relocInfo::none, 0);\n-        __ emit_data(val13, relocInfo::none, 0);\n-        __ emit_data(val14, relocInfo::none, 0);\n-        __ emit_data(val15, relocInfo::none, 0);\n-      }\n-    }\n+address StubGenerator::generate_vector_reverse_byte_perm_mask_short(const char *stub_name) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n@@ -1019,2 +883,8 @@\n-    return start;\n-  }\n+  __ emit_data64(0x0607040502030001, relocInfo::none);\n+  __ emit_data64(0x0E0F0C0D0A0B0809, relocInfo::none);\n+  __ emit_data64(0x0607040502030001, relocInfo::none);\n+  __ emit_data64(0x0E0F0C0D0A0B0809, relocInfo::none);\n+  __ emit_data64(0x0607040502030001, relocInfo::none);\n+  __ emit_data64(0x0E0F0C0D0A0B0809, relocInfo::none);\n+  __ emit_data64(0x0607040502030001, relocInfo::none);\n+  __ emit_data64(0x0E0F0C0D0A0B0809, relocInfo::none);\n@@ -1022,48 +892,2 @@\n-  \/\/ Non-destructive plausibility checks for oops\n-  \/\/\n-  \/\/ Arguments:\n-  \/\/    all args on stack!\n-  \/\/\n-  \/\/ Stack after saving c_rarg3:\n-  \/\/    [tos + 0]: saved c_rarg3\n-  \/\/    [tos + 1]: saved c_rarg2\n-  \/\/    [tos + 2]: saved r12 (several TemplateTable methods use it)\n-  \/\/    [tos + 3]: saved flags\n-  \/\/    [tos + 4]: return address\n-  \/\/  * [tos + 5]: error message (char*)\n-  \/\/  * [tos + 6]: object to verify (oop)\n-  \/\/  * [tos + 7]: saved rax - saved by caller and bashed\n-  \/\/  * [tos + 8]: saved r10 (rscratch1) - saved by caller\n-  \/\/  * = popped on exit\n-  address generate_verify_oop() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"verify_oop\");\n-    address start = __ pc();\n-\n-    Label exit, error;\n-\n-    __ pushf();\n-    __ incrementl(ExternalAddress((address) StubRoutines::verify_oop_count_addr()));\n-\n-    __ push(r12);\n-\n-    \/\/ save c_rarg2 and c_rarg3\n-    __ push(c_rarg2);\n-    __ push(c_rarg3);\n-\n-    enum {\n-           \/\/ After previous pushes.\n-           oop_to_verify = 6 * wordSize,\n-           saved_rax     = 7 * wordSize,\n-           saved_r10     = 8 * wordSize,\n-\n-           \/\/ Before the call to MacroAssembler::debug(), see below.\n-           return_addr   = 16 * wordSize,\n-           error_msg     = 17 * wordSize\n-    };\n-\n-    \/\/ get object\n-    __ movptr(rax, Address(rsp, oop_to_verify));\n-\n-    \/\/ make sure object is 'reasonable'\n-    __ testptr(rax, rax);\n-    __ jcc(Assembler::zero, exit); \/\/ if obj is NULL it is OK\n+  return start;\n+}\n@@ -1071,7 +895,4 @@\n-#if INCLUDE_ZGC\n-    if (UseZGC) {\n-      \/\/ Check if metadata bits indicate a bad oop\n-      __ testptr(rax, Address(r15_thread, ZThreadLocalData::address_bad_mask_offset()));\n-      __ jcc(Assembler::notZero, error);\n-    }\n-#endif\n+address StubGenerator::generate_vector_byte_shuffle_mask(const char *stub_name) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n@@ -1079,7 +900,4 @@\n-    \/\/ Check if the oop is in the right area of memory\n-    __ movptr(c_rarg2, rax);\n-    __ movptr(c_rarg3, (intptr_t) Universe::verify_oop_mask());\n-    __ andptr(c_rarg2, c_rarg3);\n-    __ movptr(c_rarg3, (intptr_t) Universe::verify_oop_bits());\n-    __ cmpptr(c_rarg2, c_rarg3);\n-    __ jcc(Assembler::notZero, error);\n+  __ emit_data64(0x7070707070707070, relocInfo::none);\n+  __ emit_data64(0x7070707070707070, relocInfo::none);\n+  __ emit_data64(0xF0F0F0F0F0F0F0F0, relocInfo::none);\n+  __ emit_data64(0xF0F0F0F0F0F0F0F0, relocInfo::none);\n@@ -1087,50 +905,2 @@\n-    \/\/ make sure klass is 'reasonable', which is not zero.\n-    __ load_klass(rax, rax, rscratch1);  \/\/ get klass\n-    __ testptr(rax, rax);\n-    __ jcc(Assembler::zero, error); \/\/ if klass is NULL it is broken\n-\n-    \/\/ return if everything seems ok\n-    __ bind(exit);\n-    __ movptr(rax, Address(rsp, saved_rax));     \/\/ get saved rax back\n-    __ movptr(rscratch1, Address(rsp, saved_r10)); \/\/ get saved r10 back\n-    __ pop(c_rarg3);                             \/\/ restore c_rarg3\n-    __ pop(c_rarg2);                             \/\/ restore c_rarg2\n-    __ pop(r12);                                 \/\/ restore r12\n-    __ popf();                                   \/\/ restore flags\n-    __ ret(4 * wordSize);                        \/\/ pop caller saved stuff\n-\n-    \/\/ handle errors\n-    __ bind(error);\n-    __ movptr(rax, Address(rsp, saved_rax));     \/\/ get saved rax back\n-    __ movptr(rscratch1, Address(rsp, saved_r10)); \/\/ get saved r10 back\n-    __ pop(c_rarg3);                             \/\/ get saved c_rarg3 back\n-    __ pop(c_rarg2);                             \/\/ get saved c_rarg2 back\n-    __ pop(r12);                                 \/\/ get saved r12 back\n-    __ popf();                                   \/\/ get saved flags off stack --\n-                                                 \/\/ will be ignored\n-\n-    __ pusha();                                  \/\/ push registers\n-                                                 \/\/ (rip is already\n-                                                 \/\/ already pushed)\n-    \/\/ debug(char* msg, int64_t pc, int64_t regs[])\n-    \/\/ We've popped the registers we'd saved (c_rarg3, c_rarg2 and flags), and\n-    \/\/ pushed all the registers, so now the stack looks like:\n-    \/\/     [tos +  0] 16 saved registers\n-    \/\/     [tos + 16] return address\n-    \/\/   * [tos + 17] error message (char*)\n-    \/\/   * [tos + 18] object to verify (oop)\n-    \/\/   * [tos + 19] saved rax - saved by caller and bashed\n-    \/\/   * [tos + 20] saved r10 (rscratch1) - saved by caller\n-    \/\/   * = popped on exit\n-\n-    __ movptr(c_rarg0, Address(rsp, error_msg));    \/\/ pass address of error message\n-    __ movptr(c_rarg1, Address(rsp, return_addr));  \/\/ pass return address\n-    __ movq(c_rarg2, rsp);                          \/\/ pass address of regs on stack\n-    __ mov(r12, rsp);                               \/\/ remember rsp\n-    __ subptr(rsp, frame::arg_reg_save_area_bytes); \/\/ windows\n-    __ andptr(rsp, -16);                            \/\/ align stack as required by ABI\n-    BLOCK_COMMENT(\"call MacroAssembler::debug\");\n-    __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, MacroAssembler::debug64)));\n-    __ hlt();\n-    return start;\n-  }\n+  return start;\n+}\n@@ -1138,19 +908,4 @@\n-  \/\/\n-  \/\/ Verify that a register contains clean 32-bits positive value\n-  \/\/ (high 32-bits are 0) so it could be used in 64-bits shifts.\n-  \/\/\n-  \/\/  Input:\n-  \/\/    Rint  -  32-bits value\n-  \/\/    Rtmp  -  scratch\n-  \/\/\n-  void assert_clean_int(Register Rint, Register Rtmp) {\n-#ifdef ASSERT\n-    Label L;\n-    assert_different_registers(Rtmp, Rint);\n-    __ movslq(Rtmp, Rint);\n-    __ cmpq(Rtmp, Rint);\n-    __ jcc(Assembler::equal, L);\n-    __ stop(\"high 32-bits of int value are not 0\");\n-    __ bind(L);\n-#endif\n-  }\n+address StubGenerator::generate_fp_mask(const char *stub_name, int64_t mask) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n@@ -1158,36 +913,2 @@\n-  \/\/  Generate overlap test for array copy stubs\n-  \/\/\n-  \/\/  Input:\n-  \/\/     c_rarg0 - from\n-  \/\/     c_rarg1 - to\n-  \/\/     c_rarg2 - element count\n-  \/\/\n-  \/\/  Output:\n-  \/\/     rax   - &from[element count - 1]\n-  \/\/\n-  void array_overlap_test(address no_overlap_target, Address::ScaleFactor sf) {\n-    assert(no_overlap_target != NULL, \"must be generated\");\n-    array_overlap_test(no_overlap_target, NULL, sf);\n-  }\n-  void array_overlap_test(Label& L_no_overlap, Address::ScaleFactor sf) {\n-    array_overlap_test(NULL, &L_no_overlap, sf);\n-  }\n-  void array_overlap_test(address no_overlap_target, Label* NOLp, Address::ScaleFactor sf) {\n-    const Register from     = c_rarg0;\n-    const Register to       = c_rarg1;\n-    const Register count    = c_rarg2;\n-    const Register end_from = rax;\n-\n-    __ cmpptr(to, from);\n-    __ lea(end_from, Address(from, count, sf, 0));\n-    if (NOLp == NULL) {\n-      ExternalAddress no_overlap(no_overlap_target);\n-      __ jump_cc(Assembler::belowEqual, no_overlap);\n-      __ cmpptr(to, end_from);\n-      __ jump_cc(Assembler::aboveEqual, no_overlap);\n-    } else {\n-      __ jcc(Assembler::belowEqual, (*NOLp));\n-      __ cmpptr(to, end_from);\n-      __ jcc(Assembler::aboveEqual, (*NOLp));\n-    }\n-  }\n+  __ emit_data64( mask, relocInfo::none );\n+  __ emit_data64( mask, relocInfo::none );\n@@ -1195,12 +916,2 @@\n-  \/\/ Shuffle first three arg regs on Windows into Linux\/Solaris locations.\n-  \/\/\n-  \/\/ Outputs:\n-  \/\/    rdi - rcx\n-  \/\/    rsi - rdx\n-  \/\/    rdx - r8\n-  \/\/    rcx - r9\n-  \/\/\n-  \/\/ Registers r9 and r10 are used to save rdi and rsi on Windows, which latter\n-  \/\/ are non-volatile.  r9 and r10 should not be used by the caller.\n-  \/\/\n-  DEBUG_ONLY(bool regs_in_thread;)\n+  return start;\n+}\n@@ -1208,22 +919,4 @@\n-  void setup_arg_regs(int nargs = 3) {\n-    const Register saved_rdi = r9;\n-    const Register saved_rsi = r10;\n-    assert(nargs == 3 || nargs == 4, \"else fix\");\n-#ifdef _WIN64\n-    assert(c_rarg0 == rcx && c_rarg1 == rdx && c_rarg2 == r8 && c_rarg3 == r9,\n-           \"unexpected argument registers\");\n-    if (nargs >= 4)\n-      __ mov(rax, r9);  \/\/ r9 is also saved_rdi\n-    __ movptr(saved_rdi, rdi);\n-    __ movptr(saved_rsi, rsi);\n-    __ mov(rdi, rcx); \/\/ c_rarg0\n-    __ mov(rsi, rdx); \/\/ c_rarg1\n-    __ mov(rdx, r8);  \/\/ c_rarg2\n-    if (nargs >= 4)\n-      __ mov(rcx, rax); \/\/ c_rarg3 (via rax)\n-#else\n-    assert(c_rarg0 == rdi && c_rarg1 == rsi && c_rarg2 == rdx && c_rarg3 == rcx,\n-           \"unexpected argument registers\");\n-#endif\n-    DEBUG_ONLY(regs_in_thread = false;)\n-  }\n+address StubGenerator::generate_vector_mask(const char *stub_name, int64_t mask) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n@@ -1231,9 +924,8 @@\n-  void restore_arg_regs() {\n-    assert(!regs_in_thread, \"wrong call to restore_arg_regs\");\n-    const Register saved_rdi = r9;\n-    const Register saved_rsi = r10;\n-#ifdef _WIN64\n-    __ movptr(rdi, saved_rdi);\n-    __ movptr(rsi, saved_rsi);\n-#endif\n-  }\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n@@ -1241,21 +933,2 @@\n-  \/\/ This is used in places where r10 is a scratch register, and can\n-  \/\/ be adapted if r9 is needed also.\n-  void setup_arg_regs_using_thread() {\n-    const Register saved_r15 = r9;\n-#ifdef _WIN64\n-    __ mov(saved_r15, r15);  \/\/ r15 is callee saved and needs to be restored\n-    __ get_thread(r15_thread);\n-    assert(c_rarg0 == rcx && c_rarg1 == rdx && c_rarg2 == r8 && c_rarg3 == r9,\n-           \"unexpected argument registers\");\n-    __ movptr(Address(r15_thread, in_bytes(JavaThread::windows_saved_rdi_offset())), rdi);\n-    __ movptr(Address(r15_thread, in_bytes(JavaThread::windows_saved_rsi_offset())), rsi);\n-\n-    __ mov(rdi, rcx); \/\/ c_rarg0\n-    __ mov(rsi, rdx); \/\/ c_rarg1\n-    __ mov(rdx, r8);  \/\/ c_rarg2\n-#else\n-    assert(c_rarg0 == rdi && c_rarg1 == rsi && c_rarg2 == rdx && c_rarg3 == rcx,\n-           \"unexpected argument registers\");\n-#endif\n-    DEBUG_ONLY(regs_in_thread = true;)\n-  }\n+  return start;\n+}\n@@ -1263,10 +936,4 @@\n-  void restore_arg_regs_using_thread() {\n-    assert(regs_in_thread, \"wrong call to restore_arg_regs\");\n-    const Register saved_r15 = r9;\n-#ifdef _WIN64\n-    __ get_thread(r15_thread);\n-    __ movptr(rsi, Address(r15_thread, in_bytes(JavaThread::windows_saved_rsi_offset())));\n-    __ movptr(rdi, Address(r15_thread, in_bytes(JavaThread::windows_saved_rdi_offset())));\n-    __ mov(r15, saved_r15);  \/\/ r15 is callee saved and needs to be restored\n-#endif\n-  }\n+address StubGenerator::generate_vector_byte_perm_mask(const char *stub_name) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n@@ -1274,34 +941,8 @@\n-  \/\/ Copy big chunks forward\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   end_from     - source arrays end address\n-  \/\/   end_to       - destination array end address\n-  \/\/   qword_count  - 64-bits element count, negative\n-  \/\/   to           - scratch\n-  \/\/   L_copy_bytes - entry label\n-  \/\/   L_copy_8_bytes  - exit  label\n-  \/\/\n-  void copy_bytes_forward(Register end_from, Register end_to,\n-                             Register qword_count, Register to,\n-                             Label& L_copy_bytes, Label& L_copy_8_bytes) {\n-    DEBUG_ONLY(__ stop(\"enter at entry label, not here\"));\n-    Label L_loop;\n-    __ align(OptoLoopAlignment);\n-    if (UseUnalignedLoadStores) {\n-      Label L_end;\n-      __ BIND(L_loop);\n-      if (UseAVX >= 2) {\n-        __ vmovdqu(xmm0, Address(end_from, qword_count, Address::times_8, -56));\n-        __ vmovdqu(Address(end_to, qword_count, Address::times_8, -56), xmm0);\n-        __ vmovdqu(xmm1, Address(end_from, qword_count, Address::times_8, -24));\n-        __ vmovdqu(Address(end_to, qword_count, Address::times_8, -24), xmm1);\n-      } else {\n-        __ movdqu(xmm0, Address(end_from, qword_count, Address::times_8, -56));\n-        __ movdqu(Address(end_to, qword_count, Address::times_8, -56), xmm0);\n-        __ movdqu(xmm1, Address(end_from, qword_count, Address::times_8, -40));\n-        __ movdqu(Address(end_to, qword_count, Address::times_8, -40), xmm1);\n-        __ movdqu(xmm2, Address(end_from, qword_count, Address::times_8, -24));\n-        __ movdqu(Address(end_to, qword_count, Address::times_8, -24), xmm2);\n-        __ movdqu(xmm3, Address(end_from, qword_count, Address::times_8, - 8));\n-        __ movdqu(Address(end_to, qword_count, Address::times_8, - 8), xmm3);\n-      }\n+  __ emit_data64(0x0000000000000001, relocInfo::none);\n+  __ emit_data64(0x0000000000000003, relocInfo::none);\n+  __ emit_data64(0x0000000000000005, relocInfo::none);\n+  __ emit_data64(0x0000000000000007, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000002, relocInfo::none);\n+  __ emit_data64(0x0000000000000004, relocInfo::none);\n+  __ emit_data64(0x0000000000000006, relocInfo::none);\n@@ -1309,36 +950,2 @@\n-      __ BIND(L_copy_bytes);\n-      __ addptr(qword_count, 8);\n-      __ jcc(Assembler::lessEqual, L_loop);\n-      __ subptr(qword_count, 4);  \/\/ sub(8) and add(4)\n-      __ jccb(Assembler::greater, L_end);\n-      \/\/ Copy trailing 32 bytes\n-      if (UseAVX >= 2) {\n-        __ vmovdqu(xmm0, Address(end_from, qword_count, Address::times_8, -24));\n-        __ vmovdqu(Address(end_to, qword_count, Address::times_8, -24), xmm0);\n-      } else {\n-        __ movdqu(xmm0, Address(end_from, qword_count, Address::times_8, -24));\n-        __ movdqu(Address(end_to, qword_count, Address::times_8, -24), xmm0);\n-        __ movdqu(xmm1, Address(end_from, qword_count, Address::times_8, - 8));\n-        __ movdqu(Address(end_to, qword_count, Address::times_8, - 8), xmm1);\n-      }\n-      __ addptr(qword_count, 4);\n-      __ BIND(L_end);\n-    } else {\n-      \/\/ Copy 32-bytes per iteration\n-      __ BIND(L_loop);\n-      __ movq(to, Address(end_from, qword_count, Address::times_8, -24));\n-      __ movq(Address(end_to, qword_count, Address::times_8, -24), to);\n-      __ movq(to, Address(end_from, qword_count, Address::times_8, -16));\n-      __ movq(Address(end_to, qword_count, Address::times_8, -16), to);\n-      __ movq(to, Address(end_from, qword_count, Address::times_8, - 8));\n-      __ movq(Address(end_to, qword_count, Address::times_8, - 8), to);\n-      __ movq(to, Address(end_from, qword_count, Address::times_8, - 0));\n-      __ movq(Address(end_to, qword_count, Address::times_8, - 0), to);\n-\n-      __ BIND(L_copy_bytes);\n-      __ addptr(qword_count, 4);\n-      __ jcc(Assembler::lessEqual, L_loop);\n-    }\n-    __ subptr(qword_count, 4);\n-    __ jcc(Assembler::less, L_copy_8_bytes); \/\/ Copy trailing qwords\n-  }\n+  return start;\n+}\n@@ -1346,34 +953,4 @@\n-  \/\/ Copy big chunks backward\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   from         - source arrays address\n-  \/\/   dest         - destination array address\n-  \/\/   qword_count  - 64-bits element count\n-  \/\/   to           - scratch\n-  \/\/   L_copy_bytes - entry label\n-  \/\/   L_copy_8_bytes  - exit  label\n-  \/\/\n-  void copy_bytes_backward(Register from, Register dest,\n-                              Register qword_count, Register to,\n-                              Label& L_copy_bytes, Label& L_copy_8_bytes) {\n-    DEBUG_ONLY(__ stop(\"enter at entry label, not here\"));\n-    Label L_loop;\n-    __ align(OptoLoopAlignment);\n-    if (UseUnalignedLoadStores) {\n-      Label L_end;\n-      __ BIND(L_loop);\n-      if (UseAVX >= 2) {\n-        __ vmovdqu(xmm0, Address(from, qword_count, Address::times_8, 32));\n-        __ vmovdqu(Address(dest, qword_count, Address::times_8, 32), xmm0);\n-        __ vmovdqu(xmm1, Address(from, qword_count, Address::times_8,  0));\n-        __ vmovdqu(Address(dest, qword_count, Address::times_8,  0), xmm1);\n-      } else {\n-        __ movdqu(xmm0, Address(from, qword_count, Address::times_8, 48));\n-        __ movdqu(Address(dest, qword_count, Address::times_8, 48), xmm0);\n-        __ movdqu(xmm1, Address(from, qword_count, Address::times_8, 32));\n-        __ movdqu(Address(dest, qword_count, Address::times_8, 32), xmm1);\n-        __ movdqu(xmm2, Address(from, qword_count, Address::times_8, 16));\n-        __ movdqu(Address(dest, qword_count, Address::times_8, 16), xmm2);\n-        __ movdqu(xmm3, Address(from, qword_count, Address::times_8,  0));\n-        __ movdqu(Address(dest, qword_count, Address::times_8,  0), xmm3);\n-      }\n+address StubGenerator::generate_vector_fp_mask(const char *stub_name, int64_t mask) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n@@ -1381,37 +958,8 @@\n-      __ BIND(L_copy_bytes);\n-      __ subptr(qword_count, 8);\n-      __ jcc(Assembler::greaterEqual, L_loop);\n-\n-      __ addptr(qword_count, 4);  \/\/ add(8) and sub(4)\n-      __ jccb(Assembler::less, L_end);\n-      \/\/ Copy trailing 32 bytes\n-      if (UseAVX >= 2) {\n-        __ vmovdqu(xmm0, Address(from, qword_count, Address::times_8, 0));\n-        __ vmovdqu(Address(dest, qword_count, Address::times_8, 0), xmm0);\n-      } else {\n-        __ movdqu(xmm0, Address(from, qword_count, Address::times_8, 16));\n-        __ movdqu(Address(dest, qword_count, Address::times_8, 16), xmm0);\n-        __ movdqu(xmm1, Address(from, qword_count, Address::times_8,  0));\n-        __ movdqu(Address(dest, qword_count, Address::times_8,  0), xmm1);\n-      }\n-      __ subptr(qword_count, 4);\n-      __ BIND(L_end);\n-    } else {\n-      \/\/ Copy 32-bytes per iteration\n-      __ BIND(L_loop);\n-      __ movq(to, Address(from, qword_count, Address::times_8, 24));\n-      __ movq(Address(dest, qword_count, Address::times_8, 24), to);\n-      __ movq(to, Address(from, qword_count, Address::times_8, 16));\n-      __ movq(Address(dest, qword_count, Address::times_8, 16), to);\n-      __ movq(to, Address(from, qword_count, Address::times_8,  8));\n-      __ movq(Address(dest, qword_count, Address::times_8,  8), to);\n-      __ movq(to, Address(from, qword_count, Address::times_8,  0));\n-      __ movq(Address(dest, qword_count, Address::times_8,  0), to);\n-\n-      __ BIND(L_copy_bytes);\n-      __ subptr(qword_count, 4);\n-      __ jcc(Assembler::greaterEqual, L_loop);\n-    }\n-    __ addptr(qword_count, 4);\n-    __ jcc(Assembler::greater, L_copy_8_bytes); \/\/ Copy trailing qwords\n-  }\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n@@ -1419,12 +967,2 @@\n-#ifndef PRODUCT\n-    int& get_profile_ctr(int shift) {\n-      if ( 0 == shift)\n-        return SharedRuntime::_jbyte_array_copy_ctr;\n-      else if(1 == shift)\n-        return SharedRuntime::_jshort_array_copy_ctr;\n-      else if(2 == shift)\n-        return SharedRuntime::_jint_array_copy_ctr;\n-      else\n-        return SharedRuntime::_jlong_array_copy_ctr;\n-    }\n-#endif\n+  return start;\n+}\n@@ -1432,7 +970,28 @@\n-  void setup_argument_regs(BasicType type) {\n-    if (type == T_BYTE || type == T_SHORT) {\n-      setup_arg_regs(); \/\/ from => rdi, to => rsi, count => rdx\n-                        \/\/ r9 and r10 may be used to save non-volatile registers\n-    } else {\n-      setup_arg_regs_using_thread(); \/\/ from => rdi, to => rsi, count => rdx\n-                                     \/\/ r9 is used to save r15_thread\n+address StubGenerator::generate_vector_custom_i32(const char *stub_name, Assembler::AvxVectorLen len,\n+                                   int32_t val0, int32_t val1, int32_t val2, int32_t val3,\n+                                   int32_t val4, int32_t val5, int32_t val6, int32_t val7,\n+                                   int32_t val8, int32_t val9, int32_t val10, int32_t val11,\n+                                   int32_t val12, int32_t val13, int32_t val14, int32_t val15) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n+\n+  assert(len != Assembler::AVX_NoVec, \"vector len must be specified\");\n+  __ emit_data(val0, relocInfo::none, 0);\n+  __ emit_data(val1, relocInfo::none, 0);\n+  __ emit_data(val2, relocInfo::none, 0);\n+  __ emit_data(val3, relocInfo::none, 0);\n+  if (len >= Assembler::AVX_256bit) {\n+    __ emit_data(val4, relocInfo::none, 0);\n+    __ emit_data(val5, relocInfo::none, 0);\n+    __ emit_data(val6, relocInfo::none, 0);\n+    __ emit_data(val7, relocInfo::none, 0);\n+    if (len >= Assembler::AVX_512bit) {\n+      __ emit_data(val8, relocInfo::none, 0);\n+      __ emit_data(val9, relocInfo::none, 0);\n+      __ emit_data(val10, relocInfo::none, 0);\n+      __ emit_data(val11, relocInfo::none, 0);\n+      __ emit_data(val12, relocInfo::none, 0);\n+      __ emit_data(val13, relocInfo::none, 0);\n+      __ emit_data(val14, relocInfo::none, 0);\n+      __ emit_data(val15, relocInfo::none, 0);\n@@ -1441,0 +1000,2 @@\n+  return start;\n+}\n@@ -1442,2240 +1003,41 @@\n-  void restore_argument_regs(BasicType type) {\n-    if (type == T_BYTE || type == T_SHORT) {\n-      restore_arg_regs();\n-    } else {\n-      restore_arg_regs_using_thread();\n-    }\n-  }\n-\n-#if COMPILER2_OR_JVMCI\n-  \/\/ Note: Following rules apply to AVX3 optimized arraycopy stubs:-\n-  \/\/ - If target supports AVX3 features (BW+VL+F) then implementation uses 32 byte vectors (YMMs)\n-  \/\/   for both special cases (various small block sizes) and aligned copy loop. This is the\n-  \/\/   default configuration.\n-  \/\/ - If copy length is above AVX3Threshold, then implementation use 64 byte vectors (ZMMs)\n-  \/\/   for main copy loop (and subsequent tail) since bulk of the cycles will be consumed in it.\n-  \/\/ - If user forces MaxVectorSize=32 then above 4096 bytes its seen that REP MOVs shows a\n-  \/\/   better performance for disjoint copies. For conjoint\/backward copy vector based\n-  \/\/   copy performs better.\n-  \/\/ - If user sets AVX3Threshold=0, then special cases for small blocks sizes operate over\n-  \/\/   64 byte vector registers (ZMMs).\n-\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source array address\n-  \/\/   c_rarg1   - destination array address\n-  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n-  \/\/\n-  \/\/\n-  \/\/ Side Effects:\n-  \/\/   disjoint_copy_avx3_masked is set to the no-overlap entry point\n-  \/\/   used by generate_conjoint_[byte\/int\/short\/long]_copy().\n-  \/\/\n-\n-  address generate_disjoint_copy_avx3_masked(address* entry, const char *name, int shift,\n-                                             bool aligned, bool is_oop, bool dest_uninitialized) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-    int avx3threshold = VM_Version::avx3_threshold();\n-    bool use64byteVector = (MaxVectorSize > 32) && (avx3threshold == 0);\n-    Label L_main_loop, L_main_loop_64bytes, L_tail, L_tail64, L_exit, L_entry;\n-    Label L_repmovs, L_main_pre_loop, L_main_pre_loop_64bytes, L_pre_main_post_64;\n-    const Register from        = rdi;  \/\/ source array address\n-    const Register to          = rsi;  \/\/ destination array address\n-    const Register count       = rdx;  \/\/ elements count\n-    const Register temp1       = r8;\n-    const Register temp2       = r11;\n-    const Register temp3       = rax;\n-    const Register temp4       = rcx;\n-    \/\/ End pointers are inclusive, and if count is not zero they point\n-    \/\/ to the last unit copied:  end_to[0] := end_from[0]\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n-\n-    if (entry != NULL) {\n-      *entry = __ pc();\n-       \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n-      BLOCK_COMMENT(\"Entry:\");\n-    }\n-\n-    BasicType type_vec[] = { T_BYTE,  T_SHORT,  T_INT,   T_LONG};\n-    BasicType type = is_oop ? T_OBJECT : type_vec[shift];\n-\n-    setup_argument_regs(type);\n-\n-    DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_DISJOINT;\n-    if (dest_uninitialized) {\n-      decorators |= IS_DEST_UNINITIALIZED;\n-    }\n-    if (aligned) {\n-      decorators |= ARRAYCOPY_ALIGNED;\n-    }\n-    BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    bs->arraycopy_prologue(_masm, decorators, type, from, to, count);\n-\n-    {\n-      \/\/ Type(shift)           byte(0), short(1), int(2),   long(3)\n-      int loop_size[]        = { 192,     96,       48,      24};\n-      int threshold[]        = { 4096,    2048,     1024,    512};\n-\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n-      \/\/ 'from', 'to' and 'count' are now valid\n-\n-      \/\/ temp1 holds remaining count and temp4 holds running count used to compute\n-      \/\/ next address offset for start of to\/from addresses (temp4 * scale).\n-      __ mov64(temp4, 0);\n-      __ movq(temp1, count);\n-\n-      \/\/ Zero length check.\n-      __ BIND(L_tail);\n-      __ cmpq(temp1, 0);\n-      __ jcc(Assembler::lessEqual, L_exit);\n-\n-      \/\/ Special cases using 32 byte [masked] vector copy operations.\n-      __ arraycopy_avx3_special_cases(xmm1, k2, from, to, temp1, shift,\n-                                      temp4, temp3, use64byteVector, L_entry, L_exit);\n-\n-      \/\/ PRE-MAIN-POST loop for aligned copy.\n-      __ BIND(L_entry);\n-\n-      if (avx3threshold != 0) {\n-        __ cmpq(count, threshold[shift]);\n-        if (MaxVectorSize == 64) {\n-          \/\/ Copy using 64 byte vectors.\n-          __ jcc(Assembler::greaterEqual, L_pre_main_post_64);\n-        } else {\n-          assert(MaxVectorSize < 64, \"vector size should be < 64 bytes\");\n-          \/\/ REP MOVS offer a faster copy path.\n-          __ jcc(Assembler::greaterEqual, L_repmovs);\n-        }\n-      }\n-\n-      if ((MaxVectorSize < 64)  || (avx3threshold != 0)) {\n-        \/\/ Partial copy to make dst address 32 byte aligned.\n-        __ movq(temp2, to);\n-        __ andq(temp2, 31);\n-        __ jcc(Assembler::equal, L_main_pre_loop);\n-\n-        __ negptr(temp2);\n-        __ addq(temp2, 32);\n-        if (shift) {\n-          __ shrq(temp2, shift);\n-        }\n-        __ movq(temp3, temp2);\n-        __ copy32_masked_avx(to, from, xmm1, k2, temp3, temp4, temp1, shift);\n-        __ movq(temp4, temp2);\n-        __ movq(temp1, count);\n-        __ subq(temp1, temp2);\n-\n-        __ cmpq(temp1, loop_size[shift]);\n-        __ jcc(Assembler::less, L_tail);\n-\n-        __ BIND(L_main_pre_loop);\n-        __ subq(temp1, loop_size[shift]);\n-\n-        \/\/ Main loop with aligned copy block size of 192 bytes at 32 byte granularity.\n-        __ align32();\n-        __ BIND(L_main_loop);\n-           __ copy64_avx(to, from, temp4, xmm1, false, shift, 0);\n-           __ copy64_avx(to, from, temp4, xmm1, false, shift, 64);\n-           __ copy64_avx(to, from, temp4, xmm1, false, shift, 128);\n-           __ addptr(temp4, loop_size[shift]);\n-           __ subq(temp1, loop_size[shift]);\n-           __ jcc(Assembler::greater, L_main_loop);\n-\n-        __ addq(temp1, loop_size[shift]);\n-\n-        \/\/ Tail loop.\n-        __ jmp(L_tail);\n-\n-        __ BIND(L_repmovs);\n-          __ movq(temp2, temp1);\n-          \/\/ Swap to(RSI) and from(RDI) addresses to comply with REP MOVs semantics.\n-          __ movq(temp3, to);\n-          __ movq(to,  from);\n-          __ movq(from, temp3);\n-          \/\/ Save to\/from for restoration post rep_mov.\n-          __ movq(temp1, to);\n-          __ movq(temp3, from);\n-          if(shift < 3) {\n-            __ shrq(temp2, 3-shift);     \/\/ quad word count\n-          }\n-          __ movq(temp4 , temp2);        \/\/ move quad ward count into temp4(RCX).\n-          __ rep_mov();\n-          __ shlq(temp2, 3);             \/\/ convert quad words into byte count.\n-          if(shift) {\n-            __ shrq(temp2, shift);       \/\/ type specific count.\n-          }\n-          \/\/ Restore original addresses in to\/from.\n-          __ movq(to, temp3);\n-          __ movq(from, temp1);\n-          __ movq(temp4, temp2);\n-          __ movq(temp1, count);\n-          __ subq(temp1, temp2);         \/\/ tailing part (less than a quad ward size).\n-          __ jmp(L_tail);\n-      }\n-\n-      if (MaxVectorSize > 32) {\n-        __ BIND(L_pre_main_post_64);\n-        \/\/ Partial copy to make dst address 64 byte aligned.\n-        __ movq(temp2, to);\n-        __ andq(temp2, 63);\n-        __ jcc(Assembler::equal, L_main_pre_loop_64bytes);\n-\n-        __ negptr(temp2);\n-        __ addq(temp2, 64);\n-        if (shift) {\n-          __ shrq(temp2, shift);\n-        }\n-        __ movq(temp3, temp2);\n-        __ copy64_masked_avx(to, from, xmm1, k2, temp3, temp4, temp1, shift, 0 , true);\n-        __ movq(temp4, temp2);\n-        __ movq(temp1, count);\n-        __ subq(temp1, temp2);\n-\n-        __ cmpq(temp1, loop_size[shift]);\n-        __ jcc(Assembler::less, L_tail64);\n-\n-        __ BIND(L_main_pre_loop_64bytes);\n-        __ subq(temp1, loop_size[shift]);\n-\n-        \/\/ Main loop with aligned copy block size of 192 bytes at\n-        \/\/ 64 byte copy granularity.\n-        __ align32();\n-        __ BIND(L_main_loop_64bytes);\n-           __ copy64_avx(to, from, temp4, xmm1, false, shift, 0 , true);\n-           __ copy64_avx(to, from, temp4, xmm1, false, shift, 64, true);\n-           __ copy64_avx(to, from, temp4, xmm1, false, shift, 128, true);\n-           __ addptr(temp4, loop_size[shift]);\n-           __ subq(temp1, loop_size[shift]);\n-           __ jcc(Assembler::greater, L_main_loop_64bytes);\n-\n-        __ addq(temp1, loop_size[shift]);\n-        \/\/ Zero length check.\n-        __ jcc(Assembler::lessEqual, L_exit);\n-\n-        __ BIND(L_tail64);\n-\n-        \/\/ Tail handling using 64 byte [masked] vector copy operations.\n-        use64byteVector = true;\n-        __ arraycopy_avx3_special_cases(xmm1, k2, from, to, temp1, shift,\n-                                        temp4, temp3, use64byteVector, L_entry, L_exit);\n-      }\n-      __ BIND(L_exit);\n-    }\n-\n-    address ucme_exit_pc = __ pc();\n-    \/\/ When called from generic_arraycopy r11 contains specific values\n-    \/\/ used during arraycopy epilogue, re-initializing r11.\n-    if (is_oop) {\n-      __ movq(r11, shift == 3 ? count : to);\n-    }\n-    bs->arraycopy_epilogue(_masm, decorators, type, from, to, count);\n-    restore_argument_regs(type);\n-    inc_counter_np(get_profile_ctr(shift)); \/\/ Update counter after rscratch1 is free\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-    return start;\n-  }\n-\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source array address\n-  \/\/   c_rarg1   - destination array address\n-  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n-  \/\/\n-  \/\/\n-  address generate_conjoint_copy_avx3_masked(address* entry, const char *name, int shift,\n-                                             address nooverlap_target, bool aligned, bool is_oop,\n-                                             bool dest_uninitialized) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    int avx3threshold = VM_Version::avx3_threshold();\n-    bool use64byteVector = (MaxVectorSize > 32) && (avx3threshold == 0);\n-\n-    Label L_main_pre_loop, L_main_pre_loop_64bytes, L_pre_main_post_64;\n-    Label L_main_loop, L_main_loop_64bytes, L_tail, L_tail64, L_exit, L_entry;\n-    const Register from        = rdi;  \/\/ source array address\n-    const Register to          = rsi;  \/\/ destination array address\n-    const Register count       = rdx;  \/\/ elements count\n-    const Register temp1       = r8;\n-    const Register temp2       = rcx;\n-    const Register temp3       = r11;\n-    const Register temp4       = rax;\n-    \/\/ End pointers are inclusive, and if count is not zero they point\n-    \/\/ to the last unit copied:  end_to[0] := end_from[0]\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n-\n-    if (entry != NULL) {\n-      *entry = __ pc();\n-       \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n-      BLOCK_COMMENT(\"Entry:\");\n-    }\n-\n-    array_overlap_test(nooverlap_target, (Address::ScaleFactor)(shift));\n-\n-    BasicType type_vec[] = { T_BYTE,  T_SHORT,  T_INT,   T_LONG};\n-    BasicType type = is_oop ? T_OBJECT : type_vec[shift];\n-\n-    setup_argument_regs(type);\n-\n-    DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n-    if (dest_uninitialized) {\n-      decorators |= IS_DEST_UNINITIALIZED;\n-    }\n-    if (aligned) {\n-      decorators |= ARRAYCOPY_ALIGNED;\n-    }\n-    BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    bs->arraycopy_prologue(_masm, decorators, type, from, to, count);\n-    {\n-      \/\/ Type(shift)       byte(0), short(1), int(2),   long(3)\n-      int loop_size[]   = { 192,     96,       48,      24};\n-      int threshold[]   = { 4096,    2048,     1024,    512};\n-\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n-      \/\/ 'from', 'to' and 'count' are now valid\n-\n-      \/\/ temp1 holds remaining count.\n-      __ movq(temp1, count);\n-\n-      \/\/ Zero length check.\n-      __ BIND(L_tail);\n-      __ cmpq(temp1, 0);\n-      __ jcc(Assembler::lessEqual, L_exit);\n-\n-      __ mov64(temp2, 0);\n-      __ movq(temp3, temp1);\n-      \/\/ Special cases using 32 byte [masked] vector copy operations.\n-      __ arraycopy_avx3_special_cases_conjoint(xmm1, k2, from, to, temp2, temp3, temp1, shift,\n-                                               temp4, use64byteVector, L_entry, L_exit);\n-\n-      \/\/ PRE-MAIN-POST loop for aligned copy.\n-      __ BIND(L_entry);\n-\n-      if ((MaxVectorSize > 32) && (avx3threshold != 0)) {\n-        __ cmpq(temp1, threshold[shift]);\n-        __ jcc(Assembler::greaterEqual, L_pre_main_post_64);\n-      }\n-\n-      if ((MaxVectorSize < 64)  || (avx3threshold != 0)) {\n-        \/\/ Partial copy to make dst address 32 byte aligned.\n-        __ leaq(temp2, Address(to, temp1, (Address::ScaleFactor)(shift), 0));\n-        __ andq(temp2, 31);\n-        __ jcc(Assembler::equal, L_main_pre_loop);\n-\n-        if (shift) {\n-          __ shrq(temp2, shift);\n-        }\n-        __ subq(temp1, temp2);\n-        __ copy32_masked_avx(to, from, xmm1, k2, temp2, temp1, temp3, shift);\n-\n-        __ cmpq(temp1, loop_size[shift]);\n-        __ jcc(Assembler::less, L_tail);\n-\n-        __ BIND(L_main_pre_loop);\n-\n-        \/\/ Main loop with aligned copy block size of 192 bytes at 32 byte granularity.\n-        __ align32();\n-        __ BIND(L_main_loop);\n-           __ copy64_avx(to, from, temp1, xmm1, true, shift, -64);\n-           __ copy64_avx(to, from, temp1, xmm1, true, shift, -128);\n-           __ copy64_avx(to, from, temp1, xmm1, true, shift, -192);\n-           __ subptr(temp1, loop_size[shift]);\n-           __ cmpq(temp1, loop_size[shift]);\n-           __ jcc(Assembler::greater, L_main_loop);\n-\n-        \/\/ Tail loop.\n-        __ jmp(L_tail);\n-      }\n-\n-      if (MaxVectorSize > 32) {\n-        __ BIND(L_pre_main_post_64);\n-        \/\/ Partial copy to make dst address 64 byte aligned.\n-        __ leaq(temp2, Address(to, temp1, (Address::ScaleFactor)(shift), 0));\n-        __ andq(temp2, 63);\n-        __ jcc(Assembler::equal, L_main_pre_loop_64bytes);\n-\n-        if (shift) {\n-          __ shrq(temp2, shift);\n-        }\n-        __ subq(temp1, temp2);\n-        __ copy64_masked_avx(to, from, xmm1, k2, temp2, temp1, temp3, shift, 0 , true);\n-\n-        __ cmpq(temp1, loop_size[shift]);\n-        __ jcc(Assembler::less, L_tail64);\n-\n-        __ BIND(L_main_pre_loop_64bytes);\n-\n-        \/\/ Main loop with aligned copy block size of 192 bytes at\n-        \/\/ 64 byte copy granularity.\n-        __ align32();\n-        __ BIND(L_main_loop_64bytes);\n-           __ copy64_avx(to, from, temp1, xmm1, true, shift, -64 , true);\n-           __ copy64_avx(to, from, temp1, xmm1, true, shift, -128, true);\n-           __ copy64_avx(to, from, temp1, xmm1, true, shift, -192, true);\n-           __ subq(temp1, loop_size[shift]);\n-           __ cmpq(temp1, loop_size[shift]);\n-           __ jcc(Assembler::greater, L_main_loop_64bytes);\n-\n-        \/\/ Zero length check.\n-        __ cmpq(temp1, 0);\n-        __ jcc(Assembler::lessEqual, L_exit);\n-\n-        __ BIND(L_tail64);\n-\n-        \/\/ Tail handling using 64 byte [masked] vector copy operations.\n-        use64byteVector = true;\n-        __ mov64(temp2, 0);\n-        __ movq(temp3, temp1);\n-        __ arraycopy_avx3_special_cases_conjoint(xmm1, k2, from, to, temp2, temp3, temp1, shift,\n-                                                 temp4, use64byteVector, L_entry, L_exit);\n-      }\n-      __ BIND(L_exit);\n-    }\n-    address ucme_exit_pc = __ pc();\n-    \/\/ When called from generic_arraycopy r11 contains specific values\n-    \/\/ used during arraycopy epilogue, re-initializing r11.\n-    if(is_oop) {\n-      __ movq(r11, count);\n-    }\n-    bs->arraycopy_epilogue(_masm, decorators, type, from, to, count);\n-    restore_argument_regs(type);\n-    inc_counter_np(get_profile_ctr(shift)); \/\/ Update counter after rscratch1 is free\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-    return start;\n-  }\n-#endif \/\/ COMPILER2_OR_JVMCI\n-\n-\n-  \/\/ Arguments:\n-  \/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n-  \/\/             ignored\n-  \/\/   name    - stub name string\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source array address\n-  \/\/   c_rarg1   - destination array address\n-  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n-  \/\/\n-  \/\/ If 'from' and\/or 'to' are aligned on 4-, 2-, or 1-byte boundaries,\n-  \/\/ we let the hardware handle it.  The one to eight bytes within words,\n-  \/\/ dwords or qwords that span cache line boundaries will still be loaded\n-  \/\/ and stored atomically.\n-  \/\/\n-  \/\/ Side Effects:\n-  \/\/   disjoint_byte_copy_entry is set to the no-overlap entry point\n-  \/\/   used by generate_conjoint_byte_copy().\n-  \/\/\n-  address generate_disjoint_byte_copy(bool aligned, address* entry, const char *name) {\n-#if COMPILER2_OR_JVMCI\n-    if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n-       return generate_disjoint_copy_avx3_masked(entry, \"jbyte_disjoint_arraycopy_avx3\", 0,\n-                                                 aligned, false, false);\n-    }\n-#endif\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    Label L_copy_bytes, L_copy_8_bytes, L_copy_4_bytes, L_copy_2_bytes;\n-    Label L_copy_byte, L_exit;\n-    const Register from        = rdi;  \/\/ source array address\n-    const Register to          = rsi;  \/\/ destination array address\n-    const Register count       = rdx;  \/\/ elements count\n-    const Register byte_count  = rcx;\n-    const Register qword_count = count;\n-    const Register end_from    = from; \/\/ source array end address\n-    const Register end_to      = to;   \/\/ destination array end address\n-    \/\/ End pointers are inclusive, and if count is not zero they point\n-    \/\/ to the last unit copied:  end_to[0] := end_from[0]\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n-\n-    if (entry != NULL) {\n-      *entry = __ pc();\n-       \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n-      BLOCK_COMMENT(\"Entry:\");\n-    }\n-\n-    setup_arg_regs(); \/\/ from => rdi, to => rsi, count => rdx\n-                      \/\/ r9 and r10 may be used to save non-volatile registers\n-\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n-      \/\/ 'from', 'to' and 'count' are now valid\n-      __ movptr(byte_count, count);\n-      __ shrptr(count, 3); \/\/ count => qword_count\n-\n-      \/\/ Copy from low to high addresses.  Use 'to' as scratch.\n-      __ lea(end_from, Address(from, qword_count, Address::times_8, -8));\n-      __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));\n-      __ negptr(qword_count); \/\/ make the count negative\n-      __ jmp(L_copy_bytes);\n-\n-      \/\/ Copy trailing qwords\n-    __ BIND(L_copy_8_bytes);\n-      __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));\n-      __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);\n-      __ increment(qword_count);\n-      __ jcc(Assembler::notZero, L_copy_8_bytes);\n-\n-      \/\/ Check for and copy trailing dword\n-    __ BIND(L_copy_4_bytes);\n-      __ testl(byte_count, 4);\n-      __ jccb(Assembler::zero, L_copy_2_bytes);\n-      __ movl(rax, Address(end_from, 8));\n-      __ movl(Address(end_to, 8), rax);\n-\n-      __ addptr(end_from, 4);\n-      __ addptr(end_to, 4);\n-\n-      \/\/ Check for and copy trailing word\n-    __ BIND(L_copy_2_bytes);\n-      __ testl(byte_count, 2);\n-      __ jccb(Assembler::zero, L_copy_byte);\n-      __ movw(rax, Address(end_from, 8));\n-      __ movw(Address(end_to, 8), rax);\n-\n-      __ addptr(end_from, 2);\n-      __ addptr(end_to, 2);\n-\n-      \/\/ Check for and copy trailing byte\n-    __ BIND(L_copy_byte);\n-      __ testl(byte_count, 1);\n-      __ jccb(Assembler::zero, L_exit);\n-      __ movb(rax, Address(end_from, 8));\n-      __ movb(Address(end_to, 8), rax);\n-    }\n-  __ BIND(L_exit);\n-    address ucme_exit_pc = __ pc();\n-    restore_arg_regs();\n-    inc_counter_np(SharedRuntime::_jbyte_array_copy_ctr); \/\/ Update counter after rscratch1 is free\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    {\n-      UnsafeCopyMemoryMark ucmm(this, !aligned, false, ucme_exit_pc);\n-      \/\/ Copy in multi-bytes chunks\n-      copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-      __ jmp(L_copy_4_bytes);\n-    }\n-    return start;\n-  }\n-\n-  \/\/ Arguments:\n-  \/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n-  \/\/             ignored\n-  \/\/   name    - stub name string\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source array address\n-  \/\/   c_rarg1   - destination array address\n-  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n-  \/\/\n-  \/\/ If 'from' and\/or 'to' are aligned on 4-, 2-, or 1-byte boundaries,\n-  \/\/ we let the hardware handle it.  The one to eight bytes within words,\n-  \/\/ dwords or qwords that span cache line boundaries will still be loaded\n-  \/\/ and stored atomically.\n-  \/\/\n-  address generate_conjoint_byte_copy(bool aligned, address nooverlap_target,\n-                                      address* entry, const char *name) {\n-#if COMPILER2_OR_JVMCI\n-    if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n-       return generate_conjoint_copy_avx3_masked(entry, \"jbyte_conjoint_arraycopy_avx3\", 0,\n-                                                 nooverlap_target, aligned, false, false);\n-    }\n-#endif\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    Label L_copy_bytes, L_copy_8_bytes, L_copy_4_bytes, L_copy_2_bytes;\n-    const Register from        = rdi;  \/\/ source array address\n-    const Register to          = rsi;  \/\/ destination array address\n-    const Register count       = rdx;  \/\/ elements count\n-    const Register byte_count  = rcx;\n-    const Register qword_count = count;\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n-\n-    if (entry != NULL) {\n-      *entry = __ pc();\n-      \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n-      BLOCK_COMMENT(\"Entry:\");\n-    }\n-\n-    array_overlap_test(nooverlap_target, Address::times_1);\n-    setup_arg_regs(); \/\/ from => rdi, to => rsi, count => rdx\n-                      \/\/ r9 and r10 may be used to save non-volatile registers\n-\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n-      \/\/ 'from', 'to' and 'count' are now valid\n-      __ movptr(byte_count, count);\n-      __ shrptr(count, 3);   \/\/ count => qword_count\n-\n-      \/\/ Copy from high to low addresses.\n-\n-      \/\/ Check for and copy trailing byte\n-      __ testl(byte_count, 1);\n-      __ jcc(Assembler::zero, L_copy_2_bytes);\n-      __ movb(rax, Address(from, byte_count, Address::times_1, -1));\n-      __ movb(Address(to, byte_count, Address::times_1, -1), rax);\n-      __ decrement(byte_count); \/\/ Adjust for possible trailing word\n-\n-      \/\/ Check for and copy trailing word\n-    __ BIND(L_copy_2_bytes);\n-      __ testl(byte_count, 2);\n-      __ jcc(Assembler::zero, L_copy_4_bytes);\n-      __ movw(rax, Address(from, byte_count, Address::times_1, -2));\n-      __ movw(Address(to, byte_count, Address::times_1, -2), rax);\n-\n-      \/\/ Check for and copy trailing dword\n-    __ BIND(L_copy_4_bytes);\n-      __ testl(byte_count, 4);\n-      __ jcc(Assembler::zero, L_copy_bytes);\n-      __ movl(rax, Address(from, qword_count, Address::times_8));\n-      __ movl(Address(to, qword_count, Address::times_8), rax);\n-      __ jmp(L_copy_bytes);\n-\n-      \/\/ Copy trailing qwords\n-    __ BIND(L_copy_8_bytes);\n-      __ movq(rax, Address(from, qword_count, Address::times_8, -8));\n-      __ movq(Address(to, qword_count, Address::times_8, -8), rax);\n-      __ decrement(qword_count);\n-      __ jcc(Assembler::notZero, L_copy_8_bytes);\n-    }\n-    restore_arg_regs();\n-    inc_counter_np(SharedRuntime::_jbyte_array_copy_ctr); \/\/ Update counter after rscratch1 is free\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n-      \/\/ Copy in multi-bytes chunks\n-      copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-    }\n-    restore_arg_regs();\n-    inc_counter_np(SharedRuntime::_jbyte_array_copy_ctr); \/\/ Update counter after rscratch1 is free\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    return start;\n-  }\n-\n-  \/\/ Arguments:\n-  \/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n-  \/\/             ignored\n-  \/\/   name    - stub name string\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source array address\n-  \/\/   c_rarg1   - destination array address\n-  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n-  \/\/\n-  \/\/ If 'from' and\/or 'to' are aligned on 4- or 2-byte boundaries, we\n-  \/\/ let the hardware handle it.  The two or four words within dwords\n-  \/\/ or qwords that span cache line boundaries will still be loaded\n-  \/\/ and stored atomically.\n-  \/\/\n-  \/\/ Side Effects:\n-  \/\/   disjoint_short_copy_entry is set to the no-overlap entry point\n-  \/\/   used by generate_conjoint_short_copy().\n-  \/\/\n-  address generate_disjoint_short_copy(bool aligned, address *entry, const char *name) {\n-#if COMPILER2_OR_JVMCI\n-    if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n-       return generate_disjoint_copy_avx3_masked(entry, \"jshort_disjoint_arraycopy_avx3\", 1,\n-                                                 aligned, false, false);\n-    }\n-#endif\n-\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    Label L_copy_bytes, L_copy_8_bytes, L_copy_4_bytes,L_copy_2_bytes,L_exit;\n-    const Register from        = rdi;  \/\/ source array address\n-    const Register to          = rsi;  \/\/ destination array address\n-    const Register count       = rdx;  \/\/ elements count\n-    const Register word_count  = rcx;\n-    const Register qword_count = count;\n-    const Register end_from    = from; \/\/ source array end address\n-    const Register end_to      = to;   \/\/ destination array end address\n-    \/\/ End pointers are inclusive, and if count is not zero they point\n-    \/\/ to the last unit copied:  end_to[0] := end_from[0]\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n-\n-    if (entry != NULL) {\n-      *entry = __ pc();\n-      \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n-      BLOCK_COMMENT(\"Entry:\");\n-    }\n-\n-    setup_arg_regs(); \/\/ from => rdi, to => rsi, count => rdx\n-                      \/\/ r9 and r10 may be used to save non-volatile registers\n-\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n-      \/\/ 'from', 'to' and 'count' are now valid\n-      __ movptr(word_count, count);\n-      __ shrptr(count, 2); \/\/ count => qword_count\n-\n-      \/\/ Copy from low to high addresses.  Use 'to' as scratch.\n-      __ lea(end_from, Address(from, qword_count, Address::times_8, -8));\n-      __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));\n-      __ negptr(qword_count);\n-      __ jmp(L_copy_bytes);\n-\n-      \/\/ Copy trailing qwords\n-    __ BIND(L_copy_8_bytes);\n-      __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));\n-      __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);\n-      __ increment(qword_count);\n-      __ jcc(Assembler::notZero, L_copy_8_bytes);\n-\n-      \/\/ Original 'dest' is trashed, so we can't use it as a\n-      \/\/ base register for a possible trailing word copy\n-\n-      \/\/ Check for and copy trailing dword\n-    __ BIND(L_copy_4_bytes);\n-      __ testl(word_count, 2);\n-      __ jccb(Assembler::zero, L_copy_2_bytes);\n-      __ movl(rax, Address(end_from, 8));\n-      __ movl(Address(end_to, 8), rax);\n-\n-      __ addptr(end_from, 4);\n-      __ addptr(end_to, 4);\n-\n-      \/\/ Check for and copy trailing word\n-    __ BIND(L_copy_2_bytes);\n-      __ testl(word_count, 1);\n-      __ jccb(Assembler::zero, L_exit);\n-      __ movw(rax, Address(end_from, 8));\n-      __ movw(Address(end_to, 8), rax);\n-    }\n-  __ BIND(L_exit);\n-    address ucme_exit_pc = __ pc();\n-    restore_arg_regs();\n-    inc_counter_np(SharedRuntime::_jshort_array_copy_ctr); \/\/ Update counter after rscratch1 is free\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    {\n-      UnsafeCopyMemoryMark ucmm(this, !aligned, false, ucme_exit_pc);\n-      \/\/ Copy in multi-bytes chunks\n-      copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-      __ jmp(L_copy_4_bytes);\n-    }\n-\n-    return start;\n-  }\n-\n-  address generate_fill(BasicType t, bool aligned, const char *name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    BLOCK_COMMENT(\"Entry:\");\n-\n-    const Register to       = c_rarg0;  \/\/ destination array address\n-    const Register value    = c_rarg1;  \/\/ value\n-    const Register count    = c_rarg2;  \/\/ elements count\n-    __ mov(r11, count);\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-    __ generate_fill(t, aligned, to, value, r11, rax, xmm0);\n-\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-    return start;\n-  }\n-\n-  \/\/ Arguments:\n-  \/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n-  \/\/             ignored\n-  \/\/   name    - stub name string\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source array address\n-  \/\/   c_rarg1   - destination array address\n-  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n-  \/\/\n-  \/\/ If 'from' and\/or 'to' are aligned on 4- or 2-byte boundaries, we\n-  \/\/ let the hardware handle it.  The two or four words within dwords\n-  \/\/ or qwords that span cache line boundaries will still be loaded\n-  \/\/ and stored atomically.\n-  \/\/\n-  address generate_conjoint_short_copy(bool aligned, address nooverlap_target,\n-                                       address *entry, const char *name) {\n-#if COMPILER2_OR_JVMCI\n-    if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n-       return generate_conjoint_copy_avx3_masked(entry, \"jshort_conjoint_arraycopy_avx3\", 1,\n-                                                 nooverlap_target, aligned, false, false);\n-    }\n-#endif\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    Label L_copy_bytes, L_copy_8_bytes, L_copy_4_bytes;\n-    const Register from        = rdi;  \/\/ source array address\n-    const Register to          = rsi;  \/\/ destination array address\n-    const Register count       = rdx;  \/\/ elements count\n-    const Register word_count  = rcx;\n-    const Register qword_count = count;\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n-\n-    if (entry != NULL) {\n-      *entry = __ pc();\n-      \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n-      BLOCK_COMMENT(\"Entry:\");\n-    }\n-\n-    array_overlap_test(nooverlap_target, Address::times_2);\n-    setup_arg_regs(); \/\/ from => rdi, to => rsi, count => rdx\n-                      \/\/ r9 and r10 may be used to save non-volatile registers\n-\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n-      \/\/ 'from', 'to' and 'count' are now valid\n-      __ movptr(word_count, count);\n-      __ shrptr(count, 2); \/\/ count => qword_count\n-\n-      \/\/ Copy from high to low addresses.  Use 'to' as scratch.\n-\n-      \/\/ Check for and copy trailing word\n-      __ testl(word_count, 1);\n-      __ jccb(Assembler::zero, L_copy_4_bytes);\n-      __ movw(rax, Address(from, word_count, Address::times_2, -2));\n-      __ movw(Address(to, word_count, Address::times_2, -2), rax);\n-\n-     \/\/ Check for and copy trailing dword\n-    __ BIND(L_copy_4_bytes);\n-      __ testl(word_count, 2);\n-      __ jcc(Assembler::zero, L_copy_bytes);\n-      __ movl(rax, Address(from, qword_count, Address::times_8));\n-      __ movl(Address(to, qword_count, Address::times_8), rax);\n-      __ jmp(L_copy_bytes);\n-\n-      \/\/ Copy trailing qwords\n-    __ BIND(L_copy_8_bytes);\n-      __ movq(rax, Address(from, qword_count, Address::times_8, -8));\n-      __ movq(Address(to, qword_count, Address::times_8, -8), rax);\n-      __ decrement(qword_count);\n-      __ jcc(Assembler::notZero, L_copy_8_bytes);\n-    }\n-    restore_arg_regs();\n-    inc_counter_np(SharedRuntime::_jshort_array_copy_ctr); \/\/ Update counter after rscratch1 is free\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n-      \/\/ Copy in multi-bytes chunks\n-      copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-    }\n-    restore_arg_regs();\n-    inc_counter_np(SharedRuntime::_jshort_array_copy_ctr); \/\/ Update counter after rscratch1 is free\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    return start;\n-  }\n-\n-  \/\/ Arguments:\n-  \/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n-  \/\/             ignored\n-  \/\/   is_oop  - true => oop array, so generate store check code\n-  \/\/   name    - stub name string\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source array address\n-  \/\/   c_rarg1   - destination array address\n-  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n-  \/\/\n-  \/\/ If 'from' and\/or 'to' are aligned on 4-byte boundaries, we let\n-  \/\/ the hardware handle it.  The two dwords within qwords that span\n-  \/\/ cache line boundaries will still be loaded and stored atomically.\n-  \/\/\n-  \/\/ Side Effects:\n-  \/\/   disjoint_int_copy_entry is set to the no-overlap entry point\n-  \/\/   used by generate_conjoint_int_oop_copy().\n-  \/\/\n-  address generate_disjoint_int_oop_copy(bool aligned, bool is_oop, address* entry,\n-                                         const char *name, bool dest_uninitialized = false) {\n-#if COMPILER2_OR_JVMCI\n-    if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n-       return generate_disjoint_copy_avx3_masked(entry, \"jint_disjoint_arraycopy_avx3\", 2,\n-                                                 aligned, is_oop, dest_uninitialized);\n-    }\n-#endif\n-\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    Label L_copy_bytes, L_copy_8_bytes, L_copy_4_bytes, L_exit;\n-    const Register from        = rdi;  \/\/ source array address\n-    const Register to          = rsi;  \/\/ destination array address\n-    const Register count       = rdx;  \/\/ elements count\n-    const Register dword_count = rcx;\n-    const Register qword_count = count;\n-    const Register end_from    = from; \/\/ source array end address\n-    const Register end_to      = to;   \/\/ destination array end address\n-    \/\/ End pointers are inclusive, and if count is not zero they point\n-    \/\/ to the last unit copied:  end_to[0] := end_from[0]\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n-\n-    if (entry != NULL) {\n-      *entry = __ pc();\n-      \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n-      BLOCK_COMMENT(\"Entry:\");\n-    }\n-\n-    setup_arg_regs_using_thread(); \/\/ from => rdi, to => rsi, count => rdx\n-                                   \/\/ r9 is used to save r15_thread\n-\n-    DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_DISJOINT;\n-    if (dest_uninitialized) {\n-      decorators |= IS_DEST_UNINITIALIZED;\n-    }\n-    if (aligned) {\n-      decorators |= ARRAYCOPY_ALIGNED;\n-    }\n-\n-    BasicType type = is_oop ? T_OBJECT : T_INT;\n-    BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    bs->arraycopy_prologue(_masm, decorators, type, from, to, count);\n-\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n-      \/\/ 'from', 'to' and 'count' are now valid\n-      __ movptr(dword_count, count);\n-      __ shrptr(count, 1); \/\/ count => qword_count\n-\n-      \/\/ Copy from low to high addresses.  Use 'to' as scratch.\n-      __ lea(end_from, Address(from, qword_count, Address::times_8, -8));\n-      __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));\n-      __ negptr(qword_count);\n-      __ jmp(L_copy_bytes);\n-\n-      \/\/ Copy trailing qwords\n-    __ BIND(L_copy_8_bytes);\n-      __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));\n-      __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);\n-      __ increment(qword_count);\n-      __ jcc(Assembler::notZero, L_copy_8_bytes);\n-\n-      \/\/ Check for and copy trailing dword\n-    __ BIND(L_copy_4_bytes);\n-      __ testl(dword_count, 1); \/\/ Only byte test since the value is 0 or 1\n-      __ jccb(Assembler::zero, L_exit);\n-      __ movl(rax, Address(end_from, 8));\n-      __ movl(Address(end_to, 8), rax);\n-    }\n-  __ BIND(L_exit);\n-    address ucme_exit_pc = __ pc();\n-    bs->arraycopy_epilogue(_masm, decorators, type, from, to, dword_count);\n-    restore_arg_regs_using_thread();\n-    inc_counter_np(SharedRuntime::_jint_array_copy_ctr); \/\/ Update counter after rscratch1 is free\n-    __ vzeroupper();\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    {\n-      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, false, ucme_exit_pc);\n-      \/\/ Copy in multi-bytes chunks\n-      copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-      __ jmp(L_copy_4_bytes);\n-    }\n-\n-    return start;\n-  }\n-\n-  \/\/ Arguments:\n-  \/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n-  \/\/             ignored\n-  \/\/   is_oop  - true => oop array, so generate store check code\n-  \/\/   name    - stub name string\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source array address\n-  \/\/   c_rarg1   - destination array address\n-  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n-  \/\/\n-  \/\/ If 'from' and\/or 'to' are aligned on 4-byte boundaries, we let\n-  \/\/ the hardware handle it.  The two dwords within qwords that span\n-  \/\/ cache line boundaries will still be loaded and stored atomically.\n-  \/\/\n-  address generate_conjoint_int_oop_copy(bool aligned, bool is_oop, address nooverlap_target,\n-                                         address *entry, const char *name,\n-                                         bool dest_uninitialized = false) {\n-#if COMPILER2_OR_JVMCI\n-    if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n-       return generate_conjoint_copy_avx3_masked(entry, \"jint_conjoint_arraycopy_avx3\", 2,\n-                                                 nooverlap_target, aligned, is_oop, dest_uninitialized);\n-    }\n-#endif\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    Label L_copy_bytes, L_copy_8_bytes, L_exit;\n-    const Register from        = rdi;  \/\/ source array address\n-    const Register to          = rsi;  \/\/ destination array address\n-    const Register count       = rdx;  \/\/ elements count\n-    const Register dword_count = rcx;\n-    const Register qword_count = count;\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n-\n-    if (entry != NULL) {\n-      *entry = __ pc();\n-       \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n-      BLOCK_COMMENT(\"Entry:\");\n-    }\n-\n-    array_overlap_test(nooverlap_target, Address::times_4);\n-    setup_arg_regs_using_thread(); \/\/ from => rdi, to => rsi, count => rdx\n-                                   \/\/ r9 is used to save r15_thread\n-\n-    DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n-    if (dest_uninitialized) {\n-      decorators |= IS_DEST_UNINITIALIZED;\n-    }\n-    if (aligned) {\n-      decorators |= ARRAYCOPY_ALIGNED;\n-    }\n-\n-    BasicType type = is_oop ? T_OBJECT : T_INT;\n-    BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    \/\/ no registers are destroyed by this call\n-    bs->arraycopy_prologue(_masm, decorators, type, from, to, count);\n-\n-    assert_clean_int(count, rax); \/\/ Make sure 'count' is clean int.\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n-      \/\/ 'from', 'to' and 'count' are now valid\n-      __ movptr(dword_count, count);\n-      __ shrptr(count, 1); \/\/ count => qword_count\n-\n-      \/\/ Copy from high to low addresses.  Use 'to' as scratch.\n-\n-      \/\/ Check for and copy trailing dword\n-      __ testl(dword_count, 1);\n-      __ jcc(Assembler::zero, L_copy_bytes);\n-      __ movl(rax, Address(from, dword_count, Address::times_4, -4));\n-      __ movl(Address(to, dword_count, Address::times_4, -4), rax);\n-      __ jmp(L_copy_bytes);\n-\n-      \/\/ Copy trailing qwords\n-    __ BIND(L_copy_8_bytes);\n-      __ movq(rax, Address(from, qword_count, Address::times_8, -8));\n-      __ movq(Address(to, qword_count, Address::times_8, -8), rax);\n-      __ decrement(qword_count);\n-      __ jcc(Assembler::notZero, L_copy_8_bytes);\n-    }\n-    if (is_oop) {\n-      __ jmp(L_exit);\n-    }\n-    restore_arg_regs_using_thread();\n-    inc_counter_np(SharedRuntime::_jint_array_copy_ctr); \/\/ Update counter after rscratch1 is free\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n-      \/\/ Copy in multi-bytes chunks\n-      copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-    }\n-\n-  __ BIND(L_exit);\n-    bs->arraycopy_epilogue(_masm, decorators, type, from, to, dword_count);\n-    restore_arg_regs_using_thread();\n-    inc_counter_np(SharedRuntime::_jint_array_copy_ctr); \/\/ Update counter after rscratch1 is free\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    return start;\n-  }\n-\n-  \/\/ Arguments:\n-  \/\/   aligned - true => Input and output aligned on a HeapWord boundary == 8 bytes\n-  \/\/             ignored\n-  \/\/   is_oop  - true => oop array, so generate store check code\n-  \/\/   name    - stub name string\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source array address\n-  \/\/   c_rarg1   - destination array address\n-  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n-  \/\/\n- \/\/ Side Effects:\n-  \/\/   disjoint_oop_copy_entry or disjoint_long_copy_entry is set to the\n-  \/\/   no-overlap entry point used by generate_conjoint_long_oop_copy().\n-  \/\/\n-  address generate_disjoint_long_oop_copy(bool aligned, bool is_oop, address *entry,\n-                                          const char *name, bool dest_uninitialized = false) {\n-#if COMPILER2_OR_JVMCI\n-    if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n-       return generate_disjoint_copy_avx3_masked(entry, \"jlong_disjoint_arraycopy_avx3\", 3,\n-                                                 aligned, is_oop, dest_uninitialized);\n-    }\n-#endif\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    Label L_copy_bytes, L_copy_8_bytes, L_exit;\n-    const Register from        = rdi;  \/\/ source array address\n-    const Register to          = rsi;  \/\/ destination array address\n-    const Register qword_count = rdx;  \/\/ elements count\n-    const Register end_from    = from; \/\/ source array end address\n-    const Register end_to      = rcx;  \/\/ destination array end address\n-    const Register saved_count = r11;\n-    \/\/ End pointers are inclusive, and if count is not zero they point\n-    \/\/ to the last unit copied:  end_to[0] := end_from[0]\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    \/\/ Save no-overlap entry point for generate_conjoint_long_oop_copy()\n-    assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n-\n-    if (entry != NULL) {\n-      *entry = __ pc();\n-      \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n-      BLOCK_COMMENT(\"Entry:\");\n-    }\n-\n-    setup_arg_regs_using_thread(); \/\/ from => rdi, to => rsi, count => rdx\n-                                     \/\/ r9 is used to save r15_thread\n-    \/\/ 'from', 'to' and 'qword_count' are now valid\n-\n-    DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_DISJOINT;\n-    if (dest_uninitialized) {\n-      decorators |= IS_DEST_UNINITIALIZED;\n-    }\n-    if (aligned) {\n-      decorators |= ARRAYCOPY_ALIGNED;\n-    }\n-\n-    BasicType type = is_oop ? T_OBJECT : T_LONG;\n-    BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    bs->arraycopy_prologue(_masm, decorators, type, from, to, qword_count);\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n-\n-      \/\/ Copy from low to high addresses.  Use 'to' as scratch.\n-      __ lea(end_from, Address(from, qword_count, Address::times_8, -8));\n-      __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));\n-      __ negptr(qword_count);\n-      __ jmp(L_copy_bytes);\n-\n-      \/\/ Copy trailing qwords\n-    __ BIND(L_copy_8_bytes);\n-      __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));\n-      __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);\n-      __ increment(qword_count);\n-      __ jcc(Assembler::notZero, L_copy_8_bytes);\n-    }\n-    if (is_oop) {\n-      __ jmp(L_exit);\n-    } else {\n-      restore_arg_regs_using_thread();\n-      inc_counter_np(SharedRuntime::_jlong_array_copy_ctr); \/\/ Update counter after rscratch1 is free\n-      __ xorptr(rax, rax); \/\/ return 0\n-      __ vzeroupper();\n-      __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-      __ ret(0);\n-    }\n-\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n-      \/\/ Copy in multi-bytes chunks\n-      copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-    }\n-\n-    __ BIND(L_exit);\n-    bs->arraycopy_epilogue(_masm, decorators, type, from, to, qword_count);\n-    restore_arg_regs_using_thread();\n-    if (is_oop) {\n-      inc_counter_np(SharedRuntime::_oop_array_copy_ctr); \/\/ Update counter after rscratch1 is free\n-    } else {\n-      inc_counter_np(SharedRuntime::_jlong_array_copy_ctr); \/\/ Update counter after rscratch1 is free\n-    }\n-    __ vzeroupper();\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    return start;\n-  }\n-\n-  \/\/ Arguments:\n-  \/\/   aligned - true => Input and output aligned on a HeapWord boundary == 8 bytes\n-  \/\/             ignored\n-  \/\/   is_oop  - true => oop array, so generate store check code\n-  \/\/   name    - stub name string\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source array address\n-  \/\/   c_rarg1   - destination array address\n-  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n-  \/\/\n-  address generate_conjoint_long_oop_copy(bool aligned, bool is_oop,\n-                                          address nooverlap_target, address *entry,\n-                                          const char *name, bool dest_uninitialized = false) {\n-#if COMPILER2_OR_JVMCI\n-    if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n-       return generate_conjoint_copy_avx3_masked(entry, \"jlong_conjoint_arraycopy_avx3\", 3,\n-                                                 nooverlap_target, aligned, is_oop, dest_uninitialized);\n-    }\n-#endif\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    Label L_copy_bytes, L_copy_8_bytes, L_exit;\n-    const Register from        = rdi;  \/\/ source array address\n-    const Register to          = rsi;  \/\/ destination array address\n-    const Register qword_count = rdx;  \/\/ elements count\n-    const Register saved_count = rcx;\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n-\n-    if (entry != NULL) {\n-      *entry = __ pc();\n-      \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n-      BLOCK_COMMENT(\"Entry:\");\n-    }\n-\n-    array_overlap_test(nooverlap_target, Address::times_8);\n-    setup_arg_regs_using_thread(); \/\/ from => rdi, to => rsi, count => rdx\n-                                   \/\/ r9 is used to save r15_thread\n-    \/\/ 'from', 'to' and 'qword_count' are now valid\n-\n-    DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n-    if (dest_uninitialized) {\n-      decorators |= IS_DEST_UNINITIALIZED;\n-    }\n-    if (aligned) {\n-      decorators |= ARRAYCOPY_ALIGNED;\n-    }\n-\n-    BasicType type = is_oop ? T_OBJECT : T_LONG;\n-    BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    bs->arraycopy_prologue(_masm, decorators, type, from, to, qword_count);\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n-\n-      __ jmp(L_copy_bytes);\n-\n-      \/\/ Copy trailing qwords\n-    __ BIND(L_copy_8_bytes);\n-      __ movq(rax, Address(from, qword_count, Address::times_8, -8));\n-      __ movq(Address(to, qword_count, Address::times_8, -8), rax);\n-      __ decrement(qword_count);\n-      __ jcc(Assembler::notZero, L_copy_8_bytes);\n-    }\n-    if (is_oop) {\n-      __ jmp(L_exit);\n-    } else {\n-      restore_arg_regs_using_thread();\n-      inc_counter_np(SharedRuntime::_jlong_array_copy_ctr); \/\/ Update counter after rscratch1 is free\n-      __ xorptr(rax, rax); \/\/ return 0\n-      __ vzeroupper();\n-      __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-      __ ret(0);\n-    }\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n-\n-      \/\/ Copy in multi-bytes chunks\n-      copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-    }\n-    __ BIND(L_exit);\n-    bs->arraycopy_epilogue(_masm, decorators, type, from, to, qword_count);\n-    restore_arg_regs_using_thread();\n-    if (is_oop) {\n-      inc_counter_np(SharedRuntime::_oop_array_copy_ctr); \/\/ Update counter after rscratch1 is free\n-    } else {\n-      inc_counter_np(SharedRuntime::_jlong_array_copy_ctr); \/\/ Update counter after rscratch1 is free\n-    }\n-    __ vzeroupper();\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    return start;\n-  }\n-\n-\n-  \/\/ Helper for generating a dynamic type check.\n-  \/\/ Smashes no registers.\n-  void generate_type_check(Register sub_klass,\n-                           Register super_check_offset,\n-                           Register super_klass,\n-                           Label& L_success) {\n-    assert_different_registers(sub_klass, super_check_offset, super_klass);\n-\n-    BLOCK_COMMENT(\"type_check:\");\n-\n-    Label L_miss;\n-\n-    __ check_klass_subtype_fast_path(sub_klass, super_klass, noreg,        &L_success, &L_miss, NULL,\n-                                     super_check_offset);\n-    __ check_klass_subtype_slow_path(sub_klass, super_klass, noreg, noreg, &L_success, NULL);\n-\n-    \/\/ Fall through on failure!\n-    __ BIND(L_miss);\n-  }\n-\n-  \/\/\n-  \/\/  Generate checkcasting array copy stub\n-  \/\/\n-  \/\/  Input:\n-  \/\/    c_rarg0   - source array address\n-  \/\/    c_rarg1   - destination array address\n-  \/\/    c_rarg2   - element count, treated as ssize_t, can be zero\n-  \/\/    c_rarg3   - size_t ckoff (super_check_offset)\n-  \/\/ not Win64\n-  \/\/    c_rarg4   - oop ckval (super_klass)\n-  \/\/ Win64\n-  \/\/    rsp+40    - oop ckval (super_klass)\n-  \/\/\n-  \/\/  Output:\n-  \/\/    rax ==  0  -  success\n-  \/\/    rax == -1^K - failure, where K is partial transfer count\n-  \/\/\n-  address generate_checkcast_copy(const char *name, address *entry,\n-                                  bool dest_uninitialized = false) {\n-\n-    Label L_load_element, L_store_element, L_do_card_marks, L_done;\n-\n-    \/\/ Input registers (after setup_arg_regs)\n-    const Register from        = rdi;   \/\/ source array address\n-    const Register to          = rsi;   \/\/ destination array address\n-    const Register length      = rdx;   \/\/ elements count\n-    const Register ckoff       = rcx;   \/\/ super_check_offset\n-    const Register ckval       = r8;    \/\/ super_klass\n-\n-    \/\/ Registers used as temps (r13, r14 are save-on-entry)\n-    const Register end_from    = from;  \/\/ source array end address\n-    const Register end_to      = r13;   \/\/ destination array end address\n-    const Register count       = rdx;   \/\/ -(count_remaining)\n-    const Register r14_length  = r14;   \/\/ saved copy of length\n-    \/\/ End pointers are inclusive, and if length is not zero they point\n-    \/\/ to the last unit copied:  end_to[0] := end_from[0]\n-\n-    const Register rax_oop    = rax;    \/\/ actual oop copied\n-    const Register r11_klass  = r11;    \/\/ oop._klass\n-\n-    \/\/---------------------------------------------------------------\n-    \/\/ Assembler stub will be used for this call to arraycopy\n-    \/\/ if the two arrays are subtypes of Object[] but the\n-    \/\/ destination array type is not equal to or a supertype\n-    \/\/ of the source type.  Each element must be separately\n-    \/\/ checked.\n-\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-#ifdef ASSERT\n-    \/\/ caller guarantees that the arrays really are different\n-    \/\/ otherwise, we would have to make conjoint checks\n-    { Label L;\n-      array_overlap_test(L, TIMES_OOP);\n-      __ stop(\"checkcast_copy within a single array\");\n-      __ bind(L);\n-    }\n-#endif \/\/ASSERT\n-\n-    setup_arg_regs(4); \/\/ from => rdi, to => rsi, length => rdx\n-                       \/\/ ckoff => rcx, ckval => r8\n-                       \/\/ r9 and r10 may be used to save non-volatile registers\n-#ifdef _WIN64\n-    \/\/ last argument (#4) is on stack on Win64\n-    __ movptr(ckval, Address(rsp, 6 * wordSize));\n-#endif\n-\n-    \/\/ Caller of this entry point must set up the argument registers.\n-    if (entry != NULL) {\n-      *entry = __ pc();\n-      BLOCK_COMMENT(\"Entry:\");\n-    }\n-\n-    \/\/ allocate spill slots for r13, r14\n-    enum {\n-      saved_r13_offset,\n-      saved_r14_offset,\n-      saved_r10_offset,\n-      saved_rbp_offset\n-    };\n-    __ subptr(rsp, saved_rbp_offset * wordSize);\n-    __ movptr(Address(rsp, saved_r13_offset * wordSize), r13);\n-    __ movptr(Address(rsp, saved_r14_offset * wordSize), r14);\n-    __ movptr(Address(rsp, saved_r10_offset * wordSize), r10);\n-\n-#ifdef ASSERT\n-      Label L2;\n-      __ get_thread(r14);\n-      __ cmpptr(r15_thread, r14);\n-      __ jcc(Assembler::equal, L2);\n-      __ stop(\"StubRoutines::call_stub: r15_thread is modified by call\");\n-      __ bind(L2);\n-#endif \/\/ ASSERT\n-\n-    \/\/ check that int operands are properly extended to size_t\n-    assert_clean_int(length, rax);\n-    assert_clean_int(ckoff, rax);\n-\n-#ifdef ASSERT\n-    BLOCK_COMMENT(\"assert consistent ckoff\/ckval\");\n-    \/\/ The ckoff and ckval must be mutually consistent,\n-    \/\/ even though caller generates both.\n-    { Label L;\n-      int sco_offset = in_bytes(Klass::super_check_offset_offset());\n-      __ cmpl(ckoff, Address(ckval, sco_offset));\n-      __ jcc(Assembler::equal, L);\n-      __ stop(\"super_check_offset inconsistent\");\n-      __ bind(L);\n-    }\n-#endif \/\/ASSERT\n-\n-    \/\/ Loop-invariant addresses.  They are exclusive end pointers.\n-    Address end_from_addr(from, length, TIMES_OOP, 0);\n-    Address   end_to_addr(to,   length, TIMES_OOP, 0);\n-    \/\/ Loop-variant addresses.  They assume post-incremented count < 0.\n-    Address from_element_addr(end_from, count, TIMES_OOP, 0);\n-    Address   to_element_addr(end_to,   count, TIMES_OOP, 0);\n-\n-    DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_CHECKCAST | ARRAYCOPY_DISJOINT;\n-    if (dest_uninitialized) {\n-      decorators |= IS_DEST_UNINITIALIZED;\n-    }\n-\n-    BasicType type = T_OBJECT;\n-    BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    bs->arraycopy_prologue(_masm, decorators, type, from, to, count);\n-\n-    \/\/ Copy from low to high addresses, indexed from the end of each array.\n-    __ lea(end_from, end_from_addr);\n-    __ lea(end_to,   end_to_addr);\n-    __ movptr(r14_length, length);        \/\/ save a copy of the length\n-    assert(length == count, \"\");          \/\/ else fix next line:\n-    __ negptr(count);                     \/\/ negate and test the length\n-    __ jcc(Assembler::notZero, L_load_element);\n-\n-    \/\/ Empty array:  Nothing to do.\n-    __ xorptr(rax, rax);                  \/\/ return 0 on (trivial) success\n-    __ jmp(L_done);\n-\n-    \/\/ ======== begin loop ========\n-    \/\/ (Loop is rotated; its entry is L_load_element.)\n-    \/\/ Loop control:\n-    \/\/   for (count = -count; count != 0; count++)\n-    \/\/ Base pointers src, dst are biased by 8*(count-1),to last element.\n-    __ align(OptoLoopAlignment);\n-\n-    __ BIND(L_store_element);\n-    __ store_heap_oop(to_element_addr, rax_oop, noreg, noreg, noreg, AS_RAW);  \/\/ store the oop\n-    __ increment(count);               \/\/ increment the count toward zero\n-    __ jcc(Assembler::zero, L_do_card_marks);\n-\n-    \/\/ ======== loop entry is here ========\n-    __ BIND(L_load_element);\n-    __ load_heap_oop(rax_oop, from_element_addr, noreg, noreg, AS_RAW); \/\/ load the oop\n-    __ testptr(rax_oop, rax_oop);\n-    __ jcc(Assembler::zero, L_store_element);\n-\n-    __ load_klass(r11_klass, rax_oop, rscratch1);\/\/ query the object klass\n-    generate_type_check(r11_klass, ckoff, ckval, L_store_element);\n-    \/\/ ======== end loop ========\n-\n-    \/\/ It was a real error; we must depend on the caller to finish the job.\n-    \/\/ Register rdx = -1 * number of *remaining* oops, r14 = *total* oops.\n-    \/\/ Emit GC store barriers for the oops we have copied (r14 + rdx),\n-    \/\/ and report their number to the caller.\n-    assert_different_registers(rax, r14_length, count, to, end_to, rcx, rscratch1);\n-    Label L_post_barrier;\n-    __ addptr(r14_length, count);     \/\/ K = (original - remaining) oops\n-    __ movptr(rax, r14_length);       \/\/ save the value\n-    __ notptr(rax);                   \/\/ report (-1^K) to caller (does not affect flags)\n-    __ jccb(Assembler::notZero, L_post_barrier);\n-    __ jmp(L_done); \/\/ K == 0, nothing was copied, skip post barrier\n-\n-    \/\/ Come here on success only.\n-    __ BIND(L_do_card_marks);\n-    __ xorptr(rax, rax);              \/\/ return 0 on success\n-\n-    __ BIND(L_post_barrier);\n-    bs->arraycopy_epilogue(_masm, decorators, type, from, to, r14_length);\n-\n-    \/\/ Common exit point (success or failure).\n-    __ BIND(L_done);\n-    __ movptr(r13, Address(rsp, saved_r13_offset * wordSize));\n-    __ movptr(r14, Address(rsp, saved_r14_offset * wordSize));\n-    __ movptr(r10, Address(rsp, saved_r10_offset * wordSize));\n-    restore_arg_regs();\n-    inc_counter_np(SharedRuntime::_checkcast_array_copy_ctr); \/\/ Update counter after rscratch1 is free\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    return start;\n-  }\n-\n-  \/\/\n-  \/\/  Generate 'unsafe' array copy stub\n-  \/\/  Though just as safe as the other stubs, it takes an unscaled\n-  \/\/  size_t argument instead of an element count.\n-  \/\/\n-  \/\/  Input:\n-  \/\/    c_rarg0   - source array address\n-  \/\/    c_rarg1   - destination array address\n-  \/\/    c_rarg2   - byte count, treated as ssize_t, can be zero\n-  \/\/\n-  \/\/ Examines the alignment of the operands and dispatches\n-  \/\/ to a long, int, short, or byte copy loop.\n-  \/\/\n-  address generate_unsafe_copy(const char *name,\n-                               address byte_copy_entry, address short_copy_entry,\n-                               address int_copy_entry, address long_copy_entry) {\n-\n-    Label L_long_aligned, L_int_aligned, L_short_aligned;\n-\n-    \/\/ Input registers (before setup_arg_regs)\n-    const Register from        = c_rarg0;  \/\/ source array address\n-    const Register to          = c_rarg1;  \/\/ destination array address\n-    const Register size        = c_rarg2;  \/\/ byte count (size_t)\n-\n-    \/\/ Register used as a temp\n-    const Register bits        = rax;      \/\/ test copy of low bits\n-\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-    \/\/ bump this on entry, not on exit:\n-    inc_counter_np(SharedRuntime::_unsafe_array_copy_ctr);\n-\n-    __ mov(bits, from);\n-    __ orptr(bits, to);\n-    __ orptr(bits, size);\n-\n-    __ testb(bits, BytesPerLong-1);\n-    __ jccb(Assembler::zero, L_long_aligned);\n-\n-    __ testb(bits, BytesPerInt-1);\n-    __ jccb(Assembler::zero, L_int_aligned);\n-\n-    __ testb(bits, BytesPerShort-1);\n-    __ jump_cc(Assembler::notZero, RuntimeAddress(byte_copy_entry));\n-\n-    __ BIND(L_short_aligned);\n-    __ shrptr(size, LogBytesPerShort); \/\/ size => short_count\n-    __ jump(RuntimeAddress(short_copy_entry));\n-\n-    __ BIND(L_int_aligned);\n-    __ shrptr(size, LogBytesPerInt); \/\/ size => int_count\n-    __ jump(RuntimeAddress(int_copy_entry));\n-\n-    __ BIND(L_long_aligned);\n-    __ shrptr(size, LogBytesPerLong); \/\/ size => qword_count\n-    __ jump(RuntimeAddress(long_copy_entry));\n-\n-    return start;\n-  }\n-\n-  \/\/ Perform range checks on the proposed arraycopy.\n-  \/\/ Kills temp, but nothing else.\n-  \/\/ Also, clean the sign bits of src_pos and dst_pos.\n-  void arraycopy_range_checks(Register src,     \/\/ source array oop (c_rarg0)\n-                              Register src_pos, \/\/ source position (c_rarg1)\n-                              Register dst,     \/\/ destination array oo (c_rarg2)\n-                              Register dst_pos, \/\/ destination position (c_rarg3)\n-                              Register length,\n-                              Register temp,\n-                              Label& L_failed) {\n-    BLOCK_COMMENT(\"arraycopy_range_checks:\");\n-\n-    \/\/  if (src_pos + length > arrayOop(src)->length())  FAIL;\n-    __ movl(temp, length);\n-    __ addl(temp, src_pos);             \/\/ src_pos + length\n-    __ cmpl(temp, Address(src, arrayOopDesc::length_offset_in_bytes()));\n-    __ jcc(Assembler::above, L_failed);\n-\n-    \/\/  if (dst_pos + length > arrayOop(dst)->length())  FAIL;\n-    __ movl(temp, length);\n-    __ addl(temp, dst_pos);             \/\/ dst_pos + length\n-    __ cmpl(temp, Address(dst, arrayOopDesc::length_offset_in_bytes()));\n-    __ jcc(Assembler::above, L_failed);\n-\n-    \/\/ Have to clean up high 32-bits of 'src_pos' and 'dst_pos'.\n-    \/\/ Move with sign extension can be used since they are positive.\n-    __ movslq(src_pos, src_pos);\n-    __ movslq(dst_pos, dst_pos);\n-\n-    BLOCK_COMMENT(\"arraycopy_range_checks done\");\n-  }\n-\n-  \/\/\n-  \/\/  Generate generic array copy stubs\n-  \/\/\n-  \/\/  Input:\n-  \/\/    c_rarg0    -  src oop\n-  \/\/    c_rarg1    -  src_pos (32-bits)\n-  \/\/    c_rarg2    -  dst oop\n-  \/\/    c_rarg3    -  dst_pos (32-bits)\n-  \/\/ not Win64\n-  \/\/    c_rarg4    -  element count (32-bits)\n-  \/\/ Win64\n-  \/\/    rsp+40     -  element count (32-bits)\n-  \/\/\n-  \/\/  Output:\n-  \/\/    rax ==  0  -  success\n-  \/\/    rax == -1^K - failure, where K is partial transfer count\n-  \/\/\n-  address generate_generic_copy(const char *name,\n-                                address byte_copy_entry, address short_copy_entry,\n-                                address int_copy_entry, address oop_copy_entry,\n-                                address long_copy_entry, address checkcast_copy_entry) {\n-\n-    Label L_failed, L_failed_0, L_objArray;\n-    Label L_copy_shorts, L_copy_ints, L_copy_longs;\n-\n-    \/\/ Input registers\n-    const Register src        = c_rarg0;  \/\/ source array oop\n-    const Register src_pos    = c_rarg1;  \/\/ source position\n-    const Register dst        = c_rarg2;  \/\/ destination array oop\n-    const Register dst_pos    = c_rarg3;  \/\/ destination position\n-#ifndef _WIN64\n-    const Register length     = c_rarg4;\n-    const Register rklass_tmp = r9;  \/\/ load_klass\n-#else\n-    const Address  length(rsp, 7 * wordSize);  \/\/ elements count is on stack on Win64\n-    const Register rklass_tmp = rdi;  \/\/ load_klass\n-#endif\n-\n-    { int modulus = CodeEntryAlignment;\n-      int target  = modulus - 5; \/\/ 5 = sizeof jmp(L_failed)\n-      int advance = target - (__ offset() % modulus);\n-      if (advance < 0)  advance += modulus;\n-      if (advance > 0)  __ nop(advance);\n-    }\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-\n-    \/\/ Short-hop target to L_failed.  Makes for denser prologue code.\n-    __ BIND(L_failed_0);\n-    __ jmp(L_failed);\n-    assert(__ offset() % CodeEntryAlignment == 0, \"no further alignment needed\");\n-\n-    __ align(CodeEntryAlignment);\n-    address start = __ pc();\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-#ifdef _WIN64\n-    __ push(rklass_tmp); \/\/ rdi is callee-save on Windows\n-#endif\n-\n-    \/\/ bump this on entry, not on exit:\n-    inc_counter_np(SharedRuntime::_generic_array_copy_ctr);\n-\n-    \/\/-----------------------------------------------------------------------\n-    \/\/ Assembler stub will be used for this call to arraycopy\n-    \/\/ if the following conditions are met:\n-    \/\/\n-    \/\/ (1) src and dst must not be null.\n-    \/\/ (2) src_pos must not be negative.\n-    \/\/ (3) dst_pos must not be negative.\n-    \/\/ (4) length  must not be negative.\n-    \/\/ (5) src klass and dst klass should be the same and not NULL.\n-    \/\/ (6) src and dst should be arrays.\n-    \/\/ (7) src_pos + length must not exceed length of src.\n-    \/\/ (8) dst_pos + length must not exceed length of dst.\n-    \/\/\n-\n-    \/\/  if (src == NULL) return -1;\n-    __ testptr(src, src);         \/\/ src oop\n-    size_t j1off = __ offset();\n-    __ jccb(Assembler::zero, L_failed_0);\n-\n-    \/\/  if (src_pos < 0) return -1;\n-    __ testl(src_pos, src_pos); \/\/ src_pos (32-bits)\n-    __ jccb(Assembler::negative, L_failed_0);\n-\n-    \/\/  if (dst == NULL) return -1;\n-    __ testptr(dst, dst);         \/\/ dst oop\n-    __ jccb(Assembler::zero, L_failed_0);\n-\n-    \/\/  if (dst_pos < 0) return -1;\n-    __ testl(dst_pos, dst_pos); \/\/ dst_pos (32-bits)\n-    size_t j4off = __ offset();\n-    __ jccb(Assembler::negative, L_failed_0);\n-\n-    \/\/ The first four tests are very dense code,\n-    \/\/ but not quite dense enough to put four\n-    \/\/ jumps in a 16-byte instruction fetch buffer.\n-    \/\/ That's good, because some branch predicters\n-    \/\/ do not like jumps so close together.\n-    \/\/ Make sure of this.\n-    guarantee(((j1off ^ j4off) & ~15) != 0, \"I$ line of 1st & 4th jumps\");\n-\n-    \/\/ registers used as temp\n-    const Register r11_length    = r11; \/\/ elements count to copy\n-    const Register r10_src_klass = r10; \/\/ array klass\n-\n-    \/\/  if (length < 0) return -1;\n-    __ movl(r11_length, length);        \/\/ length (elements count, 32-bits value)\n-    __ testl(r11_length, r11_length);\n-    __ jccb(Assembler::negative, L_failed_0);\n-\n-    __ load_klass(r10_src_klass, src, rklass_tmp);\n-#ifdef ASSERT\n-    \/\/  assert(src->klass() != NULL);\n-    {\n-      BLOCK_COMMENT(\"assert klasses not null {\");\n-      Label L1, L2;\n-      __ testptr(r10_src_klass, r10_src_klass);\n-      __ jcc(Assembler::notZero, L2);   \/\/ it is broken if klass is NULL\n-      __ bind(L1);\n-      __ stop(\"broken null klass\");\n-      __ bind(L2);\n-      __ load_klass(rax, dst, rklass_tmp);\n-      __ cmpq(rax, 0);\n-      __ jcc(Assembler::equal, L1);     \/\/ this would be broken also\n-      BLOCK_COMMENT(\"} assert klasses not null done\");\n-    }\n-#endif\n-\n-    \/\/ Load layout helper (32-bits)\n-    \/\/\n-    \/\/  |array_tag|     | header_size | element_type |     |log2_element_size|\n-    \/\/ 32        30    24            16              8     2                 0\n-    \/\/\n-    \/\/   array_tag: typeArray = 0x3, objArray = 0x2, non-array = 0x0\n-    \/\/\n-\n-    const int lh_offset = in_bytes(Klass::layout_helper_offset());\n-\n-    \/\/ Handle objArrays completely differently...\n-    const jint objArray_lh = Klass::array_layout_helper(T_OBJECT);\n-    __ cmpl(Address(r10_src_klass, lh_offset), objArray_lh);\n-    __ jcc(Assembler::equal, L_objArray);\n-\n-    \/\/  if (src->klass() != dst->klass()) return -1;\n-    __ load_klass(rax, dst, rklass_tmp);\n-    __ cmpq(r10_src_klass, rax);\n-    __ jcc(Assembler::notEqual, L_failed);\n-\n-    const Register rax_lh = rax;  \/\/ layout helper\n-    __ movl(rax_lh, Address(r10_src_klass, lh_offset));\n-\n-    \/\/  if (!src->is_Array()) return -1;\n-    __ cmpl(rax_lh, Klass::_lh_neutral_value);\n-    __ jcc(Assembler::greaterEqual, L_failed);\n-\n-    \/\/ At this point, it is known to be a typeArray (array_tag 0x3).\n-#ifdef ASSERT\n-    {\n-      BLOCK_COMMENT(\"assert primitive array {\");\n-      Label L;\n-      __ cmpl(rax_lh, (Klass::_lh_array_tag_type_value << Klass::_lh_array_tag_shift));\n-      __ jcc(Assembler::greaterEqual, L);\n-      __ stop(\"must be a primitive array\");\n-      __ bind(L);\n-      BLOCK_COMMENT(\"} assert primitive array done\");\n-    }\n-#endif\n-\n-    arraycopy_range_checks(src, src_pos, dst, dst_pos, r11_length,\n-                           r10, L_failed);\n-\n-    \/\/ TypeArrayKlass\n-    \/\/\n-    \/\/ src_addr = (src + array_header_in_bytes()) + (src_pos << log2elemsize);\n-    \/\/ dst_addr = (dst + array_header_in_bytes()) + (dst_pos << log2elemsize);\n-    \/\/\n-\n-    const Register r10_offset = r10;    \/\/ array offset\n-    const Register rax_elsize = rax_lh; \/\/ element size\n-\n-    __ movl(r10_offset, rax_lh);\n-    __ shrl(r10_offset, Klass::_lh_header_size_shift);\n-    __ andptr(r10_offset, Klass::_lh_header_size_mask);   \/\/ array_offset\n-    __ addptr(src, r10_offset);           \/\/ src array offset\n-    __ addptr(dst, r10_offset);           \/\/ dst array offset\n-    BLOCK_COMMENT(\"choose copy loop based on element size\");\n-    __ andl(rax_lh, Klass::_lh_log2_element_size_mask); \/\/ rax_lh -> rax_elsize\n-\n-#ifdef _WIN64\n-    __ pop(rklass_tmp); \/\/ Restore callee-save rdi\n-#endif\n-\n-    \/\/ next registers should be set before the jump to corresponding stub\n-    const Register from     = c_rarg0;  \/\/ source array address\n-    const Register to       = c_rarg1;  \/\/ destination array address\n-    const Register count    = c_rarg2;  \/\/ elements count\n-\n-    \/\/ 'from', 'to', 'count' registers should be set in such order\n-    \/\/ since they are the same as 'src', 'src_pos', 'dst'.\n-\n-    __ cmpl(rax_elsize, 0);\n-    __ jccb(Assembler::notEqual, L_copy_shorts);\n-    __ lea(from, Address(src, src_pos, Address::times_1, 0));\/\/ src_addr\n-    __ lea(to,   Address(dst, dst_pos, Address::times_1, 0));\/\/ dst_addr\n-    __ movl2ptr(count, r11_length); \/\/ length\n-    __ jump(RuntimeAddress(byte_copy_entry));\n-\n-  __ BIND(L_copy_shorts);\n-    __ cmpl(rax_elsize, LogBytesPerShort);\n-    __ jccb(Assembler::notEqual, L_copy_ints);\n-    __ lea(from, Address(src, src_pos, Address::times_2, 0));\/\/ src_addr\n-    __ lea(to,   Address(dst, dst_pos, Address::times_2, 0));\/\/ dst_addr\n-    __ movl2ptr(count, r11_length); \/\/ length\n-    __ jump(RuntimeAddress(short_copy_entry));\n-\n-  __ BIND(L_copy_ints);\n-    __ cmpl(rax_elsize, LogBytesPerInt);\n-    __ jccb(Assembler::notEqual, L_copy_longs);\n-    __ lea(from, Address(src, src_pos, Address::times_4, 0));\/\/ src_addr\n-    __ lea(to,   Address(dst, dst_pos, Address::times_4, 0));\/\/ dst_addr\n-    __ movl2ptr(count, r11_length); \/\/ length\n-    __ jump(RuntimeAddress(int_copy_entry));\n-\n-  __ BIND(L_copy_longs);\n-#ifdef ASSERT\n-    {\n-      BLOCK_COMMENT(\"assert long copy {\");\n-      Label L;\n-      __ cmpl(rax_elsize, LogBytesPerLong);\n-      __ jcc(Assembler::equal, L);\n-      __ stop(\"must be long copy, but elsize is wrong\");\n-      __ bind(L);\n-      BLOCK_COMMENT(\"} assert long copy done\");\n-    }\n-#endif\n-    __ lea(from, Address(src, src_pos, Address::times_8, 0));\/\/ src_addr\n-    __ lea(to,   Address(dst, dst_pos, Address::times_8, 0));\/\/ dst_addr\n-    __ movl2ptr(count, r11_length); \/\/ length\n-    __ jump(RuntimeAddress(long_copy_entry));\n-\n-    \/\/ ObjArrayKlass\n-  __ BIND(L_objArray);\n-    \/\/ live at this point:  r10_src_klass, r11_length, src[_pos], dst[_pos]\n-\n-    Label L_plain_copy, L_checkcast_copy;\n-    \/\/  test array classes for subtyping\n-    __ load_klass(rax, dst, rklass_tmp);\n-    __ cmpq(r10_src_klass, rax); \/\/ usual case is exact equality\n-    __ jcc(Assembler::notEqual, L_checkcast_copy);\n-\n-    \/\/ Identically typed arrays can be copied without element-wise checks.\n-    arraycopy_range_checks(src, src_pos, dst, dst_pos, r11_length,\n-                           r10, L_failed);\n-\n-    __ lea(from, Address(src, src_pos, TIMES_OOP,\n-                 arrayOopDesc::base_offset_in_bytes(T_OBJECT))); \/\/ src_addr\n-    __ lea(to,   Address(dst, dst_pos, TIMES_OOP,\n-                 arrayOopDesc::base_offset_in_bytes(T_OBJECT))); \/\/ dst_addr\n-    __ movl2ptr(count, r11_length); \/\/ length\n-  __ BIND(L_plain_copy);\n-#ifdef _WIN64\n-    __ pop(rklass_tmp); \/\/ Restore callee-save rdi\n-#endif\n-    __ jump(RuntimeAddress(oop_copy_entry));\n-\n-  __ BIND(L_checkcast_copy);\n-    \/\/ live at this point:  r10_src_klass, r11_length, rax (dst_klass)\n-    {\n-      \/\/ Before looking at dst.length, make sure dst is also an objArray.\n-      __ cmpl(Address(rax, lh_offset), objArray_lh);\n-      __ jcc(Assembler::notEqual, L_failed);\n-\n-      \/\/ It is safe to examine both src.length and dst.length.\n-      arraycopy_range_checks(src, src_pos, dst, dst_pos, r11_length,\n-                             rax, L_failed);\n-\n-      const Register r11_dst_klass = r11;\n-      __ load_klass(r11_dst_klass, dst, rklass_tmp); \/\/ reload\n-\n-      \/\/ Marshal the base address arguments now, freeing registers.\n-      __ lea(from, Address(src, src_pos, TIMES_OOP,\n-                   arrayOopDesc::base_offset_in_bytes(T_OBJECT)));\n-      __ lea(to,   Address(dst, dst_pos, TIMES_OOP,\n-                   arrayOopDesc::base_offset_in_bytes(T_OBJECT)));\n-      __ movl(count, length);           \/\/ length (reloaded)\n-      Register sco_temp = c_rarg3;      \/\/ this register is free now\n-      assert_different_registers(from, to, count, sco_temp,\n-                                 r11_dst_klass, r10_src_klass);\n-      assert_clean_int(count, sco_temp);\n-\n-      \/\/ Generate the type check.\n-      const int sco_offset = in_bytes(Klass::super_check_offset_offset());\n-      __ movl(sco_temp, Address(r11_dst_klass, sco_offset));\n-      assert_clean_int(sco_temp, rax);\n-      generate_type_check(r10_src_klass, sco_temp, r11_dst_klass, L_plain_copy);\n-\n-      \/\/ Fetch destination element klass from the ObjArrayKlass header.\n-      int ek_offset = in_bytes(ObjArrayKlass::element_klass_offset());\n-      __ movptr(r11_dst_klass, Address(r11_dst_klass, ek_offset));\n-      __ movl(  sco_temp,      Address(r11_dst_klass, sco_offset));\n-      assert_clean_int(sco_temp, rax);\n-\n-#ifdef _WIN64\n-      __ pop(rklass_tmp); \/\/ Restore callee-save rdi\n-#endif\n-\n-      \/\/ the checkcast_copy loop needs two extra arguments:\n-      assert(c_rarg3 == sco_temp, \"#3 already in place\");\n-      \/\/ Set up arguments for checkcast_copy_entry.\n-      setup_arg_regs(4);\n-      __ movptr(r8, r11_dst_klass);  \/\/ dst.klass.element_klass, r8 is c_rarg4 on Linux\/Solaris\n-      __ jump(RuntimeAddress(checkcast_copy_entry));\n-    }\n-\n-  __ BIND(L_failed);\n-#ifdef _WIN64\n-    __ pop(rklass_tmp); \/\/ Restore callee-save rdi\n-#endif\n-    __ xorptr(rax, rax);\n-    __ notptr(rax); \/\/ return -1\n-    __ leave();   \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    return start;\n-  }\n-\n-  address generate_data_cache_writeback() {\n-    const Register src        = c_rarg0;  \/\/ source address\n-\n-    __ align(CodeEntryAlignment);\n-\n-    StubCodeMark mark(this, \"StubRoutines\", \"_data_cache_writeback\");\n-\n-    address start = __ pc();\n-    __ enter();\n-    __ cache_wb(Address(src, 0));\n-    __ leave();\n-    __ ret(0);\n-\n-    return start;\n-  }\n-\n-  address generate_data_cache_writeback_sync() {\n-    const Register is_pre    = c_rarg0;  \/\/ pre or post sync\n-\n-    __ align(CodeEntryAlignment);\n-\n-    StubCodeMark mark(this, \"StubRoutines\", \"_data_cache_writeback_sync\");\n-\n-    \/\/ pre wbsync is a no-op\n-    \/\/ post wbsync translates to an sfence\n-\n-    Label skip;\n-    address start = __ pc();\n-    __ enter();\n-    __ cmpl(is_pre, 0);\n-    __ jcc(Assembler::notEqual, skip);\n-    __ cache_wbsync(false);\n-    __ bind(skip);\n-    __ leave();\n-    __ ret(0);\n-\n-    return start;\n-  }\n-\n-  void generate_arraycopy_stubs() {\n-    address entry;\n-    address entry_jbyte_arraycopy;\n-    address entry_jshort_arraycopy;\n-    address entry_jint_arraycopy;\n-    address entry_oop_arraycopy;\n-    address entry_jlong_arraycopy;\n-    address entry_checkcast_arraycopy;\n-\n-    StubRoutines::_jbyte_disjoint_arraycopy  = generate_disjoint_byte_copy(false, &entry,\n-                                                                           \"jbyte_disjoint_arraycopy\");\n-    StubRoutines::_jbyte_arraycopy           = generate_conjoint_byte_copy(false, entry, &entry_jbyte_arraycopy,\n-                                                                           \"jbyte_arraycopy\");\n-\n-    StubRoutines::_jshort_disjoint_arraycopy = generate_disjoint_short_copy(false, &entry,\n-                                                                            \"jshort_disjoint_arraycopy\");\n-    StubRoutines::_jshort_arraycopy          = generate_conjoint_short_copy(false, entry, &entry_jshort_arraycopy,\n-                                                                            \"jshort_arraycopy\");\n-\n-    StubRoutines::_jint_disjoint_arraycopy   = generate_disjoint_int_oop_copy(false, false, &entry,\n-                                                                              \"jint_disjoint_arraycopy\");\n-    StubRoutines::_jint_arraycopy            = generate_conjoint_int_oop_copy(false, false, entry,\n-                                                                              &entry_jint_arraycopy, \"jint_arraycopy\");\n-\n-    StubRoutines::_jlong_disjoint_arraycopy  = generate_disjoint_long_oop_copy(false, false, &entry,\n-                                                                               \"jlong_disjoint_arraycopy\");\n-    StubRoutines::_jlong_arraycopy           = generate_conjoint_long_oop_copy(false, false, entry,\n-                                                                               &entry_jlong_arraycopy, \"jlong_arraycopy\");\n-\n-\n-    if (UseCompressedOops) {\n-      StubRoutines::_oop_disjoint_arraycopy  = generate_disjoint_int_oop_copy(false, true, &entry,\n-                                                                              \"oop_disjoint_arraycopy\");\n-      StubRoutines::_oop_arraycopy           = generate_conjoint_int_oop_copy(false, true, entry,\n-                                                                              &entry_oop_arraycopy, \"oop_arraycopy\");\n-      StubRoutines::_oop_disjoint_arraycopy_uninit  = generate_disjoint_int_oop_copy(false, true, &entry,\n-                                                                                     \"oop_disjoint_arraycopy_uninit\",\n-                                                                                     \/*dest_uninitialized*\/true);\n-      StubRoutines::_oop_arraycopy_uninit           = generate_conjoint_int_oop_copy(false, true, entry,\n-                                                                                     NULL, \"oop_arraycopy_uninit\",\n-                                                                                     \/*dest_uninitialized*\/true);\n-    } else {\n-      StubRoutines::_oop_disjoint_arraycopy  = generate_disjoint_long_oop_copy(false, true, &entry,\n-                                                                               \"oop_disjoint_arraycopy\");\n-      StubRoutines::_oop_arraycopy           = generate_conjoint_long_oop_copy(false, true, entry,\n-                                                                               &entry_oop_arraycopy, \"oop_arraycopy\");\n-      StubRoutines::_oop_disjoint_arraycopy_uninit  = generate_disjoint_long_oop_copy(false, true, &entry,\n-                                                                                      \"oop_disjoint_arraycopy_uninit\",\n-                                                                                      \/*dest_uninitialized*\/true);\n-      StubRoutines::_oop_arraycopy_uninit           = generate_conjoint_long_oop_copy(false, true, entry,\n-                                                                                      NULL, \"oop_arraycopy_uninit\",\n-                                                                                      \/*dest_uninitialized*\/true);\n-    }\n-\n-    StubRoutines::_checkcast_arraycopy        = generate_checkcast_copy(\"checkcast_arraycopy\", &entry_checkcast_arraycopy);\n-    StubRoutines::_checkcast_arraycopy_uninit = generate_checkcast_copy(\"checkcast_arraycopy_uninit\", NULL,\n-                                                                        \/*dest_uninitialized*\/true);\n-\n-    StubRoutines::_unsafe_arraycopy    = generate_unsafe_copy(\"unsafe_arraycopy\",\n-                                                              entry_jbyte_arraycopy,\n-                                                              entry_jshort_arraycopy,\n-                                                              entry_jint_arraycopy,\n-                                                              entry_jlong_arraycopy);\n-    StubRoutines::_generic_arraycopy   = generate_generic_copy(\"generic_arraycopy\",\n-                                                               entry_jbyte_arraycopy,\n-                                                               entry_jshort_arraycopy,\n-                                                               entry_jint_arraycopy,\n-                                                               entry_oop_arraycopy,\n-                                                               entry_jlong_arraycopy,\n-                                                               entry_checkcast_arraycopy);\n-\n-    StubRoutines::_jbyte_fill = generate_fill(T_BYTE, false, \"jbyte_fill\");\n-    StubRoutines::_jshort_fill = generate_fill(T_SHORT, false, \"jshort_fill\");\n-    StubRoutines::_jint_fill = generate_fill(T_INT, false, \"jint_fill\");\n-    StubRoutines::_arrayof_jbyte_fill = generate_fill(T_BYTE, true, \"arrayof_jbyte_fill\");\n-    StubRoutines::_arrayof_jshort_fill = generate_fill(T_SHORT, true, \"arrayof_jshort_fill\");\n-    StubRoutines::_arrayof_jint_fill = generate_fill(T_INT, true, \"arrayof_jint_fill\");\n-\n-    \/\/ We don't generate specialized code for HeapWord-aligned source\n-    \/\/ arrays, so just use the code we've already generated\n-    StubRoutines::_arrayof_jbyte_disjoint_arraycopy  = StubRoutines::_jbyte_disjoint_arraycopy;\n-    StubRoutines::_arrayof_jbyte_arraycopy           = StubRoutines::_jbyte_arraycopy;\n-\n-    StubRoutines::_arrayof_jshort_disjoint_arraycopy = StubRoutines::_jshort_disjoint_arraycopy;\n-    StubRoutines::_arrayof_jshort_arraycopy          = StubRoutines::_jshort_arraycopy;\n-\n-    StubRoutines::_arrayof_jint_disjoint_arraycopy   = StubRoutines::_jint_disjoint_arraycopy;\n-    StubRoutines::_arrayof_jint_arraycopy            = StubRoutines::_jint_arraycopy;\n-\n-    StubRoutines::_arrayof_jlong_disjoint_arraycopy  = StubRoutines::_jlong_disjoint_arraycopy;\n-    StubRoutines::_arrayof_jlong_arraycopy           = StubRoutines::_jlong_arraycopy;\n-\n-    StubRoutines::_arrayof_oop_disjoint_arraycopy    = StubRoutines::_oop_disjoint_arraycopy;\n-    StubRoutines::_arrayof_oop_arraycopy             = StubRoutines::_oop_arraycopy;\n-\n-    StubRoutines::_arrayof_oop_disjoint_arraycopy_uninit    = StubRoutines::_oop_disjoint_arraycopy_uninit;\n-    StubRoutines::_arrayof_oop_arraycopy_uninit             = StubRoutines::_oop_arraycopy_uninit;\n-  }\n-\n-  \/\/ AES intrinsic stubs\n-  enum {AESBlockSize = 16};\n-\n-  address generate_key_shuffle_mask() {\n-    __ align(16);\n-    StubCodeMark mark(this, \"StubRoutines\", \"key_shuffle_mask\");\n-    address start = __ pc();\n-    __ emit_data64( 0x0405060700010203, relocInfo::none );\n-    __ emit_data64( 0x0c0d0e0f08090a0b, relocInfo::none );\n-    return start;\n-  }\n-\n-  address generate_counter_shuffle_mask() {\n-    __ align(16);\n-    StubCodeMark mark(this, \"StubRoutines\", \"counter_shuffle_mask\");\n-    address start = __ pc();\n-    __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\n-    __ emit_data64(0x0001020304050607, relocInfo::none);\n-    return start;\n-  }\n-\n-  \/\/ Utility routine for loading a 128-bit key word in little endian format\n-  \/\/ can optionally specify that the shuffle mask is already in an xmmregister\n-  void load_key(XMMRegister xmmdst, Register key, int offset, XMMRegister xmm_shuf_mask=NULL) {\n-    __ movdqu(xmmdst, Address(key, offset));\n-    if (xmm_shuf_mask != NULL) {\n-      __ pshufb(xmmdst, xmm_shuf_mask);\n-    } else {\n-      __ pshufb(xmmdst, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n-    }\n-  }\n-\n-  \/\/ Utility routine for increase 128bit counter (iv in CTR mode)\n-  void inc_counter(Register reg, XMMRegister xmmdst, int inc_delta, Label& next_block) {\n-    __ pextrq(reg, xmmdst, 0x0);\n-    __ addq(reg, inc_delta);\n-    __ pinsrq(xmmdst, reg, 0x0);\n-    __ jcc(Assembler::carryClear, next_block); \/\/ jump if no carry\n-    __ pextrq(reg, xmmdst, 0x01); \/\/ Carry\n-    __ addq(reg, 0x01);\n-    __ pinsrq(xmmdst, reg, 0x01); \/\/Carry end\n-    __ BIND(next_block);          \/\/ next instruction\n-  }\n-\n-  \/\/ Arguments:\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source byte array address\n-  \/\/   c_rarg1   - destination byte array address\n-  \/\/   c_rarg2   - K (key) in little endian int array\n-  \/\/\n-  address generate_aescrypt_encryptBlock() {\n-    assert(UseAES, \"need AES instructions and misaligned SSE support\");\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"aescrypt_encryptBlock\");\n-    Label L_doLast;\n-    address start = __ pc();\n-\n-    const Register from        = c_rarg0;  \/\/ source array address\n-    const Register to          = c_rarg1;  \/\/ destination array address\n-    const Register key         = c_rarg2;  \/\/ key array address\n-    const Register keylen      = rax;\n-\n-    const XMMRegister xmm_result = xmm0;\n-    const XMMRegister xmm_key_shuf_mask = xmm1;\n-    \/\/ On win64 xmm6-xmm15 must be preserved so don't use them.\n-    const XMMRegister xmm_temp1  = xmm2;\n-    const XMMRegister xmm_temp2  = xmm3;\n-    const XMMRegister xmm_temp3  = xmm4;\n-    const XMMRegister xmm_temp4  = xmm5;\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-    \/\/ keylen could be only {11, 13, 15} * 4 = {44, 52, 60}\n-    __ movl(keylen, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n-\n-    __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n-    __ movdqu(xmm_result, Address(from, 0));  \/\/ get 16 bytes of input\n-\n-    \/\/ For encryption, the java expanded key ordering is just what we need\n-    \/\/ we don't know if the key is aligned, hence not using load-execute form\n-\n-    load_key(xmm_temp1, key, 0x00, xmm_key_shuf_mask);\n-    __ pxor(xmm_result, xmm_temp1);\n-\n-    load_key(xmm_temp1, key, 0x10, xmm_key_shuf_mask);\n-    load_key(xmm_temp2, key, 0x20, xmm_key_shuf_mask);\n-    load_key(xmm_temp3, key, 0x30, xmm_key_shuf_mask);\n-    load_key(xmm_temp4, key, 0x40, xmm_key_shuf_mask);\n-\n-    __ aesenc(xmm_result, xmm_temp1);\n-    __ aesenc(xmm_result, xmm_temp2);\n-    __ aesenc(xmm_result, xmm_temp3);\n-    __ aesenc(xmm_result, xmm_temp4);\n-\n-    load_key(xmm_temp1, key, 0x50, xmm_key_shuf_mask);\n-    load_key(xmm_temp2, key, 0x60, xmm_key_shuf_mask);\n-    load_key(xmm_temp3, key, 0x70, xmm_key_shuf_mask);\n-    load_key(xmm_temp4, key, 0x80, xmm_key_shuf_mask);\n-\n-    __ aesenc(xmm_result, xmm_temp1);\n-    __ aesenc(xmm_result, xmm_temp2);\n-    __ aesenc(xmm_result, xmm_temp3);\n-    __ aesenc(xmm_result, xmm_temp4);\n-\n-    load_key(xmm_temp1, key, 0x90, xmm_key_shuf_mask);\n-    load_key(xmm_temp2, key, 0xa0, xmm_key_shuf_mask);\n-\n-    __ cmpl(keylen, 44);\n-    __ jccb(Assembler::equal, L_doLast);\n-\n-    __ aesenc(xmm_result, xmm_temp1);\n-    __ aesenc(xmm_result, xmm_temp2);\n-\n-    load_key(xmm_temp1, key, 0xb0, xmm_key_shuf_mask);\n-    load_key(xmm_temp2, key, 0xc0, xmm_key_shuf_mask);\n-\n-    __ cmpl(keylen, 52);\n-    __ jccb(Assembler::equal, L_doLast);\n-\n-    __ aesenc(xmm_result, xmm_temp1);\n-    __ aesenc(xmm_result, xmm_temp2);\n-\n-    load_key(xmm_temp1, key, 0xd0, xmm_key_shuf_mask);\n-    load_key(xmm_temp2, key, 0xe0, xmm_key_shuf_mask);\n-\n-    __ BIND(L_doLast);\n-    __ aesenc(xmm_result, xmm_temp1);\n-    __ aesenclast(xmm_result, xmm_temp2);\n-    __ movdqu(Address(to, 0), xmm_result);        \/\/ store the result\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    return start;\n-  }\n+\/\/ Non-destructive plausibility checks for oops\n+\/\/\n+\/\/ Arguments:\n+\/\/    all args on stack!\n+\/\/\n+\/\/ Stack after saving c_rarg3:\n+\/\/    [tos + 0]: saved c_rarg3\n+\/\/    [tos + 1]: saved c_rarg2\n+\/\/    [tos + 2]: saved r12 (several TemplateTable methods use it)\n+\/\/    [tos + 3]: saved flags\n+\/\/    [tos + 4]: return address\n+\/\/  * [tos + 5]: error message (char*)\n+\/\/  * [tos + 6]: object to verify (oop)\n+\/\/  * [tos + 7]: saved rax - saved by caller and bashed\n+\/\/  * [tos + 8]: saved r10 (rscratch1) - saved by caller\n+\/\/  * = popped on exit\n+address StubGenerator::generate_verify_oop() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"verify_oop\");\n+  address start = __ pc();\n+\n+  Label exit, error;\n+\n+  __ pushf();\n+  __ incrementl(ExternalAddress((address) StubRoutines::verify_oop_count_addr()), rscratch1);\n+\n+  __ push(r12);\n+\n+  \/\/ save c_rarg2 and c_rarg3\n+  __ push(c_rarg2);\n+  __ push(c_rarg3);\n+\n+  enum {\n+    \/\/ After previous pushes.\n+    oop_to_verify = 6 * wordSize,\n+    saved_rax     = 7 * wordSize,\n+    saved_r10     = 8 * wordSize,\n+\n+    \/\/ Before the call to MacroAssembler::debug(), see below.\n+    return_addr   = 16 * wordSize,\n+    error_msg     = 17 * wordSize\n+  };\n@@ -3683,0 +1045,2 @@\n+  \/\/ get object\n+  __ movptr(rax, Address(rsp, oop_to_verify));\n@@ -3684,90 +1048,3 @@\n-  \/\/ Arguments:\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source byte array address\n-  \/\/   c_rarg1   - destination byte array address\n-  \/\/   c_rarg2   - K (key) in little endian int array\n-  \/\/\n-  address generate_aescrypt_decryptBlock() {\n-    assert(UseAES, \"need AES instructions and misaligned SSE support\");\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"aescrypt_decryptBlock\");\n-    Label L_doLast;\n-    address start = __ pc();\n-\n-    const Register from        = c_rarg0;  \/\/ source array address\n-    const Register to          = c_rarg1;  \/\/ destination array address\n-    const Register key         = c_rarg2;  \/\/ key array address\n-    const Register keylen      = rax;\n-\n-    const XMMRegister xmm_result = xmm0;\n-    const XMMRegister xmm_key_shuf_mask = xmm1;\n-    \/\/ On win64 xmm6-xmm15 must be preserved so don't use them.\n-    const XMMRegister xmm_temp1  = xmm2;\n-    const XMMRegister xmm_temp2  = xmm3;\n-    const XMMRegister xmm_temp3  = xmm4;\n-    const XMMRegister xmm_temp4  = xmm5;\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-    \/\/ keylen could be only {11, 13, 15} * 4 = {44, 52, 60}\n-    __ movl(keylen, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n-\n-    __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n-    __ movdqu(xmm_result, Address(from, 0));\n-\n-    \/\/ for decryption java expanded key ordering is rotated one position from what we want\n-    \/\/ so we start from 0x10 here and hit 0x00 last\n-    \/\/ we don't know if the key is aligned, hence not using load-execute form\n-    load_key(xmm_temp1, key, 0x10, xmm_key_shuf_mask);\n-    load_key(xmm_temp2, key, 0x20, xmm_key_shuf_mask);\n-    load_key(xmm_temp3, key, 0x30, xmm_key_shuf_mask);\n-    load_key(xmm_temp4, key, 0x40, xmm_key_shuf_mask);\n-\n-    __ pxor  (xmm_result, xmm_temp1);\n-    __ aesdec(xmm_result, xmm_temp2);\n-    __ aesdec(xmm_result, xmm_temp3);\n-    __ aesdec(xmm_result, xmm_temp4);\n-\n-    load_key(xmm_temp1, key, 0x50, xmm_key_shuf_mask);\n-    load_key(xmm_temp2, key, 0x60, xmm_key_shuf_mask);\n-    load_key(xmm_temp3, key, 0x70, xmm_key_shuf_mask);\n-    load_key(xmm_temp4, key, 0x80, xmm_key_shuf_mask);\n-\n-    __ aesdec(xmm_result, xmm_temp1);\n-    __ aesdec(xmm_result, xmm_temp2);\n-    __ aesdec(xmm_result, xmm_temp3);\n-    __ aesdec(xmm_result, xmm_temp4);\n-\n-    load_key(xmm_temp1, key, 0x90, xmm_key_shuf_mask);\n-    load_key(xmm_temp2, key, 0xa0, xmm_key_shuf_mask);\n-    load_key(xmm_temp3, key, 0x00, xmm_key_shuf_mask);\n-\n-    __ cmpl(keylen, 44);\n-    __ jccb(Assembler::equal, L_doLast);\n-\n-    __ aesdec(xmm_result, xmm_temp1);\n-    __ aesdec(xmm_result, xmm_temp2);\n-\n-    load_key(xmm_temp1, key, 0xb0, xmm_key_shuf_mask);\n-    load_key(xmm_temp2, key, 0xc0, xmm_key_shuf_mask);\n-\n-    __ cmpl(keylen, 52);\n-    __ jccb(Assembler::equal, L_doLast);\n-\n-    __ aesdec(xmm_result, xmm_temp1);\n-    __ aesdec(xmm_result, xmm_temp2);\n-\n-    load_key(xmm_temp1, key, 0xd0, xmm_key_shuf_mask);\n-    load_key(xmm_temp2, key, 0xe0, xmm_key_shuf_mask);\n-\n-    __ BIND(L_doLast);\n-    __ aesdec(xmm_result, xmm_temp1);\n-    __ aesdec(xmm_result, xmm_temp2);\n-\n-    \/\/ for decryption the aesdeclast operation is always on key+0x00\n-    __ aesdeclast(xmm_result, xmm_temp3);\n-    __ movdqu(Address(to, 0), xmm_result);  \/\/ store the result\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+  \/\/ make sure object is 'reasonable'\n+  __ testptr(rax, rax);\n+  __ jcc(Assembler::zero, exit); \/\/ if obj is NULL it is OK\n@@ -3775,1 +1052,5 @@\n-    return start;\n+#if INCLUDE_ZGC\n+  if (UseZGC) {\n+    \/\/ Check if metadata bits indicate a bad oop\n+    __ testptr(rax, Address(r15_thread, ZThreadLocalData::address_bad_mask_offset()));\n+    __ jcc(Assembler::notZero, error);\n@@ -3777,52 +1058,56 @@\n-\n-\n-  \/\/ Arguments:\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source byte array address\n-  \/\/   c_rarg1   - destination byte array address\n-  \/\/   c_rarg2   - K (key) in little endian int array\n-  \/\/   c_rarg3   - r vector byte array address\n-  \/\/   c_rarg4   - input length\n-  \/\/\n-  \/\/ Output:\n-  \/\/   rax       - input length\n-  \/\/\n-  address generate_cipherBlockChaining_encryptAESCrypt() {\n-    assert(UseAES, \"need AES instructions and misaligned SSE support\");\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"cipherBlockChaining_encryptAESCrypt\");\n-    address start = __ pc();\n-\n-    Label L_exit, L_key_192_256, L_key_256, L_loopTop_128, L_loopTop_192, L_loopTop_256;\n-    const Register from        = c_rarg0;  \/\/ source array address\n-    const Register to          = c_rarg1;  \/\/ destination array address\n-    const Register key         = c_rarg2;  \/\/ key array address\n-    const Register rvec        = c_rarg3;  \/\/ r byte array initialized from initvector array address\n-                                           \/\/ and left with the results of the last encryption block\n-#ifndef _WIN64\n-    const Register len_reg     = c_rarg4;  \/\/ src len (must be multiple of blocksize 16)\n-#else\n-    const Address  len_mem(rbp, 6 * wordSize);  \/\/ length is on stack on Win64\n-    const Register len_reg     = r11;      \/\/ pick the volatile windows register\n-    const Register pos         = rax;\n-\n-    \/\/ xmm register assignments for the loops below\n-    const XMMRegister xmm_result = xmm0;\n-    const XMMRegister xmm_temp   = xmm1;\n-    \/\/ keys 0-10 preloaded into xmm2-xmm12\n-    const int XMM_REG_NUM_KEY_FIRST = 2;\n-    const int XMM_REG_NUM_KEY_LAST  = 15;\n-    const XMMRegister xmm_key0   = as_XMMRegister(XMM_REG_NUM_KEY_FIRST);\n-    const XMMRegister xmm_key10  = as_XMMRegister(XMM_REG_NUM_KEY_FIRST+10);\n-    const XMMRegister xmm_key11  = as_XMMRegister(XMM_REG_NUM_KEY_FIRST+11);\n-    const XMMRegister xmm_key12  = as_XMMRegister(XMM_REG_NUM_KEY_FIRST+12);\n-    const XMMRegister xmm_key13  = as_XMMRegister(XMM_REG_NUM_KEY_FIRST+13);\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-#ifdef _WIN64\n-    \/\/ on win64, fill len_reg from stack position\n-    __ movl(len_reg, len_mem);\n-#else\n-    __ push(len_reg); \/\/ Save\n-#endif\n+  \/\/ Check if the oop is in the right area of memory\n+  __ movptr(c_rarg2, rax);\n+  __ movptr(c_rarg3, (intptr_t) Universe::verify_oop_mask());\n+  __ andptr(c_rarg2, c_rarg3);\n+  __ movptr(c_rarg3, (intptr_t) Universe::verify_oop_bits());\n+  __ cmpptr(c_rarg2, c_rarg3);\n+  __ jcc(Assembler::notZero, error);\n+\n+  \/\/ make sure klass is 'reasonable', which is not zero.\n+  __ load_klass(rax, rax, rscratch1);  \/\/ get klass\n+  __ testptr(rax, rax);\n+  __ jcc(Assembler::zero, error); \/\/ if klass is NULL it is broken\n+\n+  \/\/ return if everything seems ok\n+  __ bind(exit);\n+  __ movptr(rax, Address(rsp, saved_rax));     \/\/ get saved rax back\n+  __ movptr(rscratch1, Address(rsp, saved_r10)); \/\/ get saved r10 back\n+  __ pop(c_rarg3);                             \/\/ restore c_rarg3\n+  __ pop(c_rarg2);                             \/\/ restore c_rarg2\n+  __ pop(r12);                                 \/\/ restore r12\n+  __ popf();                                   \/\/ restore flags\n+  __ ret(4 * wordSize);                        \/\/ pop caller saved stuff\n+\n+  \/\/ handle errors\n+  __ bind(error);\n+  __ movptr(rax, Address(rsp, saved_rax));     \/\/ get saved rax back\n+  __ movptr(rscratch1, Address(rsp, saved_r10)); \/\/ get saved r10 back\n+  __ pop(c_rarg3);                             \/\/ get saved c_rarg3 back\n+  __ pop(c_rarg2);                             \/\/ get saved c_rarg2 back\n+  __ pop(r12);                                 \/\/ get saved r12 back\n+  __ popf();                                   \/\/ get saved flags off stack --\n+                                               \/\/ will be ignored\n+\n+  __ pusha();                                  \/\/ push registers\n+                                               \/\/ (rip is already\n+                                               \/\/ already pushed)\n+  \/\/ debug(char* msg, int64_t pc, int64_t regs[])\n+  \/\/ We've popped the registers we'd saved (c_rarg3, c_rarg2 and flags), and\n+  \/\/ pushed all the registers, so now the stack looks like:\n+  \/\/     [tos +  0] 16 saved registers\n+  \/\/     [tos + 16] return address\n+  \/\/   * [tos + 17] error message (char*)\n+  \/\/   * [tos + 18] object to verify (oop)\n+  \/\/   * [tos + 19] saved rax - saved by caller and bashed\n+  \/\/   * [tos + 20] saved r10 (rscratch1) - saved by caller\n+  \/\/   * = popped on exit\n+\n+  __ movptr(c_rarg0, Address(rsp, error_msg));    \/\/ pass address of error message\n+  __ movptr(c_rarg1, Address(rsp, return_addr));  \/\/ pass return address\n+  __ movq(c_rarg2, rsp);                          \/\/ pass address of regs on stack\n+  __ mov(r12, rsp);                               \/\/ remember rsp\n+  __ subptr(rsp, frame::arg_reg_save_area_bytes); \/\/ windows\n+  __ andptr(rsp, -16);                            \/\/ align stack as required by ABI\n+  BLOCK_COMMENT(\"call MacroAssembler::debug\");\n+  __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, MacroAssembler::debug64)));\n+  __ hlt();\n@@ -3832,31 +1117,2 @@\n-    const XMMRegister xmm_key_shuf_mask = xmm_temp;  \/\/ used temporarily to swap key bytes up front\n-    __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n-    \/\/ load up xmm regs xmm2 thru xmm12 with key 0x00 - 0xa0\n-    for (int rnum = XMM_REG_NUM_KEY_FIRST, offset = 0x00; rnum <= XMM_REG_NUM_KEY_FIRST+10; rnum++) {\n-      load_key(as_XMMRegister(rnum), key, offset, xmm_key_shuf_mask);\n-      offset += 0x10;\n-    }\n-    __ movdqu(xmm_result, Address(rvec, 0x00));   \/\/ initialize xmm_result with r vec\n-\n-    \/\/ now split to different paths depending on the keylen (len in ints of AESCrypt.KLE array (52=192, or 60=256))\n-    __ movl(rax, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n-    __ cmpl(rax, 44);\n-    __ jcc(Assembler::notEqual, L_key_192_256);\n-\n-    \/\/ 128 bit code follows here\n-    __ movptr(pos, 0);\n-    __ align(OptoLoopAlignment);\n-\n-    __ BIND(L_loopTop_128);\n-    __ movdqu(xmm_temp, Address(from, pos, Address::times_1, 0));   \/\/ get next 16 bytes of input\n-    __ pxor  (xmm_result, xmm_temp);               \/\/ xor with the current r vector\n-    __ pxor  (xmm_result, xmm_key0);               \/\/ do the aes rounds\n-    for (int rnum = XMM_REG_NUM_KEY_FIRST + 1; rnum <= XMM_REG_NUM_KEY_FIRST + 9; rnum++) {\n-      __ aesenc(xmm_result, as_XMMRegister(rnum));\n-    }\n-    __ aesenclast(xmm_result, xmm_key10);\n-    __ movdqu(Address(to, pos, Address::times_1, 0), xmm_result);     \/\/ store into the next 16 bytes of output\n-    \/\/ no need to store r to memory until we exit\n-    __ addptr(pos, AESBlockSize);\n-    __ subptr(len_reg, AESBlockSize);\n-    __ jcc(Assembler::notEqual, L_loopTop_128);\n+  return start;\n+}\n@@ -3864,2 +1120,15 @@\n-    __ BIND(L_exit);\n-    __ movdqu(Address(rvec, 0), xmm_result);     \/\/ final value of r stored in rvec of CipherBlockChaining object\n+\/\/ Shuffle first three arg regs on Windows into Linux\/Solaris locations.\n+\/\/\n+\/\/ Outputs:\n+\/\/    rdi - rcx\n+\/\/    rsi - rdx\n+\/\/    rdx - r8\n+\/\/    rcx - r9\n+\/\/\n+\/\/ Registers r9 and r10 are used to save rdi and rsi on Windows, which latter\n+\/\/ are non-volatile.  r9 and r10 should not be used by the caller.\n+\/\/\n+void StubGenerator::setup_arg_regs(int nargs) {\n+  const Register saved_rdi = r9;\n+  const Register saved_rsi = r10;\n+  assert(nargs == 3 || nargs == 4, \"else fix\");\n@@ -3868,1 +1137,11 @@\n-    __ movl(rax, len_mem);\n+  assert(c_rarg0 == rcx && c_rarg1 == rdx && c_rarg2 == r8 && c_rarg3 == r9,\n+         \"unexpected argument registers\");\n+  if (nargs >= 4)\n+    __ mov(rax, r9);  \/\/ r9 is also saved_rdi\n+  __ movptr(saved_rdi, rdi);\n+  __ movptr(saved_rsi, rsi);\n+  __ mov(rdi, rcx); \/\/ c_rarg0\n+  __ mov(rsi, rdx); \/\/ c_rarg1\n+  __ mov(rdx, r8);  \/\/ c_rarg2\n+  if (nargs >= 4)\n+    __ mov(rcx, rax); \/\/ c_rarg3 (via rax)\n@@ -3870,1 +1149,2 @@\n-    __ pop(rax); \/\/ return length\n+  assert(c_rarg0 == rdi && c_rarg1 == rsi && c_rarg2 == rdx && c_rarg3 == rcx,\n+         \"unexpected argument registers\");\n@@ -3872,2 +1152,2 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+  DEBUG_ONLY(_regs_in_thread = false;)\n+}\n@@ -3875,81 +1155,7 @@\n-    __ BIND(L_key_192_256);\n-    \/\/ here rax = len in ints of AESCrypt.KLE array (52=192, or 60=256)\n-    load_key(xmm_key11, key, 0xb0, xmm_key_shuf_mask);\n-    load_key(xmm_key12, key, 0xc0, xmm_key_shuf_mask);\n-    __ cmpl(rax, 52);\n-    __ jcc(Assembler::notEqual, L_key_256);\n-\n-    \/\/ 192-bit code follows here (could be changed to use more xmm registers)\n-    __ movptr(pos, 0);\n-    __ align(OptoLoopAlignment);\n-\n-    __ BIND(L_loopTop_192);\n-    __ movdqu(xmm_temp, Address(from, pos, Address::times_1, 0));   \/\/ get next 16 bytes of input\n-    __ pxor  (xmm_result, xmm_temp);               \/\/ xor with the current r vector\n-    __ pxor  (xmm_result, xmm_key0);               \/\/ do the aes rounds\n-    for (int rnum = XMM_REG_NUM_KEY_FIRST + 1; rnum  <= XMM_REG_NUM_KEY_FIRST + 11; rnum++) {\n-      __ aesenc(xmm_result, as_XMMRegister(rnum));\n-    }\n-    __ aesenclast(xmm_result, xmm_key12);\n-    __ movdqu(Address(to, pos, Address::times_1, 0), xmm_result);     \/\/ store into the next 16 bytes of output\n-    \/\/ no need to store r to memory until we exit\n-    __ addptr(pos, AESBlockSize);\n-    __ subptr(len_reg, AESBlockSize);\n-    __ jcc(Assembler::notEqual, L_loopTop_192);\n-    __ jmp(L_exit);\n-\n-    __ BIND(L_key_256);\n-    \/\/ 256-bit code follows here (could be changed to use more xmm registers)\n-    load_key(xmm_key13, key, 0xd0, xmm_key_shuf_mask);\n-    __ movptr(pos, 0);\n-    __ align(OptoLoopAlignment);\n-\n-    __ BIND(L_loopTop_256);\n-    __ movdqu(xmm_temp, Address(from, pos, Address::times_1, 0));   \/\/ get next 16 bytes of input\n-    __ pxor  (xmm_result, xmm_temp);               \/\/ xor with the current r vector\n-    __ pxor  (xmm_result, xmm_key0);               \/\/ do the aes rounds\n-    for (int rnum = XMM_REG_NUM_KEY_FIRST + 1; rnum  <= XMM_REG_NUM_KEY_FIRST + 13; rnum++) {\n-      __ aesenc(xmm_result, as_XMMRegister(rnum));\n-    }\n-    load_key(xmm_temp, key, 0xe0);\n-    __ aesenclast(xmm_result, xmm_temp);\n-    __ movdqu(Address(to, pos, Address::times_1, 0), xmm_result);     \/\/ store into the next 16 bytes of output\n-    \/\/ no need to store r to memory until we exit\n-    __ addptr(pos, AESBlockSize);\n-    __ subptr(len_reg, AESBlockSize);\n-    __ jcc(Assembler::notEqual, L_loopTop_256);\n-    __ jmp(L_exit);\n-\n-    return start;\n-  }\n-  \/\/ This is a version of CBC\/AES Decrypt which does 4 blocks in a loop at a time\n-  \/\/ to hide instruction latency\n-  \/\/\n-  \/\/ Arguments:\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source byte array address\n-  \/\/   c_rarg1   - destination byte array address\n-  \/\/   c_rarg2   - K (key) in little endian int array\n-  \/\/   c_rarg3   - r vector byte array address\n-  \/\/   c_rarg4   - input length\n-  \/\/\n-  \/\/ Output:\n-  \/\/   rax       - input length\n-  \/\/\n-  address generate_cipherBlockChaining_decryptAESCrypt_Parallel() {\n-    assert(UseAES, \"need AES instructions and misaligned SSE support\");\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"cipherBlockChaining_decryptAESCrypt\");\n-    address start = __ pc();\n-\n-    const Register from        = c_rarg0;  \/\/ source array address\n-    const Register to          = c_rarg1;  \/\/ destination array address\n-    const Register key         = c_rarg2;  \/\/ key array address\n-    const Register rvec        = c_rarg3;  \/\/ r byte array initialized from initvector array address\n-                                           \/\/ and left with the results of the last encryption block\n-#ifndef _WIN64\n-    const Register len_reg     = c_rarg4;  \/\/ src len (must be multiple of blocksize 16)\n-#else\n-    const Address  len_mem(rbp, 6 * wordSize);  \/\/ length is on stack on Win64\n-    const Register len_reg     = r11;      \/\/ pick the volatile windows register\n+void StubGenerator::restore_arg_regs() {\n+  assert(!_regs_in_thread, \"wrong call to restore_arg_regs\");\n+  const Register saved_rdi = r9;\n+  const Register saved_rsi = r10;\n+#ifdef _WIN64\n+  __ movptr(rdi, saved_rdi);\n+  __ movptr(rsi, saved_rsi);\n@@ -3958,17 +1164,1 @@\n-    const Register pos         = rax;\n-\n-    const int PARALLEL_FACTOR = 4;\n-    const int ROUNDS[3] = { 10, 12, 14 }; \/\/ aes rounds for key128, key192, key256\n-\n-    Label L_exit;\n-    Label L_singleBlock_loopTopHead[3]; \/\/ 128, 192, 256\n-    Label L_singleBlock_loopTopHead2[3]; \/\/ 128, 192, 256\n-    Label L_singleBlock_loopTop[3]; \/\/ 128, 192, 256\n-    Label L_multiBlock_loopTopHead[3]; \/\/ 128, 192, 256\n-    Label L_multiBlock_loopTop[3]; \/\/ 128, 192, 256\n-\n-    \/\/ keys 0-10 preloaded into xmm5-xmm15\n-    const int XMM_REG_NUM_KEY_FIRST = 5;\n-    const int XMM_REG_NUM_KEY_LAST  = 15;\n-    const XMMRegister xmm_key_first = as_XMMRegister(XMM_REG_NUM_KEY_FIRST);\n-    const XMMRegister xmm_key_last  = as_XMMRegister(XMM_REG_NUM_KEY_LAST);\n+}\n@@ -3976,1 +1166,4 @@\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+\/\/ This is used in places where r10 is a scratch register, and can\n+\/\/ be adapted if r9 is needed also.\n+void StubGenerator::setup_arg_regs_using_thread() {\n+  const Register saved_r15 = r9;\n@@ -3979,2 +1172,10 @@\n-    \/\/ on win64, fill len_reg from stack position\n-    __ movl(len_reg, len_mem);\n+  __ mov(saved_r15, r15);  \/\/ r15 is callee saved and needs to be restored\n+  __ get_thread(r15_thread);\n+  assert(c_rarg0 == rcx && c_rarg1 == rdx && c_rarg2 == r8 && c_rarg3 == r9,\n+         \"unexpected argument registers\");\n+  __ movptr(Address(r15_thread, in_bytes(JavaThread::windows_saved_rdi_offset())), rdi);\n+  __ movptr(Address(r15_thread, in_bytes(JavaThread::windows_saved_rsi_offset())), rsi);\n+\n+  __ mov(rdi, rcx); \/\/ c_rarg0\n+  __ mov(rsi, rdx); \/\/ c_rarg1\n+  __ mov(rdx, r8);  \/\/ c_rarg2\n@@ -3982,1 +1183,2 @@\n-    __ push(len_reg); \/\/ Save\n+  assert(c_rarg0 == rdi && c_rarg1 == rsi && c_rarg2 == rdx && c_rarg3 == rcx,\n+         \"unexpected argument registers\");\n@@ -3984,172 +1186,2 @@\n-    __ push(rbx);\n-    \/\/ the java expanded key ordering is rotated one position from what we want\n-    \/\/ so we start from 0x10 here and hit 0x00 last\n-    const XMMRegister xmm_key_shuf_mask = xmm1;  \/\/ used temporarily to swap key bytes up front\n-    __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n-    \/\/ load up xmm regs 5 thru 15 with key 0x10 - 0xa0 - 0x00\n-    for (int rnum = XMM_REG_NUM_KEY_FIRST, offset = 0x10; rnum < XMM_REG_NUM_KEY_LAST; rnum++) {\n-      load_key(as_XMMRegister(rnum), key, offset, xmm_key_shuf_mask);\n-      offset += 0x10;\n-    }\n-    load_key(xmm_key_last, key, 0x00, xmm_key_shuf_mask);\n-\n-    const XMMRegister xmm_prev_block_cipher = xmm1;  \/\/ holds cipher of previous block\n-\n-    \/\/ registers holding the four results in the parallelized loop\n-    const XMMRegister xmm_result0 = xmm0;\n-    const XMMRegister xmm_result1 = xmm2;\n-    const XMMRegister xmm_result2 = xmm3;\n-    const XMMRegister xmm_result3 = xmm4;\n-\n-    __ movdqu(xmm_prev_block_cipher, Address(rvec, 0x00));   \/\/ initialize with initial rvec\n-\n-    __ xorptr(pos, pos);\n-\n-    \/\/ now split to different paths depending on the keylen (len in ints of AESCrypt.KLE array (52=192, or 60=256))\n-    __ movl(rbx, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n-    __ cmpl(rbx, 52);\n-    __ jcc(Assembler::equal, L_multiBlock_loopTopHead[1]);\n-    __ cmpl(rbx, 60);\n-    __ jcc(Assembler::equal, L_multiBlock_loopTopHead[2]);\n-\n-#define DoFour(opc, src_reg)           \\\n-  __ opc(xmm_result0, src_reg);         \\\n-  __ opc(xmm_result1, src_reg);         \\\n-  __ opc(xmm_result2, src_reg);         \\\n-  __ opc(xmm_result3, src_reg);         \\\n-\n-    for (int k = 0; k < 3; ++k) {\n-      __ BIND(L_multiBlock_loopTopHead[k]);\n-      if (k != 0) {\n-        __ cmpptr(len_reg, PARALLEL_FACTOR * AESBlockSize); \/\/ see if at least 4 blocks left\n-        __ jcc(Assembler::less, L_singleBlock_loopTopHead2[k]);\n-      }\n-      if (k == 1) {\n-        __ subptr(rsp, 6 * wordSize);\n-        __ movdqu(Address(rsp, 0), xmm15); \/\/save last_key from xmm15\n-        load_key(xmm15, key, 0xb0); \/\/ 0xb0; 192-bit key goes up to 0xc0\n-        __ movdqu(Address(rsp, 2 * wordSize), xmm15);\n-        load_key(xmm1, key, 0xc0);  \/\/ 0xc0;\n-        __ movdqu(Address(rsp, 4 * wordSize), xmm1);\n-      } else if (k == 2) {\n-        __ subptr(rsp, 10 * wordSize);\n-        __ movdqu(Address(rsp, 0), xmm15); \/\/save last_key from xmm15\n-        load_key(xmm15, key, 0xd0); \/\/ 0xd0; 256-bit key goes up to 0xe0\n-        __ movdqu(Address(rsp, 6 * wordSize), xmm15);\n-        load_key(xmm1, key, 0xe0);  \/\/ 0xe0;\n-        __ movdqu(Address(rsp, 8 * wordSize), xmm1);\n-        load_key(xmm15, key, 0xb0); \/\/ 0xb0;\n-        __ movdqu(Address(rsp, 2 * wordSize), xmm15);\n-        load_key(xmm1, key, 0xc0);  \/\/ 0xc0;\n-        __ movdqu(Address(rsp, 4 * wordSize), xmm1);\n-      }\n-      __ align(OptoLoopAlignment);\n-      __ BIND(L_multiBlock_loopTop[k]);\n-      __ cmpptr(len_reg, PARALLEL_FACTOR * AESBlockSize); \/\/ see if at least 4 blocks left\n-      __ jcc(Assembler::less, L_singleBlock_loopTopHead[k]);\n-\n-      if  (k != 0) {\n-        __ movdqu(xmm15, Address(rsp, 2 * wordSize));\n-        __ movdqu(xmm1, Address(rsp, 4 * wordSize));\n-      }\n-\n-      __ movdqu(xmm_result0, Address(from, pos, Address::times_1, 0 * AESBlockSize)); \/\/ get next 4 blocks into xmmresult registers\n-      __ movdqu(xmm_result1, Address(from, pos, Address::times_1, 1 * AESBlockSize));\n-      __ movdqu(xmm_result2, Address(from, pos, Address::times_1, 2 * AESBlockSize));\n-      __ movdqu(xmm_result3, Address(from, pos, Address::times_1, 3 * AESBlockSize));\n-\n-      DoFour(pxor, xmm_key_first);\n-      if (k == 0) {\n-        for (int rnum = 1; rnum < ROUNDS[k]; rnum++) {\n-          DoFour(aesdec, as_XMMRegister(rnum + XMM_REG_NUM_KEY_FIRST));\n-        }\n-        DoFour(aesdeclast, xmm_key_last);\n-      } else if (k == 1) {\n-        for (int rnum = 1; rnum <= ROUNDS[k]-2; rnum++) {\n-          DoFour(aesdec, as_XMMRegister(rnum + XMM_REG_NUM_KEY_FIRST));\n-        }\n-        __ movdqu(xmm_key_last, Address(rsp, 0)); \/\/ xmm15 needs to be loaded again.\n-        DoFour(aesdec, xmm1);  \/\/ key : 0xc0\n-        __ movdqu(xmm_prev_block_cipher, Address(rvec, 0x00));  \/\/ xmm1 needs to be loaded again\n-        DoFour(aesdeclast, xmm_key_last);\n-      } else if (k == 2) {\n-        for (int rnum = 1; rnum <= ROUNDS[k] - 4; rnum++) {\n-          DoFour(aesdec, as_XMMRegister(rnum + XMM_REG_NUM_KEY_FIRST));\n-        }\n-        DoFour(aesdec, xmm1);  \/\/ key : 0xc0\n-        __ movdqu(xmm15, Address(rsp, 6 * wordSize));\n-        __ movdqu(xmm1, Address(rsp, 8 * wordSize));\n-        DoFour(aesdec, xmm15);  \/\/ key : 0xd0\n-        __ movdqu(xmm_key_last, Address(rsp, 0)); \/\/ xmm15 needs to be loaded again.\n-        DoFour(aesdec, xmm1);  \/\/ key : 0xe0\n-        __ movdqu(xmm_prev_block_cipher, Address(rvec, 0x00));  \/\/ xmm1 needs to be loaded again\n-        DoFour(aesdeclast, xmm_key_last);\n-      }\n-\n-      \/\/ for each result, xor with the r vector of previous cipher block\n-      __ pxor(xmm_result0, xmm_prev_block_cipher);\n-      __ movdqu(xmm_prev_block_cipher, Address(from, pos, Address::times_1, 0 * AESBlockSize));\n-      __ pxor(xmm_result1, xmm_prev_block_cipher);\n-      __ movdqu(xmm_prev_block_cipher, Address(from, pos, Address::times_1, 1 * AESBlockSize));\n-      __ pxor(xmm_result2, xmm_prev_block_cipher);\n-      __ movdqu(xmm_prev_block_cipher, Address(from, pos, Address::times_1, 2 * AESBlockSize));\n-      __ pxor(xmm_result3, xmm_prev_block_cipher);\n-      __ movdqu(xmm_prev_block_cipher, Address(from, pos, Address::times_1, 3 * AESBlockSize));   \/\/ this will carry over to next set of blocks\n-      if (k != 0) {\n-        __ movdqu(Address(rvec, 0x00), xmm_prev_block_cipher);\n-      }\n-\n-      __ movdqu(Address(to, pos, Address::times_1, 0 * AESBlockSize), xmm_result0);     \/\/ store 4 results into the next 64 bytes of output\n-      __ movdqu(Address(to, pos, Address::times_1, 1 * AESBlockSize), xmm_result1);\n-      __ movdqu(Address(to, pos, Address::times_1, 2 * AESBlockSize), xmm_result2);\n-      __ movdqu(Address(to, pos, Address::times_1, 3 * AESBlockSize), xmm_result3);\n-\n-      __ addptr(pos, PARALLEL_FACTOR * AESBlockSize);\n-      __ subptr(len_reg, PARALLEL_FACTOR * AESBlockSize);\n-      __ jmp(L_multiBlock_loopTop[k]);\n-\n-      \/\/ registers used in the non-parallelized loops\n-      \/\/ xmm register assignments for the loops below\n-      const XMMRegister xmm_result = xmm0;\n-      const XMMRegister xmm_prev_block_cipher_save = xmm2;\n-      const XMMRegister xmm_key11 = xmm3;\n-      const XMMRegister xmm_key12 = xmm4;\n-      const XMMRegister key_tmp = xmm4;\n-\n-      __ BIND(L_singleBlock_loopTopHead[k]);\n-      if (k == 1) {\n-        __ addptr(rsp, 6 * wordSize);\n-      } else if (k == 2) {\n-        __ addptr(rsp, 10 * wordSize);\n-      }\n-      __ cmpptr(len_reg, 0); \/\/ any blocks left??\n-      __ jcc(Assembler::equal, L_exit);\n-      __ BIND(L_singleBlock_loopTopHead2[k]);\n-      if (k == 1) {\n-        load_key(xmm_key11, key, 0xb0); \/\/ 0xb0; 192-bit key goes up to 0xc0\n-        load_key(xmm_key12, key, 0xc0); \/\/ 0xc0; 192-bit key goes up to 0xc0\n-      }\n-      if (k == 2) {\n-        load_key(xmm_key11, key, 0xb0); \/\/ 0xb0; 256-bit key goes up to 0xe0\n-      }\n-      __ align(OptoLoopAlignment);\n-      __ BIND(L_singleBlock_loopTop[k]);\n-      __ movdqu(xmm_result, Address(from, pos, Address::times_1, 0)); \/\/ get next 16 bytes of cipher input\n-      __ movdqa(xmm_prev_block_cipher_save, xmm_result); \/\/ save for next r vector\n-      __ pxor(xmm_result, xmm_key_first); \/\/ do the aes dec rounds\n-      for (int rnum = 1; rnum <= 9 ; rnum++) {\n-          __ aesdec(xmm_result, as_XMMRegister(rnum + XMM_REG_NUM_KEY_FIRST));\n-      }\n-      if (k == 1) {\n-        __ aesdec(xmm_result, xmm_key11);\n-        __ aesdec(xmm_result, xmm_key12);\n-      }\n-      if (k == 2) {\n-        __ aesdec(xmm_result, xmm_key11);\n-        load_key(key_tmp, key, 0xc0);\n-        __ aesdec(xmm_result, key_tmp);\n-        load_key(key_tmp, key, 0xd0);\n-        __ aesdec(xmm_result, key_tmp);\n-        load_key(key_tmp, key, 0xe0);\n-        __ aesdec(xmm_result, key_tmp);\n-      }\n+  DEBUG_ONLY(_regs_in_thread = true;)\n+}\n@@ -4157,15 +1189,3 @@\n-      __ aesdeclast(xmm_result, xmm_key_last); \/\/ xmm15 always came from key+0\n-      __ pxor(xmm_result, xmm_prev_block_cipher); \/\/ xor with the current r vector\n-      __ movdqu(Address(to, pos, Address::times_1, 0), xmm_result); \/\/ store into the next 16 bytes of output\n-      \/\/ no need to store r to memory until we exit\n-      __ movdqa(xmm_prev_block_cipher, xmm_prev_block_cipher_save); \/\/ set up next r vector with cipher input from this block\n-      __ addptr(pos, AESBlockSize);\n-      __ subptr(len_reg, AESBlockSize);\n-      __ jcc(Assembler::notEqual, L_singleBlock_loopTop[k]);\n-      if (k != 2) {\n-        __ jmp(L_exit);\n-      }\n-    } \/\/for 128\/192\/256\n-    __ BIND(L_exit);\n-    __ movdqu(Address(rvec, 0), xmm_prev_block_cipher);     \/\/ final value of r stored in rvec of CipherBlockChaining object\n-    __ pop(rbx);\n+void StubGenerator::restore_arg_regs_using_thread() {\n+  assert(_regs_in_thread, \"wrong call to restore_arg_regs\");\n+  const Register saved_r15 = r9;\n@@ -4174,3 +1194,4 @@\n-    __ movl(rax, len_mem);\n-#else\n-    __ pop(rax); \/\/ return length\n+  __ get_thread(r15_thread);\n+  __ movptr(rsi, Address(r15_thread, in_bytes(JavaThread::windows_saved_rsi_offset())));\n+  __ movptr(rdi, Address(r15_thread, in_bytes(JavaThread::windows_saved_rdi_offset())));\n+  __ mov(r15, saved_r15);  \/\/ r15 is callee saved and needs to be restored\n@@ -4178,3 +1199,0 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-    return start;\n@@ -4183,66 +1201,7 @@\n-  address generate_electronicCodeBook_encryptAESCrypt() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"electronicCodeBook_encryptAESCrypt\");\n-    address start = __ pc();\n-    const Register from = c_rarg0;  \/\/ source array address\n-    const Register to = c_rarg1;  \/\/ destination array address\n-    const Register key = c_rarg2;  \/\/ key array address\n-    const Register len = c_rarg3;  \/\/ src len (must be multiple of blocksize 16)\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ aesecb_encrypt(from, to, key, len);\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-    return start;\n- }\n-\n-  address generate_electronicCodeBook_decryptAESCrypt() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"electronicCodeBook_decryptAESCrypt\");\n-    address start = __ pc();\n-    const Register from = c_rarg0;  \/\/ source array address\n-    const Register to = c_rarg1;  \/\/ destination array address\n-    const Register key = c_rarg2;  \/\/ key array address\n-    const Register len = c_rarg3;  \/\/ src len (must be multiple of blocksize 16)\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ aesecb_decrypt(from, to, key, len);\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-    return start;\n-  }\n-\n-  \/\/ ofs and limit are use for multi-block byte array.\n-  \/\/ int com.sun.security.provider.MD5.implCompress(byte[] b, int ofs)\n-  address generate_md5_implCompress(bool multi_block, const char *name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    const Register buf_param = r15;\n-    const Address state_param(rsp, 0 * wordSize);\n-    const Address ofs_param  (rsp, 1 * wordSize    );\n-    const Address limit_param(rsp, 1 * wordSize + 4);\n-\n-    __ enter();\n-    __ push(rbx);\n-    __ push(rdi);\n-    __ push(rsi);\n-    __ push(r15);\n-    __ subptr(rsp, 2 * wordSize);\n-\n-    __ movptr(buf_param, c_rarg0);\n-    __ movptr(state_param, c_rarg1);\n-    if (multi_block) {\n-      __ movl(ofs_param, c_rarg2);\n-      __ movl(limit_param, c_rarg3);\n-    }\n-    __ fast_md5(buf_param, state_param, ofs_param, limit_param, multi_block);\n-    __ addptr(rsp, 2 * wordSize);\n-    __ pop(r15);\n-    __ pop(rsi);\n-    __ pop(rdi);\n-    __ pop(rbx);\n-    __ leave();\n-    __ ret(0);\n-    return start;\n+void StubGenerator::setup_argument_regs(BasicType type) {\n+  if (type == T_BYTE || type == T_SHORT) {\n+    setup_arg_regs(); \/\/ from => rdi, to => rsi, count => rdx\n+                      \/\/ r9 and r10 may be used to save non-volatile registers\n+  } else {\n+    setup_arg_regs_using_thread(); \/\/ from => rdi, to => rsi, count => rdx\n+                                   \/\/ r9 is used to save r15_thread\n@@ -4251,0 +1210,1 @@\n+}\n@@ -4252,15 +1212,5 @@\n-  address generate_upper_word_mask() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"upper_word_mask\");\n-    address start = __ pc();\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0xFFFFFFFF00000000, relocInfo::none);\n-    return start;\n-  }\n-  address generate_shuffle_byte_flip_mask() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"shuffle_byte_flip_mask\");\n-    address start = __ pc();\n-    __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\n-    __ emit_data64(0x0001020304050607, relocInfo::none);\n-    return start;\n+void StubGenerator::restore_argument_regs(BasicType type) {\n+  if (type == T_BYTE || type == T_SHORT) {\n+    restore_arg_regs();\n+  } else {\n+    restore_arg_regs_using_thread();\n@@ -4269,0 +1219,1 @@\n+}\n@@ -4270,11 +1221,2 @@\n-  \/\/ ofs and limit are use for multi-block byte array.\n-  \/\/ int com.sun.security.provider.DigestBase.implCompressMultiBlock(byte[] b, int ofs, int limit)\n-  address generate_sha1_implCompress(bool multi_block, const char *name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    Register buf = c_rarg0;\n-    Register state = c_rarg1;\n-    Register ofs = c_rarg2;\n-    Register limit = c_rarg3;\n+address StubGenerator::generate_data_cache_writeback() {\n+  const Register src        = c_rarg0;  \/\/ source address\n@@ -4282,4 +1224,1 @@\n-    const XMMRegister abcd = xmm0;\n-    const XMMRegister e0 = xmm1;\n-    const XMMRegister e1 = xmm2;\n-    const XMMRegister msg0 = xmm3;\n+  __ align(CodeEntryAlignment);\n@@ -4287,4 +1226,1 @@\n-    const XMMRegister msg1 = xmm4;\n-    const XMMRegister msg2 = xmm5;\n-    const XMMRegister msg3 = xmm6;\n-    const XMMRegister shuf_mask = xmm7;\n+  StubCodeMark mark(this, \"StubRoutines\", \"_data_cache_writeback\");\n@@ -4292,1 +1228,1 @@\n-    __ enter();\n+  address start = __ pc();\n@@ -4294,1 +1230,4 @@\n-    __ subptr(rsp, 4 * wordSize);\n+  __ enter();\n+  __ cache_wb(Address(src, 0));\n+  __ leave();\n+  __ ret(0);\n@@ -4296,2 +1235,2 @@\n-    __ fast_sha1(abcd, e0, e1, msg0, msg1, msg2, msg3, shuf_mask,\n-      buf, state, ofs, limit, rsp, multi_block);\n+  return start;\n+}\n@@ -4299,1 +1238,2 @@\n-    __ addptr(rsp, 4 * wordSize);\n+address StubGenerator::generate_data_cache_writeback_sync() {\n+  const Register is_pre    = c_rarg0;  \/\/ pre or post sync\n@@ -4301,4 +1241,1 @@\n-    __ leave();\n-    __ ret(0);\n-    return start;\n-  }\n+  __ align(CodeEntryAlignment);\n@@ -4306,6 +1243,1 @@\n-  address generate_pshuffle_byte_flip_mask() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"pshuffle_byte_flip_mask\");\n-    address start = __ pc();\n-    __ emit_data64(0x0405060700010203, relocInfo::none);\n-    __ emit_data64(0x0c0d0e0f08090a0b, relocInfo::none);\n+  StubCodeMark mark(this, \"StubRoutines\", \"_data_cache_writeback_sync\");\n@@ -4313,14 +1245,2 @@\n-    if (VM_Version::supports_avx2()) {\n-      __ emit_data64(0x0405060700010203, relocInfo::none); \/\/ second copy\n-      __ emit_data64(0x0c0d0e0f08090a0b, relocInfo::none);\n-      \/\/ _SHUF_00BA\n-      __ emit_data64(0x0b0a090803020100, relocInfo::none);\n-      __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);\n-      __ emit_data64(0x0b0a090803020100, relocInfo::none);\n-      __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);\n-      \/\/ _SHUF_DC00\n-      __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);\n-      __ emit_data64(0x0b0a090803020100, relocInfo::none);\n-      __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);\n-      __ emit_data64(0x0b0a090803020100, relocInfo::none);\n-    }\n+  \/\/ pre wbsync is a no-op\n+  \/\/ post wbsync translates to an sfence\n@@ -4328,2 +1248,2 @@\n-    return start;\n-  }\n+  Label skip;\n+  address start = __ pc();\n@@ -4331,15 +1251,7 @@\n-  \/\/Mask for byte-swapping a couple of qwords in an XMM register using (v)pshufb.\n-  address generate_pshuffle_byte_flip_mask_sha512() {\n-    __ align32();\n-    StubCodeMark mark(this, \"StubRoutines\", \"pshuffle_byte_flip_mask_sha512\");\n-    address start = __ pc();\n-    if (VM_Version::supports_avx2()) {\n-      __ emit_data64(0x0001020304050607, relocInfo::none); \/\/ PSHUFFLE_BYTE_FLIP_MASK\n-      __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\n-      __ emit_data64(0x1011121314151617, relocInfo::none);\n-      __ emit_data64(0x18191a1b1c1d1e1f, relocInfo::none);\n-      __ emit_data64(0x0000000000000000, relocInfo::none); \/\/MASK_YMM_LO\n-      __ emit_data64(0x0000000000000000, relocInfo::none);\n-      __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);\n-      __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);\n-    }\n+  __ enter();\n+  __ cmpl(is_pre, 0);\n+  __ jcc(Assembler::notEqual, skip);\n+  __ cache_wbsync(false);\n+  __ bind(skip);\n+  __ leave();\n+  __ ret(0);\n@@ -4347,2 +1259,2 @@\n-    return start;\n-  }\n+  return start;\n+}\n@@ -4351,75 +1263,33 @@\n-\/\/ int com.sun.security.provider.DigestBase.implCompressMultiBlock(byte[] b, int ofs, int limit)\n-  address generate_sha256_implCompress(bool multi_block, const char *name) {\n-    assert(VM_Version::supports_sha() || VM_Version::supports_avx2(), \"\");\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    Register buf = c_rarg0;\n-    Register state = c_rarg1;\n-    Register ofs = c_rarg2;\n-    Register limit = c_rarg3;\n-\n-    const XMMRegister msg = xmm0;\n-    const XMMRegister state0 = xmm1;\n-    const XMMRegister state1 = xmm2;\n-    const XMMRegister msgtmp0 = xmm3;\n-\n-    const XMMRegister msgtmp1 = xmm4;\n-    const XMMRegister msgtmp2 = xmm5;\n-    const XMMRegister msgtmp3 = xmm6;\n-    const XMMRegister msgtmp4 = xmm7;\n-\n-    const XMMRegister shuf_mask = xmm8;\n-\n-    __ enter();\n-\n-    __ subptr(rsp, 4 * wordSize);\n-\n-    if (VM_Version::supports_sha()) {\n-      __ fast_sha256(msg, state0, state1, msgtmp0, msgtmp1, msgtmp2, msgtmp3, msgtmp4,\n-        buf, state, ofs, limit, rsp, multi_block, shuf_mask);\n-    } else if (VM_Version::supports_avx2()) {\n-      __ sha256_AVX2(msg, state0, state1, msgtmp0, msgtmp1, msgtmp2, msgtmp3, msgtmp4,\n-        buf, state, ofs, limit, rsp, multi_block, shuf_mask);\n-    }\n-    __ addptr(rsp, 4 * wordSize);\n-    __ vzeroupper();\n-    __ leave();\n-    __ ret(0);\n-    return start;\n-  }\n-\n-  address generate_sha512_implCompress(bool multi_block, const char *name) {\n-    assert(VM_Version::supports_avx2(), \"\");\n-    assert(VM_Version::supports_bmi2(), \"\");\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    Register buf = c_rarg0;\n-    Register state = c_rarg1;\n-    Register ofs = c_rarg2;\n-    Register limit = c_rarg3;\n-\n-    const XMMRegister msg = xmm0;\n-    const XMMRegister state0 = xmm1;\n-    const XMMRegister state1 = xmm2;\n-    const XMMRegister msgtmp0 = xmm3;\n-    const XMMRegister msgtmp1 = xmm4;\n-    const XMMRegister msgtmp2 = xmm5;\n-    const XMMRegister msgtmp3 = xmm6;\n-    const XMMRegister msgtmp4 = xmm7;\n-\n-    const XMMRegister shuf_mask = xmm8;\n-\n-    __ enter();\n-\n-    __ sha512_AVX2(msg, state0, state1, msgtmp0, msgtmp1, msgtmp2, msgtmp3, msgtmp4,\n-    buf, state, ofs, limit, rsp, multi_block, shuf_mask);\n-\n-    __ vzeroupper();\n-    __ leave();\n-    __ ret(0);\n-    return start;\n-  }\n+\/\/ int com.sun.security.provider.MD5.implCompress(byte[] b, int ofs)\n+address StubGenerator::generate_md5_implCompress(bool multi_block, const char *name) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n+\n+  const Register buf_param = r15;\n+  const Address state_param(rsp, 0 * wordSize);\n+  const Address ofs_param  (rsp, 1 * wordSize    );\n+  const Address limit_param(rsp, 1 * wordSize + 4);\n+\n+  __ enter();\n+  __ push(rbx);\n+  __ push(rdi);\n+  __ push(rsi);\n+  __ push(r15);\n+  __ subptr(rsp, 2 * wordSize);\n+\n+  __ movptr(buf_param, c_rarg0);\n+  __ movptr(state_param, c_rarg1);\n+  if (multi_block) {\n+    __ movl(ofs_param, c_rarg2);\n+    __ movl(limit_param, c_rarg3);\n+  }\n+  __ fast_md5(buf_param, state_param, ofs_param, limit_param, multi_block);\n+\n+  __ addptr(rsp, 2 * wordSize);\n+  __ pop(r15);\n+  __ pop(rsi);\n+  __ pop(rdi);\n+  __ pop(rbx);\n+  __ leave();\n+  __ ret(0);\n@@ -4427,17 +1297,1 @@\n-  address ghash_polynomial512_addr() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"_ghash_poly512_addr\");\n-    address start = __ pc();\n-    __ emit_data64(0x00000001C2000000, relocInfo::none); \/\/ POLY for reduction\n-    __ emit_data64(0xC200000000000000, relocInfo::none);\n-    __ emit_data64(0x00000001C2000000, relocInfo::none);\n-    __ emit_data64(0xC200000000000000, relocInfo::none);\n-    __ emit_data64(0x00000001C2000000, relocInfo::none);\n-    __ emit_data64(0xC200000000000000, relocInfo::none);\n-    __ emit_data64(0x00000001C2000000, relocInfo::none);\n-    __ emit_data64(0xC200000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000001, relocInfo::none); \/\/ POLY\n-    __ emit_data64(0xC200000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000001, relocInfo::none); \/\/ TWOONE\n-    __ emit_data64(0x0000000100000000, relocInfo::none);\n-    return start;\n+  return start;\n@@ -4446,307 +1300,4 @@\n-  \/\/ Vector AES Galois Counter Mode implementation. Parameters:\n-  \/\/ Windows regs            |  Linux regs\n-  \/\/ in = c_rarg0 (rcx)      |  c_rarg0 (rsi)\n-  \/\/ len = c_rarg1 (rdx)     |  c_rarg1 (rdi)\n-  \/\/ ct = c_rarg2 (r8)       |  c_rarg2 (rdx)\n-  \/\/ out = c_rarg3 (r9)      |  c_rarg3 (rcx)\n-  \/\/ key = r10               |  c_rarg4 (r8)\n-  \/\/ state = r13             |  c_rarg5 (r9)\n-  \/\/ subkeyHtbl = r14        |  r11\n-  \/\/ counter = rsi           |  r12\n-  \/\/ return - number of processed bytes\n-  address generate_galoisCounterMode_AESCrypt() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"galoisCounterMode_AESCrypt\");\n-    address start = __ pc();\n-    const Register in = c_rarg0;\n-    const Register len = c_rarg1;\n-    const Register ct = c_rarg2;\n-    const Register out = c_rarg3;\n-    \/\/ and updated with the incremented counter in the end\n-#ifndef _WIN64\n-    const Register key = c_rarg4;\n-    const Register state = c_rarg5;\n-    const Address subkeyH_mem(rbp, 2 * wordSize);\n-    const Register subkeyHtbl = r11;\n-    const Register avx512_subkeyHtbl = r13;\n-    const Address counter_mem(rbp, 3 * wordSize);\n-    const Register counter = r12;\n-#else\n-    const Address key_mem(rbp, 6 * wordSize);\n-    const Register key = r10;\n-    const Address state_mem(rbp, 7 * wordSize);\n-    const Register state = r13;\n-    const Address subkeyH_mem(rbp, 8 * wordSize);\n-    const Register subkeyHtbl = r14;\n-    const Register avx512_subkeyHtbl = r12;\n-    const Address counter_mem(rbp, 9 * wordSize);\n-    const Register counter = rsi;\n-#endif\n-    __ enter();\n-   \/\/ Save state before entering routine\n-    __ push(r12);\n-    __ push(r13);\n-    __ push(r14);\n-    __ push(r15);\n-    __ push(rbx);\n-#ifdef _WIN64\n-    \/\/ on win64, fill len_reg from stack position\n-    __ push(rsi);\n-    __ movptr(key, key_mem);\n-    __ movptr(state, state_mem);\n-#endif\n-    __ movptr(subkeyHtbl, subkeyH_mem);\n-    __ movptr(counter, counter_mem);\n-\/\/ Save rbp and rsp\n-    __ push(rbp);\n-    __ movq(rbp, rsp);\n-\/\/ Align stack\n-    __ andq(rsp, -64);\n-    __ subptr(rsp, 96 * longSize); \/\/ Create space on the stack for htbl entries\n-    __ movptr(avx512_subkeyHtbl, rsp);\n-\n-    __ aesgcm_encrypt(in, len, ct, out, key, state, subkeyHtbl, avx512_subkeyHtbl, counter);\n-    __ vzeroupper();\n-\n-    __ movq(rsp, rbp);\n-    __ pop(rbp);\n-\n-    \/\/ Restore state before leaving routine\n-#ifdef _WIN64\n-    __ pop(rsi);\n-#endif\n-    __ pop(rbx);\n-    __ pop(r15);\n-    __ pop(r14);\n-    __ pop(r13);\n-    __ pop(r12);\n-\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-     return start;\n-  }\n-\n-  \/\/ This mask is used for incrementing counter value(linc0, linc4, etc.)\n-  address counter_mask_addr() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"counter_mask_addr\");\n-    address start = __ pc();\n-    __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\/\/lbswapmask\n-    __ emit_data64(0x0001020304050607, relocInfo::none);\n-    __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\n-    __ emit_data64(0x0001020304050607, relocInfo::none);\n-    __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\n-    __ emit_data64(0x0001020304050607, relocInfo::none);\n-    __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\n-    __ emit_data64(0x0001020304050607, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\/\/linc0 = counter_mask_addr+64\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000001, relocInfo::none);\/\/counter_mask_addr() + 80\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000002, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000003, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000004, relocInfo::none);\/\/linc4 = counter_mask_addr() + 128\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000004, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000004, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000004, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000008, relocInfo::none);\/\/linc8 = counter_mask_addr() + 192\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000008, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000008, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000008, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000020, relocInfo::none);\/\/linc32 = counter_mask_addr() + 256\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000020, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000020, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000020, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000010, relocInfo::none);\/\/linc16 = counter_mask_addr() + 320\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000010, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000010, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000010, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    return start;\n-  }\n-\n- \/\/ Vector AES Counter implementation\n-  address generate_counterMode_VectorAESCrypt()  {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"counterMode_AESCrypt\");\n-    address start = __ pc();\n-    const Register from = c_rarg0; \/\/ source array address\n-    const Register to = c_rarg1; \/\/ destination array address\n-    const Register key = c_rarg2; \/\/ key array address r8\n-    const Register counter = c_rarg3; \/\/ counter byte array initialized from counter array address\n-    \/\/ and updated with the incremented counter in the end\n-#ifndef _WIN64\n-    const Register len_reg = c_rarg4;\n-    const Register saved_encCounter_start = c_rarg5;\n-    const Register used_addr = r10;\n-    const Address  used_mem(rbp, 2 * wordSize);\n-    const Register used = r11;\n-#else\n-    const Address len_mem(rbp, 6 * wordSize); \/\/ length is on stack on Win64\n-    const Address saved_encCounter_mem(rbp, 7 * wordSize); \/\/ saved encrypted counter is on stack on Win64\n-    const Address used_mem(rbp, 8 * wordSize); \/\/ used length is on stack on Win64\n-    const Register len_reg = r10; \/\/ pick the first volatile windows register\n-    const Register saved_encCounter_start = r11;\n-    const Register used_addr = r13;\n-    const Register used = r14;\n-#endif\n-    __ enter();\n-   \/\/ Save state before entering routine\n-    __ push(r12);\n-    __ push(r13);\n-    __ push(r14);\n-    __ push(r15);\n-#ifdef _WIN64\n-    \/\/ on win64, fill len_reg from stack position\n-    __ movl(len_reg, len_mem);\n-    __ movptr(saved_encCounter_start, saved_encCounter_mem);\n-    __ movptr(used_addr, used_mem);\n-    __ movl(used, Address(used_addr, 0));\n-#else\n-    __ push(len_reg); \/\/ Save\n-    __ movptr(used_addr, used_mem);\n-    __ movl(used, Address(used_addr, 0));\n-#endif\n-    __ push(rbx);\n-    __ aesctr_encrypt(from, to, key, counter, len_reg, used, used_addr, saved_encCounter_start);\n-    __ vzeroupper();\n-    \/\/ Restore state before leaving routine\n-    __ pop(rbx);\n-#ifdef _WIN64\n-    __ movl(rax, len_mem); \/\/ return length\n-#else\n-    __ pop(rax); \/\/ return length\n-#endif\n-    __ pop(r15);\n-    __ pop(r14);\n-    __ pop(r13);\n-    __ pop(r12);\n-\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-    return start;\n-  }\n-\n-  \/\/ This is a version of CTR\/AES crypt which does 6 blocks in a loop at a time\n-  \/\/ to hide instruction latency\n-  \/\/\n-  \/\/ Arguments:\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source byte array address\n-  \/\/   c_rarg1   - destination byte array address\n-  \/\/   c_rarg2   - K (key) in little endian int array\n-  \/\/   c_rarg3   - counter vector byte array address\n-  \/\/   Linux\n-  \/\/     c_rarg4   -          input length\n-  \/\/     c_rarg5   -          saved encryptedCounter start\n-  \/\/     rbp + 6 * wordSize - saved used length\n-  \/\/   Windows\n-  \/\/     rbp + 6 * wordSize - input length\n-  \/\/     rbp + 7 * wordSize - saved encryptedCounter start\n-  \/\/     rbp + 8 * wordSize - saved used length\n-  \/\/\n-  \/\/ Output:\n-  \/\/   rax       - input length\n-  \/\/\n-  address generate_counterMode_AESCrypt_Parallel() {\n-    assert(UseAES, \"need AES instructions and misaligned SSE support\");\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"counterMode_AESCrypt\");\n-    address start = __ pc();\n-    const Register from = c_rarg0; \/\/ source array address\n-    const Register to = c_rarg1; \/\/ destination array address\n-    const Register key = c_rarg2; \/\/ key array address\n-    const Register counter = c_rarg3; \/\/ counter byte array initialized from counter array address\n-                                      \/\/ and updated with the incremented counter in the end\n-#ifndef _WIN64\n-    const Register len_reg = c_rarg4;\n-    const Register saved_encCounter_start = c_rarg5;\n-    const Register used_addr = r10;\n-    const Address  used_mem(rbp, 2 * wordSize);\n-    const Register used = r11;\n-#else\n-    const Address len_mem(rbp, 6 * wordSize); \/\/ length is on stack on Win64\n-    const Address saved_encCounter_mem(rbp, 7 * wordSize); \/\/ length is on stack on Win64\n-    const Address used_mem(rbp, 8 * wordSize); \/\/ length is on stack on Win64\n-    const Register len_reg = r10; \/\/ pick the first volatile windows register\n-    const Register saved_encCounter_start = r11;\n-    const Register used_addr = r13;\n-    const Register used = r14;\n-#endif\n-    const Register pos = rax;\n-\n-    const int PARALLEL_FACTOR = 6;\n-    const XMMRegister xmm_counter_shuf_mask = xmm0;\n-    const XMMRegister xmm_key_shuf_mask = xmm1; \/\/ used temporarily to swap key bytes up front\n-    const XMMRegister xmm_curr_counter = xmm2;\n-\n-    const XMMRegister xmm_key_tmp0 = xmm3;\n-    const XMMRegister xmm_key_tmp1 = xmm4;\n-\n-    \/\/ registers holding the four results in the parallelized loop\n-    const XMMRegister xmm_result0 = xmm5;\n-    const XMMRegister xmm_result1 = xmm6;\n-    const XMMRegister xmm_result2 = xmm7;\n-    const XMMRegister xmm_result3 = xmm8;\n-    const XMMRegister xmm_result4 = xmm9;\n-    const XMMRegister xmm_result5 = xmm10;\n-\n-    const XMMRegister xmm_from0 = xmm11;\n-    const XMMRegister xmm_from1 = xmm12;\n-    const XMMRegister xmm_from2 = xmm13;\n-    const XMMRegister xmm_from3 = xmm14; \/\/the last one is xmm14. we have to preserve it on WIN64.\n-    const XMMRegister xmm_from4 = xmm3; \/\/reuse xmm3~4. Because xmm_key_tmp0~1 are useless when loading input text\n-    const XMMRegister xmm_from5 = xmm4;\n-\n-    \/\/for key_128, key_192, key_256\n-    const int rounds[3] = {10, 12, 14};\n-    Label L_exit_preLoop, L_preLoop_start;\n-    Label L_multiBlock_loopTop[3];\n-    Label L_singleBlockLoopTop[3];\n-    Label L__incCounter[3][6]; \/\/for 6 blocks\n-    Label L__incCounter_single[3]; \/\/for single block, key128, key192, key256\n-    Label L_processTail_insr[3], L_processTail_4_insr[3], L_processTail_2_insr[3], L_processTail_1_insr[3], L_processTail_exit_insr[3];\n-    Label L_processTail_4_extr[3], L_processTail_2_extr[3], L_processTail_1_extr[3], L_processTail_exit_extr[3];\n-\n-    Label L_exit;\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-#ifdef _WIN64\n-    \/\/ allocate spill slots for r13, r14\n-    enum {\n-        saved_r13_offset,\n-        saved_r14_offset\n-    };\n-    __ subptr(rsp, 2 * wordSize);\n-    __ movptr(Address(rsp, saved_r13_offset * wordSize), r13);\n-    __ movptr(Address(rsp, saved_r14_offset * wordSize), r14);\n-\n-    \/\/ on win64, fill len_reg from stack position\n-    __ movl(len_reg, len_mem);\n-    __ movptr(saved_encCounter_start, saved_encCounter_mem);\n-    __ movptr(used_addr, used_mem);\n-    __ movl(used, Address(used_addr, 0));\n-#else\n-    __ push(len_reg); \/\/ Save\n-    __ movptr(used_addr, used_mem);\n-    __ movl(used, Address(used_addr, 0));\n-#endif\n+address StubGenerator::generate_upper_word_mask() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"upper_word_mask\");\n+  address start = __ pc();\n@@ -4754,75 +1305,2 @@\n-    __ push(rbx); \/\/ Save RBX\n-    __ movdqu(xmm_curr_counter, Address(counter, 0x00)); \/\/ initialize counter with initial counter\n-    __ movdqu(xmm_counter_shuf_mask, ExternalAddress(StubRoutines::x86::counter_shuffle_mask_addr()), pos); \/\/ pos as scratch\n-    __ pshufb(xmm_curr_counter, xmm_counter_shuf_mask); \/\/counter is shuffled\n-    __ movptr(pos, 0);\n-\n-    \/\/ Use the partially used encrpyted counter from last invocation\n-    __ BIND(L_preLoop_start);\n-    __ cmpptr(used, 16);\n-    __ jcc(Assembler::aboveEqual, L_exit_preLoop);\n-      __ cmpptr(len_reg, 0);\n-      __ jcc(Assembler::lessEqual, L_exit_preLoop);\n-      __ movb(rbx, Address(saved_encCounter_start, used));\n-      __ xorb(rbx, Address(from, pos));\n-      __ movb(Address(to, pos), rbx);\n-      __ addptr(pos, 1);\n-      __ addptr(used, 1);\n-      __ subptr(len_reg, 1);\n-\n-    __ jmp(L_preLoop_start);\n-\n-    __ BIND(L_exit_preLoop);\n-    __ movl(Address(used_addr, 0), used);\n-\n-    \/\/ key length could be only {11, 13, 15} * 4 = {44, 52, 60}\n-    __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()), rbx); \/\/ rbx as scratch\n-    __ movl(rbx, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n-    __ cmpl(rbx, 52);\n-    __ jcc(Assembler::equal, L_multiBlock_loopTop[1]);\n-    __ cmpl(rbx, 60);\n-    __ jcc(Assembler::equal, L_multiBlock_loopTop[2]);\n-\n-#define CTR_DoSix(opc, src_reg)                \\\n-    __ opc(xmm_result0, src_reg);              \\\n-    __ opc(xmm_result1, src_reg);              \\\n-    __ opc(xmm_result2, src_reg);              \\\n-    __ opc(xmm_result3, src_reg);              \\\n-    __ opc(xmm_result4, src_reg);              \\\n-    __ opc(xmm_result5, src_reg);\n-\n-    \/\/ k == 0 :  generate code for key_128\n-    \/\/ k == 1 :  generate code for key_192\n-    \/\/ k == 2 :  generate code for key_256\n-    for (int k = 0; k < 3; ++k) {\n-      \/\/multi blocks starts here\n-      __ align(OptoLoopAlignment);\n-      __ BIND(L_multiBlock_loopTop[k]);\n-      __ cmpptr(len_reg, PARALLEL_FACTOR * AESBlockSize); \/\/ see if at least PARALLEL_FACTOR blocks left\n-      __ jcc(Assembler::less, L_singleBlockLoopTop[k]);\n-      load_key(xmm_key_tmp0, key, 0x00, xmm_key_shuf_mask);\n-\n-      \/\/load, then increase counters\n-      CTR_DoSix(movdqa, xmm_curr_counter);\n-      inc_counter(rbx, xmm_result1, 0x01, L__incCounter[k][0]);\n-      inc_counter(rbx, xmm_result2, 0x02, L__incCounter[k][1]);\n-      inc_counter(rbx, xmm_result3, 0x03, L__incCounter[k][2]);\n-      inc_counter(rbx, xmm_result4, 0x04, L__incCounter[k][3]);\n-      inc_counter(rbx, xmm_result5,  0x05, L__incCounter[k][4]);\n-      inc_counter(rbx, xmm_curr_counter, 0x06, L__incCounter[k][5]);\n-      CTR_DoSix(pshufb, xmm_counter_shuf_mask); \/\/ after increased, shuffled counters back for PXOR\n-      CTR_DoSix(pxor, xmm_key_tmp0);   \/\/PXOR with Round 0 key\n-\n-      \/\/load two ROUND_KEYs at a time\n-      for (int i = 1; i < rounds[k]; ) {\n-        load_key(xmm_key_tmp1, key, (0x10 * i), xmm_key_shuf_mask);\n-        load_key(xmm_key_tmp0, key, (0x10 * (i+1)), xmm_key_shuf_mask);\n-        CTR_DoSix(aesenc, xmm_key_tmp1);\n-        i++;\n-        if (i != rounds[k]) {\n-          CTR_DoSix(aesenc, xmm_key_tmp0);\n-        } else {\n-          CTR_DoSix(aesenclast, xmm_key_tmp0);\n-        }\n-        i++;\n-      }\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0xFFFFFFFF00000000, relocInfo::none);\n@@ -4830,105 +1308,2 @@\n-      \/\/ get next PARALLEL_FACTOR blocks into xmm_result registers\n-      __ movdqu(xmm_from0, Address(from, pos, Address::times_1, 0 * AESBlockSize));\n-      __ movdqu(xmm_from1, Address(from, pos, Address::times_1, 1 * AESBlockSize));\n-      __ movdqu(xmm_from2, Address(from, pos, Address::times_1, 2 * AESBlockSize));\n-      __ movdqu(xmm_from3, Address(from, pos, Address::times_1, 3 * AESBlockSize));\n-      __ movdqu(xmm_from4, Address(from, pos, Address::times_1, 4 * AESBlockSize));\n-      __ movdqu(xmm_from5, Address(from, pos, Address::times_1, 5 * AESBlockSize));\n-\n-      __ pxor(xmm_result0, xmm_from0);\n-      __ pxor(xmm_result1, xmm_from1);\n-      __ pxor(xmm_result2, xmm_from2);\n-      __ pxor(xmm_result3, xmm_from3);\n-      __ pxor(xmm_result4, xmm_from4);\n-      __ pxor(xmm_result5, xmm_from5);\n-\n-      \/\/ store 6 results into the next 64 bytes of output\n-      __ movdqu(Address(to, pos, Address::times_1, 0 * AESBlockSize), xmm_result0);\n-      __ movdqu(Address(to, pos, Address::times_1, 1 * AESBlockSize), xmm_result1);\n-      __ movdqu(Address(to, pos, Address::times_1, 2 * AESBlockSize), xmm_result2);\n-      __ movdqu(Address(to, pos, Address::times_1, 3 * AESBlockSize), xmm_result3);\n-      __ movdqu(Address(to, pos, Address::times_1, 4 * AESBlockSize), xmm_result4);\n-      __ movdqu(Address(to, pos, Address::times_1, 5 * AESBlockSize), xmm_result5);\n-\n-      __ addptr(pos, PARALLEL_FACTOR * AESBlockSize); \/\/ increase the length of crypt text\n-      __ subptr(len_reg, PARALLEL_FACTOR * AESBlockSize); \/\/ decrease the remaining length\n-      __ jmp(L_multiBlock_loopTop[k]);\n-\n-      \/\/ singleBlock starts here\n-      __ align(OptoLoopAlignment);\n-      __ BIND(L_singleBlockLoopTop[k]);\n-      __ cmpptr(len_reg, 0);\n-      __ jcc(Assembler::lessEqual, L_exit);\n-      load_key(xmm_key_tmp0, key, 0x00, xmm_key_shuf_mask);\n-      __ movdqa(xmm_result0, xmm_curr_counter);\n-      inc_counter(rbx, xmm_curr_counter, 0x01, L__incCounter_single[k]);\n-      __ pshufb(xmm_result0, xmm_counter_shuf_mask);\n-      __ pxor(xmm_result0, xmm_key_tmp0);\n-      for (int i = 1; i < rounds[k]; i++) {\n-        load_key(xmm_key_tmp0, key, (0x10 * i), xmm_key_shuf_mask);\n-        __ aesenc(xmm_result0, xmm_key_tmp0);\n-      }\n-      load_key(xmm_key_tmp0, key, (rounds[k] * 0x10), xmm_key_shuf_mask);\n-      __ aesenclast(xmm_result0, xmm_key_tmp0);\n-      __ cmpptr(len_reg, AESBlockSize);\n-      __ jcc(Assembler::less, L_processTail_insr[k]);\n-        __ movdqu(xmm_from0, Address(from, pos, Address::times_1, 0 * AESBlockSize));\n-        __ pxor(xmm_result0, xmm_from0);\n-        __ movdqu(Address(to, pos, Address::times_1, 0 * AESBlockSize), xmm_result0);\n-        __ addptr(pos, AESBlockSize);\n-        __ subptr(len_reg, AESBlockSize);\n-        __ jmp(L_singleBlockLoopTop[k]);\n-      __ BIND(L_processTail_insr[k]);                               \/\/ Process the tail part of the input array\n-        __ addptr(pos, len_reg);                                    \/\/ 1. Insert bytes from src array into xmm_from0 register\n-        __ testptr(len_reg, 8);\n-        __ jcc(Assembler::zero, L_processTail_4_insr[k]);\n-          __ subptr(pos,8);\n-          __ pinsrq(xmm_from0, Address(from, pos), 0);\n-        __ BIND(L_processTail_4_insr[k]);\n-        __ testptr(len_reg, 4);\n-        __ jcc(Assembler::zero, L_processTail_2_insr[k]);\n-          __ subptr(pos,4);\n-          __ pslldq(xmm_from0, 4);\n-          __ pinsrd(xmm_from0, Address(from, pos), 0);\n-        __ BIND(L_processTail_2_insr[k]);\n-        __ testptr(len_reg, 2);\n-        __ jcc(Assembler::zero, L_processTail_1_insr[k]);\n-          __ subptr(pos, 2);\n-          __ pslldq(xmm_from0, 2);\n-          __ pinsrw(xmm_from0, Address(from, pos), 0);\n-        __ BIND(L_processTail_1_insr[k]);\n-        __ testptr(len_reg, 1);\n-        __ jcc(Assembler::zero, L_processTail_exit_insr[k]);\n-          __ subptr(pos, 1);\n-          __ pslldq(xmm_from0, 1);\n-          __ pinsrb(xmm_from0, Address(from, pos), 0);\n-        __ BIND(L_processTail_exit_insr[k]);\n-\n-        __ movdqu(Address(saved_encCounter_start, 0), xmm_result0);  \/\/ 2. Perform pxor of the encrypted counter and plaintext Bytes.\n-        __ pxor(xmm_result0, xmm_from0);                             \/\/    Also the encrypted counter is saved for next invocation.\n-\n-        __ testptr(len_reg, 8);\n-        __ jcc(Assembler::zero, L_processTail_4_extr[k]);            \/\/ 3. Extract bytes from xmm_result0 into the dest. array\n-          __ pextrq(Address(to, pos), xmm_result0, 0);\n-          __ psrldq(xmm_result0, 8);\n-          __ addptr(pos, 8);\n-        __ BIND(L_processTail_4_extr[k]);\n-        __ testptr(len_reg, 4);\n-        __ jcc(Assembler::zero, L_processTail_2_extr[k]);\n-          __ pextrd(Address(to, pos), xmm_result0, 0);\n-          __ psrldq(xmm_result0, 4);\n-          __ addptr(pos, 4);\n-        __ BIND(L_processTail_2_extr[k]);\n-        __ testptr(len_reg, 2);\n-        __ jcc(Assembler::zero, L_processTail_1_extr[k]);\n-          __ pextrw(Address(to, pos), xmm_result0, 0);\n-          __ psrldq(xmm_result0, 2);\n-          __ addptr(pos, 2);\n-        __ BIND(L_processTail_1_extr[k]);\n-        __ testptr(len_reg, 1);\n-        __ jcc(Assembler::zero, L_processTail_exit_extr[k]);\n-          __ pextrb(Address(to, pos), xmm_result0, 0);\n-\n-        __ BIND(L_processTail_exit_extr[k]);\n-        __ movl(Address(used_addr, 0), len_reg);\n-        __ jmp(L_exit);\n+  return start;\n+}\n@@ -4936,1 +1311,4 @@\n-    }\n+address StubGenerator::generate_shuffle_byte_flip_mask() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"shuffle_byte_flip_mask\");\n+  address start = __ pc();\n@@ -4938,16 +1316,2 @@\n-    __ BIND(L_exit);\n-    __ pshufb(xmm_curr_counter, xmm_counter_shuf_mask); \/\/counter is shuffled back.\n-    __ movdqu(Address(counter, 0), xmm_curr_counter); \/\/save counter back\n-    __ pop(rbx); \/\/ pop the saved RBX.\n-#ifdef _WIN64\n-    __ movl(rax, len_mem);\n-    __ movptr(r13, Address(rsp, saved_r13_offset * wordSize));\n-    __ movptr(r14, Address(rsp, saved_r14_offset * wordSize));\n-    __ addptr(rsp, 2 * wordSize);\n-#else\n-    __ pop(rax); \/\/ return 'len'\n-#endif\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-    return start;\n-  }\n+  __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\n+  __ emit_data64(0x0001020304050607, relocInfo::none);\n@@ -4955,9 +1319,1 @@\n-void roundDec(XMMRegister xmm_reg) {\n-  __ vaesdec(xmm1, xmm1, xmm_reg, Assembler::AVX_512bit);\n-  __ vaesdec(xmm2, xmm2, xmm_reg, Assembler::AVX_512bit);\n-  __ vaesdec(xmm3, xmm3, xmm_reg, Assembler::AVX_512bit);\n-  __ vaesdec(xmm4, xmm4, xmm_reg, Assembler::AVX_512bit);\n-  __ vaesdec(xmm5, xmm5, xmm_reg, Assembler::AVX_512bit);\n-  __ vaesdec(xmm6, xmm6, xmm_reg, Assembler::AVX_512bit);\n-  __ vaesdec(xmm7, xmm7, xmm_reg, Assembler::AVX_512bit);\n-  __ vaesdec(xmm8, xmm8, xmm_reg, Assembler::AVX_512bit);\n+  return start;\n@@ -4966,10 +1322,6 @@\n-void roundDeclast(XMMRegister xmm_reg) {\n-  __ vaesdeclast(xmm1, xmm1, xmm_reg, Assembler::AVX_512bit);\n-  __ vaesdeclast(xmm2, xmm2, xmm_reg, Assembler::AVX_512bit);\n-  __ vaesdeclast(xmm3, xmm3, xmm_reg, Assembler::AVX_512bit);\n-  __ vaesdeclast(xmm4, xmm4, xmm_reg, Assembler::AVX_512bit);\n-  __ vaesdeclast(xmm5, xmm5, xmm_reg, Assembler::AVX_512bit);\n-  __ vaesdeclast(xmm6, xmm6, xmm_reg, Assembler::AVX_512bit);\n-  __ vaesdeclast(xmm7, xmm7, xmm_reg, Assembler::AVX_512bit);\n-  __ vaesdeclast(xmm8, xmm8, xmm_reg, Assembler::AVX_512bit);\n-}\n+\/\/ ofs and limit are use for multi-block byte array.\n+\/\/ int com.sun.security.provider.DigestBase.implCompressMultiBlock(byte[] b, int ofs, int limit)\n+address StubGenerator::generate_sha1_implCompress(bool multi_block, const char *name) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n@@ -4977,8 +1329,4 @@\n-  void ev_load_key(XMMRegister xmmdst, Register key, int offset, XMMRegister xmm_shuf_mask = NULL) {\n-    __ movdqu(xmmdst, Address(key, offset));\n-    if (xmm_shuf_mask != NULL) {\n-      __ pshufb(xmmdst, xmm_shuf_mask);\n-    } else {\n-      __ pshufb(xmmdst, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n-    }\n-    __ evshufi64x2(xmmdst, xmmdst, xmmdst, 0x0, Assembler::AVX_512bit);\n+  Register buf = c_rarg0;\n+  Register state = c_rarg1;\n+  Register ofs = c_rarg2;\n+  Register limit = c_rarg3;\n@@ -4986,1 +1334,4 @@\n-  }\n+  const XMMRegister abcd = xmm0;\n+  const XMMRegister e0 = xmm1;\n+  const XMMRegister e1 = xmm2;\n+  const XMMRegister msg0 = xmm3;\n@@ -4988,17 +1339,4 @@\n-address generate_cipherBlockChaining_decryptVectorAESCrypt() {\n-    assert(VM_Version::supports_avx512_vaes(), \"need AES instructions and misaligned SSE support\");\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"cipherBlockChaining_decryptAESCrypt\");\n-    address start = __ pc();\n-\n-    const Register from = c_rarg0;  \/\/ source array address\n-    const Register to = c_rarg1;  \/\/ destination array address\n-    const Register key = c_rarg2;  \/\/ key array address\n-    const Register rvec = c_rarg3;  \/\/ r byte array initialized from initvector array address\n-    \/\/ and left with the results of the last encryption block\n-#ifndef _WIN64\n-    const Register len_reg = c_rarg4;  \/\/ src len (must be multiple of blocksize 16)\n-#else\n-    const Address  len_mem(rbp, 6 * wordSize);  \/\/ length is on stack on Win64\n-    const Register len_reg = r11;      \/\/ pick the volatile windows register\n-#endif\n+  const XMMRegister msg1 = xmm4;\n+  const XMMRegister msg2 = xmm5;\n+  const XMMRegister msg3 = xmm6;\n+  const XMMRegister shuf_mask = xmm7;\n@@ -5006,2 +1344,1 @@\n-    Label Loop, Loop1, L_128, L_256, L_192, KEY_192, KEY_256, Loop2, Lcbc_dec_rem_loop,\n-          Lcbc_dec_rem_last, Lcbc_dec_ret, Lcbc_dec_rem, Lcbc_exit;\n+  __ enter();\n@@ -5009,1 +1346,1 @@\n-    __ enter();\n+  __ subptr(rsp, 4 * wordSize);\n@@ -5011,8 +1348,2 @@\n-#ifdef _WIN64\n-  \/\/ on win64, fill len_reg from stack position\n-    __ movl(len_reg, len_mem);\n-#else\n-    __ push(len_reg); \/\/ Save\n-#endif\n-    __ push(rbx);\n-    __ vzeroupper();\n+  __ fast_sha1(abcd, e0, e1, msg0, msg1, msg2, msg3, shuf_mask,\n+    buf, state, ofs, limit, rsp, multi_block);\n@@ -5020,241 +1351,1 @@\n-    \/\/ Temporary variable declaration for swapping key bytes\n-    const XMMRegister xmm_key_shuf_mask = xmm1;\n-    __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n-\n-    \/\/ Calculate number of rounds from key size: 44 for 10-rounds, 52 for 12-rounds, 60 for 14-rounds\n-    const Register rounds = rbx;\n-    __ movl(rounds, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n-\n-    const XMMRegister IV = xmm0;\n-    \/\/ Load IV and broadcast value to 512-bits\n-    __ evbroadcasti64x2(IV, Address(rvec, 0), Assembler::AVX_512bit);\n-\n-    \/\/ Temporary variables for storing round keys\n-    const XMMRegister RK0 = xmm30;\n-    const XMMRegister RK1 = xmm9;\n-    const XMMRegister RK2 = xmm18;\n-    const XMMRegister RK3 = xmm19;\n-    const XMMRegister RK4 = xmm20;\n-    const XMMRegister RK5 = xmm21;\n-    const XMMRegister RK6 = xmm22;\n-    const XMMRegister RK7 = xmm23;\n-    const XMMRegister RK8 = xmm24;\n-    const XMMRegister RK9 = xmm25;\n-    const XMMRegister RK10 = xmm26;\n-\n-     \/\/ Load and shuffle key\n-    \/\/ the java expanded key ordering is rotated one position from what we want\n-    \/\/ so we start from 1*16 here and hit 0*16 last\n-    ev_load_key(RK1, key, 1 * 16, xmm_key_shuf_mask);\n-    ev_load_key(RK2, key, 2 * 16, xmm_key_shuf_mask);\n-    ev_load_key(RK3, key, 3 * 16, xmm_key_shuf_mask);\n-    ev_load_key(RK4, key, 4 * 16, xmm_key_shuf_mask);\n-    ev_load_key(RK5, key, 5 * 16, xmm_key_shuf_mask);\n-    ev_load_key(RK6, key, 6 * 16, xmm_key_shuf_mask);\n-    ev_load_key(RK7, key, 7 * 16, xmm_key_shuf_mask);\n-    ev_load_key(RK8, key, 8 * 16, xmm_key_shuf_mask);\n-    ev_load_key(RK9, key, 9 * 16, xmm_key_shuf_mask);\n-    ev_load_key(RK10, key, 10 * 16, xmm_key_shuf_mask);\n-    ev_load_key(RK0, key, 0*16, xmm_key_shuf_mask);\n-\n-    \/\/ Variables for storing source cipher text\n-    const XMMRegister S0 = xmm10;\n-    const XMMRegister S1 = xmm11;\n-    const XMMRegister S2 = xmm12;\n-    const XMMRegister S3 = xmm13;\n-    const XMMRegister S4 = xmm14;\n-    const XMMRegister S5 = xmm15;\n-    const XMMRegister S6 = xmm16;\n-    const XMMRegister S7 = xmm17;\n-\n-    \/\/ Variables for storing decrypted text\n-    const XMMRegister B0 = xmm1;\n-    const XMMRegister B1 = xmm2;\n-    const XMMRegister B2 = xmm3;\n-    const XMMRegister B3 = xmm4;\n-    const XMMRegister B4 = xmm5;\n-    const XMMRegister B5 = xmm6;\n-    const XMMRegister B6 = xmm7;\n-    const XMMRegister B7 = xmm8;\n-\n-    __ cmpl(rounds, 44);\n-    __ jcc(Assembler::greater, KEY_192);\n-    __ jmp(Loop);\n-\n-    __ BIND(KEY_192);\n-    const XMMRegister RK11 = xmm27;\n-    const XMMRegister RK12 = xmm28;\n-    ev_load_key(RK11, key, 11*16, xmm_key_shuf_mask);\n-    ev_load_key(RK12, key, 12*16, xmm_key_shuf_mask);\n-\n-    __ cmpl(rounds, 52);\n-    __ jcc(Assembler::greater, KEY_256);\n-    __ jmp(Loop);\n-\n-    __ BIND(KEY_256);\n-    const XMMRegister RK13 = xmm29;\n-    const XMMRegister RK14 = xmm31;\n-    ev_load_key(RK13, key, 13*16, xmm_key_shuf_mask);\n-    ev_load_key(RK14, key, 14*16, xmm_key_shuf_mask);\n-\n-    __ BIND(Loop);\n-    __ cmpl(len_reg, 512);\n-    __ jcc(Assembler::below, Lcbc_dec_rem);\n-    __ BIND(Loop1);\n-    __ subl(len_reg, 512);\n-    __ evmovdquq(S0, Address(from, 0 * 64), Assembler::AVX_512bit);\n-    __ evmovdquq(S1, Address(from, 1 * 64), Assembler::AVX_512bit);\n-    __ evmovdquq(S2, Address(from, 2 * 64), Assembler::AVX_512bit);\n-    __ evmovdquq(S3, Address(from, 3 * 64), Assembler::AVX_512bit);\n-    __ evmovdquq(S4, Address(from, 4 * 64), Assembler::AVX_512bit);\n-    __ evmovdquq(S5, Address(from, 5 * 64), Assembler::AVX_512bit);\n-    __ evmovdquq(S6, Address(from, 6 * 64), Assembler::AVX_512bit);\n-    __ evmovdquq(S7, Address(from, 7 * 64), Assembler::AVX_512bit);\n-    __ leaq(from, Address(from, 8 * 64));\n-\n-    __ evpxorq(B0, S0, RK1, Assembler::AVX_512bit);\n-    __ evpxorq(B1, S1, RK1, Assembler::AVX_512bit);\n-    __ evpxorq(B2, S2, RK1, Assembler::AVX_512bit);\n-    __ evpxorq(B3, S3, RK1, Assembler::AVX_512bit);\n-    __ evpxorq(B4, S4, RK1, Assembler::AVX_512bit);\n-    __ evpxorq(B5, S5, RK1, Assembler::AVX_512bit);\n-    __ evpxorq(B6, S6, RK1, Assembler::AVX_512bit);\n-    __ evpxorq(B7, S7, RK1, Assembler::AVX_512bit);\n-\n-    __ evalignq(IV, S0, IV, 0x06);\n-    __ evalignq(S0, S1, S0, 0x06);\n-    __ evalignq(S1, S2, S1, 0x06);\n-    __ evalignq(S2, S3, S2, 0x06);\n-    __ evalignq(S3, S4, S3, 0x06);\n-    __ evalignq(S4, S5, S4, 0x06);\n-    __ evalignq(S5, S6, S5, 0x06);\n-    __ evalignq(S6, S7, S6, 0x06);\n-\n-    roundDec(RK2);\n-    roundDec(RK3);\n-    roundDec(RK4);\n-    roundDec(RK5);\n-    roundDec(RK6);\n-    roundDec(RK7);\n-    roundDec(RK8);\n-    roundDec(RK9);\n-    roundDec(RK10);\n-\n-    __ cmpl(rounds, 44);\n-    __ jcc(Assembler::belowEqual, L_128);\n-    roundDec(RK11);\n-    roundDec(RK12);\n-\n-    __ cmpl(rounds, 52);\n-    __ jcc(Assembler::belowEqual, L_192);\n-    roundDec(RK13);\n-    roundDec(RK14);\n-\n-    __ BIND(L_256);\n-    roundDeclast(RK0);\n-    __ jmp(Loop2);\n-\n-    __ BIND(L_128);\n-    roundDeclast(RK0);\n-    __ jmp(Loop2);\n-\n-    __ BIND(L_192);\n-    roundDeclast(RK0);\n-\n-    __ BIND(Loop2);\n-    __ evpxorq(B0, B0, IV, Assembler::AVX_512bit);\n-    __ evpxorq(B1, B1, S0, Assembler::AVX_512bit);\n-    __ evpxorq(B2, B2, S1, Assembler::AVX_512bit);\n-    __ evpxorq(B3, B3, S2, Assembler::AVX_512bit);\n-    __ evpxorq(B4, B4, S3, Assembler::AVX_512bit);\n-    __ evpxorq(B5, B5, S4, Assembler::AVX_512bit);\n-    __ evpxorq(B6, B6, S5, Assembler::AVX_512bit);\n-    __ evpxorq(B7, B7, S6, Assembler::AVX_512bit);\n-    __ evmovdquq(IV, S7, Assembler::AVX_512bit);\n-\n-    __ evmovdquq(Address(to, 0 * 64), B0, Assembler::AVX_512bit);\n-    __ evmovdquq(Address(to, 1 * 64), B1, Assembler::AVX_512bit);\n-    __ evmovdquq(Address(to, 2 * 64), B2, Assembler::AVX_512bit);\n-    __ evmovdquq(Address(to, 3 * 64), B3, Assembler::AVX_512bit);\n-    __ evmovdquq(Address(to, 4 * 64), B4, Assembler::AVX_512bit);\n-    __ evmovdquq(Address(to, 5 * 64), B5, Assembler::AVX_512bit);\n-    __ evmovdquq(Address(to, 6 * 64), B6, Assembler::AVX_512bit);\n-    __ evmovdquq(Address(to, 7 * 64), B7, Assembler::AVX_512bit);\n-    __ leaq(to, Address(to, 8 * 64));\n-    __ jmp(Loop);\n-\n-    __ BIND(Lcbc_dec_rem);\n-    __ evshufi64x2(IV, IV, IV, 0x03, Assembler::AVX_512bit);\n-\n-    __ BIND(Lcbc_dec_rem_loop);\n-    __ subl(len_reg, 16);\n-    __ jcc(Assembler::carrySet, Lcbc_dec_ret);\n-\n-    __ movdqu(S0, Address(from, 0));\n-    __ evpxorq(B0, S0, RK1, Assembler::AVX_512bit);\n-    __ vaesdec(B0, B0, RK2, Assembler::AVX_512bit);\n-    __ vaesdec(B0, B0, RK3, Assembler::AVX_512bit);\n-    __ vaesdec(B0, B0, RK4, Assembler::AVX_512bit);\n-    __ vaesdec(B0, B0, RK5, Assembler::AVX_512bit);\n-    __ vaesdec(B0, B0, RK6, Assembler::AVX_512bit);\n-    __ vaesdec(B0, B0, RK7, Assembler::AVX_512bit);\n-    __ vaesdec(B0, B0, RK8, Assembler::AVX_512bit);\n-    __ vaesdec(B0, B0, RK9, Assembler::AVX_512bit);\n-    __ vaesdec(B0, B0, RK10, Assembler::AVX_512bit);\n-    __ cmpl(rounds, 44);\n-    __ jcc(Assembler::belowEqual, Lcbc_dec_rem_last);\n-\n-    __ vaesdec(B0, B0, RK11, Assembler::AVX_512bit);\n-    __ vaesdec(B0, B0, RK12, Assembler::AVX_512bit);\n-    __ cmpl(rounds, 52);\n-    __ jcc(Assembler::belowEqual, Lcbc_dec_rem_last);\n-\n-    __ vaesdec(B0, B0, RK13, Assembler::AVX_512bit);\n-    __ vaesdec(B0, B0, RK14, Assembler::AVX_512bit);\n-\n-    __ BIND(Lcbc_dec_rem_last);\n-    __ vaesdeclast(B0, B0, RK0, Assembler::AVX_512bit);\n-\n-    __ evpxorq(B0, B0, IV, Assembler::AVX_512bit);\n-    __ evmovdquq(IV, S0, Assembler::AVX_512bit);\n-    __ movdqu(Address(to, 0), B0);\n-    __ leaq(from, Address(from, 16));\n-    __ leaq(to, Address(to, 16));\n-    __ jmp(Lcbc_dec_rem_loop);\n-\n-    __ BIND(Lcbc_dec_ret);\n-    __ movdqu(Address(rvec, 0), IV);\n-\n-    \/\/ Zero out the round keys\n-    __ evpxorq(RK0, RK0, RK0, Assembler::AVX_512bit);\n-    __ evpxorq(RK1, RK1, RK1, Assembler::AVX_512bit);\n-    __ evpxorq(RK2, RK2, RK2, Assembler::AVX_512bit);\n-    __ evpxorq(RK3, RK3, RK3, Assembler::AVX_512bit);\n-    __ evpxorq(RK4, RK4, RK4, Assembler::AVX_512bit);\n-    __ evpxorq(RK5, RK5, RK5, Assembler::AVX_512bit);\n-    __ evpxorq(RK6, RK6, RK6, Assembler::AVX_512bit);\n-    __ evpxorq(RK7, RK7, RK7, Assembler::AVX_512bit);\n-    __ evpxorq(RK8, RK8, RK8, Assembler::AVX_512bit);\n-    __ evpxorq(RK9, RK9, RK9, Assembler::AVX_512bit);\n-    __ evpxorq(RK10, RK10, RK10, Assembler::AVX_512bit);\n-    __ cmpl(rounds, 44);\n-    __ jcc(Assembler::belowEqual, Lcbc_exit);\n-    __ evpxorq(RK11, RK11, RK11, Assembler::AVX_512bit);\n-    __ evpxorq(RK12, RK12, RK12, Assembler::AVX_512bit);\n-    __ cmpl(rounds, 52);\n-    __ jcc(Assembler::belowEqual, Lcbc_exit);\n-    __ evpxorq(RK13, RK13, RK13, Assembler::AVX_512bit);\n-    __ evpxorq(RK14, RK14, RK14, Assembler::AVX_512bit);\n-\n-    __ BIND(Lcbc_exit);\n-    __ vzeroupper();\n-    __ pop(rbx);\n-#ifdef _WIN64\n-    __ movl(rax, len_mem);\n-#else\n-    __ pop(rax); \/\/ return length\n-#endif\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-    return start;\n-}\n+  __ addptr(rsp, 4 * wordSize);\n@@ -5262,9 +1353,2 @@\n-\/\/ Polynomial x^128+x^127+x^126+x^121+1\n-address ghash_polynomial_addr() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"_ghash_poly_addr\");\n-    address start = __ pc();\n-    __ emit_data64(0x0000000000000001, relocInfo::none);\n-    __ emit_data64(0xc200000000000000, relocInfo::none);\n-    return start;\n-}\n+  __ leave();\n+  __ ret(0);\n@@ -5272,7 +1356,1 @@\n-address ghash_shufflemask_addr() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"_ghash_shuffmask_addr\");\n-    address start = __ pc();\n-    __ emit_data64(0x0f0f0f0f0f0f0f0f, relocInfo::none);\n-    __ emit_data64(0x0f0f0f0f0f0f0f0f, relocInfo::none);\n-    return start;\n+  return start;\n@@ -5281,19 +1359,4 @@\n-\/\/ Ghash single and multi block operations using AVX instructions\n-address generate_avx_ghash_processBlocks() {\n-    __ align(CodeEntryAlignment);\n-\n-    StubCodeMark mark(this, \"StubRoutines\", \"ghash_processBlocks\");\n-    address start = __ pc();\n-\n-    \/\/ arguments\n-    const Register state = c_rarg0;\n-    const Register htbl = c_rarg1;\n-    const Register data = c_rarg2;\n-    const Register blocks = c_rarg3;\n-    __ enter();\n-   \/\/ Save state before entering routine\n-    __ avx_ghash(state, htbl, data, blocks);\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-    return start;\n-}\n+address StubGenerator::generate_pshuffle_byte_flip_mask() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"pshuffle_byte_flip_mask\");\n+  address start = __ pc();\n@@ -5301,9 +1364,2 @@\n-  \/\/ byte swap x86 long\n-  address generate_ghash_long_swap_mask() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"ghash_long_swap_mask\");\n-    address start = __ pc();\n-    __ emit_data64(0x0f0e0d0c0b0a0908, relocInfo::none );\n-    __ emit_data64(0x0706050403020100, relocInfo::none );\n-  return start;\n-  }\n+  __ emit_data64(0x0405060700010203, relocInfo::none);\n+  __ emit_data64(0x0c0d0e0f08090a0b, relocInfo::none);\n@@ -5311,8 +1367,13 @@\n-  \/\/ byte swap x86 byte array\n-  address generate_ghash_byte_swap_mask() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"ghash_byte_swap_mask\");\n-    address start = __ pc();\n-    __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none );\n-    __ emit_data64(0x0001020304050607, relocInfo::none );\n-  return start;\n+  if (VM_Version::supports_avx2()) {\n+    __ emit_data64(0x0405060700010203, relocInfo::none); \/\/ second copy\n+    __ emit_data64(0x0c0d0e0f08090a0b, relocInfo::none);\n+    \/\/ _SHUF_00BA\n+    __ emit_data64(0x0b0a090803020100, relocInfo::none);\n+    __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);\n+    __ emit_data64(0x0b0a090803020100, relocInfo::none);\n+    __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);\n+    \/\/ _SHUF_DC00\n+    __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);\n+    __ emit_data64(0x0b0a090803020100, relocInfo::none);\n+    __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);\n+    __ emit_data64(0x0b0a090803020100, relocInfo::none);\n@@ -5321,30 +1382,2 @@\n-  \/* Single and multi-block ghash operations *\/\n-  address generate_ghash_processBlocks() {\n-    __ align(CodeEntryAlignment);\n-    Label L_ghash_loop, L_exit;\n-    StubCodeMark mark(this, \"StubRoutines\", \"ghash_processBlocks\");\n-    address start = __ pc();\n-\n-    const Register state        = c_rarg0;\n-    const Register subkeyH      = c_rarg1;\n-    const Register data         = c_rarg2;\n-    const Register blocks       = c_rarg3;\n-\n-    const XMMRegister xmm_temp0 = xmm0;\n-    const XMMRegister xmm_temp1 = xmm1;\n-    const XMMRegister xmm_temp2 = xmm2;\n-    const XMMRegister xmm_temp3 = xmm3;\n-    const XMMRegister xmm_temp4 = xmm4;\n-    const XMMRegister xmm_temp5 = xmm5;\n-    const XMMRegister xmm_temp6 = xmm6;\n-    const XMMRegister xmm_temp7 = xmm7;\n-    const XMMRegister xmm_temp8 = xmm8;\n-    const XMMRegister xmm_temp9 = xmm9;\n-    const XMMRegister xmm_temp10 = xmm10;\n-\n-    __ enter();\n-\n-    __ movdqu(xmm_temp10, ExternalAddress(StubRoutines::x86::ghash_long_swap_mask_addr()));\n-\n-    __ movdqu(xmm_temp0, Address(state, 0));\n-    __ pshufb(xmm_temp0, xmm_temp10);\n+  return start;\n+}\n@@ -5352,0 +1385,5 @@\n+\/\/Mask for byte-swapping a couple of qwords in an XMM register using (v)pshufb.\n+address StubGenerator::generate_pshuffle_byte_flip_mask_sha512() {\n+  __ align32();\n+  StubCodeMark mark(this, \"StubRoutines\", \"pshuffle_byte_flip_mask_sha512\");\n+  address start = __ pc();\n@@ -5353,3 +1391,10 @@\n-    __ BIND(L_ghash_loop);\n-    __ movdqu(xmm_temp2, Address(data, 0));\n-    __ pshufb(xmm_temp2, ExternalAddress(StubRoutines::x86::ghash_byte_swap_mask_addr()));\n+  if (VM_Version::supports_avx2()) {\n+    __ emit_data64(0x0001020304050607, relocInfo::none); \/\/ PSHUFFLE_BYTE_FLIP_MASK\n+    __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\n+    __ emit_data64(0x1011121314151617, relocInfo::none);\n+    __ emit_data64(0x18191a1b1c1d1e1f, relocInfo::none);\n+    __ emit_data64(0x0000000000000000, relocInfo::none); \/\/MASK_YMM_LO\n+    __ emit_data64(0x0000000000000000, relocInfo::none);\n+    __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);\n+    __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);\n+  }\n@@ -5357,2 +1402,2 @@\n-    __ movdqu(xmm_temp1, Address(subkeyH, 0));\n-    __ pshufb(xmm_temp1, xmm_temp10);\n+  return start;\n+}\n@@ -5360,1 +1405,7 @@\n-    __ pxor(xmm_temp0, xmm_temp2);\n+\/\/ ofs and limit are use for multi-block byte array.\n+\/\/ int com.sun.security.provider.DigestBase.implCompressMultiBlock(byte[] b, int ofs, int limit)\n+address StubGenerator::generate_sha256_implCompress(bool multi_block, const char *name) {\n+  assert(VM_Version::supports_sha() || VM_Version::supports_avx2(), \"\");\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n@@ -5362,38 +1413,4 @@\n-    \/\/\n-    \/\/ Multiply with the hash key\n-    \/\/\n-    __ movdqu(xmm_temp3, xmm_temp0);\n-    __ pclmulqdq(xmm_temp3, xmm_temp1, 0);      \/\/ xmm3 holds a0*b0\n-    __ movdqu(xmm_temp4, xmm_temp0);\n-    __ pclmulqdq(xmm_temp4, xmm_temp1, 16);     \/\/ xmm4 holds a0*b1\n-\n-    __ movdqu(xmm_temp5, xmm_temp0);\n-    __ pclmulqdq(xmm_temp5, xmm_temp1, 1);      \/\/ xmm5 holds a1*b0\n-    __ movdqu(xmm_temp6, xmm_temp0);\n-    __ pclmulqdq(xmm_temp6, xmm_temp1, 17);     \/\/ xmm6 holds a1*b1\n-\n-    __ pxor(xmm_temp4, xmm_temp5);      \/\/ xmm4 holds a0*b1 + a1*b0\n-\n-    __ movdqu(xmm_temp5, xmm_temp4);    \/\/ move the contents of xmm4 to xmm5\n-    __ psrldq(xmm_temp4, 8);    \/\/ shift by xmm4 64 bits to the right\n-    __ pslldq(xmm_temp5, 8);    \/\/ shift by xmm5 64 bits to the left\n-    __ pxor(xmm_temp3, xmm_temp5);\n-    __ pxor(xmm_temp6, xmm_temp4);      \/\/ Register pair <xmm6:xmm3> holds the result\n-                                        \/\/ of the carry-less multiplication of\n-                                        \/\/ xmm0 by xmm1.\n-\n-    \/\/ We shift the result of the multiplication by one bit position\n-    \/\/ to the left to cope for the fact that the bits are reversed.\n-    __ movdqu(xmm_temp7, xmm_temp3);\n-    __ movdqu(xmm_temp8, xmm_temp6);\n-    __ pslld(xmm_temp3, 1);\n-    __ pslld(xmm_temp6, 1);\n-    __ psrld(xmm_temp7, 31);\n-    __ psrld(xmm_temp8, 31);\n-    __ movdqu(xmm_temp9, xmm_temp7);\n-    __ pslldq(xmm_temp8, 4);\n-    __ pslldq(xmm_temp7, 4);\n-    __ psrldq(xmm_temp9, 12);\n-    __ por(xmm_temp3, xmm_temp7);\n-    __ por(xmm_temp6, xmm_temp8);\n-    __ por(xmm_temp6, xmm_temp9);\n+  Register buf = c_rarg0;\n+  Register state = c_rarg1;\n+  Register ofs = c_rarg2;\n+  Register limit = c_rarg3;\n@@ -5401,17 +1418,4 @@\n-    \/\/\n-    \/\/ First phase of the reduction\n-    \/\/\n-    \/\/ Move xmm3 into xmm7, xmm8, xmm9 in order to perform the shifts\n-    \/\/ independently.\n-    __ movdqu(xmm_temp7, xmm_temp3);\n-    __ movdqu(xmm_temp8, xmm_temp3);\n-    __ movdqu(xmm_temp9, xmm_temp3);\n-    __ pslld(xmm_temp7, 31);    \/\/ packed right shift shifting << 31\n-    __ pslld(xmm_temp8, 30);    \/\/ packed right shift shifting << 30\n-    __ pslld(xmm_temp9, 25);    \/\/ packed right shift shifting << 25\n-    __ pxor(xmm_temp7, xmm_temp8);      \/\/ xor the shifted versions\n-    __ pxor(xmm_temp7, xmm_temp9);\n-    __ movdqu(xmm_temp8, xmm_temp7);\n-    __ pslldq(xmm_temp7, 12);\n-    __ psrldq(xmm_temp8, 4);\n-    __ pxor(xmm_temp3, xmm_temp7);      \/\/ first phase of the reduction complete\n+  const XMMRegister msg = xmm0;\n+  const XMMRegister state0 = xmm1;\n+  const XMMRegister state1 = xmm2;\n+  const XMMRegister msgtmp0 = xmm3;\n@@ -5419,22 +1423,4 @@\n-    \/\/\n-    \/\/ Second phase of the reduction\n-    \/\/\n-    \/\/ Make 3 copies of xmm3 in xmm2, xmm4, xmm5 for doing these\n-    \/\/ shift operations.\n-    __ movdqu(xmm_temp2, xmm_temp3);\n-    __ movdqu(xmm_temp4, xmm_temp3);\n-    __ movdqu(xmm_temp5, xmm_temp3);\n-    __ psrld(xmm_temp2, 1);     \/\/ packed left shifting >> 1\n-    __ psrld(xmm_temp4, 2);     \/\/ packed left shifting >> 2\n-    __ psrld(xmm_temp5, 7);     \/\/ packed left shifting >> 7\n-    __ pxor(xmm_temp2, xmm_temp4);      \/\/ xor the shifted versions\n-    __ pxor(xmm_temp2, xmm_temp5);\n-    __ pxor(xmm_temp2, xmm_temp8);\n-    __ pxor(xmm_temp3, xmm_temp2);\n-    __ pxor(xmm_temp6, xmm_temp3);      \/\/ the result is in xmm6\n-\n-    __ decrement(blocks);\n-    __ jcc(Assembler::zero, L_exit);\n-    __ movdqu(xmm_temp0, xmm_temp6);\n-    __ addptr(data, 16);\n-    __ jmp(L_ghash_loop);\n+  const XMMRegister msgtmp1 = xmm4;\n+  const XMMRegister msgtmp2 = xmm5;\n+  const XMMRegister msgtmp3 = xmm6;\n+  const XMMRegister msgtmp4 = xmm7;\n@@ -5442,7 +1428,1 @@\n-    __ BIND(L_exit);\n-    __ pshufb(xmm_temp6, xmm_temp10);          \/\/ Byte swap 16-byte result\n-    __ movdqu(Address(state, 0), xmm_temp6);   \/\/ store the result\n-    __ leave();\n-    __ ret(0);\n-    return start;\n-  }\n+  const XMMRegister shuf_mask = xmm8;\n@@ -5450,17 +1430,1 @@\n-  address base64_shuffle_addr()\n-  {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"shuffle_base64\");\n-    address start = __ pc();\n-    assert(((unsigned long long)start & 0x3f) == 0,\n-           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n-    __ emit_data64(0x0405030401020001, relocInfo::none);\n-    __ emit_data64(0x0a0b090a07080607, relocInfo::none);\n-    __ emit_data64(0x10110f100d0e0c0d, relocInfo::none);\n-    __ emit_data64(0x1617151613141213, relocInfo::none);\n-    __ emit_data64(0x1c1d1b1c191a1819, relocInfo::none);\n-    __ emit_data64(0x222321221f201e1f, relocInfo::none);\n-    __ emit_data64(0x2829272825262425, relocInfo::none);\n-    __ emit_data64(0x2e2f2d2e2b2c2a2b, relocInfo::none);\n-    return start;\n-  }\n+  __ enter();\n@@ -5468,11 +1432,1 @@\n-  address base64_avx2_shuffle_addr()\n-  {\n-    __ align32();\n-    StubCodeMark mark(this, \"StubRoutines\", \"avx2_shuffle_base64\");\n-    address start = __ pc();\n-    __ emit_data64(0x0809070805060405, relocInfo::none);\n-    __ emit_data64(0x0e0f0d0e0b0c0a0b, relocInfo::none);\n-    __ emit_data64(0x0405030401020001, relocInfo::none);\n-    __ emit_data64(0x0a0b090a07080607, relocInfo::none);\n-    return start;\n-  }\n+  __ subptr(rsp, 4 * wordSize);\n@@ -5480,10 +1434,6 @@\n-  address base64_avx2_input_mask_addr()\n-  {\n-    __ align32();\n-    StubCodeMark mark(this, \"StubRoutines\", \"avx2_input_mask_base64\");\n-    address start = __ pc();\n-    __ emit_data64(0x8000000000000000, relocInfo::none);\n-    __ emit_data64(0x8000000080000000, relocInfo::none);\n-    __ emit_data64(0x8000000080000000, relocInfo::none);\n-    __ emit_data64(0x8000000080000000, relocInfo::none);\n-    return start;\n+  if (VM_Version::supports_sha()) {\n+    __ fast_sha256(msg, state0, state1, msgtmp0, msgtmp1, msgtmp2, msgtmp3, msgtmp4,\n+      buf, state, ofs, limit, rsp, multi_block, shuf_mask);\n+  } else if (VM_Version::supports_avx2()) {\n+    __ sha256_AVX2(msg, state0, state1, msgtmp0, msgtmp1, msgtmp2, msgtmp3, msgtmp4,\n+      buf, state, ofs, limit, rsp, multi_block, shuf_mask);\n@@ -5491,0 +1441,4 @@\n+  __ addptr(rsp, 4 * wordSize);\n+  __ vzeroupper();\n+  __ leave();\n+  __ ret(0);\n@@ -5492,17 +1446,2 @@\n-  address base64_avx2_lut_addr()\n-  {\n-    __ align32();\n-    StubCodeMark mark(this, \"StubRoutines\", \"avx2_lut_base64\");\n-    address start = __ pc();\n-    __ emit_data64(0xfcfcfcfcfcfc4741, relocInfo::none);\n-    __ emit_data64(0x0000f0edfcfcfcfc, relocInfo::none);\n-    __ emit_data64(0xfcfcfcfcfcfc4741, relocInfo::none);\n-    __ emit_data64(0x0000f0edfcfcfcfc, relocInfo::none);\n-\n-    \/\/ URL LUT\n-    __ emit_data64(0xfcfcfcfcfcfc4741, relocInfo::none);\n-    __ emit_data64(0x000020effcfcfcfc, relocInfo::none);\n-    __ emit_data64(0xfcfcfcfcfcfc4741, relocInfo::none);\n-    __ emit_data64(0x000020effcfcfcfc, relocInfo::none);\n-    return start;\n-  }\n+  return start;\n+}\n@@ -5510,26 +1449,6 @@\n-  address base64_encoding_table_addr()\n-  {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"encoding_table_base64\");\n-    address start = __ pc();\n-    assert(((unsigned long long)start & 0x3f) == 0, \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n-    __ emit_data64(0x4847464544434241, relocInfo::none);\n-    __ emit_data64(0x504f4e4d4c4b4a49, relocInfo::none);\n-    __ emit_data64(0x5857565554535251, relocInfo::none);\n-    __ emit_data64(0x6665646362615a59, relocInfo::none);\n-    __ emit_data64(0x6e6d6c6b6a696867, relocInfo::none);\n-    __ emit_data64(0x767574737271706f, relocInfo::none);\n-    __ emit_data64(0x333231307a797877, relocInfo::none);\n-    __ emit_data64(0x2f2b393837363534, relocInfo::none);\n-\n-    \/\/ URL table\n-    __ emit_data64(0x4847464544434241, relocInfo::none);\n-    __ emit_data64(0x504f4e4d4c4b4a49, relocInfo::none);\n-    __ emit_data64(0x5857565554535251, relocInfo::none);\n-    __ emit_data64(0x6665646362615a59, relocInfo::none);\n-    __ emit_data64(0x6e6d6c6b6a696867, relocInfo::none);\n-    __ emit_data64(0x767574737271706f, relocInfo::none);\n-    __ emit_data64(0x333231307a797877, relocInfo::none);\n-    __ emit_data64(0x5f2d393837363534, relocInfo::none);\n-    return start;\n-  }\n+address StubGenerator::generate_sha512_implCompress(bool multi_block, const char *name) {\n+  assert(VM_Version::supports_avx2(), \"\");\n+  assert(VM_Version::supports_bmi2(), \"\");\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n@@ -5537,22 +1456,4 @@\n-  \/\/ Code for generating Base64 encoding.\n-  \/\/ Intrinsic function prototype in Base64.java:\n-  \/\/ private void encodeBlock(byte[] src, int sp, int sl, byte[] dst, int dp,\n-  \/\/ boolean isURL) {\n-  address generate_base64_encodeBlock()\n-  {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"implEncode\");\n-    address start = __ pc();\n-    __ enter();\n-\n-    \/\/ Save callee-saved registers before using them\n-    __ push(r12);\n-    __ push(r13);\n-    __ push(r14);\n-    __ push(r15);\n-\n-    \/\/ arguments\n-    const Register source = c_rarg0;       \/\/ Source Array\n-    const Register start_offset = c_rarg1; \/\/ start offset\n-    const Register end_offset = c_rarg2;   \/\/ end offset\n-    const Register dest = c_rarg3;   \/\/ destination array\n+  Register buf = c_rarg0;\n+  Register state = c_rarg1;\n+  Register ofs = c_rarg2;\n+  Register limit = c_rarg3;\n@@ -5560,11 +1461,8 @@\n-#ifndef _WIN64\n-    const Register dp = c_rarg4;    \/\/ Position for writing to dest array\n-    const Register isURL = c_rarg5; \/\/ Base64 or URL character set\n-#else\n-    const Address dp_mem(rbp, 6 * wordSize); \/\/ length is on stack on Win64\n-    const Address isURL_mem(rbp, 7 * wordSize);\n-    const Register isURL = r10; \/\/ pick the volatile windows register\n-    const Register dp = r12;\n-    __ movl(dp, dp_mem);\n-    __ movl(isURL, isURL_mem);\n-#endif\n+  const XMMRegister msg = xmm0;\n+  const XMMRegister state0 = xmm1;\n+  const XMMRegister state1 = xmm2;\n+  const XMMRegister msgtmp0 = xmm3;\n+  const XMMRegister msgtmp1 = xmm4;\n+  const XMMRegister msgtmp2 = xmm5;\n+  const XMMRegister msgtmp3 = xmm6;\n+  const XMMRegister msgtmp4 = xmm7;\n@@ -5572,3 +1470,1 @@\n-    const Register length = r14;\n-    const Register encode_table = r13;\n-    Label L_process3, L_exit, L_processdata, L_vbmiLoop, L_not512, L_32byteLoop;\n+  const XMMRegister shuf_mask = xmm8;\n@@ -5576,5 +1472,1 @@\n-    \/\/ calculate length from offsets\n-    __ movl(length, end_offset);\n-    __ subl(length, start_offset);\n-    __ cmpl(length, 0);\n-    __ jcc(Assembler::lessEqual, L_exit);\n+  __ enter();\n@@ -5582,6 +1474,2 @@\n-    \/\/ Code for 512-bit VBMI encoding.  Encodes 48 input bytes into 64\n-    \/\/ output bytes. We read 64 input bytes and ignore the last 16, so be\n-    \/\/ sure not to read past the end of the input buffer.\n-    if (VM_Version::supports_avx512_vbmi()) {\n-      __ cmpl(length, 64); \/\/ Do not overrun input buffer.\n-      __ jcc(Assembler::below, L_not512);\n+  __ sha512_AVX2(msg, state0, state1, msgtmp0, msgtmp1, msgtmp2, msgtmp3, msgtmp4,\n+  buf, state, ofs, limit, rsp, multi_block, shuf_mask);\n@@ -5589,4 +1477,3 @@\n-      __ shll(isURL, 6); \/\/ index into decode table based on isURL\n-      __ lea(encode_table, ExternalAddress(StubRoutines::x86::base64_encoding_table_addr()));\n-      __ addptr(encode_table, isURL);\n-      __ shrl(isURL, 6); \/\/ restore isURL\n+  __ vzeroupper();\n+  __ leave();\n+  __ ret(0);\n@@ -5594,4 +1481,2 @@\n-      __ mov64(rax, 0x3036242a1016040aull); \/\/ Shifts\n-      __ evmovdquq(xmm3, ExternalAddress(StubRoutines::x86::base64_shuffle_addr()), Assembler::AVX_512bit, r15);\n-      __ evmovdquq(xmm2, Address(encode_table, 0), Assembler::AVX_512bit);\n-      __ evpbroadcastq(xmm1, rax, Assembler::AVX_512bit);\n+  return start;\n+}\n@@ -5599,2 +1484,15 @@\n-      __ align32();\n-      __ BIND(L_vbmiLoop);\n+address StubGenerator::base64_shuffle_addr() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"shuffle_base64\");\n+  address start = __ pc();\n+\n+  assert(((unsigned long long)start & 0x3f) == 0,\n+         \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+  __ emit_data64(0x0405030401020001, relocInfo::none);\n+  __ emit_data64(0x0a0b090a07080607, relocInfo::none);\n+  __ emit_data64(0x10110f100d0e0c0d, relocInfo::none);\n+  __ emit_data64(0x1617151613141213, relocInfo::none);\n+  __ emit_data64(0x1c1d1b1c191a1819, relocInfo::none);\n+  __ emit_data64(0x222321221f201e1f, relocInfo::none);\n+  __ emit_data64(0x2829272825262425, relocInfo::none);\n+  __ emit_data64(0x2e2f2d2e2b2c2a2b, relocInfo::none);\n@@ -5602,2 +1500,2 @@\n-      __ vpermb(xmm0, xmm3, Address(source, start_offset), Assembler::AVX_512bit);\n-      __ subl(length, 48);\n+  return start;\n+}\n@@ -5605,4 +1503,4 @@\n-      \/\/ Put the input bytes into the proper lanes for writing, then\n-      \/\/ encode them.\n-      __ evpmultishiftqb(xmm0, xmm1, xmm0, Assembler::AVX_512bit);\n-      __ vpermb(xmm0, xmm0, xmm2, Assembler::AVX_512bit);\n+address StubGenerator::base64_avx2_shuffle_addr() {\n+  __ align32();\n+  StubCodeMark mark(this, \"StubRoutines\", \"avx2_shuffle_base64\");\n+  address start = __ pc();\n@@ -5610,2 +1508,4 @@\n-      \/\/ Write to destination\n-      __ evmovdquq(Address(dest, dp), xmm0, Assembler::AVX_512bit);\n+  __ emit_data64(0x0809070805060405, relocInfo::none);\n+  __ emit_data64(0x0e0f0d0e0b0c0a0b, relocInfo::none);\n+  __ emit_data64(0x0405030401020001, relocInfo::none);\n+  __ emit_data64(0x0a0b090a07080607, relocInfo::none);\n@@ -5613,4 +1513,2 @@\n-      __ addptr(dest, 64);\n-      __ addptr(source, 48);\n-      __ cmpl(length, 64);\n-      __ jcc(Assembler::aboveEqual, L_vbmiLoop);\n+  return start;\n+}\n@@ -5618,2 +1516,4 @@\n-      __ vzeroupper();\n-    }\n+address StubGenerator::base64_avx2_input_mask_addr() {\n+  __ align32();\n+  StubCodeMark mark(this, \"StubRoutines\", \"avx2_input_mask_base64\");\n+  address start = __ pc();\n@@ -5621,216 +1521,4 @@\n-    __ BIND(L_not512);\n-    if (VM_Version::supports_avx2()\n-        && VM_Version::supports_avx512vlbw()) {\n-      \/*\n-      ** This AVX2 encoder is based off the paper at:\n-      **      https:\/\/dl.acm.org\/doi\/10.1145\/3132709\n-      **\n-      ** We use AVX2 SIMD instructions to encode 24 bytes into 32\n-      ** output bytes.\n-      **\n-      *\/\n-      \/\/ Lengths under 32 bytes are done with scalar routine\n-      __ cmpl(length, 31);\n-      __ jcc(Assembler::belowEqual, L_process3);\n-\n-      \/\/ Set up supporting constant table data\n-      __ vmovdqu(xmm9, ExternalAddress(StubRoutines::x86::base64_avx2_shuffle_addr()), rax);\n-      \/\/ 6-bit mask for 2nd and 4th (and multiples) 6-bit values\n-      __ movl(rax, 0x0fc0fc00);\n-      __ vmovdqu(xmm1, ExternalAddress(StubRoutines::x86::base64_avx2_input_mask_addr()), rax);\n-      __ evpbroadcastd(xmm8, rax, Assembler::AVX_256bit);\n-\n-      \/\/ Multiplication constant for \"shifting\" right by 6 and 10\n-      \/\/ bits\n-      __ movl(rax, 0x04000040);\n-\n-      __ subl(length, 24);\n-      __ evpbroadcastd(xmm7, rax, Assembler::AVX_256bit);\n-\n-      \/\/ For the first load, we mask off reading of the first 4\n-      \/\/ bytes into the register. This is so we can get 4 3-byte\n-      \/\/ chunks into each lane of the register, avoiding having to\n-      \/\/ handle end conditions.  We then shuffle these bytes into a\n-      \/\/ specific order so that manipulation is easier.\n-      \/\/\n-      \/\/ The initial read loads the XMM register like this:\n-      \/\/\n-      \/\/ Lower 128-bit lane:\n-      \/\/ +----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n-      \/\/ | XX | XX | XX | XX | A0 | A1 | A2 | B0 | B1 | B2 | C0 | C1\n-      \/\/ | C2 | D0 | D1 | D2 |\n-      \/\/ +----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n-      \/\/\n-      \/\/ Upper 128-bit lane:\n-      \/\/ +----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n-      \/\/ | E0 | E1 | E2 | F0 | F1 | F2 | G0 | G1 | G2 | H0 | H1 | H2\n-      \/\/ | XX | XX | XX | XX |\n-      \/\/ +----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n-      \/\/\n-      \/\/ Where A0 is the first input byte, B0 is the fourth, etc.\n-      \/\/ The alphabetical significance denotes the 3 bytes to be\n-      \/\/ consumed and encoded into 4 bytes.\n-      \/\/\n-      \/\/ We then shuffle the register so each 32-bit word contains\n-      \/\/ the sequence:\n-      \/\/    A1 A0 A2 A1, B1, B0, B2, B1, etc.\n-      \/\/ Each of these byte sequences are then manipulated into 4\n-      \/\/ 6-bit values ready for encoding.\n-      \/\/\n-      \/\/ If we focus on one set of 3-byte chunks, changing the\n-      \/\/ nomenclature such that A0 => a, A1 => b, and A2 => c, we\n-      \/\/ shuffle such that each 24-bit chunk contains:\n-      \/\/\n-      \/\/ b7 b6 b5 b4 b3 b2 b1 b0 | a7 a6 a5 a4 a3 a2 a1 a0 | c7 c6\n-      \/\/ c5 c4 c3 c2 c1 c0 | b7 b6 b5 b4 b3 b2 b1 b0\n-      \/\/ Explain this step.\n-      \/\/ b3 b2 b1 b0 c5 c4 c3 c2 | c1 c0 d5 d4 d3 d2 d1 d0 | a5 a4\n-      \/\/ a3 a2 a1 a0 b5 b4 | b3 b2 b1 b0 c5 c4 c3 c2\n-      \/\/\n-      \/\/ W first and off all but bits 4-9 and 16-21 (c5..c0 and\n-      \/\/ a5..a0) and shift them using a vector multiplication\n-      \/\/ operation (vpmulhuw) which effectively shifts c right by 6\n-      \/\/ bits and a right by 10 bits.  We similarly mask bits 10-15\n-      \/\/ (d5..d0) and 22-27 (b5..b0) and shift them left by 8 and 4\n-      \/\/ bits respectively.  This is done using vpmullw.  We end up\n-      \/\/ with 4 6-bit values, thus splitting the 3 input bytes,\n-      \/\/ ready for encoding:\n-      \/\/    0 0 d5..d0 0 0 c5..c0 0 0 b5..b0 0 0 a5..a0\n-      \/\/\n-      \/\/ For translation, we recognize that there are 5 distinct\n-      \/\/ ranges of legal Base64 characters as below:\n-      \/\/\n-      \/\/   +-------------+-------------+------------+\n-      \/\/   | 6-bit value | ASCII range |   offset   |\n-      \/\/   +-------------+-------------+------------+\n-      \/\/   |    0..25    |    A..Z     |     65     |\n-      \/\/   |   26..51    |    a..z     |     71     |\n-      \/\/   |   52..61    |    0..9     |     -4     |\n-      \/\/   |     62      |   + or -    | -19 or -17 |\n-      \/\/   |     63      |   \/ or _    | -16 or 32  |\n-      \/\/   +-------------+-------------+------------+\n-      \/\/\n-      \/\/ We note that vpshufb does a parallel lookup in a\n-      \/\/ destination register using the lower 4 bits of bytes from a\n-      \/\/ source register.  If we use a saturated subtraction and\n-      \/\/ subtract 51 from each 6-bit value, bytes from [0,51]\n-      \/\/ saturate to 0, and [52,63] map to a range of [1,12].  We\n-      \/\/ distinguish the [0,25] and [26,51] ranges by assigning a\n-      \/\/ value of 13 for all 6-bit values less than 26.  We end up\n-      \/\/ with:\n-      \/\/\n-      \/\/   +-------------+-------------+------------+\n-      \/\/   | 6-bit value |   Reduced   |   offset   |\n-      \/\/   +-------------+-------------+------------+\n-      \/\/   |    0..25    |     13      |     65     |\n-      \/\/   |   26..51    |      0      |     71     |\n-      \/\/   |   52..61    |    0..9     |     -4     |\n-      \/\/   |     62      |     11      | -19 or -17 |\n-      \/\/   |     63      |     12      | -16 or 32  |\n-      \/\/   +-------------+-------------+------------+\n-      \/\/\n-      \/\/ We then use a final vpshufb to add the appropriate offset,\n-      \/\/ translating the bytes.\n-      \/\/\n-      \/\/ Load input bytes - only 28 bytes.  Mask the first load to\n-      \/\/ not load into the full register.\n-      __ vpmaskmovd(xmm1, xmm1, Address(source, start_offset, Address::times_1, -4), Assembler::AVX_256bit);\n-\n-      \/\/ Move 3-byte chunks of input (12 bytes) into 16 bytes,\n-      \/\/ ordering by:\n-      \/\/   1, 0, 2, 1; 4, 3, 5, 4; etc.  This groups 6-bit chunks\n-      \/\/   for easy masking\n-      __ vpshufb(xmm1, xmm1, xmm9, Assembler::AVX_256bit);\n-\n-      __ addl(start_offset, 24);\n-\n-      \/\/ Load masking register for first and third (and multiples)\n-      \/\/ 6-bit values.\n-      __ movl(rax, 0x003f03f0);\n-      __ evpbroadcastd(xmm6, rax, Assembler::AVX_256bit);\n-      \/\/ Multiplication constant for \"shifting\" left by 4 and 8 bits\n-      __ movl(rax, 0x01000010);\n-      __ evpbroadcastd(xmm5, rax, Assembler::AVX_256bit);\n-\n-      \/\/ Isolate 6-bit chunks of interest\n-      __ vpand(xmm0, xmm8, xmm1, Assembler::AVX_256bit);\n-\n-      \/\/ Load constants for encoding\n-      __ movl(rax, 0x19191919);\n-      __ evpbroadcastd(xmm3, rax, Assembler::AVX_256bit);\n-      __ movl(rax, 0x33333333);\n-      __ evpbroadcastd(xmm4, rax, Assembler::AVX_256bit);\n-\n-      \/\/ Shift output bytes 0 and 2 into proper lanes\n-      __ vpmulhuw(xmm2, xmm0, xmm7, Assembler::AVX_256bit);\n-\n-      \/\/ Mask and shift output bytes 1 and 3 into proper lanes and\n-      \/\/ combine\n-      __ vpand(xmm0, xmm6, xmm1, Assembler::AVX_256bit);\n-      __ vpmullw(xmm0, xmm5, xmm0, Assembler::AVX_256bit);\n-      __ vpor(xmm0, xmm0, xmm2, Assembler::AVX_256bit);\n-\n-      \/\/ Find out which are 0..25.  This indicates which input\n-      \/\/ values fall in the range of 'A'-'Z', which require an\n-      \/\/ additional offset (see comments above)\n-      __ vpcmpgtb(xmm2, xmm0, xmm3, Assembler::AVX_256bit);\n-      __ vpsubusb(xmm1, xmm0, xmm4, Assembler::AVX_256bit);\n-      __ vpsubb(xmm1, xmm1, xmm2, Assembler::AVX_256bit);\n-\n-      \/\/ Load the proper lookup table\n-      __ lea(r11, ExternalAddress(StubRoutines::x86::base64_avx2_lut_addr()));\n-      __ movl(r15, isURL);\n-      __ shll(r15, 5);\n-      __ vmovdqu(xmm2, Address(r11, r15));\n-\n-      \/\/ Shuffle the offsets based on the range calculation done\n-      \/\/ above. This allows us to add the correct offset to the\n-      \/\/ 6-bit value corresponding to the range documented above.\n-      __ vpshufb(xmm1, xmm2, xmm1, Assembler::AVX_256bit);\n-      __ vpaddb(xmm0, xmm1, xmm0, Assembler::AVX_256bit);\n-\n-      \/\/ Store the encoded bytes\n-      __ vmovdqu(Address(dest, dp), xmm0);\n-      __ addl(dp, 32);\n-\n-      __ cmpl(length, 31);\n-      __ jcc(Assembler::belowEqual, L_process3);\n-\n-      __ align32();\n-      __ BIND(L_32byteLoop);\n-\n-      \/\/ Get next 32 bytes\n-      __ vmovdqu(xmm1, Address(source, start_offset, Address::times_1, -4));\n-\n-      __ subl(length, 24);\n-      __ addl(start_offset, 24);\n-\n-      \/\/ This logic is identical to the above, with only constant\n-      \/\/ register loads removed.  Shuffle the input, mask off 6-bit\n-      \/\/ chunks, shift them into place, then add the offset to\n-      \/\/ encode.\n-      __ vpshufb(xmm1, xmm1, xmm9, Assembler::AVX_256bit);\n-\n-      __ vpand(xmm0, xmm8, xmm1, Assembler::AVX_256bit);\n-      __ vpmulhuw(xmm10, xmm0, xmm7, Assembler::AVX_256bit);\n-      __ vpand(xmm0, xmm6, xmm1, Assembler::AVX_256bit);\n-      __ vpmullw(xmm0, xmm5, xmm0, Assembler::AVX_256bit);\n-      __ vpor(xmm0, xmm0, xmm10, Assembler::AVX_256bit);\n-      __ vpcmpgtb(xmm10, xmm0, xmm3, Assembler::AVX_256bit);\n-      __ vpsubusb(xmm1, xmm0, xmm4, Assembler::AVX_256bit);\n-      __ vpsubb(xmm1, xmm1, xmm10, Assembler::AVX_256bit);\n-      __ vpshufb(xmm1, xmm2, xmm1, Assembler::AVX_256bit);\n-      __ vpaddb(xmm0, xmm1, xmm0, Assembler::AVX_256bit);\n-\n-      \/\/ Store the encoded bytes\n-      __ vmovdqu(Address(dest, dp), xmm0);\n-      __ addl(dp, 32);\n-\n-      __ cmpl(length, 31);\n-      __ jcc(Assembler::above, L_32byteLoop);\n-\n-      __ BIND(L_process3);\n-      __ vzeroupper();\n-    } else {\n-      __ BIND(L_process3);\n-    }\n+  __ emit_data64(0x8000000000000000, relocInfo::none);\n+  __ emit_data64(0x8000000080000000, relocInfo::none);\n+  __ emit_data64(0x8000000080000000, relocInfo::none);\n+  __ emit_data64(0x8000000080000000, relocInfo::none);\n@@ -5838,2 +1526,2 @@\n-    __ cmpl(length, 3);\n-    __ jcc(Assembler::below, L_exit);\n+  return start;\n+}\n@@ -5841,5 +1529,4 @@\n-    \/\/ Load the encoding table based on isURL\n-    __ lea(r11, ExternalAddress(StubRoutines::x86::base64_encoding_table_addr()));\n-    __ movl(r15, isURL);\n-    __ shll(r15, 6);\n-    __ addptr(r11, r15);\n+address StubGenerator::base64_avx2_lut_addr() {\n+  __ align32();\n+  StubCodeMark mark(this, \"StubRoutines\", \"avx2_lut_base64\");\n+  address start = __ pc();\n@@ -5847,1 +1534,4 @@\n-    __ BIND(L_processdata);\n+  __ emit_data64(0xfcfcfcfcfcfc4741, relocInfo::none);\n+  __ emit_data64(0x0000f0edfcfcfcfc, relocInfo::none);\n+  __ emit_data64(0xfcfcfcfcfcfc4741, relocInfo::none);\n+  __ emit_data64(0x0000f0edfcfcfcfc, relocInfo::none);\n@@ -5849,4 +1539,5 @@\n-    \/\/ Load 3 bytes\n-    __ load_unsigned_byte(r15, Address(source, start_offset));\n-    __ load_unsigned_byte(r10, Address(source, start_offset, Address::times_1, 1));\n-    __ load_unsigned_byte(r13, Address(source, start_offset, Address::times_1, 2));\n+  \/\/ URL LUT\n+  __ emit_data64(0xfcfcfcfcfcfc4741, relocInfo::none);\n+  __ emit_data64(0x000020effcfcfcfc, relocInfo::none);\n+  __ emit_data64(0xfcfcfcfcfcfc4741, relocInfo::none);\n+  __ emit_data64(0x000020effcfcfcfc, relocInfo::none);\n@@ -5854,4 +1545,2 @@\n-    \/\/ Build a 32-bit word with bytes 1, 2, 0, 1\n-    __ movl(rax, r10);\n-    __ shll(r10, 24);\n-    __ orl(rax, r10);\n+  return start;\n+}\n@@ -5859,1 +1548,24 @@\n-    __ subl(length, 3);\n+address StubGenerator::base64_encoding_table_addr() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"encoding_table_base64\");\n+  address start = __ pc();\n+\n+  assert(((unsigned long long)start & 0x3f) == 0, \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+  __ emit_data64(0x4847464544434241, relocInfo::none);\n+  __ emit_data64(0x504f4e4d4c4b4a49, relocInfo::none);\n+  __ emit_data64(0x5857565554535251, relocInfo::none);\n+  __ emit_data64(0x6665646362615a59, relocInfo::none);\n+  __ emit_data64(0x6e6d6c6b6a696867, relocInfo::none);\n+  __ emit_data64(0x767574737271706f, relocInfo::none);\n+  __ emit_data64(0x333231307a797877, relocInfo::none);\n+  __ emit_data64(0x2f2b393837363534, relocInfo::none);\n+\n+  \/\/ URL table\n+  __ emit_data64(0x4847464544434241, relocInfo::none);\n+  __ emit_data64(0x504f4e4d4c4b4a49, relocInfo::none);\n+  __ emit_data64(0x5857565554535251, relocInfo::none);\n+  __ emit_data64(0x6665646362615a59, relocInfo::none);\n+  __ emit_data64(0x6e6d6c6b6a696867, relocInfo::none);\n+  __ emit_data64(0x767574737271706f, relocInfo::none);\n+  __ emit_data64(0x333231307a797877, relocInfo::none);\n+  __ emit_data64(0x5f2d393837363534, relocInfo::none);\n@@ -5861,3 +1573,2 @@\n-    __ shll(r15, 8);\n-    __ shll(r13, 16);\n-    __ orl(rax, r15);\n+  return start;\n+}\n@@ -5865,1 +1576,23 @@\n-    __ addl(start_offset, 3);\n+\/\/ Code for generating Base64 encoding.\n+\/\/ Intrinsic function prototype in Base64.java:\n+\/\/ private void encodeBlock(byte[] src, int sp, int sl, byte[] dst, int dp,\n+\/\/ boolean isURL) {\n+address StubGenerator::generate_base64_encodeBlock()\n+{\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"implEncode\");\n+  address start = __ pc();\n+\n+  __ enter();\n+\n+  \/\/ Save callee-saved registers before using them\n+  __ push(r12);\n+  __ push(r13);\n+  __ push(r14);\n+  __ push(r15);\n+\n+  \/\/ arguments\n+  const Register source = c_rarg0;       \/\/ Source Array\n+  const Register start_offset = c_rarg1; \/\/ start offset\n+  const Register end_offset = c_rarg2;   \/\/ end offset\n+  const Register dest = c_rarg3;   \/\/ destination array\n@@ -5867,6 +1600,38 @@\n-    __ orl(rax, r13);\n-    \/\/ At this point, rax contains | byte1 | byte2 | byte0 | byte1\n-    \/\/ r13 has byte2 << 16 - need low-order 6 bits to translate.\n-    \/\/ This translated byte is the fourth output byte.\n-    __ shrl(r13, 16);\n-    __ andl(r13, 0x3f);\n+#ifndef _WIN64\n+  const Register dp = c_rarg4;    \/\/ Position for writing to dest array\n+  const Register isURL = c_rarg5; \/\/ Base64 or URL character set\n+#else\n+  const Address dp_mem(rbp, 6 * wordSize); \/\/ length is on stack on Win64\n+  const Address isURL_mem(rbp, 7 * wordSize);\n+  const Register isURL = r10; \/\/ pick the volatile windows register\n+  const Register dp = r12;\n+  __ movl(dp, dp_mem);\n+  __ movl(isURL, isURL_mem);\n+#endif\n+\n+  const Register length = r14;\n+  const Register encode_table = r13;\n+  Label L_process3, L_exit, L_processdata, L_vbmiLoop, L_not512, L_32byteLoop;\n+\n+  \/\/ calculate length from offsets\n+  __ movl(length, end_offset);\n+  __ subl(length, start_offset);\n+  __ cmpl(length, 0);\n+  __ jcc(Assembler::lessEqual, L_exit);\n+\n+  \/\/ Code for 512-bit VBMI encoding.  Encodes 48 input bytes into 64\n+  \/\/ output bytes. We read 64 input bytes and ignore the last 16, so be\n+  \/\/ sure not to read past the end of the input buffer.\n+  if (VM_Version::supports_avx512_vbmi()) {\n+    __ cmpl(length, 64); \/\/ Do not overrun input buffer.\n+    __ jcc(Assembler::below, L_not512);\n+\n+    __ shll(isURL, 6); \/\/ index into decode table based on isURL\n+    __ lea(encode_table, ExternalAddress(StubRoutines::x86::base64_encoding_table_addr()));\n+    __ addptr(encode_table, isURL);\n+    __ shrl(isURL, 6); \/\/ restore isURL\n+\n+    __ mov64(rax, 0x3036242a1016040aull); \/\/ Shifts\n+    __ evmovdquq(xmm3, ExternalAddress(StubRoutines::x86::base64_shuffle_addr()), Assembler::AVX_512bit, r15);\n+    __ evmovdquq(xmm2, Address(encode_table, 0), Assembler::AVX_512bit);\n+    __ evpbroadcastq(xmm1, rax, Assembler::AVX_512bit);\n@@ -5874,3 +1639,2 @@\n-    \/\/ The high-order 6 bits of r15 (byte0) is translated.\n-    \/\/ The translated byte is the first output byte.\n-    __ shrl(r15, 10);\n+    __ align32();\n+    __ BIND(L_vbmiLoop);\n@@ -5878,2 +1642,2 @@\n-    __ load_unsigned_byte(r13, Address(r11, r13));\n-    __ load_unsigned_byte(r15, Address(r11, r15));\n+    __ vpermb(xmm0, xmm3, Address(source, start_offset), Assembler::AVX_512bit);\n+    __ subl(length, 48);\n@@ -5881,1 +1645,4 @@\n-    __ movb(Address(dest, dp, Address::times_1, 3), r13);\n+    \/\/ Put the input bytes into the proper lanes for writing, then\n+    \/\/ encode them.\n+    __ evpmultishiftqb(xmm0, xmm1, xmm0, Assembler::AVX_512bit);\n+    __ vpermb(xmm0, xmm0, xmm2, Assembler::AVX_512bit);\n@@ -5883,5 +1650,2 @@\n-    \/\/ Extract high-order 4 bits of byte1 and low-order 2 bits of byte0.\n-    \/\/ This translated byte is the second output byte.\n-    __ shrl(rax, 4);\n-    __ movl(r10, rax);\n-    __ andl(rax, 0x3f);\n+    \/\/ Write to destination\n+    __ evmovdquq(Address(dest, dp), xmm0, Assembler::AVX_512bit);\n@@ -5889,1 +1653,4 @@\n-    __ movb(Address(dest, dp, Address::times_1, 0), r15);\n+    __ addptr(dest, 64);\n+    __ addptr(source, 48);\n+    __ cmpl(length, 64);\n+    __ jcc(Assembler::aboveEqual, L_vbmiLoop);\n@@ -5891,1 +1658,2 @@\n-    __ load_unsigned_byte(rax, Address(r11, rax));\n+    __ vzeroupper();\n+  }\n@@ -5893,4 +1661,164 @@\n-    \/\/ Extract low-order 2 bits of byte1 and high-order 4 bits of byte2.\n-    \/\/ This translated byte is the third output byte.\n-    __ shrl(r10, 18);\n-    __ andl(r10, 0x3f);\n+  __ BIND(L_not512);\n+  if (VM_Version::supports_avx2()\n+      && VM_Version::supports_avx512vlbw()) {\n+    \/*\n+    ** This AVX2 encoder is based off the paper at:\n+    **      https:\/\/dl.acm.org\/doi\/10.1145\/3132709\n+    **\n+    ** We use AVX2 SIMD instructions to encode 24 bytes into 32\n+    ** output bytes.\n+    **\n+    *\/\n+    \/\/ Lengths under 32 bytes are done with scalar routine\n+    __ cmpl(length, 31);\n+    __ jcc(Assembler::belowEqual, L_process3);\n+\n+    \/\/ Set up supporting constant table data\n+    __ vmovdqu(xmm9, ExternalAddress(StubRoutines::x86::base64_avx2_shuffle_addr()), rax);\n+    \/\/ 6-bit mask for 2nd and 4th (and multiples) 6-bit values\n+    __ movl(rax, 0x0fc0fc00);\n+    __ vmovdqu(xmm1, ExternalAddress(StubRoutines::x86::base64_avx2_input_mask_addr()), rax);\n+    __ evpbroadcastd(xmm8, rax, Assembler::AVX_256bit);\n+\n+    \/\/ Multiplication constant for \"shifting\" right by 6 and 10\n+    \/\/ bits\n+    __ movl(rax, 0x04000040);\n+\n+    __ subl(length, 24);\n+    __ evpbroadcastd(xmm7, rax, Assembler::AVX_256bit);\n+\n+    \/\/ For the first load, we mask off reading of the first 4\n+    \/\/ bytes into the register. This is so we can get 4 3-byte\n+    \/\/ chunks into each lane of the register, avoiding having to\n+    \/\/ handle end conditions.  We then shuffle these bytes into a\n+    \/\/ specific order so that manipulation is easier.\n+    \/\/\n+    \/\/ The initial read loads the XMM register like this:\n+    \/\/\n+    \/\/ Lower 128-bit lane:\n+    \/\/ +----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n+    \/\/ | XX | XX | XX | XX | A0 | A1 | A2 | B0 | B1 | B2 | C0 | C1\n+    \/\/ | C2 | D0 | D1 | D2 |\n+    \/\/ +----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n+    \/\/\n+    \/\/ Upper 128-bit lane:\n+    \/\/ +----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n+    \/\/ | E0 | E1 | E2 | F0 | F1 | F2 | G0 | G1 | G2 | H0 | H1 | H2\n+    \/\/ | XX | XX | XX | XX |\n+    \/\/ +----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n+    \/\/\n+    \/\/ Where A0 is the first input byte, B0 is the fourth, etc.\n+    \/\/ The alphabetical significance denotes the 3 bytes to be\n+    \/\/ consumed and encoded into 4 bytes.\n+    \/\/\n+    \/\/ We then shuffle the register so each 32-bit word contains\n+    \/\/ the sequence:\n+    \/\/    A1 A0 A2 A1, B1, B0, B2, B1, etc.\n+    \/\/ Each of these byte sequences are then manipulated into 4\n+    \/\/ 6-bit values ready for encoding.\n+    \/\/\n+    \/\/ If we focus on one set of 3-byte chunks, changing the\n+    \/\/ nomenclature such that A0 => a, A1 => b, and A2 => c, we\n+    \/\/ shuffle such that each 24-bit chunk contains:\n+    \/\/\n+    \/\/ b7 b6 b5 b4 b3 b2 b1 b0 | a7 a6 a5 a4 a3 a2 a1 a0 | c7 c6\n+    \/\/ c5 c4 c3 c2 c1 c0 | b7 b6 b5 b4 b3 b2 b1 b0\n+    \/\/ Explain this step.\n+    \/\/ b3 b2 b1 b0 c5 c4 c3 c2 | c1 c0 d5 d4 d3 d2 d1 d0 | a5 a4\n+    \/\/ a3 a2 a1 a0 b5 b4 | b3 b2 b1 b0 c5 c4 c3 c2\n+    \/\/\n+    \/\/ W first and off all but bits 4-9 and 16-21 (c5..c0 and\n+    \/\/ a5..a0) and shift them using a vector multiplication\n+    \/\/ operation (vpmulhuw) which effectively shifts c right by 6\n+    \/\/ bits and a right by 10 bits.  We similarly mask bits 10-15\n+    \/\/ (d5..d0) and 22-27 (b5..b0) and shift them left by 8 and 4\n+    \/\/ bits respectively.  This is done using vpmullw.  We end up\n+    \/\/ with 4 6-bit values, thus splitting the 3 input bytes,\n+    \/\/ ready for encoding:\n+    \/\/    0 0 d5..d0 0 0 c5..c0 0 0 b5..b0 0 0 a5..a0\n+    \/\/\n+    \/\/ For translation, we recognize that there are 5 distinct\n+    \/\/ ranges of legal Base64 characters as below:\n+    \/\/\n+    \/\/   +-------------+-------------+------------+\n+    \/\/   | 6-bit value | ASCII range |   offset   |\n+    \/\/   +-------------+-------------+------------+\n+    \/\/   |    0..25    |    A..Z     |     65     |\n+    \/\/   |   26..51    |    a..z     |     71     |\n+    \/\/   |   52..61    |    0..9     |     -4     |\n+    \/\/   |     62      |   + or -    | -19 or -17 |\n+    \/\/   |     63      |   \/ or _    | -16 or 32  |\n+    \/\/   +-------------+-------------+------------+\n+    \/\/\n+    \/\/ We note that vpshufb does a parallel lookup in a\n+    \/\/ destination register using the lower 4 bits of bytes from a\n+    \/\/ source register.  If we use a saturated subtraction and\n+    \/\/ subtract 51 from each 6-bit value, bytes from [0,51]\n+    \/\/ saturate to 0, and [52,63] map to a range of [1,12].  We\n+    \/\/ distinguish the [0,25] and [26,51] ranges by assigning a\n+    \/\/ value of 13 for all 6-bit values less than 26.  We end up\n+    \/\/ with:\n+    \/\/\n+    \/\/   +-------------+-------------+------------+\n+    \/\/   | 6-bit value |   Reduced   |   offset   |\n+    \/\/   +-------------+-------------+------------+\n+    \/\/   |    0..25    |     13      |     65     |\n+    \/\/   |   26..51    |      0      |     71     |\n+    \/\/   |   52..61    |    0..9     |     -4     |\n+    \/\/   |     62      |     11      | -19 or -17 |\n+    \/\/   |     63      |     12      | -16 or 32  |\n+    \/\/   +-------------+-------------+------------+\n+    \/\/\n+    \/\/ We then use a final vpshufb to add the appropriate offset,\n+    \/\/ translating the bytes.\n+    \/\/\n+    \/\/ Load input bytes - only 28 bytes.  Mask the first load to\n+    \/\/ not load into the full register.\n+    __ vpmaskmovd(xmm1, xmm1, Address(source, start_offset, Address::times_1, -4), Assembler::AVX_256bit);\n+\n+    \/\/ Move 3-byte chunks of input (12 bytes) into 16 bytes,\n+    \/\/ ordering by:\n+    \/\/   1, 0, 2, 1; 4, 3, 5, 4; etc.  This groups 6-bit chunks\n+    \/\/   for easy masking\n+    __ vpshufb(xmm1, xmm1, xmm9, Assembler::AVX_256bit);\n+\n+    __ addl(start_offset, 24);\n+\n+    \/\/ Load masking register for first and third (and multiples)\n+    \/\/ 6-bit values.\n+    __ movl(rax, 0x003f03f0);\n+    __ evpbroadcastd(xmm6, rax, Assembler::AVX_256bit);\n+    \/\/ Multiplication constant for \"shifting\" left by 4 and 8 bits\n+    __ movl(rax, 0x01000010);\n+    __ evpbroadcastd(xmm5, rax, Assembler::AVX_256bit);\n+\n+    \/\/ Isolate 6-bit chunks of interest\n+    __ vpand(xmm0, xmm8, xmm1, Assembler::AVX_256bit);\n+\n+    \/\/ Load constants for encoding\n+    __ movl(rax, 0x19191919);\n+    __ evpbroadcastd(xmm3, rax, Assembler::AVX_256bit);\n+    __ movl(rax, 0x33333333);\n+    __ evpbroadcastd(xmm4, rax, Assembler::AVX_256bit);\n+\n+    \/\/ Shift output bytes 0 and 2 into proper lanes\n+    __ vpmulhuw(xmm2, xmm0, xmm7, Assembler::AVX_256bit);\n+\n+    \/\/ Mask and shift output bytes 1 and 3 into proper lanes and\n+    \/\/ combine\n+    __ vpand(xmm0, xmm6, xmm1, Assembler::AVX_256bit);\n+    __ vpmullw(xmm0, xmm5, xmm0, Assembler::AVX_256bit);\n+    __ vpor(xmm0, xmm0, xmm2, Assembler::AVX_256bit);\n+\n+    \/\/ Find out which are 0..25.  This indicates which input\n+    \/\/ values fall in the range of 'A'-'Z', which require an\n+    \/\/ additional offset (see comments above)\n+    __ vpcmpgtb(xmm2, xmm0, xmm3, Assembler::AVX_256bit);\n+    __ vpsubusb(xmm1, xmm0, xmm4, Assembler::AVX_256bit);\n+    __ vpsubb(xmm1, xmm1, xmm2, Assembler::AVX_256bit);\n+\n+    \/\/ Load the proper lookup table\n+    __ lea(r11, ExternalAddress(StubRoutines::x86::base64_avx2_lut_addr()));\n+    __ movl(r15, isURL);\n+    __ shll(r15, 5);\n+    __ vmovdqu(xmm2, Address(r11, r15));\n@@ -5898,1 +1826,5 @@\n-    __ load_unsigned_byte(r10, Address(r11, r10));\n+    \/\/ Shuffle the offsets based on the range calculation done\n+    \/\/ above. This allows us to add the correct offset to the\n+    \/\/ 6-bit value corresponding to the range documented above.\n+    __ vpshufb(xmm1, xmm2, xmm1, Assembler::AVX_256bit);\n+    __ vpaddb(xmm0, xmm1, xmm0, Assembler::AVX_256bit);\n@@ -5900,2 +1832,3 @@\n-    __ movb(Address(dest, dp, Address::times_1, 1), rax);\n-    __ movb(Address(dest, dp, Address::times_1, 2), r10);\n+    \/\/ Store the encoded bytes\n+    __ vmovdqu(Address(dest, dp), xmm0);\n+    __ addl(dp, 32);\n@@ -5903,3 +1836,2 @@\n-    __ addl(dp, 4);\n-    __ cmpl(length, 3);\n-    __ jcc(Assembler::aboveEqual, L_processdata);\n+    __ cmpl(length, 31);\n+    __ jcc(Assembler::belowEqual, L_process3);\n@@ -5907,8 +1839,37 @@\n-    __ BIND(L_exit);\n-    __ pop(r15);\n-    __ pop(r14);\n-    __ pop(r13);\n-    __ pop(r12);\n-    __ leave();\n-    __ ret(0);\n-    return start;\n+    __ align32();\n+    __ BIND(L_32byteLoop);\n+\n+    \/\/ Get next 32 bytes\n+    __ vmovdqu(xmm1, Address(source, start_offset, Address::times_1, -4));\n+\n+    __ subl(length, 24);\n+    __ addl(start_offset, 24);\n+\n+    \/\/ This logic is identical to the above, with only constant\n+    \/\/ register loads removed.  Shuffle the input, mask off 6-bit\n+    \/\/ chunks, shift them into place, then add the offset to\n+    \/\/ encode.\n+    __ vpshufb(xmm1, xmm1, xmm9, Assembler::AVX_256bit);\n+\n+    __ vpand(xmm0, xmm8, xmm1, Assembler::AVX_256bit);\n+    __ vpmulhuw(xmm10, xmm0, xmm7, Assembler::AVX_256bit);\n+    __ vpand(xmm0, xmm6, xmm1, Assembler::AVX_256bit);\n+    __ vpmullw(xmm0, xmm5, xmm0, Assembler::AVX_256bit);\n+    __ vpor(xmm0, xmm0, xmm10, Assembler::AVX_256bit);\n+    __ vpcmpgtb(xmm10, xmm0, xmm3, Assembler::AVX_256bit);\n+    __ vpsubusb(xmm1, xmm0, xmm4, Assembler::AVX_256bit);\n+    __ vpsubb(xmm1, xmm1, xmm10, Assembler::AVX_256bit);\n+    __ vpshufb(xmm1, xmm2, xmm1, Assembler::AVX_256bit);\n+    __ vpaddb(xmm0, xmm1, xmm0, Assembler::AVX_256bit);\n+\n+    \/\/ Store the encoded bytes\n+    __ vmovdqu(Address(dest, dp), xmm0);\n+    __ addl(dp, 32);\n+\n+    __ cmpl(length, 31);\n+    __ jcc(Assembler::above, L_32byteLoop);\n+\n+    __ BIND(L_process3);\n+    __ vzeroupper();\n+  } else {\n+    __ BIND(L_process3);\n@@ -5917,17 +1878,2 @@\n-  \/\/ base64 AVX512vbmi tables\n-  address base64_vbmi_lookup_lo_addr() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"lookup_lo_base64\");\n-    address start = __ pc();\n-    assert(((unsigned long long)start & 0x3f) == 0,\n-           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n-    __ emit_data64(0x8080808080808080, relocInfo::none);\n-    __ emit_data64(0x8080808080808080, relocInfo::none);\n-    __ emit_data64(0x8080808080808080, relocInfo::none);\n-    __ emit_data64(0x8080808080808080, relocInfo::none);\n-    __ emit_data64(0x8080808080808080, relocInfo::none);\n-    __ emit_data64(0x3f8080803e808080, relocInfo::none);\n-    __ emit_data64(0x3b3a393837363534, relocInfo::none);\n-    __ emit_data64(0x8080808080803d3c, relocInfo::none);\n-    return start;\n-  }\n+  __ cmpl(length, 3);\n+  __ jcc(Assembler::below, L_exit);\n@@ -5935,32 +1881,5 @@\n-  address base64_vbmi_lookup_hi_addr() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"lookup_hi_base64\");\n-    address start = __ pc();\n-    assert(((unsigned long long)start & 0x3f) == 0,\n-           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n-    __ emit_data64(0x0605040302010080, relocInfo::none);\n-    __ emit_data64(0x0e0d0c0b0a090807, relocInfo::none);\n-    __ emit_data64(0x161514131211100f, relocInfo::none);\n-    __ emit_data64(0x8080808080191817, relocInfo::none);\n-    __ emit_data64(0x201f1e1d1c1b1a80, relocInfo::none);\n-    __ emit_data64(0x2827262524232221, relocInfo::none);\n-    __ emit_data64(0x302f2e2d2c2b2a29, relocInfo::none);\n-    __ emit_data64(0x8080808080333231, relocInfo::none);\n-    return start;\n-  }\n-  address base64_vbmi_lookup_lo_url_addr() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"lookup_lo_base64url\");\n-    address start = __ pc();\n-    assert(((unsigned long long)start & 0x3f) == 0,\n-           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n-    __ emit_data64(0x8080808080808080, relocInfo::none);\n-    __ emit_data64(0x8080808080808080, relocInfo::none);\n-    __ emit_data64(0x8080808080808080, relocInfo::none);\n-    __ emit_data64(0x8080808080808080, relocInfo::none);\n-    __ emit_data64(0x8080808080808080, relocInfo::none);\n-    __ emit_data64(0x80803e8080808080, relocInfo::none);\n-    __ emit_data64(0x3b3a393837363534, relocInfo::none);\n-    __ emit_data64(0x8080808080803d3c, relocInfo::none);\n-    return start;\n-  }\n+  \/\/ Load the encoding table based on isURL\n+  __ lea(r11, ExternalAddress(StubRoutines::x86::base64_encoding_table_addr()));\n+  __ movl(r15, isURL);\n+  __ shll(r15, 6);\n+  __ addptr(r11, r15);\n@@ -5968,16 +1887,1 @@\n-  address base64_vbmi_lookup_hi_url_addr() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"lookup_hi_base64url\");\n-    address start = __ pc();\n-    assert(((unsigned long long)start & 0x3f) == 0,\n-           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n-    __ emit_data64(0x0605040302010080, relocInfo::none);\n-    __ emit_data64(0x0e0d0c0b0a090807, relocInfo::none);\n-    __ emit_data64(0x161514131211100f, relocInfo::none);\n-    __ emit_data64(0x3f80808080191817, relocInfo::none);\n-    __ emit_data64(0x201f1e1d1c1b1a80, relocInfo::none);\n-    __ emit_data64(0x2827262524232221, relocInfo::none);\n-    __ emit_data64(0x302f2e2d2c2b2a29, relocInfo::none);\n-    __ emit_data64(0x8080808080333231, relocInfo::none);\n-    return start;\n-  }\n+  __ BIND(L_processdata);\n@@ -5985,16 +1889,4 @@\n-  address base64_vbmi_pack_vec_addr() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"pack_vec_base64\");\n-    address start = __ pc();\n-    assert(((unsigned long long)start & 0x3f) == 0,\n-           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n-    __ emit_data64(0x090a040506000102, relocInfo::none);\n-    __ emit_data64(0x161011120c0d0e08, relocInfo::none);\n-    __ emit_data64(0x1c1d1e18191a1415, relocInfo::none);\n-    __ emit_data64(0x292a242526202122, relocInfo::none);\n-    __ emit_data64(0x363031322c2d2e28, relocInfo::none);\n-    __ emit_data64(0x3c3d3e38393a3435, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    return start;\n-  }\n+  \/\/ Load 3 bytes\n+  __ load_unsigned_byte(r15, Address(source, start_offset));\n+  __ load_unsigned_byte(r10, Address(source, start_offset, Address::times_1, 1));\n+  __ load_unsigned_byte(r13, Address(source, start_offset, Address::times_1, 2));\n@@ -6002,16 +1894,4 @@\n-  address base64_vbmi_join_0_1_addr() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"join_0_1_base64\");\n-    address start = __ pc();\n-    assert(((unsigned long long)start & 0x3f) == 0,\n-           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n-    __ emit_data64(0x090a040506000102, relocInfo::none);\n-    __ emit_data64(0x161011120c0d0e08, relocInfo::none);\n-    __ emit_data64(0x1c1d1e18191a1415, relocInfo::none);\n-    __ emit_data64(0x292a242526202122, relocInfo::none);\n-    __ emit_data64(0x363031322c2d2e28, relocInfo::none);\n-    __ emit_data64(0x3c3d3e38393a3435, relocInfo::none);\n-    __ emit_data64(0x494a444546404142, relocInfo::none);\n-    __ emit_data64(0x565051524c4d4e48, relocInfo::none);\n-    return start;\n-  }\n+  \/\/ Build a 32-bit word with bytes 1, 2, 0, 1\n+  __ movl(rax, r10);\n+  __ shll(r10, 24);\n+  __ orl(rax, r10);\n@@ -6019,16 +1899,1 @@\n-  address base64_vbmi_join_1_2_addr() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"join_1_2_base64\");\n-    address start = __ pc();\n-    assert(((unsigned long long)start & 0x3f) == 0,\n-           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n-    __ emit_data64(0x1c1d1e18191a1415, relocInfo::none);\n-    __ emit_data64(0x292a242526202122, relocInfo::none);\n-    __ emit_data64(0x363031322c2d2e28, relocInfo::none);\n-    __ emit_data64(0x3c3d3e38393a3435, relocInfo::none);\n-    __ emit_data64(0x494a444546404142, relocInfo::none);\n-    __ emit_data64(0x565051524c4d4e48, relocInfo::none);\n-    __ emit_data64(0x5c5d5e58595a5455, relocInfo::none);\n-    __ emit_data64(0x696a646566606162, relocInfo::none);\n-    return start;\n-  }\n+  __ subl(length, 3);\n@@ -6036,16 +1901,3 @@\n-  address base64_vbmi_join_2_3_addr() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"join_2_3_base64\");\n-    address start = __ pc();\n-    assert(((unsigned long long)start & 0x3f) == 0,\n-           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n-    __ emit_data64(0x363031322c2d2e28, relocInfo::none);\n-    __ emit_data64(0x3c3d3e38393a3435, relocInfo::none);\n-    __ emit_data64(0x494a444546404142, relocInfo::none);\n-    __ emit_data64(0x565051524c4d4e48, relocInfo::none);\n-    __ emit_data64(0x5c5d5e58595a5455, relocInfo::none);\n-    __ emit_data64(0x696a646566606162, relocInfo::none);\n-    __ emit_data64(0x767071726c6d6e68, relocInfo::none);\n-    __ emit_data64(0x7c7d7e78797a7475, relocInfo::none);\n-    return start;\n-  }\n+  __ shll(r15, 8);\n+  __ shll(r13, 16);\n+  __ orl(rax, r15);\n@@ -6053,71 +1905,1 @@\n-  address base64_decoding_table_addr() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"decoding_table_base64\");\n-    address start = __ pc();\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0x3fffffff3effffff, relocInfo::none);\n-    __ emit_data64(0x3b3a393837363534, relocInfo::none);\n-    __ emit_data64(0xffffffffffff3d3c, relocInfo::none);\n-    __ emit_data64(0x06050403020100ff, relocInfo::none);\n-    __ emit_data64(0x0e0d0c0b0a090807, relocInfo::none);\n-    __ emit_data64(0x161514131211100f, relocInfo::none);\n-    __ emit_data64(0xffffffffff191817, relocInfo::none);\n-    __ emit_data64(0x201f1e1d1c1b1aff, relocInfo::none);\n-    __ emit_data64(0x2827262524232221, relocInfo::none);\n-    __ emit_data64(0x302f2e2d2c2b2a29, relocInfo::none);\n-    __ emit_data64(0xffffffffff333231, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-\n-    \/\/ URL table\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffff3effffffffff, relocInfo::none);\n-    __ emit_data64(0x3b3a393837363534, relocInfo::none);\n-    __ emit_data64(0xffffffffffff3d3c, relocInfo::none);\n-    __ emit_data64(0x06050403020100ff, relocInfo::none);\n-    __ emit_data64(0x0e0d0c0b0a090807, relocInfo::none);\n-    __ emit_data64(0x161514131211100f, relocInfo::none);\n-    __ emit_data64(0x3fffffffff191817, relocInfo::none);\n-    __ emit_data64(0x201f1e1d1c1b1aff, relocInfo::none);\n-    __ emit_data64(0x2827262524232221, relocInfo::none);\n-    __ emit_data64(0x302f2e2d2c2b2a29, relocInfo::none);\n-    __ emit_data64(0xffffffffff333231, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    return start;\n-  }\n+  __ addl(start_offset, 3);\n@@ -6125,0 +1907,6 @@\n+  __ orl(rax, r13);\n+  \/\/ At this point, rax contains | byte1 | byte2 | byte0 | byte1\n+  \/\/ r13 has byte2 << 16 - need low-order 6 bits to translate.\n+  \/\/ This translated byte is the fourth output byte.\n+  __ shrl(r13, 16);\n+  __ andl(r13, 0x3f);\n@@ -6126,25 +1914,3 @@\n-\/\/ Code for generating Base64 decoding.\n-\/\/\n-\/\/ Based on the article (and associated code) from https:\/\/arxiv.org\/abs\/1910.05109.\n-\/\/\n-\/\/ Intrinsic function prototype in Base64.java:\n-\/\/ private void decodeBlock(byte[] src, int sp, int sl, byte[] dst, int dp, boolean isURL, isMIME) {\n-  address generate_base64_decodeBlock() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"implDecode\");\n-    address start = __ pc();\n-    __ enter();\n-\n-    \/\/ Save callee-saved registers before using them\n-    __ push(r12);\n-    __ push(r13);\n-    __ push(r14);\n-    __ push(r15);\n-    __ push(rbx);\n-\n-    \/\/ arguments\n-    const Register source = c_rarg0; \/\/ Source Array\n-    const Register start_offset = c_rarg1; \/\/ start offset\n-    const Register end_offset = c_rarg2; \/\/ end offset\n-    const Register dest = c_rarg3; \/\/ destination array\n-    const Register isMIME = rbx;\n+  \/\/ The high-order 6 bits of r15 (byte0) is translated.\n+  \/\/ The translated byte is the first output byte.\n+  __ shrl(r15, 10);\n@@ -6152,13 +1918,2 @@\n-#ifndef _WIN64\n-    const Register dp = c_rarg4;  \/\/ Position for writing to dest array\n-    const Register isURL = c_rarg5;\/\/ Base64 or URL character set\n-    __ movl(isMIME, Address(rbp, 2 * wordSize));\n-#else\n-    const Address  dp_mem(rbp, 6 * wordSize);  \/\/ length is on stack on Win64\n-    const Address isURL_mem(rbp, 7 * wordSize);\n-    const Register isURL = r10;      \/\/ pick the volatile windows register\n-    const Register dp = r12;\n-    __ movl(dp, dp_mem);\n-    __ movl(isURL, isURL_mem);\n-    __ movl(isMIME, Address(rbp, 8 * wordSize));\n-#endif\n+  __ load_unsigned_byte(r13, Address(r11, r13));\n+  __ load_unsigned_byte(r15, Address(r11, r15));\n@@ -6166,338 +1921,1 @@\n-    const XMMRegister lookup_lo = xmm5;\n-    const XMMRegister lookup_hi = xmm6;\n-    const XMMRegister errorvec = xmm7;\n-    const XMMRegister pack16_op = xmm9;\n-    const XMMRegister pack32_op = xmm8;\n-    const XMMRegister input0 = xmm3;\n-    const XMMRegister input1 = xmm20;\n-    const XMMRegister input2 = xmm21;\n-    const XMMRegister input3 = xmm19;\n-    const XMMRegister join01 = xmm12;\n-    const XMMRegister join12 = xmm11;\n-    const XMMRegister join23 = xmm10;\n-    const XMMRegister translated0 = xmm2;\n-    const XMMRegister translated1 = xmm1;\n-    const XMMRegister translated2 = xmm0;\n-    const XMMRegister translated3 = xmm4;\n-\n-    const XMMRegister merged0 = xmm2;\n-    const XMMRegister merged1 = xmm1;\n-    const XMMRegister merged2 = xmm0;\n-    const XMMRegister merged3 = xmm4;\n-    const XMMRegister merge_ab_bc0 = xmm2;\n-    const XMMRegister merge_ab_bc1 = xmm1;\n-    const XMMRegister merge_ab_bc2 = xmm0;\n-    const XMMRegister merge_ab_bc3 = xmm4;\n-\n-    const XMMRegister pack24bits = xmm4;\n-\n-    const Register length = r14;\n-    const Register output_size = r13;\n-    const Register output_mask = r15;\n-    const KRegister input_mask = k1;\n-\n-    const XMMRegister input_initial_valid_b64 = xmm0;\n-    const XMMRegister tmp = xmm10;\n-    const XMMRegister mask = xmm0;\n-    const XMMRegister invalid_b64 = xmm1;\n-\n-    Label L_process256, L_process64, L_process64Loop, L_exit, L_processdata, L_loadURL;\n-    Label L_continue, L_finalBit, L_padding, L_donePadding, L_bruteForce;\n-    Label L_forceLoop, L_bottomLoop, L_checkMIME, L_exit_no_vzero;\n-\n-    \/\/ calculate length from offsets\n-    __ movl(length, end_offset);\n-    __ subl(length, start_offset);\n-    __ push(dest);          \/\/ Save for return value calc\n-\n-    \/\/ If AVX512 VBMI not supported, just compile non-AVX code\n-    if(VM_Version::supports_avx512_vbmi() &&\n-       VM_Version::supports_avx512bw()) {\n-      __ cmpl(length, 128);     \/\/ 128-bytes is break-even for AVX-512\n-      __ jcc(Assembler::lessEqual, L_bruteForce);\n-\n-      __ cmpl(isMIME, 0);\n-      __ jcc(Assembler::notEqual, L_bruteForce);\n-\n-      \/\/ Load lookup tables based on isURL\n-      __ cmpl(isURL, 0);\n-      __ jcc(Assembler::notZero, L_loadURL);\n-\n-      __ evmovdquq(lookup_lo, ExternalAddress(StubRoutines::x86::base64_vbmi_lookup_lo_addr()), Assembler::AVX_512bit, r13);\n-      __ evmovdquq(lookup_hi, ExternalAddress(StubRoutines::x86::base64_vbmi_lookup_hi_addr()), Assembler::AVX_512bit, r13);\n-\n-      __ BIND(L_continue);\n-\n-      __ movl(r15, 0x01400140);\n-      __ evpbroadcastd(pack16_op, r15, Assembler::AVX_512bit);\n-\n-      __ movl(r15, 0x00011000);\n-      __ evpbroadcastd(pack32_op, r15, Assembler::AVX_512bit);\n-\n-      __ cmpl(length, 0xff);\n-      __ jcc(Assembler::lessEqual, L_process64);\n-\n-      \/\/ load masks required for decoding data\n-      __ BIND(L_processdata);\n-      __ evmovdquq(join01, ExternalAddress(StubRoutines::x86::base64_vbmi_join_0_1_addr()), Assembler::AVX_512bit,r13);\n-      __ evmovdquq(join12, ExternalAddress(StubRoutines::x86::base64_vbmi_join_1_2_addr()), Assembler::AVX_512bit, r13);\n-      __ evmovdquq(join23, ExternalAddress(StubRoutines::x86::base64_vbmi_join_2_3_addr()), Assembler::AVX_512bit, r13);\n-\n-      __ align32();\n-      __ BIND(L_process256);\n-      \/\/ Grab input data\n-      __ evmovdquq(input0, Address(source, start_offset, Address::times_1, 0x00), Assembler::AVX_512bit);\n-      __ evmovdquq(input1, Address(source, start_offset, Address::times_1, 0x40), Assembler::AVX_512bit);\n-      __ evmovdquq(input2, Address(source, start_offset, Address::times_1, 0x80), Assembler::AVX_512bit);\n-      __ evmovdquq(input3, Address(source, start_offset, Address::times_1, 0xc0), Assembler::AVX_512bit);\n-\n-      \/\/ Copy the low part of the lookup table into the destination of the permutation\n-      __ evmovdquq(translated0, lookup_lo, Assembler::AVX_512bit);\n-      __ evmovdquq(translated1, lookup_lo, Assembler::AVX_512bit);\n-      __ evmovdquq(translated2, lookup_lo, Assembler::AVX_512bit);\n-      __ evmovdquq(translated3, lookup_lo, Assembler::AVX_512bit);\n-\n-      \/\/ Translate the base64 input into \"decoded\" bytes\n-      __ evpermt2b(translated0, input0, lookup_hi, Assembler::AVX_512bit);\n-      __ evpermt2b(translated1, input1, lookup_hi, Assembler::AVX_512bit);\n-      __ evpermt2b(translated2, input2, lookup_hi, Assembler::AVX_512bit);\n-      __ evpermt2b(translated3, input3, lookup_hi, Assembler::AVX_512bit);\n-\n-      \/\/ OR all of the translations together to check for errors (high-order bit of byte set)\n-      __ vpternlogd(input0, 0xfe, input1, input2, Assembler::AVX_512bit);\n-\n-      __ vpternlogd(input3, 0xfe, translated0, translated1, Assembler::AVX_512bit);\n-      __ vpternlogd(input0, 0xfe, translated2, translated3, Assembler::AVX_512bit);\n-      __ vpor(errorvec, input3, input0, Assembler::AVX_512bit);\n-\n-      \/\/ Check if there was an error - if so, try 64-byte chunks\n-      __ evpmovb2m(k3, errorvec, Assembler::AVX_512bit);\n-      __ kortestql(k3, k3);\n-      __ jcc(Assembler::notZero, L_process64);\n-\n-      \/\/ The merging and shuffling happens here\n-      \/\/ We multiply each byte pair [00dddddd | 00cccccc | 00bbbbbb | 00aaaaaa]\n-      \/\/ Multiply [00cccccc] by 2^6 added to [00dddddd] to get [0000cccc | ccdddddd]\n-      \/\/ The pack16_op is a vector of 0x01400140, so multiply D by 1 and C by 0x40\n-      __ vpmaddubsw(merge_ab_bc0, translated0, pack16_op, Assembler::AVX_512bit);\n-      __ vpmaddubsw(merge_ab_bc1, translated1, pack16_op, Assembler::AVX_512bit);\n-      __ vpmaddubsw(merge_ab_bc2, translated2, pack16_op, Assembler::AVX_512bit);\n-      __ vpmaddubsw(merge_ab_bc3, translated3, pack16_op, Assembler::AVX_512bit);\n-\n-      \/\/ Now do the same with packed 16-bit values.\n-      \/\/ We start with [0000cccc | ccdddddd | 0000aaaa | aabbbbbb]\n-      \/\/ pack32_op is 0x00011000 (2^12, 1), so this multiplies [0000aaaa | aabbbbbb] by 2^12\n-      \/\/ and adds [0000cccc | ccdddddd] to yield [00000000 | aaaaaabb | bbbbcccc | ccdddddd]\n-      __ vpmaddwd(merged0, merge_ab_bc0, pack32_op, Assembler::AVX_512bit);\n-      __ vpmaddwd(merged1, merge_ab_bc1, pack32_op, Assembler::AVX_512bit);\n-      __ vpmaddwd(merged2, merge_ab_bc2, pack32_op, Assembler::AVX_512bit);\n-      __ vpmaddwd(merged3, merge_ab_bc3, pack32_op, Assembler::AVX_512bit);\n-\n-      \/\/ The join vectors specify which byte from which vector goes into the outputs\n-      \/\/ One of every 4 bytes in the extended vector is zero, so we pack them into their\n-      \/\/ final positions in the register for storing (256 bytes in, 192 bytes out)\n-      __ evpermt2b(merged0, join01, merged1, Assembler::AVX_512bit);\n-      __ evpermt2b(merged1, join12, merged2, Assembler::AVX_512bit);\n-      __ evpermt2b(merged2, join23, merged3, Assembler::AVX_512bit);\n-\n-      \/\/ Store result\n-      __ evmovdquq(Address(dest, dp, Address::times_1, 0x00), merged0, Assembler::AVX_512bit);\n-      __ evmovdquq(Address(dest, dp, Address::times_1, 0x40), merged1, Assembler::AVX_512bit);\n-      __ evmovdquq(Address(dest, dp, Address::times_1, 0x80), merged2, Assembler::AVX_512bit);\n-\n-      __ addptr(source, 0x100);\n-      __ addptr(dest, 0xc0);\n-      __ subl(length, 0x100);\n-      __ cmpl(length, 64 * 4);\n-      __ jcc(Assembler::greaterEqual, L_process256);\n-\n-      \/\/ At this point, we've decoded 64 * 4 * n bytes.\n-      \/\/ The remaining length will be <= 64 * 4 - 1.\n-      \/\/ UNLESS there was an error decoding the first 256-byte chunk.  In this\n-      \/\/ case, the length will be arbitrarily long.\n-      \/\/\n-      \/\/ Note that this will be the path for MIME-encoded strings.\n-\n-      __ BIND(L_process64);\n-\n-      __ evmovdquq(pack24bits, ExternalAddress(StubRoutines::x86::base64_vbmi_pack_vec_addr()), Assembler::AVX_512bit, r13);\n-\n-      __ cmpl(length, 63);\n-      __ jcc(Assembler::lessEqual, L_finalBit);\n-\n-      __ mov64(rax, 0x0000ffffffffffff);\n-      __ kmovql(k2, rax);\n-\n-      __ align32();\n-      __ BIND(L_process64Loop);\n-\n-      \/\/ Handle first 64-byte block\n-\n-      __ evmovdquq(input0, Address(source, start_offset), Assembler::AVX_512bit);\n-      __ evmovdquq(translated0, lookup_lo, Assembler::AVX_512bit);\n-      __ evpermt2b(translated0, input0, lookup_hi, Assembler::AVX_512bit);\n-\n-      __ vpor(errorvec, translated0, input0, Assembler::AVX_512bit);\n-\n-      \/\/ Check for error and bomb out before updating dest\n-      __ evpmovb2m(k3, errorvec, Assembler::AVX_512bit);\n-      __ kortestql(k3, k3);\n-      __ jcc(Assembler::notZero, L_exit);\n-\n-      \/\/ Pack output register, selecting correct byte ordering\n-      __ vpmaddubsw(merge_ab_bc0, translated0, pack16_op, Assembler::AVX_512bit);\n-      __ vpmaddwd(merged0, merge_ab_bc0, pack32_op, Assembler::AVX_512bit);\n-      __ vpermb(merged0, pack24bits, merged0, Assembler::AVX_512bit);\n-\n-      __ evmovdqub(Address(dest, dp), k2, merged0, true, Assembler::AVX_512bit);\n-\n-      __ subl(length, 64);\n-      __ addptr(source, 64);\n-      __ addptr(dest, 48);\n-\n-      __ cmpl(length, 64);\n-      __ jcc(Assembler::greaterEqual, L_process64Loop);\n-\n-      __ cmpl(length, 0);\n-      __ jcc(Assembler::lessEqual, L_exit);\n-\n-      __ BIND(L_finalBit);\n-      \/\/ Now have 1 to 63 bytes left to decode\n-\n-      \/\/ I was going to let Java take care of the final fragment\n-      \/\/ however it will repeatedly call this routine for every 4 bytes\n-      \/\/ of input data, so handle the rest here.\n-      __ movq(rax, -1);\n-      __ bzhiq(rax, rax, length);    \/\/ Input mask in rax\n-\n-      __ movl(output_size, length);\n-      __ shrl(output_size, 2);   \/\/ Find (len \/ 4) * 3 (output length)\n-      __ lea(output_size, Address(output_size, output_size, Address::times_2, 0));\n-      \/\/ output_size in r13\n-\n-      \/\/ Strip pad characters, if any, and adjust length and mask\n-      __ cmpb(Address(source, length, Address::times_1, -1), '=');\n-      __ jcc(Assembler::equal, L_padding);\n-\n-      __ BIND(L_donePadding);\n-\n-      \/\/ Output size is (64 - output_size), output mask is (all 1s >> output_size).\n-      __ kmovql(input_mask, rax);\n-      __ movq(output_mask, -1);\n-      __ bzhiq(output_mask, output_mask, output_size);\n-\n-      \/\/ Load initial input with all valid base64 characters.  Will be used\n-      \/\/ in merging source bytes to avoid masking when determining if an error occurred.\n-      __ movl(rax, 0x61616161);\n-      __ evpbroadcastd(input_initial_valid_b64, rax, Assembler::AVX_512bit);\n-\n-      \/\/ A register containing all invalid base64 decoded values\n-      __ movl(rax, 0x80808080);\n-      __ evpbroadcastd(invalid_b64, rax, Assembler::AVX_512bit);\n-\n-      \/\/ input_mask is in k1\n-      \/\/ output_size is in r13\n-      \/\/ output_mask is in r15\n-      \/\/ zmm0 - free\n-      \/\/ zmm1 - 0x00011000\n-      \/\/ zmm2 - 0x01400140\n-      \/\/ zmm3 - errorvec\n-      \/\/ zmm4 - pack vector\n-      \/\/ zmm5 - lookup_lo\n-      \/\/ zmm6 - lookup_hi\n-      \/\/ zmm7 - errorvec\n-      \/\/ zmm8 - 0x61616161\n-      \/\/ zmm9 - 0x80808080\n-\n-      \/\/ Load only the bytes from source, merging into our \"fully-valid\" register\n-      __ evmovdqub(input_initial_valid_b64, input_mask, Address(source, start_offset, Address::times_1, 0x0), true, Assembler::AVX_512bit);\n-\n-      \/\/ Decode all bytes within our merged input\n-      __ evmovdquq(tmp, lookup_lo, Assembler::AVX_512bit);\n-      __ evpermt2b(tmp, input_initial_valid_b64, lookup_hi, Assembler::AVX_512bit);\n-      __ vporq(mask, tmp, input_initial_valid_b64, Assembler::AVX_512bit);\n-\n-      \/\/ Check for error.  Compare (decoded | initial) to all invalid.\n-      \/\/ If any bytes have their high-order bit set, then we have an error.\n-      __ evptestmb(k2, mask, invalid_b64, Assembler::AVX_512bit);\n-      __ kortestql(k2, k2);\n-\n-      \/\/ If we have an error, use the brute force loop to decode what we can (4-byte chunks).\n-      __ jcc(Assembler::notZero, L_bruteForce);\n-\n-      \/\/ Shuffle output bytes\n-      __ vpmaddubsw(tmp, tmp, pack16_op, Assembler::AVX_512bit);\n-      __ vpmaddwd(tmp, tmp, pack32_op, Assembler::AVX_512bit);\n-\n-      __ vpermb(tmp, pack24bits, tmp, Assembler::AVX_512bit);\n-      __ kmovql(k1, output_mask);\n-      __ evmovdqub(Address(dest, dp), k1, tmp, true, Assembler::AVX_512bit);\n-\n-      __ addptr(dest, output_size);\n-\n-      __ BIND(L_exit);\n-      __ vzeroupper();\n-      __ pop(rax);             \/\/ Get original dest value\n-      __ subptr(dest, rax);      \/\/ Number of bytes converted\n-      __ movptr(rax, dest);\n-      __ pop(rbx);\n-      __ pop(r15);\n-      __ pop(r14);\n-      __ pop(r13);\n-      __ pop(r12);\n-      __ leave();\n-      __ ret(0);\n-\n-      __ BIND(L_loadURL);\n-      __ evmovdquq(lookup_lo, ExternalAddress(StubRoutines::x86::base64_vbmi_lookup_lo_url_addr()), Assembler::AVX_512bit, r13);\n-      __ evmovdquq(lookup_hi, ExternalAddress(StubRoutines::x86::base64_vbmi_lookup_hi_url_addr()), Assembler::AVX_512bit, r13);\n-      __ jmp(L_continue);\n-\n-      __ BIND(L_padding);\n-      __ decrementq(output_size, 1);\n-      __ shrq(rax, 1);\n-\n-      __ cmpb(Address(source, length, Address::times_1, -2), '=');\n-      __ jcc(Assembler::notEqual, L_donePadding);\n-\n-      __ decrementq(output_size, 1);\n-      __ shrq(rax, 1);\n-      __ jmp(L_donePadding);\n-\n-      __ align32();\n-      __ BIND(L_bruteForce);\n-    }   \/\/ End of if(avx512_vbmi)\n-\n-    \/\/ Use non-AVX code to decode 4-byte chunks into 3 bytes of output\n-\n-    \/\/ Register state (Linux):\n-    \/\/ r12-15 - saved on stack\n-    \/\/ rdi - src\n-    \/\/ rsi - sp\n-    \/\/ rdx - sl\n-    \/\/ rcx - dst\n-    \/\/ r8 - dp\n-    \/\/ r9 - isURL\n-\n-    \/\/ Register state (Windows):\n-    \/\/ r12-15 - saved on stack\n-    \/\/ rcx - src\n-    \/\/ rdx - sp\n-    \/\/ r8 - sl\n-    \/\/ r9 - dst\n-    \/\/ r12 - dp\n-    \/\/ r10 - isURL\n-\n-    \/\/ Registers (common):\n-    \/\/ length (r14) - bytes in src\n-\n-    const Register decode_table = r11;\n-    const Register out_byte_count = rbx;\n-    const Register byte1 = r13;\n-    const Register byte2 = r15;\n-    const Register byte3 = WINDOWS_ONLY(r8) NOT_WINDOWS(rdx);\n-    const Register byte4 = WINDOWS_ONLY(r10) NOT_WINDOWS(r9);\n-\n-    __ shrl(length, 2);    \/\/ Multiple of 4 bytes only - length is # 4-byte chunks\n-    __ cmpl(length, 0);\n-    __ jcc(Assembler::lessEqual, L_exit_no_vzero);\n+  __ movb(Address(dest, dp, Address::times_1, 3), r13);\n@@ -6505,3 +1923,5 @@\n-    __ shll(isURL, 8);    \/\/ index into decode table based on isURL\n-    __ lea(decode_table, ExternalAddress(StubRoutines::x86::base64_decoding_table_addr()));\n-    __ addptr(decode_table, isURL);\n+  \/\/ Extract high-order 4 bits of byte1 and low-order 2 bits of byte0.\n+  \/\/ This translated byte is the second output byte.\n+  __ shrl(rax, 4);\n+  __ movl(r10, rax);\n+  __ andl(rax, 0x3f);\n@@ -6509,1 +1929,1 @@\n-    __ jmp(L_bottomLoop);\n+  __ movb(Address(dest, dp, Address::times_1, 0), r15);\n@@ -6511,48 +1931,1 @@\n-    __ align32();\n-    __ BIND(L_forceLoop);\n-    __ shll(byte1, 18);\n-    __ shll(byte2, 12);\n-    __ shll(byte3, 6);\n-    __ orl(byte1, byte2);\n-    __ orl(byte1, byte3);\n-    __ orl(byte1, byte4);\n-\n-    __ addptr(source, 4);\n-\n-    __ movb(Address(dest, dp, Address::times_1, 2), byte1);\n-    __ shrl(byte1, 8);\n-    __ movb(Address(dest, dp, Address::times_1, 1), byte1);\n-    __ shrl(byte1, 8);\n-    __ movb(Address(dest, dp, Address::times_1, 0), byte1);\n-\n-    __ addptr(dest, 3);\n-    __ decrementl(length, 1);\n-    __ jcc(Assembler::zero, L_exit_no_vzero);\n-\n-    __ BIND(L_bottomLoop);\n-    __ load_unsigned_byte(byte1, Address(source, start_offset, Address::times_1, 0x00));\n-    __ load_unsigned_byte(byte2, Address(source, start_offset, Address::times_1, 0x01));\n-    __ load_signed_byte(byte1, Address(decode_table, byte1));\n-    __ load_signed_byte(byte2, Address(decode_table, byte2));\n-    __ load_unsigned_byte(byte3, Address(source, start_offset, Address::times_1, 0x02));\n-    __ load_unsigned_byte(byte4, Address(source, start_offset, Address::times_1, 0x03));\n-    __ load_signed_byte(byte3, Address(decode_table, byte3));\n-    __ load_signed_byte(byte4, Address(decode_table, byte4));\n-\n-    __ mov(rax, byte1);\n-    __ orl(rax, byte2);\n-    __ orl(rax, byte3);\n-    __ orl(rax, byte4);\n-    __ jcc(Assembler::positive, L_forceLoop);\n-\n-    __ BIND(L_exit_no_vzero);\n-    __ pop(rax);             \/\/ Get original dest value\n-    __ subptr(dest, rax);      \/\/ Number of bytes converted\n-    __ movptr(rax, dest);\n-    __ pop(rbx);\n-    __ pop(r15);\n-    __ pop(r14);\n-    __ pop(r13);\n-    __ pop(r12);\n-    __ leave();\n-    __ ret(0);\n+  __ load_unsigned_byte(rax, Address(r11, rax));\n@@ -6560,2 +1933,4 @@\n-    return start;\n-  }\n+  \/\/ Extract low-order 2 bits of byte1 and high-order 4 bits of byte2.\n+  \/\/ This translated byte is the third output byte.\n+  __ shrl(r10, 18);\n+  __ andl(r10, 0x3f);\n@@ -6563,0 +1938,1 @@\n+  __ load_unsigned_byte(r10, Address(r11, r10));\n@@ -6564,45 +1940,2 @@\n-  \/**\n-   *  Arguments:\n-   *\n-   * Inputs:\n-   *   c_rarg0   - int crc\n-   *   c_rarg1   - byte* buf\n-   *   c_rarg2   - int length\n-   *\n-   * Output:\n-   *       rax   - int crc result\n-   *\/\n-  address generate_updateBytesCRC32() {\n-    assert(UseCRC32Intrinsics, \"need AVX and CLMUL instructions\");\n-\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"updateBytesCRC32\");\n-\n-    address start = __ pc();\n-    \/\/ Win64: rcx, rdx, r8, r9 (c_rarg0, c_rarg1, ...)\n-    \/\/ Unix:  rdi, rsi, rdx, rcx, r8, r9 (c_rarg0, c_rarg1, ...)\n-    \/\/ rscratch1: r10\n-    const Register crc   = c_rarg0;  \/\/ crc\n-    const Register buf   = c_rarg1;  \/\/ source java byte array address\n-    const Register len   = c_rarg2;  \/\/ length\n-    const Register table = c_rarg3;  \/\/ crc_table address (reuse register)\n-    const Register tmp1   = r11;\n-    const Register tmp2   = r10;\n-    assert_different_registers(crc, buf, len, table, tmp1, tmp2, rax);\n-\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-    if (VM_Version::supports_sse4_1() && VM_Version::supports_avx512_vpclmulqdq() &&\n-        VM_Version::supports_avx512bw() &&\n-        VM_Version::supports_avx512vl()) {\n-        \/\/ The constants used in the CRC32 algorithm requires the 1's compliment of the initial crc value.\n-        \/\/ However, the constant table for CRC32-C assumes the original crc value.  Account for this\n-        \/\/ difference before calling and after returning.\n-      __ lea(table, ExternalAddress(StubRoutines::x86::crc_table_avx512_addr()));\n-      __ notl(crc);\n-      __ kernel_crc32_avx512(crc, buf, len, table, tmp1, tmp2);\n-      __ notl(crc);\n-    } else {\n-      __ kernel_crc32(crc, buf, len, table, tmp1);\n-    }\n+  __ movb(Address(dest, dp, Address::times_1, 1), rax);\n+  __ movb(Address(dest, dp, Address::times_1, 2), r10);\n@@ -6610,4 +1943,3 @@\n-    __ movl(rax, crc);\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+  __ addl(dp, 4);\n+  __ cmpl(length, 3);\n+  __ jcc(Assembler::aboveEqual, L_processdata);\n@@ -6615,2 +1947,7 @@\n-    return start;\n-  }\n+  __ BIND(L_exit);\n+  __ pop(r15);\n+  __ pop(r14);\n+  __ pop(r13);\n+  __ pop(r12);\n+  __ leave();\n+  __ ret(0);\n@@ -6618,63 +1955,2 @@\n-  \/**\n-  *  Arguments:\n-  *\n-  * Inputs:\n-  *   c_rarg0   - int crc\n-  *   c_rarg1   - byte* buf\n-  *   c_rarg2   - long length\n-  *   c_rarg3   - table_start - optional (present only when doing a library_call,\n-  *              not used by x86 algorithm)\n-  *\n-  * Output:\n-  *       rax   - int crc result\n-  *\/\n-  address generate_updateBytesCRC32C(bool is_pclmulqdq_supported) {\n-      assert(UseCRC32CIntrinsics, \"need SSE4_2\");\n-      __ align(CodeEntryAlignment);\n-      StubCodeMark mark(this, \"StubRoutines\", \"updateBytesCRC32C\");\n-      address start = __ pc();\n-      \/\/reg.arg        int#0        int#1        int#2        int#3        int#4        int#5        float regs\n-      \/\/Windows        RCX          RDX          R8           R9           none         none         XMM0..XMM3\n-      \/\/Lin \/ Sol      RDI          RSI          RDX          RCX          R8           R9           XMM0..XMM7\n-      const Register crc = c_rarg0;  \/\/ crc\n-      const Register buf = c_rarg1;  \/\/ source java byte array address\n-      const Register len = c_rarg2;  \/\/ length\n-      const Register a = rax;\n-      const Register j = r9;\n-      const Register k = r10;\n-      const Register l = r11;\n-#ifdef _WIN64\n-      const Register y = rdi;\n-      const Register z = rsi;\n-#else\n-      const Register y = rcx;\n-      const Register z = r8;\n-#endif\n-      assert_different_registers(crc, buf, len, a, j, k, l, y, z);\n-\n-      BLOCK_COMMENT(\"Entry:\");\n-      __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-      if (VM_Version::supports_sse4_1() && VM_Version::supports_avx512_vpclmulqdq() &&\n-          VM_Version::supports_avx512bw() &&\n-          VM_Version::supports_avx512vl()) {\n-        __ lea(j, ExternalAddress(StubRoutines::x86::crc32c_table_avx512_addr()));\n-        __ kernel_crc32_avx512(crc, buf, len, j, l, k);\n-      } else {\n-#ifdef _WIN64\n-        __ push(y);\n-        __ push(z);\n-#endif\n-        __ crc32c_ipl_alg2_alt2(crc, buf, len,\n-                                a, j, k,\n-                                l, y, z,\n-                                c_farg0, c_farg1, c_farg2,\n-                                is_pclmulqdq_supported);\n-#ifdef _WIN64\n-        __ pop(z);\n-        __ pop(y);\n-#endif\n-      }\n-      __ movl(rax, crc);\n-      __ vzeroupper();\n-      __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-      __ ret(0);\n+  return start;\n+}\n@@ -6682,2 +1958,16 @@\n-      return start;\n-  }\n+\/\/ base64 AVX512vbmi tables\n+address StubGenerator::base64_vbmi_lookup_lo_addr() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"lookup_lo_base64\");\n+  address start = __ pc();\n+\n+  assert(((unsigned long long)start & 0x3f) == 0,\n+         \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+  __ emit_data64(0x8080808080808080, relocInfo::none);\n+  __ emit_data64(0x8080808080808080, relocInfo::none);\n+  __ emit_data64(0x8080808080808080, relocInfo::none);\n+  __ emit_data64(0x8080808080808080, relocInfo::none);\n+  __ emit_data64(0x8080808080808080, relocInfo::none);\n+  __ emit_data64(0x3f8080803e808080, relocInfo::none);\n+  __ emit_data64(0x3b3a393837363534, relocInfo::none);\n+  __ emit_data64(0x8080808080803d3c, relocInfo::none);\n@@ -6685,0 +1975,2 @@\n+  return start;\n+}\n@@ -6686,39 +1978,15 @@\n-  \/***\n-   *  Arguments:\n-   *\n-   *  Inputs:\n-   *   c_rarg0   - int   adler\n-   *   c_rarg1   - byte* buff\n-   *   c_rarg2   - int   len\n-   *\n-   * Output:\n-   *   rax   - int adler result\n-   *\/\n-\n-  address generate_updateBytesAdler32() {\n-      assert(UseAdler32Intrinsics, \"need AVX2\");\n-\n-      __ align(CodeEntryAlignment);\n-      StubCodeMark mark(this, \"StubRoutines\", \"updateBytesAdler32\");\n-\n-      address start = __ pc();\n-\n-      const Register data = r9;\n-      const Register size = r10;\n-\n-      const XMMRegister yshuf0 = xmm6;\n-      const XMMRegister yshuf1 = xmm7;\n-      assert_different_registers(c_rarg0, c_rarg1, c_rarg2, data, size);\n-\n-      BLOCK_COMMENT(\"Entry:\");\n-      __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-      __ vmovdqu(yshuf0, ExternalAddress((address) StubRoutines::x86::_adler32_shuf0_table), r9);\n-      __ vmovdqu(yshuf1, ExternalAddress((address) StubRoutines::x86::_adler32_shuf1_table), r9);\n-      __ movptr(data, c_rarg1); \/\/data\n-      __ movl(size, c_rarg2); \/\/length\n-      __ updateBytesAdler32(c_rarg0, data, size, yshuf0, yshuf1, ExternalAddress((address) StubRoutines::x86::_adler32_ascale_table));\n-      __ leave();\n-      __ ret(0);\n-      return start;\n-  }\n+address StubGenerator::base64_vbmi_lookup_hi_addr() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"lookup_hi_base64\");\n+  address start = __ pc();\n+\n+  assert(((unsigned long long)start & 0x3f) == 0,\n+         \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+  __ emit_data64(0x0605040302010080, relocInfo::none);\n+  __ emit_data64(0x0e0d0c0b0a090807, relocInfo::none);\n+  __ emit_data64(0x161514131211100f, relocInfo::none);\n+  __ emit_data64(0x8080808080191817, relocInfo::none);\n+  __ emit_data64(0x201f1e1d1c1b1a80, relocInfo::none);\n+  __ emit_data64(0x2827262524232221, relocInfo::none);\n+  __ emit_data64(0x302f2e2d2c2b2a29, relocInfo::none);\n+  __ emit_data64(0x8080808080333231, relocInfo::none);\n@@ -6726,38 +1994,17 @@\n-  \/**\n-   *  Arguments:\n-   *\n-   *  Input:\n-   *    c_rarg0   - x address\n-   *    c_rarg1   - x length\n-   *    c_rarg2   - y address\n-   *    c_rarg3   - y length\n-   * not Win64\n-   *    c_rarg4   - z address\n-   *    c_rarg5   - z length\n-   * Win64\n-   *    rsp+40    - z address\n-   *    rsp+48    - z length\n-   *\/\n-  address generate_multiplyToLen() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"multiplyToLen\");\n-\n-    address start = __ pc();\n-    \/\/ Win64: rcx, rdx, r8, r9 (c_rarg0, c_rarg1, ...)\n-    \/\/ Unix:  rdi, rsi, rdx, rcx, r8, r9 (c_rarg0, c_rarg1, ...)\n-    const Register x     = rdi;\n-    const Register xlen  = rax;\n-    const Register y     = rsi;\n-    const Register ylen  = rcx;\n-    const Register z     = r8;\n-    const Register zlen  = r11;\n-\n-    \/\/ Next registers will be saved on stack in multiply_to_len().\n-    const Register tmp1  = r12;\n-    const Register tmp2  = r13;\n-    const Register tmp3  = r14;\n-    const Register tmp4  = r15;\n-    const Register tmp5  = rbx;\n-\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  return start;\n+}\n+address StubGenerator::base64_vbmi_lookup_lo_url_addr() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"lookup_lo_base64url\");\n+  address start = __ pc();\n+\n+  assert(((unsigned long long)start & 0x3f) == 0,\n+         \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+  __ emit_data64(0x8080808080808080, relocInfo::none);\n+  __ emit_data64(0x8080808080808080, relocInfo::none);\n+  __ emit_data64(0x8080808080808080, relocInfo::none);\n+  __ emit_data64(0x8080808080808080, relocInfo::none);\n+  __ emit_data64(0x8080808080808080, relocInfo::none);\n+  __ emit_data64(0x80803e8080808080, relocInfo::none);\n+  __ emit_data64(0x3b3a393837363534, relocInfo::none);\n+  __ emit_data64(0x8080808080803d3c, relocInfo::none);\n@@ -6765,11 +2012,2 @@\n-#ifndef _WIN64\n-    __ movptr(zlen, r9); \/\/ Save r9 in r11 - zlen\n-#endif\n-    setup_arg_regs(4); \/\/ x => rdi, xlen => rsi, y => rdx\n-                       \/\/ ylen => rcx, z => r8, zlen => r11\n-                       \/\/ r9 and r10 may be used to save non-volatile registers\n-#ifdef _WIN64\n-    \/\/ last 2 arguments (#4, #5) are on stack on Win64\n-    __ movptr(z, Address(rsp, 6 * wordSize));\n-    __ movptr(zlen, Address(rsp, 7 * wordSize));\n-#endif\n+  return start;\n+}\n@@ -6777,3 +2015,15 @@\n-    __ movptr(xlen, rsi);\n-    __ movptr(y,    rdx);\n-    __ multiply_to_len(x, xlen, y, ylen, z, zlen, tmp1, tmp2, tmp3, tmp4, tmp5);\n+address StubGenerator::base64_vbmi_lookup_hi_url_addr() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"lookup_hi_base64url\");\n+  address start = __ pc();\n+\n+  assert(((unsigned long long)start & 0x3f) == 0,\n+         \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+  __ emit_data64(0x0605040302010080, relocInfo::none);\n+  __ emit_data64(0x0e0d0c0b0a090807, relocInfo::none);\n+  __ emit_data64(0x161514131211100f, relocInfo::none);\n+  __ emit_data64(0x3f80808080191817, relocInfo::none);\n+  __ emit_data64(0x201f1e1d1c1b1a80, relocInfo::none);\n+  __ emit_data64(0x2827262524232221, relocInfo::none);\n+  __ emit_data64(0x302f2e2d2c2b2a29, relocInfo::none);\n+  __ emit_data64(0x8080808080333231, relocInfo::none);\n@@ -6781,1 +2031,2 @@\n-    restore_arg_regs();\n+  return start;\n+}\n@@ -6783,2 +2034,15 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+address StubGenerator::base64_vbmi_pack_vec_addr() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"pack_vec_base64\");\n+  address start = __ pc();\n+\n+  assert(((unsigned long long)start & 0x3f) == 0,\n+         \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+  __ emit_data64(0x090a040506000102, relocInfo::none);\n+  __ emit_data64(0x161011120c0d0e08, relocInfo::none);\n+  __ emit_data64(0x1c1d1e18191a1415, relocInfo::none);\n+  __ emit_data64(0x292a242526202122, relocInfo::none);\n+  __ emit_data64(0x363031322c2d2e28, relocInfo::none);\n+  __ emit_data64(0x3c3d3e38393a3435, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n@@ -6786,2 +2050,2 @@\n-    return start;\n-  }\n+  return start;\n+}\n@@ -6789,19 +2053,15 @@\n-  \/**\n-  *  Arguments:\n-  *\n-  *  Input:\n-  *    c_rarg0   - obja     address\n-  *    c_rarg1   - objb     address\n-  *    c_rarg3   - length   length\n-  *    c_rarg4   - scale    log2_array_indxscale\n-  *\n-  *  Output:\n-  *        rax   - int >= mismatched index, < 0 bitwise complement of tail\n-  *\/\n-  address generate_vectorizedMismatch() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"vectorizedMismatch\");\n-    address start = __ pc();\n-\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter();\n+address StubGenerator::base64_vbmi_join_0_1_addr() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"join_0_1_base64\");\n+  address start = __ pc();\n+\n+  assert(((unsigned long long)start & 0x3f) == 0,\n+         \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+  __ emit_data64(0x090a040506000102, relocInfo::none);\n+  __ emit_data64(0x161011120c0d0e08, relocInfo::none);\n+  __ emit_data64(0x1c1d1e18191a1415, relocInfo::none);\n+  __ emit_data64(0x292a242526202122, relocInfo::none);\n+  __ emit_data64(0x363031322c2d2e28, relocInfo::none);\n+  __ emit_data64(0x3c3d3e38393a3435, relocInfo::none);\n+  __ emit_data64(0x494a444546404142, relocInfo::none);\n+  __ emit_data64(0x565051524c4d4e48, relocInfo::none);\n@@ -6809,22 +2069,2 @@\n-#ifdef _WIN64  \/\/ Win64: rcx, rdx, r8, r9 (c_rarg0, c_rarg1, ...)\n-    const Register scale = c_rarg0;  \/\/rcx, will exchange with r9\n-    const Register objb = c_rarg1;   \/\/rdx\n-    const Register length = c_rarg2; \/\/r8\n-    const Register obja = c_rarg3;   \/\/r9\n-    __ xchgq(obja, scale);  \/\/now obja and scale contains the correct contents\n-\n-    const Register tmp1 = r10;\n-    const Register tmp2 = r11;\n-#endif\n-#ifndef _WIN64 \/\/ Unix:  rdi, rsi, rdx, rcx, r8, r9 (c_rarg0, c_rarg1, ...)\n-    const Register obja = c_rarg0;   \/\/U:rdi\n-    const Register objb = c_rarg1;   \/\/U:rsi\n-    const Register length = c_rarg2; \/\/U:rdx\n-    const Register scale = c_rarg3;  \/\/U:rcx\n-    const Register tmp1 = r8;\n-    const Register tmp2 = r9;\n-#endif\n-    const Register result = rax; \/\/return value\n-    const XMMRegister vec0 = xmm0;\n-    const XMMRegister vec1 = xmm1;\n-    const XMMRegister vec2 = xmm2;\n+  return start;\n+}\n@@ -6832,1 +2072,15 @@\n-    __ vectorized_mismatch(obja, objb, length, scale, result, tmp1, tmp2, vec0, vec1, vec2);\n+address StubGenerator::base64_vbmi_join_1_2_addr() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"join_1_2_base64\");\n+  address start = __ pc();\n+\n+  assert(((unsigned long long)start & 0x3f) == 0,\n+         \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+  __ emit_data64(0x1c1d1e18191a1415, relocInfo::none);\n+  __ emit_data64(0x292a242526202122, relocInfo::none);\n+  __ emit_data64(0x363031322c2d2e28, relocInfo::none);\n+  __ emit_data64(0x3c3d3e38393a3435, relocInfo::none);\n+  __ emit_data64(0x494a444546404142, relocInfo::none);\n+  __ emit_data64(0x565051524c4d4e48, relocInfo::none);\n+  __ emit_data64(0x5c5d5e58595a5455, relocInfo::none);\n+  __ emit_data64(0x696a646566606162, relocInfo::none);\n@@ -6834,3 +2088,2 @@\n-    __ vzeroupper();\n-    __ leave();\n-    __ ret(0);\n+  return start;\n+}\n@@ -6838,2 +2091,15 @@\n-    return start;\n-  }\n+address StubGenerator::base64_vbmi_join_2_3_addr() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"join_2_3_base64\");\n+  address start = __ pc();\n+\n+  assert(((unsigned long long)start & 0x3f) == 0,\n+         \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+  __ emit_data64(0x363031322c2d2e28, relocInfo::none);\n+  __ emit_data64(0x3c3d3e38393a3435, relocInfo::none);\n+  __ emit_data64(0x494a444546404142, relocInfo::none);\n+  __ emit_data64(0x565051524c4d4e48, relocInfo::none);\n+  __ emit_data64(0x5c5d5e58595a5455, relocInfo::none);\n+  __ emit_data64(0x696a646566606162, relocInfo::none);\n+  __ emit_data64(0x767071726c6d6e68, relocInfo::none);\n+  __ emit_data64(0x7c7d7e78797a7475, relocInfo::none);\n@@ -6841,37 +2107,2 @@\n-\/**\n-   *  Arguments:\n-   *\n-  \/\/  Input:\n-  \/\/    c_rarg0   - x address\n-  \/\/    c_rarg1   - x length\n-  \/\/    c_rarg2   - z address\n-  \/\/    c_rarg3   - z length\n-   *\n-   *\/\n-  address generate_squareToLen() {\n-\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"squareToLen\");\n-\n-    address start = __ pc();\n-    \/\/ Win64: rcx, rdx, r8, r9 (c_rarg0, c_rarg1, ...)\n-    \/\/ Unix:  rdi, rsi, rdx, rcx (c_rarg0, c_rarg1, ...)\n-    const Register x      = rdi;\n-    const Register len    = rsi;\n-    const Register z      = r8;\n-    const Register zlen   = rcx;\n-\n-   const Register tmp1      = r12;\n-   const Register tmp2      = r13;\n-   const Register tmp3      = r14;\n-   const Register tmp4      = r15;\n-   const Register tmp5      = rbx;\n-\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-    setup_arg_regs(4); \/\/ x => rdi, len => rsi, z => rdx\n-                       \/\/ zlen => rcx\n-                       \/\/ r9 and r10 may be used to save non-volatile registers\n-    __ movptr(r8, rdx);\n-    __ square_to_len(x, len, z, zlen, tmp1, tmp2, tmp3, tmp4, tmp5, rdx, rax);\n+  return start;\n+}\n@@ -6879,1 +2110,70 @@\n-    restore_arg_regs();\n+address StubGenerator::base64_decoding_table_addr() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"decoding_table_base64\");\n+  address start = __ pc();\n+\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0x3fffffff3effffff, relocInfo::none);\n+  __ emit_data64(0x3b3a393837363534, relocInfo::none);\n+  __ emit_data64(0xffffffffffff3d3c, relocInfo::none);\n+  __ emit_data64(0x06050403020100ff, relocInfo::none);\n+  __ emit_data64(0x0e0d0c0b0a090807, relocInfo::none);\n+  __ emit_data64(0x161514131211100f, relocInfo::none);\n+  __ emit_data64(0xffffffffff191817, relocInfo::none);\n+  __ emit_data64(0x201f1e1d1c1b1aff, relocInfo::none);\n+  __ emit_data64(0x2827262524232221, relocInfo::none);\n+  __ emit_data64(0x302f2e2d2c2b2a29, relocInfo::none);\n+  __ emit_data64(0xffffffffff333231, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+\n+  \/\/ URL table\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffff3effffffffff, relocInfo::none);\n+  __ emit_data64(0x3b3a393837363534, relocInfo::none);\n+  __ emit_data64(0xffffffffffff3d3c, relocInfo::none);\n+  __ emit_data64(0x06050403020100ff, relocInfo::none);\n+  __ emit_data64(0x0e0d0c0b0a090807, relocInfo::none);\n+  __ emit_data64(0x161514131211100f, relocInfo::none);\n+  __ emit_data64(0x3fffffffff191817, relocInfo::none);\n+  __ emit_data64(0x201f1e1d1c1b1aff, relocInfo::none);\n+  __ emit_data64(0x2827262524232221, relocInfo::none);\n+  __ emit_data64(0x302f2e2d2c2b2a29, relocInfo::none);\n+  __ emit_data64(0xffffffffff333231, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n@@ -6881,2 +2181,2 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+  return start;\n+}\n@@ -6884,5 +2184,26 @@\n-    return start;\n-  }\n-  address generate_method_entry_barrier() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"nmethod_entry_barrier\");\n+\/\/ Code for generating Base64 decoding.\n+\/\/\n+\/\/ Based on the article (and associated code) from https:\/\/arxiv.org\/abs\/1910.05109.\n+\/\/\n+\/\/ Intrinsic function prototype in Base64.java:\n+\/\/ private void decodeBlock(byte[] src, int sp, int sl, byte[] dst, int dp, boolean isURL, isMIME) {\n+address StubGenerator::generate_base64_decodeBlock() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"implDecode\");\n+  address start = __ pc();\n+\n+  __ enter();\n+\n+  \/\/ Save callee-saved registers before using them\n+  __ push(r12);\n+  __ push(r13);\n+  __ push(r14);\n+  __ push(r15);\n+  __ push(rbx);\n+\n+  \/\/ arguments\n+  const Register source = c_rarg0; \/\/ Source Array\n+  const Register start_offset = c_rarg1; \/\/ start offset\n+  const Register end_offset = c_rarg2; \/\/ end offset\n+  const Register dest = c_rarg3; \/\/ destination array\n+  const Register isMIME = rbx;\n@@ -6891,1 +2212,93 @@\n-    Label deoptimize_label;\n+#ifndef _WIN64\n+  const Register dp = c_rarg4;  \/\/ Position for writing to dest array\n+  const Register isURL = c_rarg5;\/\/ Base64 or URL character set\n+  __ movl(isMIME, Address(rbp, 2 * wordSize));\n+#else\n+  const Address  dp_mem(rbp, 6 * wordSize);  \/\/ length is on stack on Win64\n+  const Address isURL_mem(rbp, 7 * wordSize);\n+  const Register isURL = r10;      \/\/ pick the volatile windows register\n+  const Register dp = r12;\n+  __ movl(dp, dp_mem);\n+  __ movl(isURL, isURL_mem);\n+  __ movl(isMIME, Address(rbp, 8 * wordSize));\n+#endif\n+\n+  const XMMRegister lookup_lo = xmm5;\n+  const XMMRegister lookup_hi = xmm6;\n+  const XMMRegister errorvec = xmm7;\n+  const XMMRegister pack16_op = xmm9;\n+  const XMMRegister pack32_op = xmm8;\n+  const XMMRegister input0 = xmm3;\n+  const XMMRegister input1 = xmm20;\n+  const XMMRegister input2 = xmm21;\n+  const XMMRegister input3 = xmm19;\n+  const XMMRegister join01 = xmm12;\n+  const XMMRegister join12 = xmm11;\n+  const XMMRegister join23 = xmm10;\n+  const XMMRegister translated0 = xmm2;\n+  const XMMRegister translated1 = xmm1;\n+  const XMMRegister translated2 = xmm0;\n+  const XMMRegister translated3 = xmm4;\n+\n+  const XMMRegister merged0 = xmm2;\n+  const XMMRegister merged1 = xmm1;\n+  const XMMRegister merged2 = xmm0;\n+  const XMMRegister merged3 = xmm4;\n+  const XMMRegister merge_ab_bc0 = xmm2;\n+  const XMMRegister merge_ab_bc1 = xmm1;\n+  const XMMRegister merge_ab_bc2 = xmm0;\n+  const XMMRegister merge_ab_bc3 = xmm4;\n+\n+  const XMMRegister pack24bits = xmm4;\n+\n+  const Register length = r14;\n+  const Register output_size = r13;\n+  const Register output_mask = r15;\n+  const KRegister input_mask = k1;\n+\n+  const XMMRegister input_initial_valid_b64 = xmm0;\n+  const XMMRegister tmp = xmm10;\n+  const XMMRegister mask = xmm0;\n+  const XMMRegister invalid_b64 = xmm1;\n+\n+  Label L_process256, L_process64, L_process64Loop, L_exit, L_processdata, L_loadURL;\n+  Label L_continue, L_finalBit, L_padding, L_donePadding, L_bruteForce;\n+  Label L_forceLoop, L_bottomLoop, L_checkMIME, L_exit_no_vzero;\n+\n+  \/\/ calculate length from offsets\n+  __ movl(length, end_offset);\n+  __ subl(length, start_offset);\n+  __ push(dest);          \/\/ Save for return value calc\n+\n+  \/\/ If AVX512 VBMI not supported, just compile non-AVX code\n+  if(VM_Version::supports_avx512_vbmi() &&\n+     VM_Version::supports_avx512bw()) {\n+    __ cmpl(length, 128);     \/\/ 128-bytes is break-even for AVX-512\n+    __ jcc(Assembler::lessEqual, L_bruteForce);\n+\n+    __ cmpl(isMIME, 0);\n+    __ jcc(Assembler::notEqual, L_bruteForce);\n+\n+    \/\/ Load lookup tables based on isURL\n+    __ cmpl(isURL, 0);\n+    __ jcc(Assembler::notZero, L_loadURL);\n+\n+    __ evmovdquq(lookup_lo, ExternalAddress(StubRoutines::x86::base64_vbmi_lookup_lo_addr()), Assembler::AVX_512bit, r13);\n+    __ evmovdquq(lookup_hi, ExternalAddress(StubRoutines::x86::base64_vbmi_lookup_hi_addr()), Assembler::AVX_512bit, r13);\n+\n+    __ BIND(L_continue);\n+\n+    __ movl(r15, 0x01400140);\n+    __ evpbroadcastd(pack16_op, r15, Assembler::AVX_512bit);\n+\n+    __ movl(r15, 0x00011000);\n+    __ evpbroadcastd(pack32_op, r15, Assembler::AVX_512bit);\n+\n+    __ cmpl(length, 0xff);\n+    __ jcc(Assembler::lessEqual, L_process64);\n+\n+    \/\/ load masks required for decoding data\n+    __ BIND(L_processdata);\n+    __ evmovdquq(join01, ExternalAddress(StubRoutines::x86::base64_vbmi_join_0_1_addr()), Assembler::AVX_512bit,r13);\n+    __ evmovdquq(join12, ExternalAddress(StubRoutines::x86::base64_vbmi_join_1_2_addr()), Assembler::AVX_512bit, r13);\n+    __ evmovdquq(join23, ExternalAddress(StubRoutines::x86::base64_vbmi_join_2_3_addr()), Assembler::AVX_512bit, r13);\n@@ -6893,1 +2306,74 @@\n-    address start = __ pc();\n+    __ align32();\n+    __ BIND(L_process256);\n+    \/\/ Grab input data\n+    __ evmovdquq(input0, Address(source, start_offset, Address::times_1, 0x00), Assembler::AVX_512bit);\n+    __ evmovdquq(input1, Address(source, start_offset, Address::times_1, 0x40), Assembler::AVX_512bit);\n+    __ evmovdquq(input2, Address(source, start_offset, Address::times_1, 0x80), Assembler::AVX_512bit);\n+    __ evmovdquq(input3, Address(source, start_offset, Address::times_1, 0xc0), Assembler::AVX_512bit);\n+\n+    \/\/ Copy the low part of the lookup table into the destination of the permutation\n+    __ evmovdquq(translated0, lookup_lo, Assembler::AVX_512bit);\n+    __ evmovdquq(translated1, lookup_lo, Assembler::AVX_512bit);\n+    __ evmovdquq(translated2, lookup_lo, Assembler::AVX_512bit);\n+    __ evmovdquq(translated3, lookup_lo, Assembler::AVX_512bit);\n+\n+    \/\/ Translate the base64 input into \"decoded\" bytes\n+    __ evpermt2b(translated0, input0, lookup_hi, Assembler::AVX_512bit);\n+    __ evpermt2b(translated1, input1, lookup_hi, Assembler::AVX_512bit);\n+    __ evpermt2b(translated2, input2, lookup_hi, Assembler::AVX_512bit);\n+    __ evpermt2b(translated3, input3, lookup_hi, Assembler::AVX_512bit);\n+\n+    \/\/ OR all of the translations together to check for errors (high-order bit of byte set)\n+    __ vpternlogd(input0, 0xfe, input1, input2, Assembler::AVX_512bit);\n+\n+    __ vpternlogd(input3, 0xfe, translated0, translated1, Assembler::AVX_512bit);\n+    __ vpternlogd(input0, 0xfe, translated2, translated3, Assembler::AVX_512bit);\n+    __ vpor(errorvec, input3, input0, Assembler::AVX_512bit);\n+\n+    \/\/ Check if there was an error - if so, try 64-byte chunks\n+    __ evpmovb2m(k3, errorvec, Assembler::AVX_512bit);\n+    __ kortestql(k3, k3);\n+    __ jcc(Assembler::notZero, L_process64);\n+\n+    \/\/ The merging and shuffling happens here\n+    \/\/ We multiply each byte pair [00dddddd | 00cccccc | 00bbbbbb | 00aaaaaa]\n+    \/\/ Multiply [00cccccc] by 2^6 added to [00dddddd] to get [0000cccc | ccdddddd]\n+    \/\/ The pack16_op is a vector of 0x01400140, so multiply D by 1 and C by 0x40\n+    __ vpmaddubsw(merge_ab_bc0, translated0, pack16_op, Assembler::AVX_512bit);\n+    __ vpmaddubsw(merge_ab_bc1, translated1, pack16_op, Assembler::AVX_512bit);\n+    __ vpmaddubsw(merge_ab_bc2, translated2, pack16_op, Assembler::AVX_512bit);\n+    __ vpmaddubsw(merge_ab_bc3, translated3, pack16_op, Assembler::AVX_512bit);\n+\n+    \/\/ Now do the same with packed 16-bit values.\n+    \/\/ We start with [0000cccc | ccdddddd | 0000aaaa | aabbbbbb]\n+    \/\/ pack32_op is 0x00011000 (2^12, 1), so this multiplies [0000aaaa | aabbbbbb] by 2^12\n+    \/\/ and adds [0000cccc | ccdddddd] to yield [00000000 | aaaaaabb | bbbbcccc | ccdddddd]\n+    __ vpmaddwd(merged0, merge_ab_bc0, pack32_op, Assembler::AVX_512bit);\n+    __ vpmaddwd(merged1, merge_ab_bc1, pack32_op, Assembler::AVX_512bit);\n+    __ vpmaddwd(merged2, merge_ab_bc2, pack32_op, Assembler::AVX_512bit);\n+    __ vpmaddwd(merged3, merge_ab_bc3, pack32_op, Assembler::AVX_512bit);\n+\n+    \/\/ The join vectors specify which byte from which vector goes into the outputs\n+    \/\/ One of every 4 bytes in the extended vector is zero, so we pack them into their\n+    \/\/ final positions in the register for storing (256 bytes in, 192 bytes out)\n+    __ evpermt2b(merged0, join01, merged1, Assembler::AVX_512bit);\n+    __ evpermt2b(merged1, join12, merged2, Assembler::AVX_512bit);\n+    __ evpermt2b(merged2, join23, merged3, Assembler::AVX_512bit);\n+\n+    \/\/ Store result\n+    __ evmovdquq(Address(dest, dp, Address::times_1, 0x00), merged0, Assembler::AVX_512bit);\n+    __ evmovdquq(Address(dest, dp, Address::times_1, 0x40), merged1, Assembler::AVX_512bit);\n+    __ evmovdquq(Address(dest, dp, Address::times_1, 0x80), merged2, Assembler::AVX_512bit);\n+\n+    __ addptr(source, 0x100);\n+    __ addptr(dest, 0xc0);\n+    __ subl(length, 0x100);\n+    __ cmpl(length, 64 * 4);\n+    __ jcc(Assembler::greaterEqual, L_process256);\n+\n+    \/\/ At this point, we've decoded 64 * 4 * n bytes.\n+    \/\/ The remaining length will be <= 64 * 4 - 1.\n+    \/\/ UNLESS there was an error decoding the first 256-byte chunk.  In this\n+    \/\/ case, the length will be arbitrarily long.\n+    \/\/\n+    \/\/ Note that this will be the path for MIME-encoded strings.\n@@ -6895,1 +2381,1 @@\n-    __ push(-1); \/\/ cookie, this is used for writing the new rsp when deoptimizing\n+    __ BIND(L_process64);\n@@ -6897,2 +2383,1 @@\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ save rbp\n+    __ evmovdquq(pack24bits, ExternalAddress(StubRoutines::x86::base64_vbmi_pack_vec_addr()), Assembler::AVX_512bit, r13);\n@@ -6900,3 +2385,2 @@\n-    \/\/ save c_rarg0, because we want to use that value.\n-    \/\/ We could do without it but then we depend on the number of slots used by pusha\n-    __ push(c_rarg0);\n+    __ cmpl(length, 63);\n+    __ jcc(Assembler::lessEqual, L_finalBit);\n@@ -6904,1 +2388,2 @@\n-    __ lea(c_rarg0, Address(rsp, wordSize * 3)); \/\/ 1 for cookie, 1 for rbp, 1 for c_rarg0 - this should be the return address\n+    __ mov64(rax, 0x0000ffffffffffff);\n+    __ kmovql(k2, rax);\n@@ -6906,1 +2391,2 @@\n-    __ pusha();\n+    __ align32();\n+    __ BIND(L_process64Loop);\n@@ -6908,14 +2394,1 @@\n-    \/\/ The method may have floats as arguments, and we must spill them before calling\n-    \/\/ the VM runtime.\n-    assert(Argument::n_float_register_parameters_j == 8, \"Assumption\");\n-    const int xmm_size = wordSize * 2;\n-    const int xmm_spill_size = xmm_size * Argument::n_float_register_parameters_j;\n-    __ subptr(rsp, xmm_spill_size);\n-    __ movdqu(Address(rsp, xmm_size * 7), xmm7);\n-    __ movdqu(Address(rsp, xmm_size * 6), xmm6);\n-    __ movdqu(Address(rsp, xmm_size * 5), xmm5);\n-    __ movdqu(Address(rsp, xmm_size * 4), xmm4);\n-    __ movdqu(Address(rsp, xmm_size * 3), xmm3);\n-    __ movdqu(Address(rsp, xmm_size * 2), xmm2);\n-    __ movdqu(Address(rsp, xmm_size * 1), xmm1);\n-    __ movdqu(Address(rsp, xmm_size * 0), xmm0);\n+    \/\/ Handle first 64-byte block\n@@ -6923,1 +2396,3 @@\n-    __ call_VM_leaf(CAST_FROM_FN_PTR(address, static_cast<int (*)(address*)>(BarrierSetNMethod::nmethod_stub_entry_barrier)), 1);\n+    __ evmovdquq(input0, Address(source, start_offset), Assembler::AVX_512bit);\n+    __ evmovdquq(translated0, lookup_lo, Assembler::AVX_512bit);\n+    __ evpermt2b(translated0, input0, lookup_hi, Assembler::AVX_512bit);\n@@ -6925,9 +2400,1 @@\n-    __ movdqu(xmm0, Address(rsp, xmm_size * 0));\n-    __ movdqu(xmm1, Address(rsp, xmm_size * 1));\n-    __ movdqu(xmm2, Address(rsp, xmm_size * 2));\n-    __ movdqu(xmm3, Address(rsp, xmm_size * 3));\n-    __ movdqu(xmm4, Address(rsp, xmm_size * 4));\n-    __ movdqu(xmm5, Address(rsp, xmm_size * 5));\n-    __ movdqu(xmm6, Address(rsp, xmm_size * 6));\n-    __ movdqu(xmm7, Address(rsp, xmm_size * 7));\n-    __ addptr(rsp, xmm_spill_size);\n+    __ vpor(errorvec, translated0, input0, Assembler::AVX_512bit);\n@@ -6935,2 +2402,4 @@\n-    __ cmpl(rax, 1); \/\/ 1 means deoptimize\n-    __ jcc(Assembler::equal, deoptimize_label);\n+    \/\/ Check for error and bomb out before updating dest\n+    __ evpmovb2m(k3, errorvec, Assembler::AVX_512bit);\n+    __ kortestql(k3, k3);\n+    __ jcc(Assembler::notZero, L_exit);\n@@ -6938,2 +2407,4 @@\n-    __ popa();\n-    __ pop(c_rarg0);\n+    \/\/ Pack output register, selecting correct byte ordering\n+    __ vpmaddubsw(merge_ab_bc0, translated0, pack16_op, Assembler::AVX_512bit);\n+    __ vpmaddwd(merged0, merge_ab_bc0, pack32_op, Assembler::AVX_512bit);\n+    __ vpermb(merged0, pack24bits, merged0, Assembler::AVX_512bit);\n@@ -6941,1 +2412,1 @@\n-    __ leave();\n+    __ evmovdqub(Address(dest, dp), k2, merged0, true, Assembler::AVX_512bit);\n@@ -6943,2 +2414,3 @@\n-    __ addptr(rsp, 1 * wordSize); \/\/ cookie\n-    __ ret(0);\n+    __ subl(length, 64);\n+    __ addptr(source, 64);\n+    __ addptr(dest, 48);\n@@ -6946,0 +2418,2 @@\n+    __ cmpl(length, 64);\n+    __ jcc(Assembler::greaterEqual, L_process64Loop);\n@@ -6947,1 +2421,2 @@\n-    __ BIND(deoptimize_label);\n+    __ cmpl(length, 0);\n+    __ jcc(Assembler::lessEqual, L_exit);\n@@ -6949,2 +2424,73 @@\n-    __ popa();\n-    __ pop(c_rarg0);\n+    __ BIND(L_finalBit);\n+    \/\/ Now have 1 to 63 bytes left to decode\n+\n+    \/\/ I was going to let Java take care of the final fragment\n+    \/\/ however it will repeatedly call this routine for every 4 bytes\n+    \/\/ of input data, so handle the rest here.\n+    __ movq(rax, -1);\n+    __ bzhiq(rax, rax, length);    \/\/ Input mask in rax\n+\n+    __ movl(output_size, length);\n+    __ shrl(output_size, 2);   \/\/ Find (len \/ 4) * 3 (output length)\n+    __ lea(output_size, Address(output_size, output_size, Address::times_2, 0));\n+    \/\/ output_size in r13\n+\n+    \/\/ Strip pad characters, if any, and adjust length and mask\n+    __ cmpb(Address(source, length, Address::times_1, -1), '=');\n+    __ jcc(Assembler::equal, L_padding);\n+\n+    __ BIND(L_donePadding);\n+\n+    \/\/ Output size is (64 - output_size), output mask is (all 1s >> output_size).\n+    __ kmovql(input_mask, rax);\n+    __ movq(output_mask, -1);\n+    __ bzhiq(output_mask, output_mask, output_size);\n+\n+    \/\/ Load initial input with all valid base64 characters.  Will be used\n+    \/\/ in merging source bytes to avoid masking when determining if an error occurred.\n+    __ movl(rax, 0x61616161);\n+    __ evpbroadcastd(input_initial_valid_b64, rax, Assembler::AVX_512bit);\n+\n+    \/\/ A register containing all invalid base64 decoded values\n+    __ movl(rax, 0x80808080);\n+    __ evpbroadcastd(invalid_b64, rax, Assembler::AVX_512bit);\n+\n+    \/\/ input_mask is in k1\n+    \/\/ output_size is in r13\n+    \/\/ output_mask is in r15\n+    \/\/ zmm0 - free\n+    \/\/ zmm1 - 0x00011000\n+    \/\/ zmm2 - 0x01400140\n+    \/\/ zmm3 - errorvec\n+    \/\/ zmm4 - pack vector\n+    \/\/ zmm5 - lookup_lo\n+    \/\/ zmm6 - lookup_hi\n+    \/\/ zmm7 - errorvec\n+    \/\/ zmm8 - 0x61616161\n+    \/\/ zmm9 - 0x80808080\n+\n+    \/\/ Load only the bytes from source, merging into our \"fully-valid\" register\n+    __ evmovdqub(input_initial_valid_b64, input_mask, Address(source, start_offset, Address::times_1, 0x0), true, Assembler::AVX_512bit);\n+\n+    \/\/ Decode all bytes within our merged input\n+    __ evmovdquq(tmp, lookup_lo, Assembler::AVX_512bit);\n+    __ evpermt2b(tmp, input_initial_valid_b64, lookup_hi, Assembler::AVX_512bit);\n+    __ vporq(mask, tmp, input_initial_valid_b64, Assembler::AVX_512bit);\n+\n+    \/\/ Check for error.  Compare (decoded | initial) to all invalid.\n+    \/\/ If any bytes have their high-order bit set, then we have an error.\n+    __ evptestmb(k2, mask, invalid_b64, Assembler::AVX_512bit);\n+    __ kortestql(k2, k2);\n+\n+    \/\/ If we have an error, use the brute force loop to decode what we can (4-byte chunks).\n+    __ jcc(Assembler::notZero, L_bruteForce);\n+\n+    \/\/ Shuffle output bytes\n+    __ vpmaddubsw(tmp, tmp, pack16_op, Assembler::AVX_512bit);\n+    __ vpmaddwd(tmp, tmp, pack32_op, Assembler::AVX_512bit);\n+\n+    __ vpermb(tmp, pack24bits, tmp, Assembler::AVX_512bit);\n+    __ kmovql(k1, output_mask);\n+    __ evmovdqub(Address(dest, dp), k1, tmp, true, Assembler::AVX_512bit);\n+\n+    __ addptr(dest, output_size);\n@@ -6952,0 +2498,10 @@\n+    __ BIND(L_exit);\n+    __ vzeroupper();\n+    __ pop(rax);             \/\/ Get original dest value\n+    __ subptr(dest, rax);      \/\/ Number of bytes converted\n+    __ movptr(rax, dest);\n+    __ pop(rbx);\n+    __ pop(r15);\n+    __ pop(r14);\n+    __ pop(r13);\n+    __ pop(r12);\n@@ -6953,0 +2509,1 @@\n+    __ ret(0);\n@@ -6954,6 +2511,4 @@\n-    \/\/ this can be taken out, but is good for verification purposes. getting a SIGSEGV\n-    \/\/ here while still having a correct stack is valuable\n-    __ testptr(rsp, Address(rsp, 0));\n-\n-    __ movptr(rsp, Address(rsp, 0)); \/\/ new rsp was written in the barrier\n-    __ jmp(Address(rsp, -1 * wordSize)); \/\/ jmp target should be callers verified_entry_point\n+    __ BIND(L_loadURL);\n+    __ evmovdquq(lookup_lo, ExternalAddress(StubRoutines::x86::base64_vbmi_lookup_lo_url_addr()), Assembler::AVX_512bit, r13);\n+    __ evmovdquq(lookup_hi, ExternalAddress(StubRoutines::x86::base64_vbmi_lookup_hi_url_addr()), Assembler::AVX_512bit, r13);\n+    __ jmp(L_continue);\n@@ -6961,2 +2516,3 @@\n-    return start;\n-  }\n+    __ BIND(L_padding);\n+    __ decrementq(output_size, 1);\n+    __ shrq(rax, 1);\n@@ -6964,45 +2520,2 @@\n-   \/**\n-   *  Arguments:\n-   *\n-   *  Input:\n-   *    c_rarg0   - out address\n-   *    c_rarg1   - in address\n-   *    c_rarg2   - offset\n-   *    c_rarg3   - len\n-   * not Win64\n-   *    c_rarg4   - k\n-   * Win64\n-   *    rsp+40    - k\n-   *\/\n-  address generate_mulAdd() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"mulAdd\");\n-\n-    address start = __ pc();\n-    \/\/ Win64: rcx, rdx, r8, r9 (c_rarg0, c_rarg1, ...)\n-    \/\/ Unix:  rdi, rsi, rdx, rcx, r8, r9 (c_rarg0, c_rarg1, ...)\n-    const Register out     = rdi;\n-    const Register in      = rsi;\n-    const Register offset  = r11;\n-    const Register len     = rcx;\n-    const Register k       = r8;\n-\n-    \/\/ Next registers will be saved on stack in mul_add().\n-    const Register tmp1  = r12;\n-    const Register tmp2  = r13;\n-    const Register tmp3  = r14;\n-    const Register tmp4  = r15;\n-    const Register tmp5  = rbx;\n-\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-    setup_arg_regs(4); \/\/ out => rdi, in => rsi, offset => rdx\n-                       \/\/ len => rcx, k => r8\n-                       \/\/ r9 and r10 may be used to save non-volatile registers\n-#ifdef _WIN64\n-    \/\/ last argument is on stack on Win64\n-    __ movl(k, Address(rsp, 6 * wordSize));\n-#endif\n-    __ movptr(r11, rdx);  \/\/ move offset in rdx to offset(r11)\n-    __ mul_add(out, in, offset, len, k, tmp1, tmp2, tmp3, tmp4, tmp5, rdx, rax);\n+    __ cmpb(Address(source, length, Address::times_1, -2), '=');\n+    __ jcc(Assembler::notEqual, L_donePadding);\n@@ -7010,1 +2523,3 @@\n-    restore_arg_regs();\n+    __ decrementq(output_size, 1);\n+    __ shrq(rax, 1);\n+    __ jmp(L_donePadding);\n@@ -7012,2 +2527,92 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+    __ align32();\n+    __ BIND(L_bruteForce);\n+  }   \/\/ End of if(avx512_vbmi)\n+\n+  \/\/ Use non-AVX code to decode 4-byte chunks into 3 bytes of output\n+\n+  \/\/ Register state (Linux):\n+  \/\/ r12-15 - saved on stack\n+  \/\/ rdi - src\n+  \/\/ rsi - sp\n+  \/\/ rdx - sl\n+  \/\/ rcx - dst\n+  \/\/ r8 - dp\n+  \/\/ r9 - isURL\n+\n+  \/\/ Register state (Windows):\n+  \/\/ r12-15 - saved on stack\n+  \/\/ rcx - src\n+  \/\/ rdx - sp\n+  \/\/ r8 - sl\n+  \/\/ r9 - dst\n+  \/\/ r12 - dp\n+  \/\/ r10 - isURL\n+\n+  \/\/ Registers (common):\n+  \/\/ length (r14) - bytes in src\n+\n+  const Register decode_table = r11;\n+  const Register out_byte_count = rbx;\n+  const Register byte1 = r13;\n+  const Register byte2 = r15;\n+  const Register byte3 = WIN64_ONLY(r8) NOT_WIN64(rdx);\n+  const Register byte4 = WIN64_ONLY(r10) NOT_WIN64(r9);\n+\n+  __ shrl(length, 2);    \/\/ Multiple of 4 bytes only - length is # 4-byte chunks\n+  __ cmpl(length, 0);\n+  __ jcc(Assembler::lessEqual, L_exit_no_vzero);\n+\n+  __ shll(isURL, 8);    \/\/ index into decode table based on isURL\n+  __ lea(decode_table, ExternalAddress(StubRoutines::x86::base64_decoding_table_addr()));\n+  __ addptr(decode_table, isURL);\n+\n+  __ jmp(L_bottomLoop);\n+\n+  __ align32();\n+  __ BIND(L_forceLoop);\n+  __ shll(byte1, 18);\n+  __ shll(byte2, 12);\n+  __ shll(byte3, 6);\n+  __ orl(byte1, byte2);\n+  __ orl(byte1, byte3);\n+  __ orl(byte1, byte4);\n+\n+  __ addptr(source, 4);\n+\n+  __ movb(Address(dest, dp, Address::times_1, 2), byte1);\n+  __ shrl(byte1, 8);\n+  __ movb(Address(dest, dp, Address::times_1, 1), byte1);\n+  __ shrl(byte1, 8);\n+  __ movb(Address(dest, dp, Address::times_1, 0), byte1);\n+\n+  __ addptr(dest, 3);\n+  __ decrementl(length, 1);\n+  __ jcc(Assembler::zero, L_exit_no_vzero);\n+\n+  __ BIND(L_bottomLoop);\n+  __ load_unsigned_byte(byte1, Address(source, start_offset, Address::times_1, 0x00));\n+  __ load_unsigned_byte(byte2, Address(source, start_offset, Address::times_1, 0x01));\n+  __ load_signed_byte(byte1, Address(decode_table, byte1));\n+  __ load_signed_byte(byte2, Address(decode_table, byte2));\n+  __ load_unsigned_byte(byte3, Address(source, start_offset, Address::times_1, 0x02));\n+  __ load_unsigned_byte(byte4, Address(source, start_offset, Address::times_1, 0x03));\n+  __ load_signed_byte(byte3, Address(decode_table, byte3));\n+  __ load_signed_byte(byte4, Address(decode_table, byte4));\n+\n+  __ mov(rax, byte1);\n+  __ orl(rax, byte2);\n+  __ orl(rax, byte3);\n+  __ orl(rax, byte4);\n+  __ jcc(Assembler::positive, L_forceLoop);\n+\n+  __ BIND(L_exit_no_vzero);\n+  __ pop(rax);             \/\/ Get original dest value\n+  __ subptr(dest, rax);      \/\/ Number of bytes converted\n+  __ movptr(rax, dest);\n+  __ pop(rbx);\n+  __ pop(r15);\n+  __ pop(r14);\n+  __ pop(r13);\n+  __ pop(r12);\n+  __ leave();\n+  __ ret(0);\n@@ -7015,2 +2620,2 @@\n-    return start;\n-  }\n+  return start;\n+}\n@@ -7018,119 +2623,51 @@\n-  address generate_bigIntegerRightShift() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"bigIntegerRightShiftWorker\");\n-\n-    address start = __ pc();\n-    Label Shift512Loop, ShiftTwo, ShiftTwoLoop, ShiftOne, Exit;\n-    \/\/ For Unix, the arguments are as follows: rdi, rsi, rdx, rcx, r8.\n-    const Register newArr = rdi;\n-    const Register oldArr = rsi;\n-    const Register newIdx = rdx;\n-    const Register shiftCount = rcx;  \/\/ It was intentional to have shiftCount in rcx since it is used implicitly for shift.\n-    const Register totalNumIter = r8;\n-\n-    \/\/ For windows, we use r9 and r10 as temps to save rdi and rsi. Thus we cannot allocate them for our temps.\n-    \/\/ For everything else, we prefer using r9 and r10 since we do not have to save them before use.\n-    const Register tmp1 = r11;                    \/\/ Caller save.\n-    const Register tmp2 = rax;                    \/\/ Caller save.\n-    const Register tmp3 = WINDOWS_ONLY(r12) NOT_WINDOWS(r9);   \/\/ Windows: Callee save. Linux: Caller save.\n-    const Register tmp4 = WINDOWS_ONLY(r13) NOT_WINDOWS(r10);  \/\/ Windows: Callee save. Linux: Caller save.\n-    const Register tmp5 = r14;                    \/\/ Callee save.\n-    const Register tmp6 = r15;\n-\n-    const XMMRegister x0 = xmm0;\n-    const XMMRegister x1 = xmm1;\n-    const XMMRegister x2 = xmm2;\n-\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-#ifdef _WINDOWS\n-    setup_arg_regs(4);\n-    \/\/ For windows, since last argument is on stack, we need to move it to the appropriate register.\n-    __ movl(totalNumIter, Address(rsp, 6 * wordSize));\n-    \/\/ Save callee save registers.\n-    __ push(tmp3);\n-    __ push(tmp4);\n-#endif\n-    __ push(tmp5);\n-\n-    \/\/ Rename temps used throughout the code.\n-    const Register idx = tmp1;\n-    const Register nIdx = tmp2;\n-\n-    __ xorl(idx, idx);\n-\n-    \/\/ Start right shift from end of the array.\n-    \/\/ For example, if #iteration = 4 and newIdx = 1\n-    \/\/ then dest[4] = src[4] >> shiftCount  | src[3] <<< (shiftCount - 32)\n-    \/\/ if #iteration = 4 and newIdx = 0\n-    \/\/ then dest[3] = src[4] >> shiftCount  | src[3] <<< (shiftCount - 32)\n-    __ movl(idx, totalNumIter);\n-    __ movl(nIdx, idx);\n-    __ addl(nIdx, newIdx);\n-\n-    \/\/ If vectorization is enabled, check if the number of iterations is at least 64\n-    \/\/ If not, then go to ShifTwo processing 2 iterations\n-    if (VM_Version::supports_avx512_vbmi2()) {\n-      __ cmpptr(totalNumIter, (AVX3Threshold\/64));\n-      __ jcc(Assembler::less, ShiftTwo);\n-      if (AVX3Threshold < 16 * 64) {\n-        __ cmpl(totalNumIter, 16);\n-        __ jcc(Assembler::less, ShiftTwo);\n-      }\n-      __ evpbroadcastd(x0, shiftCount, Assembler::AVX_512bit);\n-      __ subl(idx, 16);\n-      __ subl(nIdx, 16);\n-      __ BIND(Shift512Loop);\n-      __ evmovdqul(x2, Address(oldArr, idx, Address::times_4, 4), Assembler::AVX_512bit);\n-      __ evmovdqul(x1, Address(oldArr, idx, Address::times_4), Assembler::AVX_512bit);\n-      __ vpshrdvd(x2, x1, x0, Assembler::AVX_512bit);\n-      __ evmovdqul(Address(newArr, nIdx, Address::times_4), x2, Assembler::AVX_512bit);\n-      __ subl(nIdx, 16);\n-      __ subl(idx, 16);\n-      __ jcc(Assembler::greaterEqual, Shift512Loop);\n-      __ addl(idx, 16);\n-      __ addl(nIdx, 16);\n-    }\n-    __ BIND(ShiftTwo);\n-    __ cmpl(idx, 2);\n-    __ jcc(Assembler::less, ShiftOne);\n-    __ subl(idx, 2);\n-    __ subl(nIdx, 2);\n-    __ BIND(ShiftTwoLoop);\n-    __ movl(tmp5, Address(oldArr, idx, Address::times_4, 8));\n-    __ movl(tmp4, Address(oldArr, idx, Address::times_4, 4));\n-    __ movl(tmp3, Address(oldArr, idx, Address::times_4));\n-    __ shrdl(tmp5, tmp4);\n-    __ shrdl(tmp4, tmp3);\n-    __ movl(Address(newArr, nIdx, Address::times_4, 4), tmp5);\n-    __ movl(Address(newArr, nIdx, Address::times_4), tmp4);\n-    __ subl(nIdx, 2);\n-    __ subl(idx, 2);\n-    __ jcc(Assembler::greaterEqual, ShiftTwoLoop);\n-    __ addl(idx, 2);\n-    __ addl(nIdx, 2);\n-\n-    \/\/ Do the last iteration\n-    __ BIND(ShiftOne);\n-    __ cmpl(idx, 1);\n-    __ jcc(Assembler::less, Exit);\n-    __ subl(idx, 1);\n-    __ subl(nIdx, 1);\n-    __ movl(tmp4, Address(oldArr, idx, Address::times_4, 4));\n-    __ movl(tmp3, Address(oldArr, idx, Address::times_4));\n-    __ shrdl(tmp4, tmp3);\n-    __ movl(Address(newArr, nIdx, Address::times_4), tmp4);\n-    __ BIND(Exit);\n-    __ vzeroupper();\n-    \/\/ Restore callee save registers.\n-    __ pop(tmp5);\n-#ifdef _WINDOWS\n-    __ pop(tmp4);\n-    __ pop(tmp3);\n-    restore_arg_regs();\n-#endif\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-    return start;\n-  }\n+\/**\n+ *  Arguments:\n+ *\n+ * Inputs:\n+ *   c_rarg0   - int crc\n+ *   c_rarg1   - byte* buf\n+ *   c_rarg2   - int length\n+ *\n+ * Output:\n+ *       rax   - int crc result\n+ *\/\n+address StubGenerator::generate_updateBytesCRC32() {\n+  assert(UseCRC32Intrinsics, \"need AVX and CLMUL instructions\");\n+\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"updateBytesCRC32\");\n+\n+  address start = __ pc();\n+\n+  \/\/ Win64: rcx, rdx, r8, r9 (c_rarg0, c_rarg1, ...)\n+  \/\/ Unix:  rdi, rsi, rdx, rcx, r8, r9 (c_rarg0, c_rarg1, ...)\n+  \/\/ rscratch1: r10\n+  const Register crc   = c_rarg0;  \/\/ crc\n+  const Register buf   = c_rarg1;  \/\/ source java byte array address\n+  const Register len   = c_rarg2;  \/\/ length\n+  const Register table = c_rarg3;  \/\/ crc_table address (reuse register)\n+  const Register tmp1   = r11;\n+  const Register tmp2   = r10;\n+  assert_different_registers(crc, buf, len, table, tmp1, tmp2, rax);\n+\n+  BLOCK_COMMENT(\"Entry:\");\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+\n+  if (VM_Version::supports_sse4_1() && VM_Version::supports_avx512_vpclmulqdq() &&\n+      VM_Version::supports_avx512bw() &&\n+      VM_Version::supports_avx512vl()) {\n+      \/\/ The constants used in the CRC32 algorithm requires the 1's compliment of the initial crc value.\n+      \/\/ However, the constant table for CRC32-C assumes the original crc value.  Account for this\n+      \/\/ difference before calling and after returning.\n+    __ lea(table, ExternalAddress(StubRoutines::x86::crc_table_avx512_addr()));\n+    __ notl(crc);\n+    __ kernel_crc32_avx512(crc, buf, len, table, tmp1, tmp2);\n+    __ notl(crc);\n+  } else {\n+    __ kernel_crc32(crc, buf, len, table, tmp1);\n+  }\n+\n+  __ movl(rax, crc);\n+  __ vzeroupper();\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -7139,63 +2676,2 @@\n-   \/**\n-   *  Arguments:\n-   *\n-   *  Input:\n-   *    c_rarg0   - newArr address\n-   *    c_rarg1   - oldArr address\n-   *    c_rarg2   - newIdx\n-   *    c_rarg3   - shiftCount\n-   * not Win64\n-   *    c_rarg4   - numIter\n-   * Win64\n-   *    rsp40    - numIter\n-   *\/\n-  address generate_bigIntegerLeftShift() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this,  \"StubRoutines\", \"bigIntegerLeftShiftWorker\");\n-    address start = __ pc();\n-    Label Shift512Loop, ShiftTwo, ShiftTwoLoop, ShiftOne, Exit;\n-    \/\/ For Unix, the arguments are as follows: rdi, rsi, rdx, rcx, r8.\n-    const Register newArr = rdi;\n-    const Register oldArr = rsi;\n-    const Register newIdx = rdx;\n-    const Register shiftCount = rcx;  \/\/ It was intentional to have shiftCount in rcx since it is used implicitly for shift.\n-    const Register totalNumIter = r8;\n-    \/\/ For windows, we use r9 and r10 as temps to save rdi and rsi. Thus we cannot allocate them for our temps.\n-    \/\/ For everything else, we prefer using r9 and r10 since we do not have to save them before use.\n-    const Register tmp1 = r11;                    \/\/ Caller save.\n-    const Register tmp2 = rax;                    \/\/ Caller save.\n-    const Register tmp3 = WINDOWS_ONLY(r12) NOT_WINDOWS(r9);   \/\/ Windows: Callee save. Linux: Caller save.\n-    const Register tmp4 = WINDOWS_ONLY(r13) NOT_WINDOWS(r10);  \/\/ Windows: Callee save. Linux: Caller save.\n-    const Register tmp5 = r14;                    \/\/ Callee save.\n-\n-    const XMMRegister x0 = xmm0;\n-    const XMMRegister x1 = xmm1;\n-    const XMMRegister x2 = xmm2;\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-#ifdef _WINDOWS\n-    setup_arg_regs(4);\n-    \/\/ For windows, since last argument is on stack, we need to move it to the appropriate register.\n-    __ movl(totalNumIter, Address(rsp, 6 * wordSize));\n-    \/\/ Save callee save registers.\n-    __ push(tmp3);\n-    __ push(tmp4);\n-#endif\n-    __ push(tmp5);\n-\n-    \/\/ Rename temps used throughout the code\n-    const Register idx = tmp1;\n-    const Register numIterTmp = tmp2;\n-\n-    \/\/ Start idx from zero.\n-    __ xorl(idx, idx);\n-    \/\/ Compute interior pointer for new array. We do this so that we can use same index for both old and new arrays.\n-    __ lea(newArr, Address(newArr, newIdx, Address::times_4));\n-    __ movl(numIterTmp, totalNumIter);\n-\n-    \/\/ If vectorization is enabled, check if the number of iterations is at least 64\n-    \/\/ If not, then go to ShiftTwo shifting two numbers at a time\n-    if (VM_Version::supports_avx512_vbmi2()) {\n-      __ cmpl(totalNumIter, (AVX3Threshold\/64));\n-      __ jcc(Assembler::less, ShiftTwo);\n+  return start;\n+}\n@@ -7203,52 +2679,58 @@\n-      if (AVX3Threshold < 16 * 64) {\n-        __ cmpl(totalNumIter, 16);\n-        __ jcc(Assembler::less, ShiftTwo);\n-      }\n-      __ evpbroadcastd(x0, shiftCount, Assembler::AVX_512bit);\n-      __ subl(numIterTmp, 16);\n-      __ BIND(Shift512Loop);\n-      __ evmovdqul(x1, Address(oldArr, idx, Address::times_4), Assembler::AVX_512bit);\n-      __ evmovdqul(x2, Address(oldArr, idx, Address::times_4, 0x4), Assembler::AVX_512bit);\n-      __ vpshldvd(x1, x2, x0, Assembler::AVX_512bit);\n-      __ evmovdqul(Address(newArr, idx, Address::times_4), x1, Assembler::AVX_512bit);\n-      __ addl(idx, 16);\n-      __ subl(numIterTmp, 16);\n-      __ jcc(Assembler::greaterEqual, Shift512Loop);\n-      __ addl(numIterTmp, 16);\n-    }\n-    __ BIND(ShiftTwo);\n-    __ cmpl(totalNumIter, 1);\n-    __ jcc(Assembler::less, Exit);\n-    __ movl(tmp3, Address(oldArr, idx, Address::times_4));\n-    __ subl(numIterTmp, 2);\n-    __ jcc(Assembler::less, ShiftOne);\n-\n-    __ BIND(ShiftTwoLoop);\n-    __ movl(tmp4, Address(oldArr, idx, Address::times_4, 0x4));\n-    __ movl(tmp5, Address(oldArr, idx, Address::times_4, 0x8));\n-    __ shldl(tmp3, tmp4);\n-    __ shldl(tmp4, tmp5);\n-    __ movl(Address(newArr, idx, Address::times_4), tmp3);\n-    __ movl(Address(newArr, idx, Address::times_4, 0x4), tmp4);\n-    __ movl(tmp3, tmp5);\n-    __ addl(idx, 2);\n-    __ subl(numIterTmp, 2);\n-    __ jcc(Assembler::greaterEqual, ShiftTwoLoop);\n-\n-    \/\/ Do the last iteration\n-    __ BIND(ShiftOne);\n-    __ addl(numIterTmp, 2);\n-    __ cmpl(numIterTmp, 1);\n-    __ jcc(Assembler::less, Exit);\n-    __ movl(tmp4, Address(oldArr, idx, Address::times_4, 0x4));\n-    __ shldl(tmp3, tmp4);\n-    __ movl(Address(newArr, idx, Address::times_4), tmp3);\n-\n-    __ BIND(Exit);\n-    __ vzeroupper();\n-    \/\/ Restore callee save registers.\n-    __ pop(tmp5);\n-#ifdef _WINDOWS\n-    __ pop(tmp4);\n-    __ pop(tmp3);\n-    restore_arg_regs();\n+\/**\n+*  Arguments:\n+*\n+* Inputs:\n+*   c_rarg0   - int crc\n+*   c_rarg1   - byte* buf\n+*   c_rarg2   - long length\n+*   c_rarg3   - table_start - optional (present only when doing a library_call,\n+*              not used by x86 algorithm)\n+*\n+* Output:\n+*       rax   - int crc result\n+*\/\n+address StubGenerator::generate_updateBytesCRC32C(bool is_pclmulqdq_supported) {\n+  assert(UseCRC32CIntrinsics, \"need SSE4_2\");\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"updateBytesCRC32C\");\n+  address start = __ pc();\n+\n+  \/\/reg.arg        int#0        int#1        int#2        int#3        int#4        int#5        float regs\n+  \/\/Windows        RCX          RDX          R8           R9           none         none         XMM0..XMM3\n+  \/\/Lin \/ Sol      RDI          RSI          RDX          RCX          R8           R9           XMM0..XMM7\n+  const Register crc = c_rarg0;  \/\/ crc\n+  const Register buf = c_rarg1;  \/\/ source java byte array address\n+  const Register len = c_rarg2;  \/\/ length\n+  const Register a = rax;\n+  const Register j = r9;\n+  const Register k = r10;\n+  const Register l = r11;\n+#ifdef _WIN64\n+  const Register y = rdi;\n+  const Register z = rsi;\n+#else\n+  const Register y = rcx;\n+  const Register z = r8;\n+#endif\n+  assert_different_registers(crc, buf, len, a, j, k, l, y, z);\n+\n+  BLOCK_COMMENT(\"Entry:\");\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  if (VM_Version::supports_sse4_1() && VM_Version::supports_avx512_vpclmulqdq() &&\n+      VM_Version::supports_avx512bw() &&\n+      VM_Version::supports_avx512vl()) {\n+    __ lea(j, ExternalAddress(StubRoutines::x86::crc32c_table_avx512_addr()));\n+    __ kernel_crc32_avx512(crc, buf, len, j, l, k);\n+  } else {\n+#ifdef _WIN64\n+    __ push(y);\n+    __ push(z);\n+#endif\n+    __ crc32c_ipl_alg2_alt2(crc, buf, len,\n+                            a, j, k,\n+                            l, y, z,\n+                            c_farg0, c_farg1, c_farg2,\n+                            is_pclmulqdq_supported);\n+#ifdef _WIN64\n+    __ pop(z);\n+    __ pop(y);\n@@ -7256,3 +2738,4 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-    return start;\n+  __ movl(rax, crc);\n+  __ vzeroupper();\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -7261,14 +2744,2 @@\n-  address generate_libmExp() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"libmExp\");\n-\n-    address start = __ pc();\n-\n-    const XMMRegister x0  = xmm0;\n-    const XMMRegister x1  = xmm1;\n-    const XMMRegister x2  = xmm2;\n-    const XMMRegister x3  = xmm3;\n-\n-    const XMMRegister x4  = xmm4;\n-    const XMMRegister x5  = xmm5;\n-    const XMMRegister x6  = xmm6;\n-    const XMMRegister x7  = xmm7;\n+  return start;\n+}\n@@ -7276,3 +2747,38 @@\n-    const Register tmp   = r11;\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+\/**\n+ *  Arguments:\n+ *\n+ *  Input:\n+ *    c_rarg0   - x address\n+ *    c_rarg1   - x length\n+ *    c_rarg2   - y address\n+ *    c_rarg3   - y length\n+ * not Win64\n+ *    c_rarg4   - z address\n+ *    c_rarg5   - z length\n+ * Win64\n+ *    rsp+40    - z address\n+ *    rsp+48    - z length\n+ *\/\n+address StubGenerator::generate_multiplyToLen() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"multiplyToLen\");\n+  address start = __ pc();\n+\n+  \/\/ Win64: rcx, rdx, r8, r9 (c_rarg0, c_rarg1, ...)\n+  \/\/ Unix:  rdi, rsi, rdx, rcx, r8, r9 (c_rarg0, c_rarg1, ...)\n+  const Register x     = rdi;\n+  const Register xlen  = rax;\n+  const Register y     = rsi;\n+  const Register ylen  = rcx;\n+  const Register z     = r8;\n+  const Register zlen  = r11;\n+\n+  \/\/ Next registers will be saved on stack in multiply_to_len().\n+  const Register tmp1  = r12;\n+  const Register tmp2  = r13;\n+  const Register tmp3  = r14;\n+  const Register tmp4  = r15;\n+  const Register tmp5  = rbx;\n+\n+  BLOCK_COMMENT(\"Entry:\");\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n@@ -7281,1 +2787,11 @@\n-    __ fast_exp(x0, x1, x2, x3, x4, x5, x6, x7, rax, rcx, rdx, tmp);\n+#ifndef _WIN64\n+  __ movptr(zlen, r9); \/\/ Save r9 in r11 - zlen\n+#endif\n+  setup_arg_regs(4); \/\/ x => rdi, xlen => rsi, y => rdx\n+                     \/\/ ylen => rcx, z => r8, zlen => r11\n+                     \/\/ r9 and r10 may be used to save non-volatile registers\n+#ifdef _WIN64\n+  \/\/ last 2 arguments (#4, #5) are on stack on Win64\n+  __ movptr(z, Address(rsp, 6 * wordSize));\n+  __ movptr(zlen, Address(rsp, 7 * wordSize));\n+#endif\n@@ -7283,2 +2799,3 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+  __ movptr(xlen, rsi);\n+  __ movptr(y,    rdx);\n+  __ multiply_to_len(x, xlen, y, ylen, z, zlen, tmp1, tmp2, tmp3, tmp4, tmp5);\n@@ -7286,1 +2803,1 @@\n-    return start;\n+  restore_arg_regs();\n@@ -7288,1 +2805,2 @@\n-  }\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -7290,2 +2808,2 @@\n-  address generate_libmLog() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"libmLog\");\n+  return start;\n+}\n@@ -7293,1 +2811,19 @@\n-    address start = __ pc();\n+\/**\n+*  Arguments:\n+*\n+*  Input:\n+*    c_rarg0   - obja     address\n+*    c_rarg1   - objb     address\n+*    c_rarg3   - length   length\n+*    c_rarg4   - scale    log2_array_indxscale\n+*\n+*  Output:\n+*        rax   - int >= mismatched index, < 0 bitwise complement of tail\n+*\/\n+address StubGenerator::generate_vectorizedMismatch() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"vectorizedMismatch\");\n+  address start = __ pc();\n+\n+  BLOCK_COMMENT(\"Entry:\");\n+  __ enter();\n@@ -7295,4 +2831,6 @@\n-    const XMMRegister x0 = xmm0;\n-    const XMMRegister x1 = xmm1;\n-    const XMMRegister x2 = xmm2;\n-    const XMMRegister x3 = xmm3;\n+#ifdef _WIN64  \/\/ Win64: rcx, rdx, r8, r9 (c_rarg0, c_rarg1, ...)\n+  const Register scale = c_rarg0;  \/\/rcx, will exchange with r9\n+  const Register objb = c_rarg1;   \/\/rdx\n+  const Register length = c_rarg2; \/\/r8\n+  const Register obja = c_rarg3;   \/\/r9\n+  __ xchgq(obja, scale);  \/\/now obja and scale contains the correct contents\n@@ -7300,4 +2838,15 @@\n-    const XMMRegister x4 = xmm4;\n-    const XMMRegister x5 = xmm5;\n-    const XMMRegister x6 = xmm6;\n-    const XMMRegister x7 = xmm7;\n+  const Register tmp1 = r10;\n+  const Register tmp2 = r11;\n+#endif\n+#ifndef _WIN64 \/\/ Unix:  rdi, rsi, rdx, rcx, r8, r9 (c_rarg0, c_rarg1, ...)\n+  const Register obja = c_rarg0;   \/\/U:rdi\n+  const Register objb = c_rarg1;   \/\/U:rsi\n+  const Register length = c_rarg2; \/\/U:rdx\n+  const Register scale = c_rarg3;  \/\/U:rcx\n+  const Register tmp1 = r8;\n+  const Register tmp2 = r9;\n+#endif\n+  const Register result = rax; \/\/return value\n+  const XMMRegister vec0 = xmm0;\n+  const XMMRegister vec1 = xmm1;\n+  const XMMRegister vec2 = xmm2;\n@@ -7305,2 +2854,1 @@\n-    const Register tmp1 = r11;\n-    const Register tmp2 = r8;\n+  __ vectorized_mismatch(obja, objb, length, scale, result, tmp1, tmp2, vec0, vec1, vec2);\n@@ -7308,2 +2856,3 @@\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ vzeroupper();\n+  __ leave();\n+  __ ret(0);\n@@ -7311,1 +2860,2 @@\n-    __ fast_log(x0, x1, x2, x3, x4, x5, x6, x7, rax, rcx, rdx, tmp1, tmp2);\n+  return start;\n+}\n@@ -7313,2 +2863,11 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+\/**\n+ *  Arguments:\n+ *\n+\/\/  Input:\n+\/\/    c_rarg0   - x address\n+\/\/    c_rarg1   - x length\n+\/\/    c_rarg2   - z address\n+\/\/    c_rarg3   - z length\n+ *\n+ *\/\n+address StubGenerator::generate_squareToLen() {\n@@ -7316,1 +2875,3 @@\n-    return start;\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"squareToLen\");\n+  address start = __ pc();\n@@ -7318,1 +2879,6 @@\n-  }\n+  \/\/ Win64: rcx, rdx, r8, r9 (c_rarg0, c_rarg1, ...)\n+  \/\/ Unix:  rdi, rsi, rdx, rcx (c_rarg0, c_rarg1, ...)\n+  const Register x      = rdi;\n+  const Register len    = rsi;\n+  const Register z      = r8;\n+  const Register zlen   = rcx;\n@@ -7320,2 +2886,5 @@\n-  address generate_libmLog10() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"libmLog10\");\n+ const Register tmp1      = r12;\n+ const Register tmp2      = r13;\n+ const Register tmp3      = r14;\n+ const Register tmp4      = r15;\n+ const Register tmp5      = rbx;\n@@ -7323,1 +2892,2 @@\n-    address start = __ pc();\n+  BLOCK_COMMENT(\"Entry:\");\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n@@ -7325,4 +2895,5 @@\n-    const XMMRegister x0 = xmm0;\n-    const XMMRegister x1 = xmm1;\n-    const XMMRegister x2 = xmm2;\n-    const XMMRegister x3 = xmm3;\n+  setup_arg_regs(4); \/\/ x => rdi, len => rsi, z => rdx\n+                     \/\/ zlen => rcx\n+                     \/\/ r9 and r10 may be used to save non-volatile registers\n+  __ movptr(r8, rdx);\n+  __ square_to_len(x, len, z, zlen, tmp1, tmp2, tmp3, tmp4, tmp5, rdx, rax);\n@@ -7330,4 +2901,1 @@\n-    const XMMRegister x4 = xmm4;\n-    const XMMRegister x5 = xmm5;\n-    const XMMRegister x6 = xmm6;\n-    const XMMRegister x7 = xmm7;\n+  restore_arg_regs();\n@@ -7335,1 +2903,2 @@\n-    const Register tmp = r11;\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -7337,2 +2906,2 @@\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  return start;\n+}\n@@ -7340,1 +2909,4 @@\n-    __ fast_log10(x0, x1, x2, x3, x4, x5, x6, x7, rax, rcx, rdx, tmp);\n+address StubGenerator::generate_method_entry_barrier() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"nmethod_entry_barrier\");\n+  address start = __ pc();\n@@ -7342,2 +2914,1 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+  Label deoptimize_label;\n@@ -7345,1 +2916,1 @@\n-    return start;\n+  __ push(-1); \/\/ cookie, this is used for writing the new rsp when deoptimizing\n@@ -7347,1 +2918,2 @@\n-  }\n+  BLOCK_COMMENT(\"Entry:\");\n+  __ enter(); \/\/ save rbp\n@@ -7349,2 +2921,3 @@\n-  address generate_libmPow() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"libmPow\");\n+  \/\/ save c_rarg0, because we want to use that value.\n+  \/\/ We could do without it but then we depend on the number of slots used by pusha\n+  __ push(c_rarg0);\n@@ -7352,1 +2925,1 @@\n-    address start = __ pc();\n+  __ lea(c_rarg0, Address(rsp, wordSize * 3)); \/\/ 1 for cookie, 1 for rbp, 1 for c_rarg0 - this should be the return address\n@@ -7354,4 +2927,1 @@\n-    const XMMRegister x0 = xmm0;\n-    const XMMRegister x1 = xmm1;\n-    const XMMRegister x2 = xmm2;\n-    const XMMRegister x3 = xmm3;\n+  __ pusha();\n@@ -7359,4 +2929,14 @@\n-    const XMMRegister x4 = xmm4;\n-    const XMMRegister x5 = xmm5;\n-    const XMMRegister x6 = xmm6;\n-    const XMMRegister x7 = xmm7;\n+  \/\/ The method may have floats as arguments, and we must spill them before calling\n+  \/\/ the VM runtime.\n+  assert(Argument::n_float_register_parameters_j == 8, \"Assumption\");\n+  const int xmm_size = wordSize * 2;\n+  const int xmm_spill_size = xmm_size * Argument::n_float_register_parameters_j;\n+  __ subptr(rsp, xmm_spill_size);\n+  __ movdqu(Address(rsp, xmm_size * 7), xmm7);\n+  __ movdqu(Address(rsp, xmm_size * 6), xmm6);\n+  __ movdqu(Address(rsp, xmm_size * 5), xmm5);\n+  __ movdqu(Address(rsp, xmm_size * 4), xmm4);\n+  __ movdqu(Address(rsp, xmm_size * 3), xmm3);\n+  __ movdqu(Address(rsp, xmm_size * 2), xmm2);\n+  __ movdqu(Address(rsp, xmm_size * 1), xmm1);\n+  __ movdqu(Address(rsp, xmm_size * 0), xmm0);\n@@ -7364,4 +2944,1 @@\n-    const Register tmp1 = r8;\n-    const Register tmp2 = r9;\n-    const Register tmp3 = r10;\n-    const Register tmp4 = r11;\n+  __ call_VM_leaf(CAST_FROM_FN_PTR(address, static_cast<int (*)(address*)>(BarrierSetNMethod::nmethod_stub_entry_barrier)), 1);\n@@ -7369,2 +2946,9 @@\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ movdqu(xmm0, Address(rsp, xmm_size * 0));\n+  __ movdqu(xmm1, Address(rsp, xmm_size * 1));\n+  __ movdqu(xmm2, Address(rsp, xmm_size * 2));\n+  __ movdqu(xmm3, Address(rsp, xmm_size * 3));\n+  __ movdqu(xmm4, Address(rsp, xmm_size * 4));\n+  __ movdqu(xmm5, Address(rsp, xmm_size * 5));\n+  __ movdqu(xmm6, Address(rsp, xmm_size * 6));\n+  __ movdqu(xmm7, Address(rsp, xmm_size * 7));\n+  __ addptr(rsp, xmm_spill_size);\n@@ -7372,1 +2956,2 @@\n-    __ fast_pow(x0, x1, x2, x3, x4, x5, x6, x7, rax, rcx, rdx, tmp1, tmp2, tmp3, tmp4);\n+  __ cmpl(rax, 1); \/\/ 1 means deoptimize\n+  __ jcc(Assembler::equal, deoptimize_label);\n@@ -7374,2 +2959,2 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+  __ popa();\n+  __ pop(c_rarg0);\n@@ -7377,1 +2962,1 @@\n-    return start;\n+  __ leave();\n@@ -7379,1 +2964,2 @@\n-  }\n+  __ addptr(rsp, 1 * wordSize); \/\/ cookie\n+  __ ret(0);\n@@ -7381,3 +2967,1 @@\n-  address generate_libmSin() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"libmSin\");\n-    address start = __ pc();\n+  __ BIND(deoptimize_label);\n@@ -7386,4 +2970,2 @@\n-    const XMMRegister x0 = xmm0;\n-    const XMMRegister x1 = xmm1;\n-    const XMMRegister x2 = xmm2;\n-    const XMMRegister x3 = xmm3;\n+  __ popa();\n+  __ pop(c_rarg0);\n@@ -7391,4 +2973,1 @@\n-    const XMMRegister x4 = xmm4;\n-    const XMMRegister x5 = xmm5;\n-    const XMMRegister x6 = xmm6;\n-    const XMMRegister x7 = xmm7;\n+  __ leave();\n@@ -7396,4 +2975,3 @@\n-    const Register tmp1 = r8;\n-    const Register tmp2 = r9;\n-    const Register tmp3 = r10;\n-    const Register tmp4 = r11;\n+  \/\/ this can be taken out, but is good for verification purposes. getting a SIGSEGV\n+  \/\/ here while still having a correct stack is valuable\n+  __ testptr(rsp, Address(rsp, 0));\n@@ -7401,2 +2979,2 @@\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ movptr(rsp, Address(rsp, 0)); \/\/ new rsp was written in the barrier\n+  __ jmp(Address(rsp, -1 * wordSize)); \/\/ jmp target should be callers verified_entry_point\n@@ -7404,5 +2982,2 @@\n-#ifdef _WIN64\n-    __ push(rsi);\n-    __ push(rdi);\n-#endif\n-    __ fast_sin(x0, x1, x2, x3, x4, x5, x6, x7, rax, rbx, rcx, rdx, tmp1, tmp2, tmp3, tmp4);\n+  return start;\n+}\n@@ -7410,0 +2985,39 @@\n+ \/**\n+ *  Arguments:\n+ *\n+ *  Input:\n+ *    c_rarg0   - out address\n+ *    c_rarg1   - in address\n+ *    c_rarg2   - offset\n+ *    c_rarg3   - len\n+ * not Win64\n+ *    c_rarg4   - k\n+ * Win64\n+ *    rsp+40    - k\n+ *\/\n+address StubGenerator::generate_mulAdd() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"mulAdd\");\n+  address start = __ pc();\n+\n+  \/\/ Win64: rcx, rdx, r8, r9 (c_rarg0, c_rarg1, ...)\n+  \/\/ Unix:  rdi, rsi, rdx, rcx, r8, r9 (c_rarg0, c_rarg1, ...)\n+  const Register out     = rdi;\n+  const Register in      = rsi;\n+  const Register offset  = r11;\n+  const Register len     = rcx;\n+  const Register k       = r8;\n+\n+  \/\/ Next registers will be saved on stack in mul_add().\n+  const Register tmp1  = r12;\n+  const Register tmp2  = r13;\n+  const Register tmp3  = r14;\n+  const Register tmp4  = r15;\n+  const Register tmp5  = rbx;\n+\n+  BLOCK_COMMENT(\"Entry:\");\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+\n+  setup_arg_regs(4); \/\/ out => rdi, in => rsi, offset => rdx\n+                     \/\/ len => rcx, k => r8\n+                     \/\/ r9 and r10 may be used to save non-volatile registers\n@@ -7411,2 +3025,2 @@\n-    __ pop(rdi);\n-    __ pop(rsi);\n+  \/\/ last argument is on stack on Win64\n+  __ movl(k, Address(rsp, 6 * wordSize));\n@@ -7414,0 +3028,2 @@\n+  __ movptr(r11, rdx);  \/\/ move offset in rdx to offset(r11)\n+  __ mul_add(out, in, offset, len, k, tmp1, tmp2, tmp3, tmp4, tmp5, rdx, rax);\n@@ -7415,4 +3031,1 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    return start;\n+  restore_arg_regs();\n@@ -7420,4 +3033,2 @@\n-  }\n-\n-  address generate_libmCos() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"libmCos\");\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -7425,1 +3036,2 @@\n-    address start = __ pc();\n+  return start;\n+}\n@@ -7427,4 +3039,28 @@\n-    const XMMRegister x0 = xmm0;\n-    const XMMRegister x1 = xmm1;\n-    const XMMRegister x2 = xmm2;\n-    const XMMRegister x3 = xmm3;\n+address StubGenerator::generate_bigIntegerRightShift() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"bigIntegerRightShiftWorker\");\n+  address start = __ pc();\n+\n+  Label Shift512Loop, ShiftTwo, ShiftTwoLoop, ShiftOne, Exit;\n+  \/\/ For Unix, the arguments are as follows: rdi, rsi, rdx, rcx, r8.\n+  const Register newArr = rdi;\n+  const Register oldArr = rsi;\n+  const Register newIdx = rdx;\n+  const Register shiftCount = rcx;  \/\/ It was intentional to have shiftCount in rcx since it is used implicitly for shift.\n+  const Register totalNumIter = r8;\n+\n+  \/\/ For windows, we use r9 and r10 as temps to save rdi and rsi. Thus we cannot allocate them for our temps.\n+  \/\/ For everything else, we prefer using r9 and r10 since we do not have to save them before use.\n+  const Register tmp1 = r11;                    \/\/ Caller save.\n+  const Register tmp2 = rax;                    \/\/ Caller save.\n+  const Register tmp3 = WIN64_ONLY(r12) NOT_WIN64(r9);   \/\/ Windows: Callee save. Linux: Caller save.\n+  const Register tmp4 = WIN64_ONLY(r13) NOT_WIN64(r10);  \/\/ Windows: Callee save. Linux: Caller save.\n+  const Register tmp5 = r14;                    \/\/ Callee save.\n+  const Register tmp6 = r15;\n+\n+  const XMMRegister x0 = xmm0;\n+  const XMMRegister x1 = xmm1;\n+  const XMMRegister x2 = xmm2;\n+\n+  BLOCK_COMMENT(\"Entry:\");\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n@@ -7432,4 +3068,89 @@\n-    const XMMRegister x4 = xmm4;\n-    const XMMRegister x5 = xmm5;\n-    const XMMRegister x6 = xmm6;\n-    const XMMRegister x7 = xmm7;\n+#ifdef _WIN64\n+  setup_arg_regs(4);\n+  \/\/ For windows, since last argument is on stack, we need to move it to the appropriate register.\n+  __ movl(totalNumIter, Address(rsp, 6 * wordSize));\n+  \/\/ Save callee save registers.\n+  __ push(tmp3);\n+  __ push(tmp4);\n+#endif\n+  __ push(tmp5);\n+\n+  \/\/ Rename temps used throughout the code.\n+  const Register idx = tmp1;\n+  const Register nIdx = tmp2;\n+\n+  __ xorl(idx, idx);\n+\n+  \/\/ Start right shift from end of the array.\n+  \/\/ For example, if #iteration = 4 and newIdx = 1\n+  \/\/ then dest[4] = src[4] >> shiftCount  | src[3] <<< (shiftCount - 32)\n+  \/\/ if #iteration = 4 and newIdx = 0\n+  \/\/ then dest[3] = src[4] >> shiftCount  | src[3] <<< (shiftCount - 32)\n+  __ movl(idx, totalNumIter);\n+  __ movl(nIdx, idx);\n+  __ addl(nIdx, newIdx);\n+\n+  \/\/ If vectorization is enabled, check if the number of iterations is at least 64\n+  \/\/ If not, then go to ShifTwo processing 2 iterations\n+  if (VM_Version::supports_avx512_vbmi2()) {\n+    __ cmpptr(totalNumIter, (AVX3Threshold\/64));\n+    __ jcc(Assembler::less, ShiftTwo);\n+\n+    if (AVX3Threshold < 16 * 64) {\n+      __ cmpl(totalNumIter, 16);\n+      __ jcc(Assembler::less, ShiftTwo);\n+    }\n+    __ evpbroadcastd(x0, shiftCount, Assembler::AVX_512bit);\n+    __ subl(idx, 16);\n+    __ subl(nIdx, 16);\n+    __ BIND(Shift512Loop);\n+    __ evmovdqul(x2, Address(oldArr, idx, Address::times_4, 4), Assembler::AVX_512bit);\n+    __ evmovdqul(x1, Address(oldArr, idx, Address::times_4), Assembler::AVX_512bit);\n+    __ vpshrdvd(x2, x1, x0, Assembler::AVX_512bit);\n+    __ evmovdqul(Address(newArr, nIdx, Address::times_4), x2, Assembler::AVX_512bit);\n+    __ subl(nIdx, 16);\n+    __ subl(idx, 16);\n+    __ jcc(Assembler::greaterEqual, Shift512Loop);\n+    __ addl(idx, 16);\n+    __ addl(nIdx, 16);\n+  }\n+  __ BIND(ShiftTwo);\n+  __ cmpl(idx, 2);\n+  __ jcc(Assembler::less, ShiftOne);\n+  __ subl(idx, 2);\n+  __ subl(nIdx, 2);\n+  __ BIND(ShiftTwoLoop);\n+  __ movl(tmp5, Address(oldArr, idx, Address::times_4, 8));\n+  __ movl(tmp4, Address(oldArr, idx, Address::times_4, 4));\n+  __ movl(tmp3, Address(oldArr, idx, Address::times_4));\n+  __ shrdl(tmp5, tmp4);\n+  __ shrdl(tmp4, tmp3);\n+  __ movl(Address(newArr, nIdx, Address::times_4, 4), tmp5);\n+  __ movl(Address(newArr, nIdx, Address::times_4), tmp4);\n+  __ subl(nIdx, 2);\n+  __ subl(idx, 2);\n+  __ jcc(Assembler::greaterEqual, ShiftTwoLoop);\n+  __ addl(idx, 2);\n+  __ addl(nIdx, 2);\n+\n+  \/\/ Do the last iteration\n+  __ BIND(ShiftOne);\n+  __ cmpl(idx, 1);\n+  __ jcc(Assembler::less, Exit);\n+  __ subl(idx, 1);\n+  __ subl(nIdx, 1);\n+  __ movl(tmp4, Address(oldArr, idx, Address::times_4, 4));\n+  __ movl(tmp3, Address(oldArr, idx, Address::times_4));\n+  __ shrdl(tmp4, tmp3);\n+  __ movl(Address(newArr, nIdx, Address::times_4), tmp4);\n+  __ BIND(Exit);\n+  __ vzeroupper();\n+  \/\/ Restore callee save registers.\n+  __ pop(tmp5);\n+#ifdef _WIN64\n+  __ pop(tmp4);\n+  __ pop(tmp3);\n+  restore_arg_regs();\n+#endif\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -7437,4 +3158,2 @@\n-    const Register tmp1 = r8;\n-    const Register tmp2 = r9;\n-    const Register tmp3 = r10;\n-    const Register tmp4 = r11;\n+  return start;\n+}\n@@ -7442,2 +3161,38 @@\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+ \/**\n+ *  Arguments:\n+ *\n+ *  Input:\n+ *    c_rarg0   - newArr address\n+ *    c_rarg1   - oldArr address\n+ *    c_rarg2   - newIdx\n+ *    c_rarg3   - shiftCount\n+ * not Win64\n+ *    c_rarg4   - numIter\n+ * Win64\n+ *    rsp40    - numIter\n+ *\/\n+address StubGenerator::generate_bigIntegerLeftShift() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this,  \"StubRoutines\", \"bigIntegerLeftShiftWorker\");\n+  address start = __ pc();\n+\n+  Label Shift512Loop, ShiftTwo, ShiftTwoLoop, ShiftOne, Exit;\n+  \/\/ For Unix, the arguments are as follows: rdi, rsi, rdx, rcx, r8.\n+  const Register newArr = rdi;\n+  const Register oldArr = rsi;\n+  const Register newIdx = rdx;\n+  const Register shiftCount = rcx;  \/\/ It was intentional to have shiftCount in rcx since it is used implicitly for shift.\n+  const Register totalNumIter = r8;\n+  \/\/ For windows, we use r9 and r10 as temps to save rdi and rsi. Thus we cannot allocate them for our temps.\n+  \/\/ For everything else, we prefer using r9 and r10 since we do not have to save them before use.\n+  const Register tmp1 = r11;                    \/\/ Caller save.\n+  const Register tmp2 = rax;                    \/\/ Caller save.\n+  const Register tmp3 = WIN64_ONLY(r12) NOT_WIN64(r9);   \/\/ Windows: Callee save. Linux: Caller save.\n+  const Register tmp4 = WIN64_ONLY(r13) NOT_WIN64(r10);  \/\/ Windows: Callee save. Linux: Caller save.\n+  const Register tmp5 = r14;                    \/\/ Callee save.\n+\n+  const XMMRegister x0 = xmm0;\n+  const XMMRegister x1 = xmm1;\n+  const XMMRegister x2 = xmm2;\n+  BLOCK_COMMENT(\"Entry:\");\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n@@ -7446,5 +3201,73 @@\n-    __ push(rsi);\n-    __ push(rdi);\n-#endif\n-    __ fast_cos(x0, x1, x2, x3, x4, x5, x6, x7, rax, rcx, rdx, tmp1, tmp2, tmp3, tmp4);\n-\n+  setup_arg_regs(4);\n+  \/\/ For windows, since last argument is on stack, we need to move it to the appropriate register.\n+  __ movl(totalNumIter, Address(rsp, 6 * wordSize));\n+  \/\/ Save callee save registers.\n+  __ push(tmp3);\n+  __ push(tmp4);\n+#endif\n+  __ push(tmp5);\n+\n+  \/\/ Rename temps used throughout the code\n+  const Register idx = tmp1;\n+  const Register numIterTmp = tmp2;\n+\n+  \/\/ Start idx from zero.\n+  __ xorl(idx, idx);\n+  \/\/ Compute interior pointer for new array. We do this so that we can use same index for both old and new arrays.\n+  __ lea(newArr, Address(newArr, newIdx, Address::times_4));\n+  __ movl(numIterTmp, totalNumIter);\n+\n+  \/\/ If vectorization is enabled, check if the number of iterations is at least 64\n+  \/\/ If not, then go to ShiftTwo shifting two numbers at a time\n+  if (VM_Version::supports_avx512_vbmi2()) {\n+    __ cmpl(totalNumIter, (AVX3Threshold\/64));\n+    __ jcc(Assembler::less, ShiftTwo);\n+\n+    if (AVX3Threshold < 16 * 64) {\n+      __ cmpl(totalNumIter, 16);\n+      __ jcc(Assembler::less, ShiftTwo);\n+    }\n+    __ evpbroadcastd(x0, shiftCount, Assembler::AVX_512bit);\n+    __ subl(numIterTmp, 16);\n+    __ BIND(Shift512Loop);\n+    __ evmovdqul(x1, Address(oldArr, idx, Address::times_4), Assembler::AVX_512bit);\n+    __ evmovdqul(x2, Address(oldArr, idx, Address::times_4, 0x4), Assembler::AVX_512bit);\n+    __ vpshldvd(x1, x2, x0, Assembler::AVX_512bit);\n+    __ evmovdqul(Address(newArr, idx, Address::times_4), x1, Assembler::AVX_512bit);\n+    __ addl(idx, 16);\n+    __ subl(numIterTmp, 16);\n+    __ jcc(Assembler::greaterEqual, Shift512Loop);\n+    __ addl(numIterTmp, 16);\n+  }\n+  __ BIND(ShiftTwo);\n+  __ cmpl(totalNumIter, 1);\n+  __ jcc(Assembler::less, Exit);\n+  __ movl(tmp3, Address(oldArr, idx, Address::times_4));\n+  __ subl(numIterTmp, 2);\n+  __ jcc(Assembler::less, ShiftOne);\n+\n+  __ BIND(ShiftTwoLoop);\n+  __ movl(tmp4, Address(oldArr, idx, Address::times_4, 0x4));\n+  __ movl(tmp5, Address(oldArr, idx, Address::times_4, 0x8));\n+  __ shldl(tmp3, tmp4);\n+  __ shldl(tmp4, tmp5);\n+  __ movl(Address(newArr, idx, Address::times_4), tmp3);\n+  __ movl(Address(newArr, idx, Address::times_4, 0x4), tmp4);\n+  __ movl(tmp3, tmp5);\n+  __ addl(idx, 2);\n+  __ subl(numIterTmp, 2);\n+  __ jcc(Assembler::greaterEqual, ShiftTwoLoop);\n+\n+  \/\/ Do the last iteration\n+  __ BIND(ShiftOne);\n+  __ addl(numIterTmp, 2);\n+  __ cmpl(numIterTmp, 1);\n+  __ jcc(Assembler::less, Exit);\n+  __ movl(tmp4, Address(oldArr, idx, Address::times_4, 0x4));\n+  __ shldl(tmp3, tmp4);\n+  __ movl(Address(newArr, idx, Address::times_4), tmp3);\n+\n+  __ BIND(Exit);\n+  __ vzeroupper();\n+  \/\/ Restore callee save registers.\n+  __ pop(tmp5);\n@@ -7452,2 +3275,3 @@\n-    __ pop(rdi);\n-    __ pop(rsi);\n+  __ pop(tmp4);\n+  __ pop(tmp3);\n+  restore_arg_regs();\n@@ -7455,0 +3279,2 @@\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -7456,4 +3282,2 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    return start;\n+  return start;\n+}\n@@ -7461,0 +3285,23 @@\n+void StubGenerator::generate_libm_stubs() {\n+  if (UseLibmIntrinsic && InlineIntrinsics) {\n+    if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dsin)) {\n+      StubRoutines::_dsin = generate_libmSin(); \/\/ from stubGenerator_x86_64_sin.cpp\n+    }\n+    if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dcos)) {\n+      StubRoutines::_dcos = generate_libmCos(); \/\/ from stubGenerator_x86_64_cos.cpp\n+    }\n+    if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dtan)) {\n+      StubRoutines::_dtan = generate_libmTan();\n+    }\n+    if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dexp)) {\n+      StubRoutines::_dexp = generate_libmExp();\n+    }\n+    if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dpow)) {\n+      StubRoutines::_dpow = generate_libmPow();\n+    }\n+    if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dlog)) {\n+      StubRoutines::_dlog = generate_libmLog();\n+    }\n+    if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dlog10)) {\n+      StubRoutines::_dlog10 = generate_libmLog10();\n+    }\n@@ -7462,0 +3309,1 @@\n+}\n@@ -7463,13 +3311,39 @@\n-  address generate_libmTan() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"libmTan\");\n-\n-    address start = __ pc();\n-\n-    const XMMRegister x0 = xmm0;\n-    const XMMRegister x1 = xmm1;\n-    const XMMRegister x2 = xmm2;\n-    const XMMRegister x3 = xmm3;\n-    const XMMRegister x4 = xmm4;\n-    const XMMRegister x5 = xmm5;\n-    const XMMRegister x6 = xmm6;\n-    const XMMRegister x7 = xmm7;\n+  \/\/ Call stub to call runtime oopDesc::load_nklass_runtime().\n+  \/\/ rax: call argument (object)\n+  \/\/ rax: return object's narrowKlass\n+  \/\/ Preserves all caller-saved registers, except rax\n+#ifdef _LP64\n+address StubGenerator::generate_load_nklass() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark(this, \"StubRoutines\", \"load_nklass\");\n+  address start = __ pc();\n+  __ enter(); \/\/ save rbp\n+\n+  __ andptr(rsp, -(StackAlignmentInBytes));    \/\/ Align stack\n+  __ push_FPU_state();\n+\n+  __ push(rdi);\n+  __ push(rsi);\n+  __ push(rdx);\n+  __ push(rcx);\n+  __ push(r8);\n+  __ push(r9);\n+  __ push(r10);\n+  __ push(r11);\n+  __ call_VM_leaf(CAST_FROM_FN_PTR(address, oopDesc::load_nklass_runtime), rax);\n+  __ pop(r11);\n+  __ pop(r10);\n+  __ pop(r9);\n+  __ pop(r8);\n+  __ pop(rcx);\n+  __ pop(rdx);\n+  __ pop(rsi);\n+  __ pop(rdi);\n+\n+  __ pop_FPU_state();\n+\n+  __ leave();\n+  __ ret(0);\n+  return start;\n+}\n+#endif \/\/ _LP64\n@@ -7477,4 +3351,2 @@\n-    const Register tmp1 = r8;\n-    const Register tmp2 = r9;\n-    const Register tmp3 = r10;\n-    const Register tmp4 = r11;\n+address StubGenerator::generate_cont_thaw(const char* label, Continuation::thaw_kind kind) {\n+  if (!Continuations::enabled()) return nullptr;\n@@ -7483,2 +3355,2 @@\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  bool return_barrier = Continuation::is_thaw_return_barrier(kind);\n+  bool return_barrier_exception = Continuation::is_thaw_return_barrier_exception(kind);\n@@ -7486,5 +3358,2 @@\n-#ifdef _WIN64\n-    __ push(rsi);\n-    __ push(rdi);\n-#endif\n-    __ fast_tan(x0, x1, x2, x3, x4, x5, x6, x7, rax, rcx, rdx, tmp1, tmp2, tmp3, tmp4);\n+  StubCodeMark mark(this, \"StubRoutines\", label);\n+  address start = __ pc();\n@@ -7492,4 +3361,1 @@\n-#ifdef _WIN64\n-    __ pop(rdi);\n-    __ pop(rsi);\n-#endif\n+  \/\/ TODO: Handle Valhalla return types. May require generating different return barriers.\n@@ -7497,2 +3363,7 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+  if (!return_barrier) {\n+    \/\/ Pop return address. If we don't do this, we get a drift,\n+    \/\/ where the bottom-most frozen frame continuously grows.\n+    __ pop(c_rarg3);\n+  } else {\n+    __ movptr(rsp, Address(r15_thread, JavaThread::cont_entry_offset()));\n+  }\n@@ -7500,1 +3371,9 @@\n-    return start;\n+#ifdef ASSERT\n+  {\n+    Label L_good_sp;\n+    __ cmpptr(rsp, Address(r15_thread, JavaThread::cont_entry_offset()));\n+    __ jcc(Assembler::equal, L_good_sp);\n+    __ stop(\"Incorrect rsp at thaw entry\");\n+    __ BIND(L_good_sp);\n+  }\n+#endif \/\/ ASSERT\n@@ -7502,0 +3381,4 @@\n+  if (return_barrier) {\n+    \/\/ Preserve possible return value from a method returning to the return barrier.\n+    __ push(rax);\n+    __ push_d(xmm0);\n@@ -7504,31 +3387,4 @@\n-  \/\/ Call stub to call runtime oopDesc::load_nklass_runtime().\n-  \/\/ rax: call argument (object)\n-  \/\/ rax: return object's narrowKlass\n-  \/\/ Preserves all caller-saved registers, except rax\n-#ifdef _LP64\n-  address generate_load_nklass() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark(this, \"StubRoutines\", \"load_nklass\");\n-    address start = __ pc();\n-    __ enter(); \/\/ save rbp\n-\n-    __ andptr(rsp, -(StackAlignmentInBytes));    \/\/ Align stack\n-    __ push_FPU_state();\n-\n-    __ push(rdi);\n-    __ push(rsi);\n-    __ push(rdx);\n-    __ push(rcx);\n-    __ push(r8);\n-    __ push(r9);\n-    __ push(r10);\n-    __ push(r11);\n-    __ call_VM_leaf(CAST_FROM_FN_PTR(address, oopDesc::load_nklass_runtime), rax);\n-    __ pop(r11);\n-    __ pop(r10);\n-    __ pop(r9);\n-    __ pop(r8);\n-    __ pop(rcx);\n-    __ pop(rdx);\n-    __ pop(rsi);\n-    __ pop(rdi);\n+  __ movptr(c_rarg0, r15_thread);\n+  __ movptr(c_rarg1, (return_barrier ? 1 : 0));\n+  __ call_VM_leaf(CAST_FROM_FN_PTR(address, Continuation::prepare_thaw), 2);\n+  __ movptr(rbx, rax);\n@@ -7536,1 +3392,6 @@\n-    __ pop_FPU_state();\n+  if (return_barrier) {\n+    \/\/ Restore return value from a method returning to the return barrier.\n+    \/\/ No safepoint in the call to thaw, so even an oop return value should be OK.\n+    __ pop_d(xmm0);\n+    __ pop(rax);\n+  }\n@@ -7538,3 +3399,7 @@\n-    __ leave();\n-    __ ret(0);\n-    return start;\n+#ifdef ASSERT\n+  {\n+    Label L_good_sp;\n+    __ cmpptr(rsp, Address(r15_thread, JavaThread::cont_entry_offset()));\n+    __ jcc(Assembler::equal, L_good_sp);\n+    __ stop(\"Incorrect rsp after prepare thaw\");\n+    __ BIND(L_good_sp);\n@@ -7542,1 +3407,1 @@\n-#endif \/\/ _LP64\n+#endif \/\/ ASSERT\n@@ -7544,2 +3409,6 @@\n-  RuntimeStub* generate_cont_doYield() {\n-    if (!Continuations::enabled()) return nullptr;\n+  \/\/ rbx contains the size of the frames to thaw, 0 if overflow or no more frames\n+  Label L_thaw_success;\n+  __ testptr(rbx, rbx);\n+  __ jccb(Assembler::notZero, L_thaw_success);\n+  __ jump(ExternalAddress(StubRoutines::throw_StackOverflowError_entry()));\n+  __ bind(L_thaw_success);\n@@ -7547,7 +3416,3 @@\n-    enum layout {\n-      rbp_off,\n-      rbpH_off,\n-      return_off,\n-      return_off2,\n-      framesize \/\/ inclusive of return address\n-    };\n+  \/\/ Make room for the thawed frames and align the stack.\n+  __ subptr(rsp, rbx);\n+  __ andptr(rsp, -StackAlignmentInBytes);\n@@ -7555,2 +3420,5 @@\n-    CodeBuffer code(\"cont_doYield\", 512, 64);\n-    MacroAssembler* _masm = new MacroAssembler(&code);\n+  if (return_barrier) {\n+    \/\/ Preserve possible return value from a method returning to the return barrier. (Again.)\n+    __ push(rax);\n+    __ push_d(xmm0);\n+  }\n@@ -7558,3 +3426,5 @@\n-    address start = __ pc();\n-    __ enter();\n-    address the_pc = __ pc();\n+  \/\/ If we want, we can templatize thaw by kind, and have three different entries.\n+  __ movptr(c_rarg0, r15_thread);\n+  __ movptr(c_rarg1, kind);\n+  __ call_VM_leaf(Continuation::thaw_entry(), 2);\n+  __ movptr(rbx, rax);\n@@ -7562,1 +3432,9 @@\n-    int frame_complete = the_pc - start;\n+  if (return_barrier) {\n+    \/\/ Restore return value from a method returning to the return barrier. (Again.)\n+    \/\/ No safepoint in the call to thaw, so even an oop return value should be OK.\n+    __ pop_d(xmm0);\n+    __ pop(rax);\n+  } else {\n+    \/\/ Return 0 (success) from doYield.\n+    __ xorptr(rax, rax);\n+  }\n@@ -7564,7 +3442,4 @@\n-    \/\/ This nop must be exactly at the PC we push into the frame info.\n-    \/\/ We use this nop for fast CodeBlob lookup, associate the OopMap\n-    \/\/ with it right away.\n-    __ post_call_nop();\n-    OopMapSet* oop_maps = new OopMapSet();\n-    OopMap* map = new OopMap(framesize, 1);\n-    oop_maps->add_gc_map(frame_complete, map);\n+  \/\/ After thawing, rbx is the SP of the yielding frame.\n+  \/\/ Move there, and then to saved RBP slot.\n+  __ movptr(rsp, rbx);\n+  __ subptr(rsp, 2*wordSize);\n@@ -7572,1 +3447,1 @@\n-    __ set_last_Java_frame(rsp, rbp, the_pc);\n+  if (return_barrier_exception) {\n@@ -7574,3 +3449,1 @@\n-    __ movptr(c_rarg1, rsp);\n-    __ call_VM_leaf(Continuation::freeze_entry(), 2);\n-    __ reset_last_Java_frame(true);\n+    __ movptr(c_rarg1, Address(rsp, wordSize)); \/\/ return address\n@@ -7578,1 +3451,2 @@\n-    Label L_pinned;\n+    \/\/ rax still holds the original exception oop, save it before the call\n+    __ push(rax);\n@@ -7580,2 +3454,2 @@\n-    __ testptr(rax, rax);\n-    __ jcc(Assembler::notZero, L_pinned);\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::exception_handler_for_return_address), 2);\n+    __ movptr(rbx, rax);\n@@ -7583,2 +3457,11 @@\n-    __ movptr(rsp, Address(r15_thread, JavaThread::cont_entry_offset()));\n-    continuation_enter_cleanup(_masm);\n+    \/\/ Continue at exception handler:\n+    \/\/   rax: exception oop\n+    \/\/   rbx: exception handler\n+    \/\/   rdx: exception pc\n+    __ pop(rax);\n+    __ verify_oop(rax);\n+    __ pop(rbp); \/\/ pop out RBP here too\n+    __ pop(rdx);\n+    __ jmp(rbx);\n+  } else {\n+    \/\/ We are \"returning\" into the topmost thawed frame; see Thaw::push_return_frame\n@@ -7587,0 +3470,1 @@\n+  }\n@@ -7588,1 +3472,2 @@\n-    __ bind(L_pinned);\n+  return start;\n+}\n@@ -7590,3 +3475,3 @@\n-    \/\/ Pinned, return to caller\n-    __ leave();\n-    __ ret(0);\n+address StubGenerator::generate_cont_thaw() {\n+  return generate_cont_thaw(\"Cont thaw\", Continuation::thaw_top);\n+}\n@@ -7594,9 +3479,1 @@\n-    RuntimeStub* stub =\n-      RuntimeStub::new_runtime_stub(code.name(),\n-                                    &code,\n-                                    frame_complete,\n-                                    (framesize >> (LogBytesPerWord - LogBytesPerInt)),\n-                                    oop_maps,\n-                                    false);\n-    return stub;\n-  }\n+\/\/ TODO: will probably need multiple return barriers depending on return type\n@@ -7604,2 +3481,3 @@\n-  address generate_cont_thaw(const char* label, Continuation::thaw_kind kind) {\n-    if (!Continuations::enabled()) return nullptr;\n+address StubGenerator::generate_cont_returnBarrier() {\n+  return generate_cont_thaw(\"Cont thaw return barrier\", Continuation::thaw_return_barrier);\n+}\n@@ -7607,2 +3485,3 @@\n-    bool return_barrier = Continuation::is_thaw_return_barrier(kind);\n-    bool return_barrier_exception = Continuation::is_thaw_return_barrier_exception(kind);\n+address StubGenerator::generate_cont_returnBarrier_exception() {\n+  return generate_cont_thaw(\"Cont thaw return barrier exception\", Continuation::thaw_return_barrier_exception);\n+}\n@@ -7610,2 +3489,1 @@\n-    StubCodeMark mark(this, \"StubRoutines\", label);\n-    address start = __ pc();\n+#if INCLUDE_JFR\n@@ -7613,1 +3491,11 @@\n-    \/\/ TODO: Handle Valhalla return types. May require generating different return barriers.\n+\/\/ For c2: c_rarg0 is junk, call to runtime to write a checkpoint.\n+\/\/ It returns a jobject handle to the event writer.\n+\/\/ The handle is dereferenced and the return value is the event writer oop.\n+RuntimeStub* StubGenerator::generate_jfr_write_checkpoint() {\n+  enum layout {\n+    rbp_off,\n+    rbpH_off,\n+    return_off,\n+    return_off2,\n+    framesize \/\/ inclusive of return address\n+  };\n@@ -7615,7 +3503,3 @@\n-    if (!return_barrier) {\n-      \/\/ Pop return address. If we don't do this, we get a drift,\n-      \/\/ where the bottom-most frozen frame continuously grows.\n-      __ pop(c_rarg3);\n-    } else {\n-      __ movptr(rsp, Address(r15_thread, JavaThread::cont_entry_offset()));\n-    }\n+  CodeBuffer code(\"jfr_write_checkpoint\", 512, 64);\n+  MacroAssembler* _masm = new MacroAssembler(&code);\n+  address start = __ pc();\n@@ -7623,9 +3507,2 @@\n-#ifdef ASSERT\n-    {\n-      Label L_good_sp;\n-      __ cmpptr(rsp, Address(r15_thread, JavaThread::cont_entry_offset()));\n-      __ jcc(Assembler::equal, L_good_sp);\n-      __ stop(\"Incorrect rsp at thaw entry\");\n-      __ BIND(L_good_sp);\n-    }\n-#endif\n+  __ enter();\n+  address the_pc = __ pc();\n@@ -7633,5 +3510,1 @@\n-    if (return_barrier) {\n-      \/\/ Preserve possible return value from a method returning to the return barrier.\n-      __ push(rax);\n-      __ push_d(xmm0);\n-    }\n+  int frame_complete = the_pc - start;\n@@ -7639,4 +3512,4 @@\n-    __ movptr(c_rarg0, r15_thread);\n-    __ movptr(c_rarg1, (return_barrier ? 1 : 0));\n-    __ call_VM_leaf(CAST_FROM_FN_PTR(address, Continuation::prepare_thaw), 2);\n-    __ movptr(rbx, rax);\n+  __ set_last_Java_frame(rsp, rbp, the_pc, rscratch1);\n+  __ movptr(c_rarg0, r15_thread);\n+  __ call_VM_leaf(CAST_FROM_FN_PTR(address, JfrIntrinsicSupport::write_checkpoint), 1);\n+  __ reset_last_Java_frame(true);\n@@ -7644,6 +3517,4 @@\n-    if (return_barrier) {\n-      \/\/ Restore return value from a method returning to the return barrier.\n-      \/\/ No safepoint in the call to thaw, so even an oop return value should be OK.\n-      __ pop_d(xmm0);\n-      __ pop(rax);\n-    }\n+  \/\/ rax is jobject handle result, unpack and process it through a barrier.\n+  Label L_null_jobject;\n+  __ testptr(rax, rax);\n+  __ jcc(Assembler::zero, L_null_jobject);\n@@ -7651,9 +3522,2 @@\n-#ifdef ASSERT\n-    {\n-      Label L_good_sp;\n-      __ cmpptr(rsp, Address(r15_thread, JavaThread::cont_entry_offset()));\n-      __ jcc(Assembler::equal, L_good_sp);\n-      __ stop(\"Incorrect rsp after prepare thaw\");\n-      __ BIND(L_good_sp);\n-    }\n-#endif\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->load_at(_masm, ACCESS_READ | IN_NATIVE, T_OBJECT, rax, Address(rax, 0), c_rarg0, r15_thread);\n@@ -7661,16 +3525,1 @@\n-    \/\/ rbx contains the size of the frames to thaw, 0 if overflow or no more frames\n-    Label L_thaw_success;\n-    __ testptr(rbx, rbx);\n-    __ jccb(Assembler::notZero, L_thaw_success);\n-    __ jump(ExternalAddress(StubRoutines::throw_StackOverflowError_entry()));\n-    __ bind(L_thaw_success);\n-\n-    \/\/ Make room for the thawed frames and align the stack.\n-    __ subptr(rsp, rbx);\n-    __ andptr(rsp, -StackAlignmentInBytes);\n-\n-    if (return_barrier) {\n-      \/\/ Preserve possible return value from a method returning to the return barrier. (Again.)\n-      __ push(rax);\n-      __ push_d(xmm0);\n-    }\n+  __ bind(L_null_jobject);\n@@ -7678,5 +3527,2 @@\n-    \/\/ If we want, we can templatize thaw by kind, and have three different entries.\n-    __ movptr(c_rarg0, r15_thread);\n-    __ movptr(c_rarg1, kind);\n-    __ call_VM_leaf(Continuation::thaw_entry(), 2);\n-    __ movptr(rbx, rax);\n+  __ leave();\n+  __ ret(0);\n@@ -7684,9 +3530,3 @@\n-    if (return_barrier) {\n-      \/\/ Restore return value from a method returning to the return barrier. (Again.)\n-      \/\/ No safepoint in the call to thaw, so even an oop return value should be OK.\n-      __ pop_d(xmm0);\n-      __ pop(rax);\n-    } else {\n-      \/\/ Return 0 (success) from doYield.\n-      __ xorptr(rax, rax);\n-    }\n+  OopMapSet* oop_maps = new OopMapSet();\n+  OopMap* map = new OopMap(framesize, 1);\n+  oop_maps->add_gc_map(frame_complete, map);\n@@ -7694,29 +3534,9 @@\n-    \/\/ After thawing, rbx is the SP of the yielding frame.\n-    \/\/ Move there, and then to saved RBP slot.\n-    __ movptr(rsp, rbx);\n-    __ subptr(rsp, 2*wordSize);\n-\n-    if (return_barrier_exception) {\n-      __ movptr(c_rarg0, r15_thread);\n-      __ movptr(c_rarg1, Address(rsp, wordSize)); \/\/ return address\n-\n-      \/\/ rax still holds the original exception oop, save it before the call\n-      __ push(rax);\n-\n-      __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::exception_handler_for_return_address), 2);\n-      __ movptr(rbx, rax);\n-\n-      \/\/ Continue at exception handler:\n-      \/\/   rax: exception oop\n-      \/\/   rbx: exception handler\n-      \/\/   rdx: exception pc\n-      __ pop(rax);\n-      __ verify_oop(rax);\n-      __ pop(rbp); \/\/ pop out RBP here too\n-      __ pop(rdx);\n-      __ jmp(rbx);\n-    } else {\n-      \/\/ We are \"returning\" into the topmost thawed frame; see Thaw::push_return_frame\n-      __ pop(rbp);\n-      __ ret(0);\n-    }\n+  RuntimeStub* stub =\n+    RuntimeStub::new_runtime_stub(code.name(),\n+                                  &code,\n+                                  frame_complete,\n+                                  (framesize >> (LogBytesPerWord - LogBytesPerInt)),\n+                                  oop_maps,\n+                                  false);\n+  return stub;\n+}\n@@ -7724,2 +3544,1 @@\n-    return start;\n-  }\n+#endif \/\/ INCLUDE_JFR\n@@ -7727,3 +3546,30 @@\n-  address generate_cont_thaw() {\n-    return generate_cont_thaw(\"Cont thaw\", Continuation::thaw_top);\n-  }\n+\/\/ Continuation point for throwing of implicit exceptions that are\n+\/\/ not handled in the current activation. Fabricates an exception\n+\/\/ oop and initiates normal exception dispatching in this\n+\/\/ frame. Since we need to preserve callee-saved values (currently\n+\/\/ only for C2, but done for C1 as well) we need a callee-saved oop\n+\/\/ map and therefore have to make these stubs into RuntimeStubs\n+\/\/ rather than BufferBlobs.  If the compiler needs all registers to\n+\/\/ be preserved between the fault point and the exception handler\n+\/\/ then it must assume responsibility for that in\n+\/\/ AbstractCompiler::continuation_for_implicit_null_exception or\n+\/\/ continuation_for_implicit_division_by_zero_exception. All other\n+\/\/ implicit exceptions (e.g., NullPointerException or\n+\/\/ AbstractMethodError on entry) are either at call sites or\n+\/\/ otherwise assume that stack unwinding will be initiated, so\n+\/\/ caller saved registers were assumed volatile in the compiler.\n+address StubGenerator::generate_throw_exception(const char* name,\n+                                                address runtime_entry,\n+                                                Register arg1,\n+                                                Register arg2) {\n+  \/\/ Information about frame layout at time of blocking runtime call.\n+  \/\/ Note that we only have to preserve callee-saved registers since\n+  \/\/ the compilers are responsible for supplying a continuation point\n+  \/\/ if they expect all registers to be preserved.\n+  enum layout {\n+    rbp_off = frame::arg_reg_save_area_bytes\/BytesPerInt,\n+    rbp_off2,\n+    return_off,\n+    return_off2,\n+    framesize \/\/ inclusive of return address\n+  };\n@@ -7731,1 +3577,2 @@\n-  \/\/ TODO: will probably need multiple return barriers depending on return type\n+  int insts_size = 512;\n+  int locs_size  = 64;\n@@ -7733,3 +3580,3 @@\n-  address generate_cont_returnBarrier() {\n-    return generate_cont_thaw(\"Cont thaw return barrier\", Continuation::thaw_return_barrier);\n-  }\n+  CodeBuffer code(name, insts_size, locs_size);\n+  OopMapSet* oop_maps  = new OopMapSet();\n+  MacroAssembler* _masm = new MacroAssembler(&code);\n@@ -7737,3 +3584,1 @@\n-  address generate_cont_returnBarrier_exception() {\n-    return generate_cont_thaw(\"Cont thaw return barrier exception\", Continuation::thaw_return_barrier_exception);\n-  }\n+  address start = __ pc();\n@@ -7741,1 +3586,4 @@\n-#if INCLUDE_JFR\n+  \/\/ This is an inlined and slightly modified version of call_VM\n+  \/\/ which has the ability to fetch the return PC out of\n+  \/\/ thread-local storage and also sets up last_Java_sp slightly\n+  \/\/ differently than the real call_VM\n@@ -7743,11 +3591,1 @@\n-  \/\/ For c2: c_rarg0 is junk, call to runtime to write a checkpoint.\n-  \/\/ It returns a jobject handle to the event writer.\n-  \/\/ The handle is dereferenced and the return value is the event writer oop.\n-  RuntimeStub* generate_jfr_write_checkpoint() {\n-    enum layout {\n-      rbp_off,\n-      rbpH_off,\n-      return_off,\n-      return_off2,\n-      framesize \/\/ inclusive of return address\n-    };\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n@@ -7755,2 +3593,1 @@\n-    CodeBuffer code(\"jfr_write_checkpoint\", 512, 64);\n-    MacroAssembler* _masm = new MacroAssembler(&code);\n+  assert(is_even(framesize\/2), \"sp not 16-byte aligned\");\n@@ -7758,3 +3595,2 @@\n-    address start = __ pc();\n-    __ enter();\n-    address the_pc = __ pc();\n+  \/\/ return address and rbp are already in place\n+  __ subptr(rsp, (framesize-4) << LogBytesPerInt); \/\/ prolog\n@@ -7762,1 +3598,1 @@\n-    int frame_complete = the_pc - start;\n+  int frame_complete = __ pc() - start;\n@@ -7764,4 +3600,4 @@\n-    __ set_last_Java_frame(rsp, rbp, the_pc);\n-    __ movptr(c_rarg0, r15_thread);\n-    __ call_VM_leaf(CAST_FROM_FN_PTR(address, JfrIntrinsicSupport::write_checkpoint), 1);\n-    __ reset_last_Java_frame(true);\n+  \/\/ Set up last_Java_sp and last_Java_fp\n+  address the_pc = __ pc();\n+  __ set_last_Java_frame(rsp, rbp, the_pc, rscratch1);\n+  __ andptr(rsp, -(StackAlignmentInBytes));    \/\/ Align stack\n@@ -7769,4 +3605,11 @@\n-    \/\/ rax is jobject handle result, unpack and process it through a barrier.\n-    Label L_null_jobject;\n-    __ testptr(rax, rax);\n-    __ jcc(Assembler::zero, L_null_jobject);\n+  \/\/ Call runtime\n+  if (arg1 != noreg) {\n+    assert(arg2 != c_rarg1, \"clobbered\");\n+    __ movptr(c_rarg1, arg1);\n+  }\n+  if (arg2 != noreg) {\n+    __ movptr(c_rarg2, arg2);\n+  }\n+  __ movptr(c_rarg0, r15_thread);\n+  BLOCK_COMMENT(\"call runtime_entry\");\n+  __ call(RuntimeAddress(runtime_entry));\n@@ -7774,2 +3617,2 @@\n-    BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    bs->load_at(_masm, ACCESS_READ | IN_NATIVE, T_OBJECT, rax, Address(rax, 0), c_rarg0, r15_thread);\n+  \/\/ Generate oop map\n+  OopMap* map = new OopMap(framesize, 0);\n@@ -7777,1 +3620,1 @@\n-    __ bind(L_null_jobject);\n+  oop_maps->add_gc_map(the_pc - start, map);\n@@ -7779,2 +3622,1 @@\n-    __ leave();\n-    __ ret(0);\n+  __ reset_last_Java_frame(true);\n@@ -7782,13 +3624,1 @@\n-    OopMapSet* oop_maps = new OopMapSet();\n-    OopMap* map = new OopMap(framesize, 1);\n-    oop_maps->add_gc_map(frame_complete, map);\n-\n-    RuntimeStub* stub =\n-      RuntimeStub::new_runtime_stub(code.name(),\n-                                    &code,\n-                                    frame_complete,\n-                                    (framesize >> (LogBytesPerWord - LogBytesPerInt)),\n-                                    oop_maps,\n-                                    false);\n-    return stub;\n-  }\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n@@ -7796,1 +3626,9 @@\n-#endif \/\/ INCLUDE_JFR\n+  \/\/ check for pending exceptions\n+#ifdef ASSERT\n+  Label L;\n+  __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), NULL_WORD);\n+  __ jcc(Assembler::notEqual, L);\n+  __ should_not_reach_here();\n+  __ bind(L);\n+#endif \/\/ ASSERT\n+  __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n@@ -7798,75 +3636,9 @@\n-#undef __\n-#define __ masm->\n-\n-  \/\/ Continuation point for throwing of implicit exceptions that are\n-  \/\/ not handled in the current activation. Fabricates an exception\n-  \/\/ oop and initiates normal exception dispatching in this\n-  \/\/ frame. Since we need to preserve callee-saved values (currently\n-  \/\/ only for C2, but done for C1 as well) we need a callee-saved oop\n-  \/\/ map and therefore have to make these stubs into RuntimeStubs\n-  \/\/ rather than BufferBlobs.  If the compiler needs all registers to\n-  \/\/ be preserved between the fault point and the exception handler\n-  \/\/ then it must assume responsibility for that in\n-  \/\/ AbstractCompiler::continuation_for_implicit_null_exception or\n-  \/\/ continuation_for_implicit_division_by_zero_exception. All other\n-  \/\/ implicit exceptions (e.g., NullPointerException or\n-  \/\/ AbstractMethodError on entry) are either at call sites or\n-  \/\/ otherwise assume that stack unwinding will be initiated, so\n-  \/\/ caller saved registers were assumed volatile in the compiler.\n-  address generate_throw_exception(const char* name,\n-                                   address runtime_entry,\n-                                   Register arg1 = noreg,\n-                                   Register arg2 = noreg) {\n-    \/\/ Information about frame layout at time of blocking runtime call.\n-    \/\/ Note that we only have to preserve callee-saved registers since\n-    \/\/ the compilers are responsible for supplying a continuation point\n-    \/\/ if they expect all registers to be preserved.\n-    enum layout {\n-      rbp_off = frame::arg_reg_save_area_bytes\/BytesPerInt,\n-      rbp_off2,\n-      return_off,\n-      return_off2,\n-      framesize \/\/ inclusive of return address\n-    };\n-\n-    int insts_size = 512;\n-    int locs_size  = 64;\n-\n-    CodeBuffer code(name, insts_size, locs_size);\n-    OopMapSet* oop_maps  = new OopMapSet();\n-    MacroAssembler* masm = new MacroAssembler(&code);\n-\n-    address start = __ pc();\n-\n-    \/\/ This is an inlined and slightly modified version of call_VM\n-    \/\/ which has the ability to fetch the return PC out of\n-    \/\/ thread-local storage and also sets up last_Java_sp slightly\n-    \/\/ differently than the real call_VM\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-    assert(is_even(framesize\/2), \"sp not 16-byte aligned\");\n-\n-    \/\/ return address and rbp are already in place\n-    __ subptr(rsp, (framesize-4) << LogBytesPerInt); \/\/ prolog\n-\n-    int frame_complete = __ pc() - start;\n-\n-    \/\/ Set up last_Java_sp and last_Java_fp\n-    address the_pc = __ pc();\n-    __ set_last_Java_frame(rsp, rbp, the_pc);\n-    __ andptr(rsp, -(StackAlignmentInBytes));    \/\/ Align stack\n-\n-    \/\/ Call runtime\n-    if (arg1 != noreg) {\n-      assert(arg2 != c_rarg1, \"clobbered\");\n-      __ movptr(c_rarg1, arg1);\n-    }\n-    if (arg2 != noreg) {\n-      __ movptr(c_rarg2, arg2);\n-    }\n-    __ movptr(c_rarg0, r15_thread);\n-    BLOCK_COMMENT(\"call runtime_entry\");\n-    __ call(RuntimeAddress(runtime_entry));\n-    \/\/ Generate oop map\n-    OopMap* map = new OopMap(framesize, 0);\n+  \/\/ codeBlob framesize is in words (not VMRegImpl::slot_size)\n+  RuntimeStub* stub =\n+    RuntimeStub::new_runtime_stub(name,\n+                                  &code,\n+                                  frame_complete,\n+                                  (framesize >> (LogBytesPerWord - LogBytesPerInt)),\n+                                  oop_maps, false);\n+  return stub->entry_point();\n+}\n@@ -7875,1 +3647,4 @@\n-    oop_maps->add_gc_map(the_pc - start, map);\n+void StubGenerator::create_control_words() {\n+  \/\/ Round to nearest, 64-bit mode, exceptions masked\n+  StubRoutines::x86::_mxcsr_std = 0x1F80;\n+}\n@@ -7877,1 +3652,3 @@\n-    __ reset_last_Java_frame(true);\n+\/\/ Initialization\n+void StubGenerator::generate_initial() {\n+  \/\/ Generates all stubs and initializes the entry points\n@@ -7879,1 +3656,2 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  \/\/ This platform-specific settings are needed by generate_call_stub()\n+  create_control_words();\n@@ -7881,10 +3659,5 @@\n-    \/\/ check for pending exceptions\n-#ifdef ASSERT\n-    Label L;\n-    __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()),\n-            (int32_t) NULL_WORD);\n-    __ jcc(Assembler::notEqual, L);\n-    __ should_not_reach_here();\n-    __ bind(L);\n-#endif \/\/ ASSERT\n-    __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+  \/\/ entry points that exist in all platforms Note: This is code\n+  \/\/ that could be shared among different platforms - however the\n+  \/\/ benefit seems to be smaller than the disadvantage of having a\n+  \/\/ much more complicated generator structure. See also comment in\n+  \/\/ stubRoutines.hpp.\n@@ -7892,0 +3665,1 @@\n+  StubRoutines::_forward_exception_entry = generate_forward_exception();\n@@ -7893,9 +3667,2 @@\n-    \/\/ codeBlob framesize is in words (not VMRegImpl::slot_size)\n-    RuntimeStub* stub =\n-      RuntimeStub::new_runtime_stub(name,\n-                                    &code,\n-                                    frame_complete,\n-                                    (framesize >> (LogBytesPerWord - LogBytesPerInt)),\n-                                    oop_maps, false);\n-    return stub->entry_point();\n-  }\n+  StubRoutines::_call_stub_entry =\n+    generate_call_stub(StubRoutines::_call_stub_return_address);\n@@ -7903,4 +3670,2 @@\n-  void create_control_words() {\n-    \/\/ Round to nearest, 64-bit mode, exceptions masked\n-    StubRoutines::x86::_mxcsr_std = 0x1F80;\n-  }\n+  \/\/ is referenced by megamorphic call\n+  StubRoutines::_catch_exception_entry = generate_catch_exception();\n@@ -7908,55 +3673,2 @@\n-  \/\/ Initialization\n-  void generate_initial() {\n-    \/\/ Generates all stubs and initializes the entry points\n-\n-    \/\/ This platform-specific settings are needed by generate_call_stub()\n-    create_control_words();\n-\n-    \/\/ entry points that exist in all platforms Note: This is code\n-    \/\/ that could be shared among different platforms - however the\n-    \/\/ benefit seems to be smaller than the disadvantage of having a\n-    \/\/ much more complicated generator structure. See also comment in\n-    \/\/ stubRoutines.hpp.\n-\n-    StubRoutines::_forward_exception_entry = generate_forward_exception();\n-\n-    StubRoutines::_call_stub_entry =\n-      generate_call_stub(StubRoutines::_call_stub_return_address);\n-\n-    \/\/ is referenced by megamorphic call\n-    StubRoutines::_catch_exception_entry = generate_catch_exception();\n-\n-    \/\/ atomic calls\n-    StubRoutines::_fence_entry                = generate_orderaccess_fence();\n-\n-    \/\/ platform dependent\n-    StubRoutines::x86::_get_previous_sp_entry = generate_get_previous_sp();\n-\n-    StubRoutines::x86::_verify_mxcsr_entry    = generate_verify_mxcsr();\n-\n-    StubRoutines::x86::_f2i_fixup             = generate_f2i_fixup();\n-    StubRoutines::x86::_f2l_fixup             = generate_f2l_fixup();\n-    StubRoutines::x86::_d2i_fixup             = generate_d2i_fixup();\n-    StubRoutines::x86::_d2l_fixup             = generate_d2l_fixup();\n-\n-    StubRoutines::x86::_float_sign_mask       = generate_fp_mask(\"float_sign_mask\",  0x7FFFFFFF7FFFFFFF);\n-    StubRoutines::x86::_float_sign_flip       = generate_fp_mask(\"float_sign_flip\",  0x8000000080000000);\n-    StubRoutines::x86::_double_sign_mask      = generate_fp_mask(\"double_sign_mask\", 0x7FFFFFFFFFFFFFFF);\n-    StubRoutines::x86::_double_sign_flip      = generate_fp_mask(\"double_sign_flip\", 0x8000000000000000);\n-\n-    \/\/ Build this early so it's available for the interpreter.\n-    StubRoutines::_throw_StackOverflowError_entry =\n-      generate_throw_exception(\"StackOverflowError throw_exception\",\n-                               CAST_FROM_FN_PTR(address,\n-                                                SharedRuntime::\n-                                                throw_StackOverflowError));\n-    StubRoutines::_throw_delayed_StackOverflowError_entry =\n-      generate_throw_exception(\"delayed StackOverflowError throw_exception\",\n-                               CAST_FROM_FN_PTR(address,\n-                                                SharedRuntime::\n-                                                throw_delayed_StackOverflowError));\n-    if (UseCRC32Intrinsics) {\n-      \/\/ set table address before stub generation which use it\n-      StubRoutines::_crc_table_adr = (address)StubRoutines::x86::_crc_table;\n-      StubRoutines::_updateBytesCRC32 = generate_updateBytesCRC32();\n-    }\n+  \/\/ atomic calls\n+  StubRoutines::_fence_entry                = generate_orderaccess_fence();\n@@ -7964,6 +3676,2 @@\n-    if (UseCRC32CIntrinsics) {\n-      bool supports_clmul = VM_Version::supports_clmul();\n-      StubRoutines::x86::generate_CRC32C_table(supports_clmul);\n-      StubRoutines::_crc32c_table_addr = (address)StubRoutines::x86::_crc32c_table;\n-      StubRoutines::_updateBytesCRC32C = generate_updateBytesCRC32C(supports_clmul);\n-    }\n+  \/\/ platform dependent\n+  StubRoutines::x86::_get_previous_sp_entry = generate_get_previous_sp();\n@@ -7971,3 +3679,1 @@\n-    if (UseAdler32Intrinsics) {\n-       StubRoutines::_updateBytesAdler32 = generate_updateBytesAdler32();\n-    }\n+  StubRoutines::x86::_verify_mxcsr_entry    = generate_verify_mxcsr();\n@@ -7975,41 +3681,4 @@\n-    if (UseLibmIntrinsic && InlineIntrinsics) {\n-      if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dsin) ||\n-          vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dcos) ||\n-          vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dtan)) {\n-        StubRoutines::x86::_ONEHALF_adr = (address)StubRoutines::x86::_ONEHALF;\n-        StubRoutines::x86::_P_2_adr = (address)StubRoutines::x86::_P_2;\n-        StubRoutines::x86::_SC_4_adr = (address)StubRoutines::x86::_SC_4;\n-        StubRoutines::x86::_Ctable_adr = (address)StubRoutines::x86::_Ctable;\n-        StubRoutines::x86::_SC_2_adr = (address)StubRoutines::x86::_SC_2;\n-        StubRoutines::x86::_SC_3_adr = (address)StubRoutines::x86::_SC_3;\n-        StubRoutines::x86::_SC_1_adr = (address)StubRoutines::x86::_SC_1;\n-        StubRoutines::x86::_PI_INV_TABLE_adr = (address)StubRoutines::x86::_PI_INV_TABLE;\n-        StubRoutines::x86::_PI_4_adr = (address)StubRoutines::x86::_PI_4;\n-        StubRoutines::x86::_PI32INV_adr = (address)StubRoutines::x86::_PI32INV;\n-        StubRoutines::x86::_SIGN_MASK_adr = (address)StubRoutines::x86::_SIGN_MASK;\n-        StubRoutines::x86::_P_1_adr = (address)StubRoutines::x86::_P_1;\n-        StubRoutines::x86::_P_3_adr = (address)StubRoutines::x86::_P_3;\n-        StubRoutines::x86::_NEG_ZERO_adr = (address)StubRoutines::x86::_NEG_ZERO;\n-      }\n-      if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dexp)) {\n-        StubRoutines::_dexp = generate_libmExp();\n-      }\n-      if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dlog)) {\n-        StubRoutines::_dlog = generate_libmLog();\n-      }\n-      if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dlog10)) {\n-        StubRoutines::_dlog10 = generate_libmLog10();\n-      }\n-      if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dpow)) {\n-        StubRoutines::_dpow = generate_libmPow();\n-      }\n-      if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dsin)) {\n-        StubRoutines::_dsin = generate_libmSin();\n-      }\n-      if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dcos)) {\n-        StubRoutines::_dcos = generate_libmCos();\n-      }\n-      if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dtan)) {\n-        StubRoutines::_dtan = generate_libmTan();\n-      }\n-    }\n+  StubRoutines::x86::_f2i_fixup             = generate_f2i_fixup();\n+  StubRoutines::x86::_f2l_fixup             = generate_f2l_fixup();\n+  StubRoutines::x86::_d2i_fixup             = generate_d2i_fixup();\n+  StubRoutines::x86::_d2l_fixup             = generate_d2l_fixup();\n@@ -8017,4 +3686,4 @@\n-#ifdef _LP64\n-    StubRoutines::_load_nklass = generate_load_nklass();\n-#endif\n-  }\n+  StubRoutines::x86::_float_sign_mask       = generate_fp_mask(\"float_sign_mask\",  0x7FFFFFFF7FFFFFFF);\n+  StubRoutines::x86::_float_sign_flip       = generate_fp_mask(\"float_sign_flip\",  0x8000000080000000);\n+  StubRoutines::x86::_double_sign_mask      = generate_fp_mask(\"double_sign_mask\", 0x7FFFFFFFFFFFFFFF);\n+  StubRoutines::x86::_double_sign_flip      = generate_fp_mask(\"double_sign_flip\", 0x8000000000000000);\n@@ -8022,11 +3691,15 @@\n-  void generate_phase1() {\n-    \/\/ Continuation stubs:\n-    StubRoutines::_cont_thaw          = generate_cont_thaw();\n-    StubRoutines::_cont_returnBarrier = generate_cont_returnBarrier();\n-    StubRoutines::_cont_returnBarrierExc = generate_cont_returnBarrier_exception();\n-    StubRoutines::_cont_doYield_stub = generate_cont_doYield();\n-    StubRoutines::_cont_doYield      = StubRoutines::_cont_doYield_stub == nullptr ? nullptr\n-                                        : StubRoutines::_cont_doYield_stub->entry_point();\n-\n-    JFR_ONLY(StubRoutines::_jfr_write_checkpoint_stub = generate_jfr_write_checkpoint();)\n-    JFR_ONLY(StubRoutines::_jfr_write_checkpoint = StubRoutines::_jfr_write_checkpoint_stub->entry_point();)\n+  \/\/ Build this early so it's available for the interpreter.\n+  StubRoutines::_throw_StackOverflowError_entry =\n+    generate_throw_exception(\"StackOverflowError throw_exception\",\n+                             CAST_FROM_FN_PTR(address,\n+                                              SharedRuntime::\n+                                              throw_StackOverflowError));\n+  StubRoutines::_throw_delayed_StackOverflowError_entry =\n+    generate_throw_exception(\"delayed StackOverflowError throw_exception\",\n+                             CAST_FROM_FN_PTR(address,\n+                                              SharedRuntime::\n+                                              throw_delayed_StackOverflowError));\n+  if (UseCRC32Intrinsics) {\n+    \/\/ set table address before stub generation which use it\n+    StubRoutines::_crc_table_adr = (address)StubRoutines::x86::_crc_table;\n+    StubRoutines::_updateBytesCRC32 = generate_updateBytesCRC32();\n@@ -8035,86 +3708,6 @@\n-  void generate_all() {\n-    \/\/ Generates all stubs and initializes the entry points\n-\n-    \/\/ These entry points require SharedInfo::stack0 to be set up in\n-    \/\/ non-core builds and need to be relocatable, so they each\n-    \/\/ fabricate a RuntimeStub internally.\n-    StubRoutines::_throw_AbstractMethodError_entry =\n-      generate_throw_exception(\"AbstractMethodError throw_exception\",\n-                               CAST_FROM_FN_PTR(address,\n-                                                SharedRuntime::\n-                                                throw_AbstractMethodError));\n-\n-    StubRoutines::_throw_IncompatibleClassChangeError_entry =\n-      generate_throw_exception(\"IncompatibleClassChangeError throw_exception\",\n-                               CAST_FROM_FN_PTR(address,\n-                                                SharedRuntime::\n-                                                throw_IncompatibleClassChangeError));\n-\n-    StubRoutines::_throw_NullPointerException_at_call_entry =\n-      generate_throw_exception(\"NullPointerException at call throw_exception\",\n-                               CAST_FROM_FN_PTR(address,\n-                                                SharedRuntime::\n-                                                throw_NullPointerException_at_call));\n-\n-    \/\/ entry points that are platform specific\n-    StubRoutines::x86::_vector_float_sign_mask = generate_vector_mask(\"vector_float_sign_mask\", 0x7FFFFFFF7FFFFFFF);\n-    StubRoutines::x86::_vector_float_sign_flip = generate_vector_mask(\"vector_float_sign_flip\", 0x8000000080000000);\n-    StubRoutines::x86::_vector_double_sign_mask = generate_vector_mask(\"vector_double_sign_mask\", 0x7FFFFFFFFFFFFFFF);\n-    StubRoutines::x86::_vector_double_sign_flip = generate_vector_mask(\"vector_double_sign_flip\", 0x8000000000000000);\n-    StubRoutines::x86::_vector_all_bits_set = generate_vector_mask(\"vector_all_bits_set\", 0xFFFFFFFFFFFFFFFF);\n-    StubRoutines::x86::_vector_int_mask_cmp_bits = generate_vector_mask(\"vector_int_mask_cmp_bits\", 0x0000000100000001);\n-    StubRoutines::x86::_vector_short_to_byte_mask = generate_vector_mask(\"vector_short_to_byte_mask\", 0x00ff00ff00ff00ff);\n-    StubRoutines::x86::_vector_byte_perm_mask = generate_vector_byte_perm_mask(\"vector_byte_perm_mask\");\n-    StubRoutines::x86::_vector_int_to_byte_mask = generate_vector_mask(\"vector_int_to_byte_mask\", 0x000000ff000000ff);\n-    StubRoutines::x86::_vector_int_to_short_mask = generate_vector_mask(\"vector_int_to_short_mask\", 0x0000ffff0000ffff);\n-    StubRoutines::x86::_vector_32_bit_mask = generate_vector_custom_i32(\"vector_32_bit_mask\", Assembler::AVX_512bit,\n-                                                                        0xFFFFFFFF, 0, 0, 0);\n-    StubRoutines::x86::_vector_64_bit_mask = generate_vector_custom_i32(\"vector_64_bit_mask\", Assembler::AVX_512bit,\n-                                                                        0xFFFFFFFF, 0xFFFFFFFF, 0, 0);\n-    StubRoutines::x86::_vector_int_shuffle_mask = generate_vector_mask(\"vector_int_shuffle_mask\", 0x0302010003020100);\n-    StubRoutines::x86::_vector_byte_shuffle_mask = generate_vector_byte_shuffle_mask(\"vector_byte_shuffle_mask\");\n-    StubRoutines::x86::_vector_short_shuffle_mask = generate_vector_mask(\"vector_short_shuffle_mask\", 0x0100010001000100);\n-    StubRoutines::x86::_vector_long_shuffle_mask = generate_vector_mask(\"vector_long_shuffle_mask\", 0x0000000100000000);\n-    StubRoutines::x86::_vector_long_sign_mask = generate_vector_mask(\"vector_long_sign_mask\", 0x8000000000000000);\n-    StubRoutines::x86::_vector_iota_indices = generate_iota_indices(\"iota_indices\");\n-    StubRoutines::x86::_vector_count_leading_zeros_lut = generate_count_leading_zeros_lut(\"count_leading_zeros_lut\");\n-    StubRoutines::x86::_vector_reverse_bit_lut = generate_vector_reverse_bit_lut(\"reverse_bit_lut\");\n-    StubRoutines::x86::_vector_reverse_byte_perm_mask_long = generate_vector_reverse_byte_perm_mask_long(\"perm_mask_long\");\n-    StubRoutines::x86::_vector_reverse_byte_perm_mask_int = generate_vector_reverse_byte_perm_mask_int(\"perm_mask_int\");\n-    StubRoutines::x86::_vector_reverse_byte_perm_mask_short = generate_vector_reverse_byte_perm_mask_short(\"perm_mask_short\");\n-\n-    if (VM_Version::supports_avx2() && !VM_Version::supports_avx512_vpopcntdq()) {\n-      \/\/ lut implementation influenced by counting 1s algorithm from section 5-1 of Hackers' Delight.\n-      StubRoutines::x86::_vector_popcount_lut = generate_popcount_avx_lut(\"popcount_lut\");\n-    }\n-\n-    \/\/ support for verify_oop (must happen after universe_init)\n-    if (VerifyOops) {\n-      StubRoutines::_verify_oop_subroutine_entry = generate_verify_oop();\n-    }\n-\n-    \/\/ data cache line writeback\n-    StubRoutines::_data_cache_writeback = generate_data_cache_writeback();\n-    StubRoutines::_data_cache_writeback_sync = generate_data_cache_writeback_sync();\n-\n-    \/\/ arraycopy stubs used by compilers\n-    generate_arraycopy_stubs();\n-\n-    \/\/ don't bother generating these AES intrinsic stubs unless global flag is set\n-    if (UseAESIntrinsics) {\n-      StubRoutines::x86::_key_shuffle_mask_addr = generate_key_shuffle_mask();  \/\/ needed by the others\n-      StubRoutines::_aescrypt_encryptBlock = generate_aescrypt_encryptBlock();\n-      StubRoutines::_aescrypt_decryptBlock = generate_aescrypt_decryptBlock();\n-      StubRoutines::_cipherBlockChaining_encryptAESCrypt = generate_cipherBlockChaining_encryptAESCrypt();\n-      if (VM_Version::supports_avx512_vaes() &&  VM_Version::supports_avx512vl() && VM_Version::supports_avx512dq() ) {\n-        StubRoutines::_cipherBlockChaining_decryptAESCrypt = generate_cipherBlockChaining_decryptVectorAESCrypt();\n-        StubRoutines::_electronicCodeBook_encryptAESCrypt = generate_electronicCodeBook_encryptAESCrypt();\n-        StubRoutines::_electronicCodeBook_decryptAESCrypt = generate_electronicCodeBook_decryptAESCrypt();\n-        StubRoutines::x86::_counter_mask_addr = counter_mask_addr();\n-        StubRoutines::x86::_ghash_poly512_addr = ghash_polynomial512_addr();\n-        StubRoutines::x86::_ghash_long_swap_mask_addr = generate_ghash_long_swap_mask();\n-        StubRoutines::_galoisCounterMode_AESCrypt = generate_galoisCounterMode_AESCrypt();\n-      } else {\n-        StubRoutines::_cipherBlockChaining_decryptAESCrypt = generate_cipherBlockChaining_decryptAESCrypt_Parallel();\n-      }\n-    }\n+  if (UseCRC32CIntrinsics) {\n+    bool supports_clmul = VM_Version::supports_clmul();\n+    StubRoutines::x86::generate_CRC32C_table(supports_clmul);\n+    StubRoutines::_crc32c_table_addr = (address)StubRoutines::x86::_crc32c_table;\n+    StubRoutines::_updateBytesCRC32C = generate_updateBytesCRC32C(supports_clmul);\n+  }\n@@ -8122,11 +3715,4 @@\n-    if (UseAESCTRIntrinsics) {\n-      if (VM_Version::supports_avx512_vaes() && VM_Version::supports_avx512bw() && VM_Version::supports_avx512vl()) {\n-        if (StubRoutines::x86::_counter_mask_addr == NULL) {\n-          StubRoutines::x86::_counter_mask_addr = counter_mask_addr();\n-        }\n-        StubRoutines::_counterMode_AESCrypt = generate_counterMode_VectorAESCrypt();\n-      } else {\n-        StubRoutines::x86::_counter_shuffle_mask_addr = generate_counter_shuffle_mask();\n-        StubRoutines::_counterMode_AESCrypt = generate_counterMode_AESCrypt_Parallel();\n-      }\n-    }\n+  if (UseAdler32Intrinsics) {\n+     StubRoutines::_updateBytesAdler32 = generate_updateBytesAdler32();\n+  }\n+}\n@@ -8134,29 +3720,4 @@\n-    if (UseMD5Intrinsics) {\n-      StubRoutines::_md5_implCompress = generate_md5_implCompress(false, \"md5_implCompress\");\n-      StubRoutines::_md5_implCompressMB = generate_md5_implCompress(true, \"md5_implCompressMB\");\n-    }\n-    if (UseSHA1Intrinsics) {\n-      StubRoutines::x86::_upper_word_mask_addr = generate_upper_word_mask();\n-      StubRoutines::x86::_shuffle_byte_flip_mask_addr = generate_shuffle_byte_flip_mask();\n-      StubRoutines::_sha1_implCompress = generate_sha1_implCompress(false, \"sha1_implCompress\");\n-      StubRoutines::_sha1_implCompressMB = generate_sha1_implCompress(true, \"sha1_implCompressMB\");\n-    }\n-    if (UseSHA256Intrinsics) {\n-      StubRoutines::x86::_k256_adr = (address)StubRoutines::x86::_k256;\n-      char* dst = (char*)StubRoutines::x86::_k256_W;\n-      char* src = (char*)StubRoutines::x86::_k256;\n-      for (int ii = 0; ii < 16; ++ii) {\n-        memcpy(dst + 32 * ii,      src + 16 * ii, 16);\n-        memcpy(dst + 32 * ii + 16, src + 16 * ii, 16);\n-      }\n-      StubRoutines::x86::_k256_W_adr = (address)StubRoutines::x86::_k256_W;\n-      StubRoutines::x86::_pshuffle_byte_flip_mask_addr = generate_pshuffle_byte_flip_mask();\n-      StubRoutines::_sha256_implCompress = generate_sha256_implCompress(false, \"sha256_implCompress\");\n-      StubRoutines::_sha256_implCompressMB = generate_sha256_implCompress(true, \"sha256_implCompressMB\");\n-    }\n-    if (UseSHA512Intrinsics) {\n-      StubRoutines::x86::_k512_W_addr = (address)StubRoutines::x86::_k512_W;\n-      StubRoutines::x86::_pshuffle_byte_flip_mask_addr_sha512 = generate_pshuffle_byte_flip_mask_sha512();\n-      StubRoutines::_sha512_implCompress = generate_sha512_implCompress(false, \"sha512_implCompress\");\n-      StubRoutines::_sha512_implCompressMB = generate_sha512_implCompress(true, \"sha512_implCompressMB\");\n-    }\n+void StubGenerator::generate_phase1() {\n+#ifdef _LP64\n+  StubRoutines::_load_nklass = generate_load_nklass();\n+#endif\n@@ -8164,14 +3725,4 @@\n-    \/\/ Generate GHASH intrinsics code\n-    if (UseGHASHIntrinsics) {\n-      if (StubRoutines::x86::_ghash_long_swap_mask_addr == NULL) {\n-        StubRoutines::x86::_ghash_long_swap_mask_addr = generate_ghash_long_swap_mask();\n-      }\n-    StubRoutines::x86::_ghash_byte_swap_mask_addr = generate_ghash_byte_swap_mask();\n-      if (VM_Version::supports_avx()) {\n-        StubRoutines::x86::_ghash_shuffmask_addr = ghash_shufflemask_addr();\n-        StubRoutines::x86::_ghash_poly_addr = ghash_polynomial_addr();\n-        StubRoutines::_ghash_processBlocks = generate_avx_ghash_processBlocks();\n-      } else {\n-        StubRoutines::_ghash_processBlocks = generate_ghash_processBlocks();\n-      }\n-    }\n+  \/\/ Continuation stubs:\n+  StubRoutines::_cont_thaw          = generate_cont_thaw();\n+  StubRoutines::_cont_returnBarrier = generate_cont_returnBarrier();\n+  StubRoutines::_cont_returnBarrierExc = generate_cont_returnBarrier_exception();\n@@ -8179,0 +3730,3 @@\n+  JFR_ONLY(StubRoutines::_jfr_write_checkpoint_stub = generate_jfr_write_checkpoint();)\n+  JFR_ONLY(StubRoutines::_jfr_write_checkpoint = StubRoutines::_jfr_write_checkpoint_stub->entry_point();)\n+}\n@@ -8180,23 +3734,121 @@\n-    if (UseBASE64Intrinsics) {\n-      if(VM_Version::supports_avx2() &&\n-         VM_Version::supports_avx512bw() &&\n-         VM_Version::supports_avx512vl()) {\n-        StubRoutines::x86::_avx2_shuffle_base64 = base64_avx2_shuffle_addr();\n-        StubRoutines::x86::_avx2_input_mask_base64 = base64_avx2_input_mask_addr();\n-        StubRoutines::x86::_avx2_lut_base64 = base64_avx2_lut_addr();\n-      }\n-      StubRoutines::x86::_encoding_table_base64 = base64_encoding_table_addr();\n-      if (VM_Version::supports_avx512_vbmi()) {\n-        StubRoutines::x86::_shuffle_base64 = base64_shuffle_addr();\n-        StubRoutines::x86::_lookup_lo_base64 = base64_vbmi_lookup_lo_addr();\n-        StubRoutines::x86::_lookup_hi_base64 = base64_vbmi_lookup_hi_addr();\n-        StubRoutines::x86::_lookup_lo_base64url = base64_vbmi_lookup_lo_url_addr();\n-        StubRoutines::x86::_lookup_hi_base64url = base64_vbmi_lookup_hi_url_addr();\n-        StubRoutines::x86::_pack_vec_base64 = base64_vbmi_pack_vec_addr();\n-        StubRoutines::x86::_join_0_1_base64 = base64_vbmi_join_0_1_addr();\n-        StubRoutines::x86::_join_1_2_base64 = base64_vbmi_join_1_2_addr();\n-        StubRoutines::x86::_join_2_3_base64 = base64_vbmi_join_2_3_addr();\n-      }\n-      StubRoutines::x86::_decoding_table_base64 = base64_decoding_table_addr();\n-      StubRoutines::_base64_encodeBlock = generate_base64_encodeBlock();\n-      StubRoutines::_base64_decodeBlock = generate_base64_decodeBlock();\n+void StubGenerator::generate_all() {\n+  \/\/ Generates all stubs and initializes the entry points\n+\n+  \/\/ These entry points require SharedInfo::stack0 to be set up in\n+  \/\/ non-core builds and need to be relocatable, so they each\n+  \/\/ fabricate a RuntimeStub internally.\n+  StubRoutines::_throw_AbstractMethodError_entry =\n+    generate_throw_exception(\"AbstractMethodError throw_exception\",\n+                             CAST_FROM_FN_PTR(address,\n+                                              SharedRuntime::\n+                                              throw_AbstractMethodError));\n+\n+  StubRoutines::_throw_IncompatibleClassChangeError_entry =\n+    generate_throw_exception(\"IncompatibleClassChangeError throw_exception\",\n+                             CAST_FROM_FN_PTR(address,\n+                                              SharedRuntime::\n+                                              throw_IncompatibleClassChangeError));\n+\n+  StubRoutines::_throw_NullPointerException_at_call_entry =\n+    generate_throw_exception(\"NullPointerException at call throw_exception\",\n+                             CAST_FROM_FN_PTR(address,\n+                                              SharedRuntime::\n+                                              throw_NullPointerException_at_call));\n+\n+  \/\/ entry points that are platform specific\n+  StubRoutines::x86::_vector_float_sign_mask = generate_vector_mask(\"vector_float_sign_mask\", 0x7FFFFFFF7FFFFFFF);\n+  StubRoutines::x86::_vector_float_sign_flip = generate_vector_mask(\"vector_float_sign_flip\", 0x8000000080000000);\n+  StubRoutines::x86::_vector_double_sign_mask = generate_vector_mask(\"vector_double_sign_mask\", 0x7FFFFFFFFFFFFFFF);\n+  StubRoutines::x86::_vector_double_sign_flip = generate_vector_mask(\"vector_double_sign_flip\", 0x8000000000000000);\n+  StubRoutines::x86::_vector_all_bits_set = generate_vector_mask(\"vector_all_bits_set\", 0xFFFFFFFFFFFFFFFF);\n+  StubRoutines::x86::_vector_int_mask_cmp_bits = generate_vector_mask(\"vector_int_mask_cmp_bits\", 0x0000000100000001);\n+  StubRoutines::x86::_vector_short_to_byte_mask = generate_vector_mask(\"vector_short_to_byte_mask\", 0x00ff00ff00ff00ff);\n+  StubRoutines::x86::_vector_byte_perm_mask = generate_vector_byte_perm_mask(\"vector_byte_perm_mask\");\n+  StubRoutines::x86::_vector_int_to_byte_mask = generate_vector_mask(\"vector_int_to_byte_mask\", 0x000000ff000000ff);\n+  StubRoutines::x86::_vector_int_to_short_mask = generate_vector_mask(\"vector_int_to_short_mask\", 0x0000ffff0000ffff);\n+  StubRoutines::x86::_vector_32_bit_mask = generate_vector_custom_i32(\"vector_32_bit_mask\", Assembler::AVX_512bit,\n+                                                                      0xFFFFFFFF, 0, 0, 0);\n+  StubRoutines::x86::_vector_64_bit_mask = generate_vector_custom_i32(\"vector_64_bit_mask\", Assembler::AVX_512bit,\n+                                                                      0xFFFFFFFF, 0xFFFFFFFF, 0, 0);\n+  StubRoutines::x86::_vector_int_shuffle_mask = generate_vector_mask(\"vector_int_shuffle_mask\", 0x0302010003020100);\n+  StubRoutines::x86::_vector_byte_shuffle_mask = generate_vector_byte_shuffle_mask(\"vector_byte_shuffle_mask\");\n+  StubRoutines::x86::_vector_short_shuffle_mask = generate_vector_mask(\"vector_short_shuffle_mask\", 0x0100010001000100);\n+  StubRoutines::x86::_vector_long_shuffle_mask = generate_vector_mask(\"vector_long_shuffle_mask\", 0x0000000100000000);\n+  StubRoutines::x86::_vector_long_sign_mask = generate_vector_mask(\"vector_long_sign_mask\", 0x8000000000000000);\n+  StubRoutines::x86::_vector_iota_indices = generate_iota_indices(\"iota_indices\");\n+  StubRoutines::x86::_vector_count_leading_zeros_lut = generate_count_leading_zeros_lut(\"count_leading_zeros_lut\");\n+  StubRoutines::x86::_vector_reverse_bit_lut = generate_vector_reverse_bit_lut(\"reverse_bit_lut\");\n+  StubRoutines::x86::_vector_reverse_byte_perm_mask_long = generate_vector_reverse_byte_perm_mask_long(\"perm_mask_long\");\n+  StubRoutines::x86::_vector_reverse_byte_perm_mask_int = generate_vector_reverse_byte_perm_mask_int(\"perm_mask_int\");\n+  StubRoutines::x86::_vector_reverse_byte_perm_mask_short = generate_vector_reverse_byte_perm_mask_short(\"perm_mask_short\");\n+\n+  if (VM_Version::supports_avx2() && !VM_Version::supports_avx512_vpopcntdq()) {\n+    \/\/ lut implementation influenced by counting 1s algorithm from section 5-1 of Hackers' Delight.\n+    StubRoutines::x86::_vector_popcount_lut = generate_popcount_avx_lut(\"popcount_lut\");\n+  }\n+\n+  \/\/ support for verify_oop (must happen after universe_init)\n+  if (VerifyOops) {\n+    StubRoutines::_verify_oop_subroutine_entry = generate_verify_oop();\n+  }\n+\n+  \/\/ data cache line writeback\n+  StubRoutines::_data_cache_writeback = generate_data_cache_writeback();\n+  StubRoutines::_data_cache_writeback_sync = generate_data_cache_writeback_sync();\n+\n+  \/\/ arraycopy stubs used by compilers\n+  generate_arraycopy_stubs();\n+\n+  generate_aes_stubs();\n+\n+  generate_ghash_stubs();\n+\n+  if (UseMD5Intrinsics) {\n+    StubRoutines::_md5_implCompress = generate_md5_implCompress(false, \"md5_implCompress\");\n+    StubRoutines::_md5_implCompressMB = generate_md5_implCompress(true, \"md5_implCompressMB\");\n+  }\n+  if (UseSHA1Intrinsics) {\n+    StubRoutines::x86::_upper_word_mask_addr = generate_upper_word_mask();\n+    StubRoutines::x86::_shuffle_byte_flip_mask_addr = generate_shuffle_byte_flip_mask();\n+    StubRoutines::_sha1_implCompress = generate_sha1_implCompress(false, \"sha1_implCompress\");\n+    StubRoutines::_sha1_implCompressMB = generate_sha1_implCompress(true, \"sha1_implCompressMB\");\n+  }\n+  if (UseSHA256Intrinsics) {\n+    StubRoutines::x86::_k256_adr = (address)StubRoutines::x86::_k256;\n+    char* dst = (char*)StubRoutines::x86::_k256_W;\n+    char* src = (char*)StubRoutines::x86::_k256;\n+    for (int ii = 0; ii < 16; ++ii) {\n+      memcpy(dst + 32 * ii,      src + 16 * ii, 16);\n+      memcpy(dst + 32 * ii + 16, src + 16 * ii, 16);\n+    }\n+    StubRoutines::x86::_k256_W_adr = (address)StubRoutines::x86::_k256_W;\n+    StubRoutines::x86::_pshuffle_byte_flip_mask_addr = generate_pshuffle_byte_flip_mask();\n+    StubRoutines::_sha256_implCompress = generate_sha256_implCompress(false, \"sha256_implCompress\");\n+    StubRoutines::_sha256_implCompressMB = generate_sha256_implCompress(true, \"sha256_implCompressMB\");\n+  }\n+  if (UseSHA512Intrinsics) {\n+    StubRoutines::x86::_k512_W_addr = (address)StubRoutines::x86::_k512_W;\n+    StubRoutines::x86::_pshuffle_byte_flip_mask_addr_sha512 = generate_pshuffle_byte_flip_mask_sha512();\n+    StubRoutines::_sha512_implCompress = generate_sha512_implCompress(false, \"sha512_implCompress\");\n+    StubRoutines::_sha512_implCompressMB = generate_sha512_implCompress(true, \"sha512_implCompressMB\");\n+  }\n+\n+  if (UseBASE64Intrinsics) {\n+    if(VM_Version::supports_avx2() &&\n+       VM_Version::supports_avx512bw() &&\n+       VM_Version::supports_avx512vl()) {\n+      StubRoutines::x86::_avx2_shuffle_base64 = base64_avx2_shuffle_addr();\n+      StubRoutines::x86::_avx2_input_mask_base64 = base64_avx2_input_mask_addr();\n+      StubRoutines::x86::_avx2_lut_base64 = base64_avx2_lut_addr();\n+    }\n+    StubRoutines::x86::_encoding_table_base64 = base64_encoding_table_addr();\n+    if (VM_Version::supports_avx512_vbmi()) {\n+      StubRoutines::x86::_shuffle_base64 = base64_shuffle_addr();\n+      StubRoutines::x86::_lookup_lo_base64 = base64_vbmi_lookup_lo_addr();\n+      StubRoutines::x86::_lookup_hi_base64 = base64_vbmi_lookup_hi_addr();\n+      StubRoutines::x86::_lookup_lo_base64url = base64_vbmi_lookup_lo_url_addr();\n+      StubRoutines::x86::_lookup_hi_base64url = base64_vbmi_lookup_hi_url_addr();\n+      StubRoutines::x86::_pack_vec_base64 = base64_vbmi_pack_vec_addr();\n+      StubRoutines::x86::_join_0_1_base64 = base64_vbmi_join_0_1_addr();\n+      StubRoutines::x86::_join_1_2_base64 = base64_vbmi_join_1_2_addr();\n+      StubRoutines::x86::_join_2_3_base64 = base64_vbmi_join_2_3_addr();\n@@ -8204,0 +3856,4 @@\n+    StubRoutines::x86::_decoding_table_base64 = base64_decoding_table_addr();\n+    StubRoutines::_base64_encodeBlock = generate_base64_encodeBlock();\n+    StubRoutines::_base64_decodeBlock = generate_base64_decodeBlock();\n+  }\n@@ -8205,4 +3861,4 @@\n-    BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n-    if (bs_nm != NULL) {\n-      StubRoutines::x86::_method_entry_barrier = generate_method_entry_barrier();\n-    }\n+  BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n+  if (bs_nm != NULL) {\n+    StubRoutines::x86::_method_entry_barrier = generate_method_entry_barrier();\n+  }\n@@ -8210,61 +3866,47 @@\n-    if (UseMultiplyToLenIntrinsic) {\n-      StubRoutines::_multiplyToLen = generate_multiplyToLen();\n-    }\n-    if (UseSquareToLenIntrinsic) {\n-      StubRoutines::_squareToLen = generate_squareToLen();\n-    }\n-    if (UseMulAddIntrinsic) {\n-      StubRoutines::_mulAdd = generate_mulAdd();\n-    }\n-    if (VM_Version::supports_avx512_vbmi2()) {\n-      StubRoutines::_bigIntegerRightShiftWorker = generate_bigIntegerRightShift();\n-      StubRoutines::_bigIntegerLeftShiftWorker = generate_bigIntegerLeftShift();\n-    }\n-    if (UseMontgomeryMultiplyIntrinsic) {\n-      StubRoutines::_montgomeryMultiply\n-        = CAST_FROM_FN_PTR(address, SharedRuntime::montgomery_multiply);\n-    }\n-    if (UseMontgomerySquareIntrinsic) {\n-      StubRoutines::_montgomerySquare\n-        = CAST_FROM_FN_PTR(address, SharedRuntime::montgomery_square);\n-    }\n-\n-    \/\/ Get svml stub routine addresses\n-    void *libjsvml = NULL;\n-    char ebuf[1024];\n-    char dll_name[JVM_MAXPATHLEN];\n-    if (os::dll_locate_lib(dll_name, sizeof(dll_name), Arguments::get_dll_dir(), \"jsvml\")) {\n-      libjsvml = os::dll_load(dll_name, ebuf, sizeof ebuf);\n-    }\n-    if (libjsvml != NULL) {\n-      \/\/ SVML method naming convention\n-      \/\/   All the methods are named as __jsvml_op<T><N>_ha_<VV>\n-      \/\/   Where:\n-      \/\/      ha stands for high accuracy\n-      \/\/      <T> is optional to indicate float\/double\n-      \/\/              Set to f for vector float operation\n-      \/\/              Omitted for vector double operation\n-      \/\/      <N> is the number of elements in the vector\n-      \/\/              1, 2, 4, 8, 16\n-      \/\/              e.g. 128 bit float vector has 4 float elements\n-      \/\/      <VV> indicates the avx\/sse level:\n-      \/\/              z0 is AVX512, l9 is AVX2, e9 is AVX1 and ex is for SSE2\n-      \/\/      e.g. __jsvml_expf16_ha_z0 is the method for computing 16 element vector float exp using AVX 512 insns\n-      \/\/           __jsvml_exp8_ha_z0 is the method for computing 8 element vector double exp using AVX 512 insns\n-\n-      log_info(library)(\"Loaded library %s, handle \" INTPTR_FORMAT, JNI_LIB_PREFIX \"jsvml\" JNI_LIB_SUFFIX, p2i(libjsvml));\n-      if (UseAVX > 2) {\n-        for (int op = 0; op < VectorSupport::NUM_SVML_OP; op++) {\n-          int vop = VectorSupport::VECTOR_OP_SVML_START + op;\n-          if ((!VM_Version::supports_avx512dq()) &&\n-              (vop == VectorSupport::VECTOR_OP_LOG || vop == VectorSupport::VECTOR_OP_LOG10 || vop == VectorSupport::VECTOR_OP_POW)) {\n-            continue;\n-          }\n-          snprintf(ebuf, sizeof(ebuf), \"__jsvml_%sf16_ha_z0\", VectorSupport::svmlname[op]);\n-          StubRoutines::_vector_f_math[VectorSupport::VEC_SIZE_512][op] = (address)os::dll_lookup(libjsvml, ebuf);\n-\n-          snprintf(ebuf, sizeof(ebuf), \"__jsvml_%s8_ha_z0\", VectorSupport::svmlname[op]);\n-          StubRoutines::_vector_d_math[VectorSupport::VEC_SIZE_512][op] = (address)os::dll_lookup(libjsvml, ebuf);\n-        }\n-      }\n-      const char* avx_sse_str = (UseAVX >= 2) ? \"l9\" : ((UseAVX == 1) ? \"e9\" : \"ex\");\n+  if (UseMultiplyToLenIntrinsic) {\n+    StubRoutines::_multiplyToLen = generate_multiplyToLen();\n+  }\n+  if (UseSquareToLenIntrinsic) {\n+    StubRoutines::_squareToLen = generate_squareToLen();\n+  }\n+  if (UseMulAddIntrinsic) {\n+    StubRoutines::_mulAdd = generate_mulAdd();\n+  }\n+  if (VM_Version::supports_avx512_vbmi2()) {\n+    StubRoutines::_bigIntegerRightShiftWorker = generate_bigIntegerRightShift();\n+    StubRoutines::_bigIntegerLeftShiftWorker = generate_bigIntegerLeftShift();\n+  }\n+  if (UseMontgomeryMultiplyIntrinsic) {\n+    StubRoutines::_montgomeryMultiply\n+      = CAST_FROM_FN_PTR(address, SharedRuntime::montgomery_multiply);\n+  }\n+  if (UseMontgomerySquareIntrinsic) {\n+    StubRoutines::_montgomerySquare\n+      = CAST_FROM_FN_PTR(address, SharedRuntime::montgomery_square);\n+  }\n+\n+  \/\/ Get svml stub routine addresses\n+  void *libjsvml = NULL;\n+  char ebuf[1024];\n+  char dll_name[JVM_MAXPATHLEN];\n+  if (os::dll_locate_lib(dll_name, sizeof(dll_name), Arguments::get_dll_dir(), \"jsvml\")) {\n+    libjsvml = os::dll_load(dll_name, ebuf, sizeof ebuf);\n+  }\n+  if (libjsvml != NULL) {\n+    \/\/ SVML method naming convention\n+    \/\/   All the methods are named as __jsvml_op<T><N>_ha_<VV>\n+    \/\/   Where:\n+    \/\/      ha stands for high accuracy\n+    \/\/      <T> is optional to indicate float\/double\n+    \/\/              Set to f for vector float operation\n+    \/\/              Omitted for vector double operation\n+    \/\/      <N> is the number of elements in the vector\n+    \/\/              1, 2, 4, 8, 16\n+    \/\/              e.g. 128 bit float vector has 4 float elements\n+    \/\/      <VV> indicates the avx\/sse level:\n+    \/\/              z0 is AVX512, l9 is AVX2, e9 is AVX1 and ex is for SSE2\n+    \/\/      e.g. __jsvml_expf16_ha_z0 is the method for computing 16 element vector float exp using AVX 512 insns\n+    \/\/           __jsvml_exp8_ha_z0 is the method for computing 8 element vector double exp using AVX 512 insns\n+\n+    log_info(library)(\"Loaded library %s, handle \" INTPTR_FORMAT, JNI_LIB_PREFIX \"jsvml\" JNI_LIB_SUFFIX, p2i(libjsvml));\n+    if (UseAVX > 2) {\n@@ -8273,1 +3915,2 @@\n-        if (vop == VectorSupport::VECTOR_OP_POW) {\n+        if ((!VM_Version::supports_avx512dq()) &&\n+            (vop == VectorSupport::VECTOR_OP_LOG || vop == VectorSupport::VECTOR_OP_LOG10 || vop == VectorSupport::VECTOR_OP_POW)) {\n@@ -8276,2 +3919,2 @@\n-        snprintf(ebuf, sizeof(ebuf), \"__jsvml_%sf4_ha_%s\", VectorSupport::svmlname[op], avx_sse_str);\n-        StubRoutines::_vector_f_math[VectorSupport::VEC_SIZE_64][op] = (address)os::dll_lookup(libjsvml, ebuf);\n+        snprintf(ebuf, sizeof(ebuf), \"__jsvml_%sf16_ha_z0\", VectorSupport::svmlname[op]);\n+        StubRoutines::_vector_f_math[VectorSupport::VEC_SIZE_512][op] = (address)os::dll_lookup(libjsvml, ebuf);\n@@ -8279,2 +3922,12 @@\n-        snprintf(ebuf, sizeof(ebuf), \"__jsvml_%sf4_ha_%s\", VectorSupport::svmlname[op], avx_sse_str);\n-        StubRoutines::_vector_f_math[VectorSupport::VEC_SIZE_128][op] = (address)os::dll_lookup(libjsvml, ebuf);\n+        snprintf(ebuf, sizeof(ebuf), \"__jsvml_%s8_ha_z0\", VectorSupport::svmlname[op]);\n+        StubRoutines::_vector_d_math[VectorSupport::VEC_SIZE_512][op] = (address)os::dll_lookup(libjsvml, ebuf);\n+      }\n+    }\n+    const char* avx_sse_str = (UseAVX >= 2) ? \"l9\" : ((UseAVX == 1) ? \"e9\" : \"ex\");\n+    for (int op = 0; op < VectorSupport::NUM_SVML_OP; op++) {\n+      int vop = VectorSupport::VECTOR_OP_SVML_START + op;\n+      if (vop == VectorSupport::VECTOR_OP_POW) {\n+        continue;\n+      }\n+      snprintf(ebuf, sizeof(ebuf), \"__jsvml_%sf4_ha_%s\", VectorSupport::svmlname[op], avx_sse_str);\n+      StubRoutines::_vector_f_math[VectorSupport::VEC_SIZE_64][op] = (address)os::dll_lookup(libjsvml, ebuf);\n@@ -8282,2 +3935,2 @@\n-        snprintf(ebuf, sizeof(ebuf), \"__jsvml_%sf8_ha_%s\", VectorSupport::svmlname[op], avx_sse_str);\n-        StubRoutines::_vector_f_math[VectorSupport::VEC_SIZE_256][op] = (address)os::dll_lookup(libjsvml, ebuf);\n+      snprintf(ebuf, sizeof(ebuf), \"__jsvml_%sf4_ha_%s\", VectorSupport::svmlname[op], avx_sse_str);\n+      StubRoutines::_vector_f_math[VectorSupport::VEC_SIZE_128][op] = (address)os::dll_lookup(libjsvml, ebuf);\n@@ -8285,2 +3938,2 @@\n-        snprintf(ebuf, sizeof(ebuf), \"__jsvml_%s1_ha_%s\", VectorSupport::svmlname[op], avx_sse_str);\n-        StubRoutines::_vector_d_math[VectorSupport::VEC_SIZE_64][op] = (address)os::dll_lookup(libjsvml, ebuf);\n+      snprintf(ebuf, sizeof(ebuf), \"__jsvml_%sf8_ha_%s\", VectorSupport::svmlname[op], avx_sse_str);\n+      StubRoutines::_vector_f_math[VectorSupport::VEC_SIZE_256][op] = (address)os::dll_lookup(libjsvml, ebuf);\n@@ -8288,2 +3941,2 @@\n-        snprintf(ebuf, sizeof(ebuf), \"__jsvml_%s2_ha_%s\", VectorSupport::svmlname[op], avx_sse_str);\n-        StubRoutines::_vector_d_math[VectorSupport::VEC_SIZE_128][op] = (address)os::dll_lookup(libjsvml, ebuf);\n+      snprintf(ebuf, sizeof(ebuf), \"__jsvml_%s1_ha_%s\", VectorSupport::svmlname[op], avx_sse_str);\n+      StubRoutines::_vector_d_math[VectorSupport::VEC_SIZE_64][op] = (address)os::dll_lookup(libjsvml, ebuf);\n@@ -8291,5 +3944,2 @@\n-        snprintf(ebuf, sizeof(ebuf), \"__jsvml_%s4_ha_%s\", VectorSupport::svmlname[op], avx_sse_str);\n-        StubRoutines::_vector_d_math[VectorSupport::VEC_SIZE_256][op] = (address)os::dll_lookup(libjsvml, ebuf);\n-      }\n-    }\n-#endif \/\/ COMPILER2\n+      snprintf(ebuf, sizeof(ebuf), \"__jsvml_%s2_ha_%s\", VectorSupport::svmlname[op], avx_sse_str);\n+      StubRoutines::_vector_d_math[VectorSupport::VEC_SIZE_128][op] = (address)os::dll_lookup(libjsvml, ebuf);\n@@ -8297,2 +3947,2 @@\n-    if (UseVectorizedMismatchIntrinsic) {\n-      StubRoutines::_vectorizedMismatch = generate_vectorizedMismatch();\n+      snprintf(ebuf, sizeof(ebuf), \"__jsvml_%s4_ha_%s\", VectorSupport::svmlname[op], avx_sse_str);\n+      StubRoutines::_vector_d_math[VectorSupport::VEC_SIZE_256][op] = (address)os::dll_lookup(libjsvml, ebuf);\n@@ -8301,0 +3951,1 @@\n+#endif \/\/ COMPILER2\n@@ -8302,9 +3953,2 @@\n- public:\n-  StubGenerator(CodeBuffer* code, int phase) : StubCodeGenerator(code) {\n-    if (phase == 0) {\n-      generate_initial();\n-    } else if (phase == 1) {\n-      generate_phase1(); \/\/ stubs that must be available for the interpreter\n-    } else {\n-      generate_all();\n-    }\n+  if (UseVectorizedMismatchIntrinsic) {\n+    StubRoutines::_vectorizedMismatch = generate_vectorizedMismatch();\n@@ -8312,1 +3956,1 @@\n-}; \/\/ end class declaration\n+}\n@@ -8314,1 +3958,0 @@\n-#define UCM_TABLE_MAX_ENTRIES 16\n@@ -8317,1 +3960,1 @@\n-    UnsafeCopyMemory::create_table(UCM_TABLE_MAX_ENTRIES);\n+    UnsafeCopyMemory::create_table(16);\n@@ -8323,96 +3966,0 @@\n-#define __ masm->\n-\n-\/\/---------------------------- continuation_enter_setup ---------------------------\n-\/\/\n-\/\/ Arguments:\n-\/\/   None.\n-\/\/\n-\/\/ Results:\n-\/\/   rsp: pointer to blank ContinuationEntry\n-\/\/\n-\/\/ Kills:\n-\/\/   rax\n-\/\/\n-OopMap* continuation_enter_setup(MacroAssembler* masm, int& stack_slots) {\n-  assert(ContinuationEntry::size() % VMRegImpl::stack_slot_size == 0, \"\");\n-  assert(in_bytes(ContinuationEntry::cont_offset())  % VMRegImpl::stack_slot_size == 0, \"\");\n-  assert(in_bytes(ContinuationEntry::chunk_offset()) % VMRegImpl::stack_slot_size == 0, \"\");\n-\n-  stack_slots += checked_cast<int>(ContinuationEntry::size()) \/ wordSize;\n-  __ subptr(rsp, checked_cast<int32_t>(ContinuationEntry::size()));\n-\n-  int frame_size = (checked_cast<int>(ContinuationEntry::size()) + wordSize) \/ VMRegImpl::stack_slot_size;\n-  OopMap* map = new OopMap(frame_size, 0);\n-  ContinuationEntry::setup_oopmap(map);\n-\n-  __ movptr(rax, Address(r15_thread, JavaThread::cont_entry_offset()));\n-  __ movptr(Address(rsp, ContinuationEntry::parent_offset()), rax);\n-  __ movptr(Address(r15_thread, JavaThread::cont_entry_offset()), rsp);\n-\n-  return map;\n-}\n-\n-\/\/---------------------------- fill_continuation_entry ---------------------------\n-\/\/\n-\/\/ Arguments:\n-\/\/   rsp: pointer to blank Continuation entry\n-\/\/   reg_cont_obj: pointer to the continuation\n-\/\/   reg_flags: flags\n-\/\/\n-\/\/ Results:\n-\/\/   rsp: pointer to filled out ContinuationEntry\n-\/\/\n-\/\/ Kills:\n-\/\/   rax\n-\/\/\n-void fill_continuation_entry(MacroAssembler* masm, Register reg_cont_obj, Register reg_flags) {\n-  assert_different_registers(rax, reg_cont_obj, reg_flags);\n-\n-  DEBUG_ONLY(__ movl(Address(rsp, ContinuationEntry::cookie_offset()), ContinuationEntry::cookie_value());)\n-\n-  __ movptr(Address(rsp, ContinuationEntry::cont_offset()), reg_cont_obj);\n-  __ movl  (Address(rsp, ContinuationEntry::flags_offset()), reg_flags);\n-  __ movptr(Address(rsp, ContinuationEntry::chunk_offset()), 0);\n-  __ movl(Address(rsp, ContinuationEntry::argsize_offset()), 0);\n-  __ movl(Address(rsp, ContinuationEntry::pin_count_offset()), 0);\n-\n-  __ movptr(rax, Address(r15_thread, JavaThread::cont_fastpath_offset()));\n-  __ movptr(Address(rsp, ContinuationEntry::parent_cont_fastpath_offset()), rax);\n-  __ movq(rax, Address(r15_thread, JavaThread::held_monitor_count_offset()));\n-  __ movq(Address(rsp, ContinuationEntry::parent_held_monitor_count_offset()), rax);\n-\n-  __ movptr(Address(r15_thread, JavaThread::cont_fastpath_offset()), 0);\n-  __ movq(Address(r15_thread, JavaThread::held_monitor_count_offset()), 0);\n-}\n-\n-\/\/---------------------------- continuation_enter_cleanup ---------------------------\n-\/\/\n-\/\/ Arguments:\n-\/\/   rsp: pointer to the ContinuationEntry\n-\/\/\n-\/\/ Results:\n-\/\/   rsp: pointer to the spilled rbp in the entry frame\n-\/\/\n-\/\/ Kills:\n-\/\/   rbx\n-\/\/\n-void continuation_enter_cleanup(MacroAssembler* masm) {\n-#ifdef ASSERT\n-  Label L_good_sp;\n-  __ cmpptr(rsp, Address(r15_thread, JavaThread::cont_entry_offset()));\n-  __ jcc(Assembler::equal, L_good_sp);\n-  __ stop(\"Incorrect rsp at continuation_enter_cleanup\");\n-  __ bind(L_good_sp);\n-#endif\n-\n-  __ movptr(rbx, Address(rsp, ContinuationEntry::parent_cont_fastpath_offset()));\n-  __ movptr(Address(r15_thread, JavaThread::cont_fastpath_offset()), rbx);\n-  __ movq(rbx, Address(rsp, ContinuationEntry::parent_held_monitor_count_offset()));\n-  __ movq(Address(r15_thread, JavaThread::held_monitor_count_offset()), rbx);\n-\n-  __ movptr(rbx, Address(rsp, ContinuationEntry::parent_offset()));\n-  __ movptr(Address(r15_thread, JavaThread::cont_entry_offset()), rbx);\n-  __ addptr(rsp, (int32_t)ContinuationEntry::size());\n-}\n-\n-#undef __\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":3410,"deletions":7863,"binary":false,"changes":11273,"status":"modified"},{"patch":"@@ -0,0 +1,535 @@\n+\/*\n+ * Copyright (c) 2003, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_X86_STUBGENERATOR_X86_64_HPP\n+#define CPU_X86_STUBGENERATOR_X86_64_HPP\n+\n+#include \"code\/codeBlob.hpp\"\n+#include \"runtime\/continuation.hpp\"\n+#include \"runtime\/stubCodeGenerator.hpp\"\n+\n+\/\/ Stub Code definitions\n+\n+class StubGenerator: public StubCodeGenerator {\n+ private:\n+\n+  \/\/ Call stubs are used to call Java from C.\n+  address generate_call_stub(address& return_address);\n+\n+  \/\/ Return point for a Java call if there's an exception thrown in\n+  \/\/ Java code.  The exception is caught and transformed into a\n+  \/\/ pending exception stored in JavaThread that can be tested from\n+  \/\/ within the VM.\n+  \/\/\n+  \/\/ Note: Usually the parameters are removed by the callee. In case\n+  \/\/ of an exception crossing an activation frame boundary, that is\n+  \/\/ not the case if the callee is compiled code => need to setup the\n+  \/\/ rsp.\n+  \/\/\n+  \/\/ rax: exception oop\n+\n+  address generate_catch_exception();\n+\n+  \/\/ Continuation point for runtime calls returning with a pending\n+  \/\/ exception.  The pending exception check happened in the runtime\n+  \/\/ or native call stub.  The pending exception in Thread is\n+  \/\/ converted into a Java-level exception.\n+  \/\/\n+  \/\/ Contract with Java-level exception handlers:\n+  \/\/ rax: exception\n+  \/\/ rdx: throwing pc\n+  \/\/\n+  \/\/ NOTE: At entry of this stub, exception-pc must be on stack !!\n+\n+  address generate_forward_exception();\n+\n+  \/\/ Support for intptr_t OrderAccess::fence()\n+  address generate_orderaccess_fence();\n+\n+  \/\/ Support for intptr_t get_previous_sp()\n+  \/\/\n+  \/\/ This routine is used to find the previous stack pointer for the\n+  \/\/ caller.\n+  address generate_get_previous_sp();\n+\n+  \/\/----------------------------------------------------------------------------------------------------\n+  \/\/ Support for void verify_mxcsr()\n+  \/\/\n+  \/\/ This routine is used with -Xcheck:jni to verify that native\n+  \/\/ JNI code does not return to Java code without restoring the\n+  \/\/ MXCSR register to our expected state.\n+\n+  address generate_verify_mxcsr();\n+\n+  address generate_f2i_fixup();\n+  address generate_f2l_fixup();\n+  address generate_d2i_fixup();\n+  address generate_d2l_fixup();\n+\n+  address generate_count_leading_zeros_lut(const char *stub_name);\n+  address generate_popcount_avx_lut(const char *stub_name);\n+  address generate_iota_indices(const char *stub_name);\n+  address generate_vector_reverse_bit_lut(const char *stub_name);\n+\n+  address generate_vector_reverse_byte_perm_mask_long(const char *stub_name);\n+  address generate_vector_reverse_byte_perm_mask_int(const char *stub_name);\n+  address generate_vector_reverse_byte_perm_mask_short(const char *stub_name);\n+  address generate_vector_byte_shuffle_mask(const char *stub_name);\n+\n+  address generate_fp_mask(const char *stub_name, int64_t mask);\n+\n+  address generate_vector_mask(const char *stub_name, int64_t mask);\n+\n+  address generate_vector_byte_perm_mask(const char *stub_name);\n+\n+  address generate_vector_fp_mask(const char *stub_name, int64_t mask);\n+\n+  address generate_vector_custom_i32(const char *stub_name, Assembler::AvxVectorLen len,\n+                                     int32_t val0, int32_t val1, int32_t val2, int32_t val3,\n+                                     int32_t val4 = 0, int32_t val5 = 0, int32_t val6 = 0, int32_t val7 = 0,\n+                                     int32_t val8 = 0, int32_t val9 = 0, int32_t val10 = 0, int32_t val11 = 0,\n+                                     int32_t val12 = 0, int32_t val13 = 0, int32_t val14 = 0, int32_t val15 = 0);\n+\n+  \/\/ Non-destructive plausibility checks for oops\n+  address generate_verify_oop();\n+\n+  \/\/ Verify that a register contains clean 32-bits positive value\n+  \/\/ (high 32-bits are 0) so it could be used in 64-bits shifts.\n+  void assert_clean_int(Register Rint, Register Rtmp);\n+\n+  \/\/  Generate overlap test for array copy stubs\n+  void array_overlap_test(address no_overlap_target, Label* NOLp, Address::ScaleFactor sf);\n+\n+  void array_overlap_test(address no_overlap_target, Address::ScaleFactor sf) {\n+    assert(no_overlap_target != NULL, \"must be generated\");\n+    array_overlap_test(no_overlap_target, NULL, sf);\n+  }\n+  void array_overlap_test(Label& L_no_overlap, Address::ScaleFactor sf) {\n+    array_overlap_test(NULL, &L_no_overlap, sf);\n+  }\n+\n+\n+  \/\/ Shuffle first three arg regs on Windows into Linux\/Solaris locations.\n+  void setup_arg_regs(int nargs = 3);\n+  void restore_arg_regs();\n+\n+#ifdef ASSERT\n+  bool _regs_in_thread;\n+#endif\n+\n+  \/\/ This is used in places where r10 is a scratch register, and can\n+  \/\/ be adapted if r9 is needed also.\n+  void setup_arg_regs_using_thread();\n+\n+  void restore_arg_regs_using_thread();\n+\n+  \/\/ Copy big chunks forward\n+  void copy_bytes_forward(Register end_from, Register end_to,\n+                          Register qword_count, Register to,\n+                          Label& L_copy_bytes, Label& L_copy_8_bytes);\n+\n+  \/\/ Copy big chunks backward\n+  void copy_bytes_backward(Register from, Register dest,\n+                           Register qword_count, Register to,\n+                           Label& L_copy_bytes, Label& L_copy_8_bytes);\n+\n+  void setup_argument_regs(BasicType type);\n+\n+  void restore_argument_regs(BasicType type);\n+\n+#if COMPILER2_OR_JVMCI\n+  \/\/ Following rules apply to AVX3 optimized arraycopy stubs:\n+  \/\/ - If target supports AVX3 features (BW+VL+F) then implementation uses 32 byte vectors (YMMs)\n+  \/\/   for both special cases (various small block sizes) and aligned copy loop. This is the\n+  \/\/   default configuration.\n+  \/\/ - If copy length is above AVX3Threshold, then implementation use 64 byte vectors (ZMMs)\n+  \/\/   for main copy loop (and subsequent tail) since bulk of the cycles will be consumed in it.\n+  \/\/ - If user forces MaxVectorSize=32 then above 4096 bytes its seen that REP MOVs shows a\n+  \/\/   better performance for disjoint copies. For conjoint\/backward copy vector based\n+  \/\/   copy performs better.\n+  \/\/ - If user sets AVX3Threshold=0, then special cases for small blocks sizes operate over\n+  \/\/   64 byte vector registers (ZMMs).\n+\n+  address generate_disjoint_copy_avx3_masked(address* entry, const char *name, int shift,\n+                                             bool aligned, bool is_oop, bool dest_uninitialized);\n+\n+  address generate_conjoint_copy_avx3_masked(address* entry, const char *name, int shift,\n+                                             address nooverlap_target, bool aligned, bool is_oop,\n+                                             bool dest_uninitialized);\n+\n+  void arraycopy_avx3_special_cases(XMMRegister xmm, KRegister mask, Register from,\n+                                    Register to, Register count, int shift,\n+                                    Register index, Register temp,\n+                                    bool use64byteVector, Label& L_entry, Label& L_exit);\n+\n+  void arraycopy_avx3_special_cases_conjoint(XMMRegister xmm, KRegister mask, Register from,\n+                                             Register to, Register start_index, Register end_index,\n+                                             Register count, int shift, Register temp,\n+                                             bool use64byteVector, Label& L_entry, Label& L_exit);\n+\n+  void copy32_avx(Register dst, Register src, Register index, XMMRegister xmm,\n+                  int shift = Address::times_1, int offset = 0);\n+\n+  void copy64_avx(Register dst, Register src, Register index, XMMRegister xmm,\n+                  bool conjoint, int shift = Address::times_1, int offset = 0,\n+                  bool use64byteVector = false);\n+\n+  void copy64_masked_avx(Register dst, Register src, XMMRegister xmm,\n+                         KRegister mask, Register length, Register index,\n+                         Register temp, int shift = Address::times_1, int offset = 0,\n+                         bool use64byteVector = false);\n+\n+  void copy32_masked_avx(Register dst, Register src, XMMRegister xmm,\n+                         KRegister mask, Register length, Register index,\n+                         Register temp, int shift = Address::times_1, int offset = 0);\n+#endif \/\/ COMPILER2_OR_JVMCI\n+\n+  address generate_disjoint_byte_copy(bool aligned, address* entry, const char *name);\n+\n+  address generate_conjoint_byte_copy(bool aligned, address nooverlap_target,\n+                                      address* entry, const char *name);\n+\n+  address generate_disjoint_short_copy(bool aligned, address *entry, const char *name);\n+\n+  address generate_fill(BasicType t, bool aligned, const char *name);\n+\n+  address generate_conjoint_short_copy(bool aligned, address nooverlap_target,\n+                                       address *entry, const char *name);\n+  address generate_disjoint_int_oop_copy(bool aligned, bool is_oop, address* entry,\n+                                         const char *name, bool dest_uninitialized = false);\n+  address generate_conjoint_int_oop_copy(bool aligned, bool is_oop, address nooverlap_target,\n+                                         address *entry, const char *name,\n+                                         bool dest_uninitialized = false);\n+  address generate_disjoint_long_oop_copy(bool aligned, bool is_oop, address *entry,\n+                                          const char *name, bool dest_uninitialized = false);\n+  address generate_conjoint_long_oop_copy(bool aligned, bool is_oop,\n+                                          address nooverlap_target, address *entry,\n+                                          const char *name, bool dest_uninitialized = false);\n+\n+  \/\/ Helper for generating a dynamic type check.\n+  \/\/ Smashes no registers.\n+  void generate_type_check(Register sub_klass,\n+                           Register super_check_offset,\n+                           Register super_klass,\n+                           Label& L_success);\n+\n+  \/\/ Generate checkcasting array copy stub\n+  address generate_checkcast_copy(const char *name, address *entry,\n+                                  bool dest_uninitialized = false);\n+\n+  \/\/ Generate 'unsafe' array copy stub\n+  \/\/ Though just as safe as the other stubs, it takes an unscaled\n+  \/\/ size_t argument instead of an element count.\n+  \/\/\n+  \/\/ Examines the alignment of the operands and dispatches\n+  \/\/ to a long, int, short, or byte copy loop.\n+  address generate_unsafe_copy(const char *name,\n+                               address byte_copy_entry, address short_copy_entry,\n+                               address int_copy_entry, address long_copy_entry);\n+\n+  \/\/ Perform range checks on the proposed arraycopy.\n+  \/\/ Kills temp, but nothing else.\n+  \/\/ Also, clean the sign bits of src_pos and dst_pos.\n+  void arraycopy_range_checks(Register src,     \/\/ source array oop (c_rarg0)\n+                              Register src_pos, \/\/ source position (c_rarg1)\n+                              Register dst,     \/\/ destination array oo (c_rarg2)\n+                              Register dst_pos, \/\/ destination position (c_rarg3)\n+                              Register length,\n+                              Register temp,\n+                              Label& L_failed);\n+\n+  \/\/ Generate generic array copy stubs\n+  address generate_generic_copy(const char *name,\n+                                address byte_copy_entry, address short_copy_entry,\n+                                address int_copy_entry, address oop_copy_entry,\n+                                address long_copy_entry, address checkcast_copy_entry);\n+\n+  address generate_data_cache_writeback();\n+\n+  address generate_data_cache_writeback_sync();\n+\n+  void generate_arraycopy_stubs();\n+\n+\n+  \/\/ MD5 stubs\n+\n+  \/\/ ofs and limit are use for multi-block byte array.\n+  \/\/ int com.sun.security.provider.MD5.implCompress(byte[] b, int ofs)\n+  address generate_md5_implCompress(bool multi_block, const char *name);\n+\n+\n+  \/\/ SHA stubs\n+\n+  \/\/ ofs and limit are use for multi-block byte array.\n+  \/\/ int com.sun.security.provider.DigestBase.implCompressMultiBlock(byte[] b, int ofs, int limit)\n+  address generate_sha1_implCompress(bool multi_block, const char *name);\n+\n+  \/\/ ofs and limit are use for multi-block byte array.\n+  \/\/ int com.sun.security.provider.DigestBase.implCompressMultiBlock(byte[] b, int ofs, int limit)\n+  address generate_sha256_implCompress(bool multi_block, const char *name);\n+  address generate_sha512_implCompress(bool multi_block, const char *name);\n+\n+  \/\/ Mask for byte-swapping a couple of qwords in an XMM register using (v)pshufb.\n+  address generate_pshuffle_byte_flip_mask_sha512();\n+\n+  address generate_upper_word_mask();\n+  address generate_shuffle_byte_flip_mask();\n+  address generate_pshuffle_byte_flip_mask();\n+\n+\n+  \/\/ AES intrinsic stubs\n+\n+  address generate_aescrypt_encryptBlock();\n+\n+  address generate_aescrypt_decryptBlock();\n+\n+  address generate_cipherBlockChaining_encryptAESCrypt();\n+\n+  \/\/ A version of CBC\/AES Decrypt which does 4 blocks in a loop at a time\n+  \/\/ to hide instruction latency\n+  address generate_cipherBlockChaining_decryptAESCrypt_Parallel();\n+\n+  address generate_electronicCodeBook_encryptAESCrypt();\n+\n+  void aesecb_encrypt(Register source_addr, Register dest_addr, Register key, Register len);\n+\n+  address generate_electronicCodeBook_decryptAESCrypt();\n+\n+  void aesecb_decrypt(Register source_addr, Register dest_addr, Register key, Register len);\n+\n+  \/\/ Vector AES Galois Counter Mode implementation\n+  address generate_galoisCounterMode_AESCrypt();\n+  void aesgcm_encrypt(Register in, Register len, Register ct, Register out, Register key,\n+                      Register state, Register subkeyHtbl, Register avx512_subkeyHtbl, Register counter);\n+\n+\n+ \/\/ Vector AES Counter implementation\n+  address generate_counterMode_VectorAESCrypt();\n+  void aesctr_encrypt(Register src_addr, Register dest_addr, Register key, Register counter,\n+                      Register len_reg, Register used, Register used_addr, Register saved_encCounter_start);\n+\n+  \/\/ This is a version of CTR\/AES crypt which does 6 blocks in a loop at a time\n+  \/\/ to hide instruction latency\n+  address generate_counterMode_AESCrypt_Parallel();\n+\n+  address generate_cipherBlockChaining_decryptVectorAESCrypt();\n+\n+  address generate_key_shuffle_mask();\n+\n+  void roundDec(XMMRegister xmm_reg);\n+  void roundDeclast(XMMRegister xmm_reg);\n+  void roundEnc(XMMRegister key, int rnum);\n+  void lastroundEnc(XMMRegister key, int rnum);\n+  void roundDec(XMMRegister key, int rnum);\n+  void lastroundDec(XMMRegister key, int rnum);\n+  void gfmul_avx512(XMMRegister ghash, XMMRegister hkey);\n+  void generateHtbl_48_block_zmm(Register htbl, Register avx512_subkeyHtbl, Register rscratch);\n+  void ghash16_encrypt16_parallel(Register key, Register subkeyHtbl, XMMRegister ctr_blockx,\n+                                  XMMRegister aad_hashx, Register in, Register out, Register data, Register pos, bool reduction,\n+                                  XMMRegister addmask, bool no_ghash_input, Register rounds, Register ghash_pos,\n+                                  bool final_reduction, int index, XMMRegister counter_inc_mask);\n+  \/\/ Load key and shuffle operation\n+  void ev_load_key(XMMRegister xmmdst, Register key, int offset, XMMRegister xmm_shuf_mask);\n+  void ev_load_key(XMMRegister xmmdst, Register key, int offset, Register rscratch);\n+\n+  \/\/ Utility routine for loading a 128-bit key word in little endian format\n+  \/\/ can optionally specify that the shuffle mask is already in an xmmregister\n+  void load_key(XMMRegister xmmdst, Register key, int offset, XMMRegister xmm_shuf_mask);\n+  void load_key(XMMRegister xmmdst, Register key, int offset, Register rscratch);\n+\n+  \/\/ Utility routine for increase 128bit counter (iv in CTR mode)\n+  void inc_counter(Register reg, XMMRegister xmmdst, int inc_delta, Label& next_block);\n+\n+  void generate_aes_stubs();\n+\n+\n+  \/\/ GHASH stubs\n+\n+  void generate_ghash_stubs();\n+\n+  void schoolbookAAD(int i, Register subkeyH, XMMRegister data, XMMRegister tmp0,\n+                     XMMRegister tmp1, XMMRegister tmp2, XMMRegister tmp3);\n+  void gfmul(XMMRegister tmp0, XMMRegister t);\n+  void generateHtbl_one_block(Register htbl, Register rscratch);\n+  void generateHtbl_eight_blocks(Register htbl);\n+  void avx_ghash(Register state, Register htbl, Register data, Register blocks);\n+\n+  \/\/ Used by GHASH and AES stubs.\n+  address ghash_polynomial_addr();\n+  address ghash_shufflemask_addr();\n+  address ghash_long_swap_mask_addr(); \/\/ byte swap x86 long\n+  address ghash_byte_swap_mask_addr(); \/\/ byte swap x86 byte array\n+\n+  \/\/ Single and multi-block ghash operations\n+  address generate_ghash_processBlocks();\n+\n+  \/\/ Ghash single and multi block operations using AVX instructions\n+  address generate_avx_ghash_processBlocks();\n+\n+\n+  \/\/ BASE64 stubs\n+\n+  address base64_shuffle_addr();\n+  address base64_avx2_shuffle_addr();\n+  address base64_avx2_input_mask_addr();\n+  address base64_avx2_lut_addr();\n+  address base64_encoding_table_addr();\n+\n+  \/\/ Code for generating Base64 encoding.\n+  \/\/ Intrinsic function prototype in Base64.java:\n+  \/\/ private void encodeBlock(byte[] src, int sp, int sl, byte[] dst, int dp, boolean isURL)\n+  address generate_base64_encodeBlock();\n+\n+  \/\/ base64 AVX512vbmi tables\n+  address base64_vbmi_lookup_lo_addr();\n+  address base64_vbmi_lookup_hi_addr();\n+  address base64_vbmi_lookup_lo_url_addr();\n+  address base64_vbmi_lookup_hi_url_addr();\n+  address base64_vbmi_pack_vec_addr();\n+  address base64_vbmi_join_0_1_addr();\n+  address base64_vbmi_join_1_2_addr();\n+  address base64_vbmi_join_2_3_addr();\n+  address base64_decoding_table_addr();\n+\n+  \/\/ Code for generating Base64 decoding.\n+  \/\/\n+  \/\/ Based on the article (and associated code) from https:\/\/arxiv.org\/abs\/1910.05109.\n+  \/\/\n+  \/\/ Intrinsic function prototype in Base64.java:\n+  \/\/ private void decodeBlock(byte[] src, int sp, int sl, byte[] dst, int dp, boolean isURL, isMIME);\n+  address generate_base64_decodeBlock();\n+\n+  address generate_updateBytesCRC32();\n+  address generate_updateBytesCRC32C(bool is_pclmulqdq_supported);\n+\n+  address generate_updateBytesAdler32();\n+\n+  address generate_multiplyToLen();\n+\n+  address generate_vectorizedMismatch();\n+\n+  address generate_squareToLen();\n+\n+  address generate_method_entry_barrier();\n+\n+  address generate_mulAdd();\n+\n+  address generate_bigIntegerRightShift();\n+  address generate_bigIntegerLeftShift();\n+\n+\n+  \/\/ Libm trigonometric stubs\n+\n+  address generate_libmSin();\n+  address generate_libmCos();\n+  address generate_libmTan();\n+  address generate_libmExp();\n+  address generate_libmPow();\n+  address generate_libmLog();\n+  address generate_libmLog10();\n+\n+  \/\/ Shared constants\n+  static address ZERO;\n+  static address NEG_ZERO;\n+  static address ONE;\n+  static address ONEHALF;\n+  static address SIGN_MASK;\n+  static address TWO_POW_55;\n+  static address TWO_POW_M55;\n+  static address SHIFTER;\n+  static address PI32INV;\n+  static address PI_INV_TABLE;\n+  static address Ctable;\n+  static address SC_1;\n+  static address SC_2;\n+  static address SC_3;\n+  static address SC_4;\n+  static address PI_4;\n+  static address P_1;\n+  static address P_3;\n+  static address P_2;\n+\n+  void generate_libm_stubs();\n+\n+#ifdef _LP64\n+  address generate_load_nklass();\n+#endif\n+\n+  address generate_cont_thaw(const char* label, Continuation::thaw_kind kind);\n+  address generate_cont_thaw();\n+\n+  \/\/ TODO: will probably need multiple return barriers depending on return type\n+  address generate_cont_returnBarrier();\n+  address generate_cont_returnBarrier_exception();\n+\n+#if INCLUDE_JFR\n+\n+  \/\/ For c2: c_rarg0 is junk, call to runtime to write a checkpoint.\n+  \/\/ It returns a jobject handle to the event writer.\n+  \/\/ The handle is dereferenced and the return value is the event writer oop.\n+  RuntimeStub* generate_jfr_write_checkpoint();\n+\n+#endif \/\/ INCLUDE_JFR\n+\n+  \/\/ Continuation point for throwing of implicit exceptions that are\n+  \/\/ not handled in the current activation. Fabricates an exception\n+  \/\/ oop and initiates normal exception dispatching in this\n+  \/\/ frame. Since we need to preserve callee-saved values (currently\n+  \/\/ only for C2, but done for C1 as well) we need a callee-saved oop\n+  \/\/ map and therefore have to make these stubs into RuntimeStubs\n+  \/\/ rather than BufferBlobs.  If the compiler needs all registers to\n+  \/\/ be preserved between the fault point and the exception handler\n+  \/\/ then it must assume responsibility for that in\n+  \/\/ AbstractCompiler::continuation_for_implicit_null_exception or\n+  \/\/ continuation_for_implicit_division_by_zero_exception. All other\n+  \/\/ implicit exceptions (e.g., NullPointerException or\n+  \/\/ AbstractMethodError on entry) are either at call sites or\n+  \/\/ otherwise assume that stack unwinding will be initiated, so\n+  \/\/ caller saved registers were assumed volatile in the compiler.\n+  address generate_throw_exception(const char* name,\n+                                   address runtime_entry,\n+                                   Register arg1 = noreg,\n+                                   Register arg2 = noreg);\n+\n+  void create_control_words();\n+\n+  \/\/ Initialization\n+  void generate_initial();\n+  void generate_phase1();\n+  void generate_all();\n+\n+ public:\n+  StubGenerator(CodeBuffer* code, int phase) : StubCodeGenerator(code) {\n+    DEBUG_ONLY( _regs_in_thread = false; )\n+    if (phase == 0) {\n+      generate_initial();\n+    } else if (phase == 1) {\n+      generate_phase1(); \/\/ stubs that must be available for the interpreter\n+    } else {\n+      generate_all();\n+    }\n+  }\n+};\n+\n+#endif \/\/ CPU_X86_STUBGENERATOR_X86_64_HPP\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.hpp","additions":535,"deletions":0,"binary":false,"changes":535,"status":"added"},{"patch":"@@ -292,1 +292,1 @@\n-      __ movflt(xmm0, ExternalAddress((address) &one));\n+      __ movflt(xmm0, ExternalAddress((address) &one), rscratch1);\n@@ -295,1 +295,1 @@\n-      __ movflt(xmm0, ExternalAddress((address) &two));\n+      __ movflt(xmm0, ExternalAddress((address) &two), rscratch1);\n@@ -323,1 +323,1 @@\n-      __ movdbl(xmm0, ExternalAddress((address) &one));\n+      __ movdbl(xmm0, ExternalAddress((address) &one), rscratch1);\n@@ -449,1 +449,1 @@\n-    __ resolve_oop_handle(tmp);\n+    __ resolve_oop_handle(tmp, rscratch2);\n@@ -1130,2 +1130,1 @@\n-  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n-  __ load_klass(rbx, rax, tmp_load_klass);\n+  __ load_klass(rbx, rax, rscratch1);\n@@ -1134,1 +1133,1 @@\n-  __ load_klass(rax, rdx, tmp_load_klass);\n+  __ load_klass(rax, rdx, rscratch1);\n@@ -1177,2 +1176,1 @@\n-  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n-  __ load_klass(rcx, rdx, tmp_load_klass);\n+  __ load_klass(rcx, rdx, rscratch1);\n@@ -1549,1 +1547,1 @@\n-#else\n+#else \/\/ !_LP64\n@@ -1558,1 +1556,1 @@\n-#endif\n+#endif \/\/ _LP64\n@@ -1567,1 +1565,1 @@\n-#else\n+#else \/\/ !_LP64\n@@ -1612,1 +1610,1 @@\n-#else\n+#else \/\/ !_LP64\n@@ -1622,1 +1620,1 @@\n-#endif\n+#endif \/\/ _LP64\n@@ -1631,1 +1629,1 @@\n-#else\n+#else \/\/ !_LP64\n@@ -1660,1 +1658,1 @@\n-#endif\n+#endif \/\/ _LP64\n@@ -1694,1 +1692,1 @@\n-    __ xorps(xmm0, ExternalAddress((address) float_signflip));\n+    __ xorps(xmm0, ExternalAddress((address) float_signflip), rscratch1);\n@@ -1706,1 +1704,1 @@\n-    __ xorpd(xmm0, ExternalAddress((address) double_signflip));\n+    __ xorpd(xmm0, ExternalAddress((address) double_signflip), rscratch1);\n@@ -1827,1 +1825,1 @@\n-    __ cmp64(rax, ExternalAddress((address) &is_nan));\n+    __ cmp64(rax, ExternalAddress((address) &is_nan), rscratch1);\n@@ -1851,1 +1849,1 @@\n-    __ cmp64(rax, ExternalAddress((address) &is_nan));\n+    __ cmp64(rax, ExternalAddress((address) &is_nan), rscratch1);\n@@ -1863,1 +1861,1 @@\n-#else\n+#else \/\/ !_LP64\n@@ -2054,1 +2052,1 @@\n-#endif\n+#endif \/\/ _LP64\n@@ -2108,1 +2106,1 @@\n-#else\n+#else \/\/ !_LP64\n@@ -2571,2 +2569,1 @@\n-    Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n-    __ load_klass(rdi, robj, tmp_load_klass);\n+    __ load_klass(rdi, robj, rscratch1);\n@@ -2720,1 +2717,1 @@\n-    __ resolve_oop_handle(obj);\n+    __ resolve_oop_handle(obj, rscratch2);\n@@ -3612,3 +3609,6 @@\n-    LP64_ONLY(__ lea(rscratch1, table));\n-    LP64_ONLY(__ movptr(flags, Address(rscratch1, flags, Address::times_ptr)));\n-    NOT_LP64(__ movptr(flags, ArrayAddress(table, Address(noreg, flags, Address::times_ptr))));\n+#ifdef _LP64\n+    __ lea(rscratch1, table);\n+    __ movptr(flags, Address(rscratch1, flags, Address::times_ptr));\n+#else\n+    __ movptr(flags, ArrayAddress(table, Address(noreg, flags, Address::times_ptr)));\n+#endif \/\/ _LP64\n@@ -3661,2 +3661,1 @@\n-  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n-  __ load_klass(rax, recv, tmp_load_klass, true);\n+  __ load_klass(rax, recv, rscratch1, true);\n@@ -3753,2 +3752,1 @@\n-  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n-  __ load_klass(rlocals, rcx, tmp_load_klass, true);\n+  __ load_klass(rlocals, rcx, rscratch1, true);\n@@ -3776,1 +3774,1 @@\n-  __ load_klass(rdx, rcx, tmp_load_klass, true);\n+  __ load_klass(rdx, rcx, rscratch1, true);\n@@ -4006,1 +4004,1 @@\n-      SkipIfEqual skip_if(_masm, &DTraceAllocProbes, 0);\n+      SkipIfEqual skip_if(_masm, &DTraceAllocProbes, 0, rscratch1);\n@@ -4097,2 +4095,1 @@\n-  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n-  __ load_klass(rbx, rdx, tmp_load_klass);\n+  __ load_klass(rbx, rdx, rscratch1);\n@@ -4155,2 +4152,1 @@\n-  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n-  __ load_klass(rdx, rdx, tmp_load_klass);\n+  __ load_klass(rdx, rdx, rscratch1);\n@@ -4161,1 +4157,1 @@\n-  __ load_klass(rdx, rax, tmp_load_klass);\n+  __ load_klass(rdx, rax, rscratch1);\n@@ -4278,1 +4274,1 @@\n-    __ cmpptr(Address(rtop, BasicObjectLock::obj_offset_in_bytes()), (int32_t) NULL_WORD);\n+    __ cmpptr(Address(rtop, BasicObjectLock::obj_offset_in_bytes()), NULL_WORD);\n@@ -4400,1 +4396,1 @@\n-  __ jump(ArrayAddress(wtable, Address(noreg, rbx, Address::times_ptr)));\n+  __ jump(ArrayAddress(wtable, Address(noreg, rbx, Address::times_ptr)), rscratch1);\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.cpp","additions":38,"deletions":42,"binary":false,"changes":80,"status":"modified"},{"patch":"@@ -51,1 +51,0 @@\n-  Register tmp_load_klass = rscratch1;\n@@ -73,1 +72,1 @@\n-    __ incrementq(ExternalAddress((address) SharedRuntime::nof_megamorphic_calls_addr()));\n+    __ incrementq(ExternalAddress(SharedRuntime::nof_megamorphic_calls_addr()), rscratch1);\n@@ -84,1 +83,1 @@\n-  __ load_klass(rax, j_rarg0, tmp_load_klass);\n+  __ load_klass(rax, j_rarg0, rscratch1);\n@@ -120,1 +119,1 @@\n-    __ cmpptr(method, (int32_t)NULL_WORD);\n+    __ cmpptr(method, NULL_WORD);\n@@ -122,1 +121,1 @@\n-    __ cmpptr(Address(method, Method::from_compiled_offset()), (int32_t)NULL_WORD);\n+    __ cmpptr(Address(method, Method::from_compiled_offset()), NULL_WORD);\n@@ -167,1 +166,1 @@\n-    __ incrementq(ExternalAddress((address) SharedRuntime::nof_megamorphic_calls_addr()));\n+    __ incrementq(ExternalAddress(SharedRuntime::nof_megamorphic_calls_addr()), rscratch1);\n@@ -239,1 +238,1 @@\n-    __ cmpptr(method, (int32_t)NULL_WORD);\n+    __ cmpptr(method, NULL_WORD);\n@@ -241,1 +240,1 @@\n-    __ cmpptr(Address(method, Method::from_compiled_offset()), (int32_t)NULL_WORD);\n+    __ cmpptr(Address(method, Method::from_compiled_offset()), NULL_WORD);\n","filename":"src\/hotspot\/cpu\/x86\/vtableStubs_x86_64.cpp","additions":7,"deletions":8,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -3681,11 +3681,0 @@\n-operand no_rax_RegL()\n-%{\n-  constraint(ALLOC_IN_RC(long_no_rax_rdx_reg));\n-  match(RegL);\n-  match(rRegL);\n-  match(rdx_RegL);\n-\n-  format %{ %}\n-  interface(REG_INTER);\n-%}\n-\n@@ -6163,1 +6152,1 @@\n-      __ movl($mem$$Address, (int32_t)0);\n+      __ movl($mem$$Address, 0);\n@@ -6726,0 +6715,44 @@\n+\/\/--------------- Reverse Operation Instructions ----------------\n+instruct bytes_reversebit_int(rRegI dst, rRegI src, rRegI rtmp, rFlagsReg cr) %{\n+  predicate(!VM_Version::supports_gfni());\n+  match(Set dst (ReverseI src));\n+  effect(TEMP dst, TEMP rtmp, KILL cr);\n+  format %{ \"reverse_int $dst $src\\t! using $rtmp as TEMP\" %}\n+  ins_encode %{\n+    __ reverseI($dst$$Register, $src$$Register, xnoreg, xnoreg, $rtmp$$Register);\n+  %}\n+  ins_pipe( ialu_reg );\n+%}\n+\n+instruct bytes_reversebit_int_gfni(rRegI dst, rRegI src, regF xtmp1, regF xtmp2, rRegL rtmp, rFlagsReg cr) %{\n+  predicate(VM_Version::supports_gfni());\n+  match(Set dst (ReverseI src));\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP rtmp, KILL cr);\n+  format %{ \"reverse_int $dst $src\\t! using $rtmp, $xtmp1 and $xtmp2 as TEMP\" %}\n+  ins_encode %{\n+    __ reverseI($dst$$Register, $src$$Register, $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, $rtmp$$Register);\n+  %}\n+  ins_pipe( ialu_reg );\n+%}\n+\n+instruct bytes_reversebit_long(rRegL dst, rRegL src, rRegL rtmp1, rRegL rtmp2, rFlagsReg cr) %{\n+  predicate(!VM_Version::supports_gfni());\n+  match(Set dst (ReverseL src));\n+  effect(TEMP dst, TEMP rtmp1, TEMP rtmp2, KILL cr);\n+  format %{ \"reverse_long $dst $src\\t! using $rtmp1 and $rtmp2 as TEMP\" %}\n+  ins_encode %{\n+    __ reverseL($dst$$Register, $src$$Register, xnoreg, xnoreg, $rtmp1$$Register, $rtmp2$$Register);\n+  %}\n+  ins_pipe( ialu_reg );\n+%}\n+\n+instruct bytes_reversebit_long_gfni(rRegL dst, rRegL src, regD xtmp1, regD xtmp2, rRegL rtmp, rFlagsReg cr) %{\n+  predicate(VM_Version::supports_gfni());\n+  match(Set dst (ReverseL src));\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP rtmp, KILL cr);\n+  format %{ \"reverse_long $dst $src\\t! using $rtmp, $xtmp1 and $xtmp2 as TEMP\" %}\n+  ins_encode %{\n+    __ reverseL($dst$$Register, $src$$Register, $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, $rtmp$$Register, noreg);\n+  %}\n+  ins_pipe( ialu_reg );\n+%}\n@@ -7628,1 +7661,1 @@\n-  ins_cost(125); \/\/ XXX\n+  ins_cost(150); \/\/ XXX\n@@ -7798,1 +7831,1 @@\n-  ins_cost(125); \/\/ XXX\n+  ins_cost(150); \/\/ XXX\n@@ -8029,65 +8062,0 @@\n-\/\/ LoadP-locked same as a regular LoadP when used with compare-swap\n-instruct loadPLocked(rRegP dst, memory mem)\n-%{\n-  match(Set dst (LoadPLocked mem));\n-\n-  ins_cost(125); \/\/ XXX\n-  format %{ \"movq    $dst, $mem\\t# ptr locked\" %}\n-  ins_encode %{\n-    __ movq($dst$$Register, $mem$$Address);\n-  %}\n-  ins_pipe(ialu_reg_mem); \/\/ XXX\n-%}\n-\n-\/\/ Conditional-store of the updated heap-top.\n-\/\/ Used during allocation of the shared heap.\n-\/\/ Sets flags (EQ) on success.  Implemented with a CMPXCHG on Intel.\n-\n-instruct storePConditional(memory heap_top_ptr,\n-                           rax_RegP oldval, rRegP newval,\n-                           rFlagsReg cr)\n-%{\n-  predicate(n->as_LoadStore()->barrier_data() == 0);\n-  match(Set cr (StorePConditional heap_top_ptr (Binary oldval newval)));\n-\n-  format %{ \"cmpxchgq $heap_top_ptr, $newval\\t# (ptr) \"\n-            \"If rax == $heap_top_ptr then store $newval into $heap_top_ptr\" %}\n-  ins_encode %{\n-    __ lock();\n-    __ cmpxchgq($newval$$Register, $heap_top_ptr$$Address);\n-  %}\n-  ins_pipe(pipe_cmpxchg);\n-%}\n-\n-\/\/ Conditional-store of an int value.\n-\/\/ ZF flag is set on success, reset otherwise.  Implemented with a CMPXCHG.\n-instruct storeIConditional(memory mem, rax_RegI oldval, rRegI newval, rFlagsReg cr)\n-%{\n-  match(Set cr (StoreIConditional mem (Binary oldval newval)));\n-  effect(KILL oldval);\n-\n-  format %{ \"cmpxchgl $mem, $newval\\t# If rax == $mem then store $newval into $mem\" %}\n-  opcode(0x0F, 0xB1);\n-  ins_encode(lock_prefix,\n-             REX_reg_mem(newval, mem),\n-             OpcP, OpcS,\n-             reg_mem(newval, mem));\n-  ins_pipe(pipe_cmpxchg);\n-%}\n-\n-\/\/ Conditional-store of a long value.\n-\/\/ ZF flag is set on success, reset otherwise.  Implemented with a CMPXCHG.\n-instruct storeLConditional(memory mem, rax_RegL oldval, rRegL newval, rFlagsReg cr)\n-%{\n-  match(Set cr (StoreLConditional mem (Binary oldval newval)));\n-  effect(KILL oldval);\n-\n-  format %{ \"cmpxchgq $mem, $newval\\t# If rax == $mem then store $newval into $mem\" %}\n-  ins_encode %{\n-    __ lock();\n-    __ cmpxchgq($newval$$Register, $mem$$Address);\n-  %}\n-  ins_pipe(pipe_cmpxchg);\n-%}\n-\n-\n@@ -8539,12 +8507,0 @@\n-instruct subI_rReg_imm(rRegI dst, immI src, rFlagsReg cr)\n-%{\n-  match(Set dst (SubI dst src));\n-  effect(KILL cr);\n-\n-  format %{ \"subl    $dst, $src\\t# int\" %}\n-  ins_encode %{\n-    __ subl($dst$$Register, $src$$constant);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n@@ -8556,1 +8512,1 @@\n-  ins_cost(125);\n+  ins_cost(150);\n@@ -8577,13 +8533,0 @@\n-instruct subI_mem_imm(memory dst, immI src, rFlagsReg cr)\n-%{\n-  match(Set dst (StoreI dst (SubI (LoadI dst) src)));\n-  effect(KILL cr);\n-\n-  ins_cost(125); \/\/ XXX\n-  format %{ \"subl    $dst, $src\\t# int\" %}\n-  ins_encode %{\n-    __ subl($dst$$Address, $src$$constant);\n-  %}\n-  ins_pipe(ialu_mem_imm);\n-%}\n-\n@@ -8602,12 +8545,0 @@\n-instruct subL_rReg_imm(rRegI dst, immL32 src, rFlagsReg cr)\n-%{\n-  match(Set dst (SubL dst src));\n-  effect(KILL cr);\n-\n-  format %{ \"subq    $dst, $src\\t# long\" %}\n-  ins_encode %{\n-    __ subq($dst$$Register, $src$$constant);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n@@ -8619,1 +8550,1 @@\n-  ins_cost(125);\n+  ins_cost(150);\n@@ -8640,13 +8571,0 @@\n-instruct subL_mem_imm(memory dst, immL32 src, rFlagsReg cr)\n-%{\n-  match(Set dst (StoreL dst (SubL (LoadL dst) src)));\n-  effect(KILL cr);\n-\n-  ins_cost(125); \/\/ XXX\n-  format %{ \"subq    $dst, $src\\t# long\" %}\n-  ins_encode %{\n-    __ subq($dst$$Address, $src$$constant);\n-  %}\n-  ins_pipe(ialu_mem_imm);\n-%}\n-\n@@ -8856,1 +8774,1 @@\n-instruct mulHiL_rReg(rdx_RegL dst, no_rax_RegL src, rax_RegL rax, rFlagsReg cr)\n+instruct mulHiL_rReg(rdx_RegL dst, rRegL src, rax_RegL rax, rFlagsReg cr)\n@@ -8869,1 +8787,1 @@\n-instruct umulHiL_rReg(rdx_RegL dst, no_rax_RegL src, rax_RegL rax, rFlagsReg cr)\n+instruct umulHiL_rReg(rdx_RegL dst, rRegL src, rax_RegL rax, rFlagsReg cr)\n@@ -9022,65 +8940,0 @@\n-\n-\/\/----------- DivL-By-Constant-Expansions--------------------------------------\n-\/\/ DivI cases are handled by the compiler\n-\n-\/\/ Magic constant, reciprocal of 10\n-instruct loadConL_0x6666666666666667(rRegL dst)\n-%{\n-  effect(DEF dst);\n-\n-  format %{ \"movq    $dst, #0x666666666666667\\t# Used in div-by-10\" %}\n-  ins_encode(load_immL(dst, 0x6666666666666667));\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct mul_hi(rdx_RegL dst, no_rax_RegL src, rax_RegL rax, rFlagsReg cr)\n-%{\n-  effect(DEF dst, USE src, USE_KILL rax, KILL cr);\n-\n-  format %{ \"imulq   rdx:rax, rax, $src\\t# Used in div-by-10\" %}\n-  ins_encode %{\n-    __ imulq($src$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg_alu0);\n-%}\n-\n-instruct sarL_rReg_63(rRegL dst, rFlagsReg cr)\n-%{\n-  effect(USE_DEF dst, KILL cr);\n-\n-  format %{ \"sarq    $dst, #63\\t# Used in div-by-10\" %}\n-  ins_encode %{\n-    __ sarq($dst$$Register, 63);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct sarL_rReg_2(rRegL dst, rFlagsReg cr)\n-%{\n-  effect(USE_DEF dst, KILL cr);\n-\n-  format %{ \"sarq    $dst, #2\\t# Used in div-by-10\" %}\n-  ins_encode %{\n-    __ sarq($dst$$Register, 2);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct divL_10(rdx_RegL dst, no_rax_RegL src, immL10 div)\n-%{\n-  match(Set dst (DivL src div));\n-\n-  ins_cost((5+8)*100);\n-  expand %{\n-    rax_RegL rax;                     \/\/ Killed temp\n-    rFlagsReg cr;                     \/\/ Killed\n-    loadConL_0x6666666666666667(rax); \/\/ movq  rax, 0x6666666666666667\n-    mul_hi(dst, src, rax, cr);        \/\/ mulq  rdx:rax <= rax * $src\n-    sarL_rReg_63(src, cr);            \/\/ sarq  src, 63\n-    sarL_rReg_2(dst, cr);             \/\/ sarq  rdx, 2\n-    subL_rReg(dst, src, cr);          \/\/ subl  rdx, src\n-  %}\n-%}\n-\n-\/\/-----------------------------------------------------------------------------\n-\n@@ -9153,26 +9006,0 @@\n-\/\/ Shift Left by one\n-instruct salI_rReg_1(rRegI dst, immI_1 shift, rFlagsReg cr)\n-%{\n-  match(Set dst (LShiftI dst shift));\n-  effect(KILL cr);\n-\n-  format %{ \"sall    $dst, $shift\" %}\n-  ins_encode %{\n-    __ sall($dst$$Register, $shift$$constant);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\/\/ Shift Left by one\n-instruct salI_mem_1(memory dst, immI_1 shift, rFlagsReg cr)\n-%{\n-  match(Set dst (StoreI dst (LShiftI (LoadI dst) shift)));\n-  effect(KILL cr);\n-\n-  format %{ \"sall    $dst, $shift\\t\" %}\n-  ins_encode %{\n-    __ sall($dst$$Address, $shift$$constant);\n-  %}\n-  ins_pipe(ialu_mem_imm);\n-%}\n-\n@@ -9257,26 +9084,0 @@\n-\/\/ Arithmetic shift right by one\n-instruct sarI_rReg_1(rRegI dst, immI_1 shift, rFlagsReg cr)\n-%{\n-  match(Set dst (RShiftI dst shift));\n-  effect(KILL cr);\n-\n-  format %{ \"sarl    $dst, $shift\" %}\n-  ins_encode %{\n-    __ sarl($dst$$Register, $shift$$constant);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\/\/ Arithmetic shift right by one\n-instruct sarI_mem_1(memory dst, immI_1 shift, rFlagsReg cr)\n-%{\n-  match(Set dst (StoreI dst (RShiftI (LoadI dst) shift)));\n-  effect(KILL cr);\n-\n-  format %{ \"sarl    $dst, $shift\" %}\n-  ins_encode %{\n-    __ sarl($dst$$Address, $shift$$constant);\n-  %}\n-  ins_pipe(ialu_mem_imm);\n-%}\n-\n@@ -9360,26 +9161,0 @@\n-\/\/ Logical shift right by one\n-instruct shrI_rReg_1(rRegI dst, immI_1 shift, rFlagsReg cr)\n-%{\n-  match(Set dst (URShiftI dst shift));\n-  effect(KILL cr);\n-\n-  format %{ \"shrl    $dst, $shift\" %}\n-  ins_encode %{\n-    __ shrl($dst$$Register, $shift$$constant);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\/\/ Logical shift right by one\n-instruct shrI_mem_1(memory dst, immI_1 shift, rFlagsReg cr)\n-%{\n-  match(Set dst (StoreI dst (URShiftI (LoadI dst) shift)));\n-  effect(KILL cr);\n-\n-  format %{ \"shrl    $dst, $shift\" %}\n-  ins_encode %{\n-    __ shrl($dst$$Address, $shift$$constant);\n-  %}\n-  ins_pipe(ialu_mem_imm);\n-%}\n-\n@@ -9465,26 +9240,0 @@\n-\/\/ Shift Left by one\n-instruct salL_rReg_1(rRegL dst, immI_1 shift, rFlagsReg cr)\n-%{\n-  match(Set dst (LShiftL dst shift));\n-  effect(KILL cr);\n-\n-  format %{ \"salq    $dst, $shift\" %}\n-  ins_encode %{\n-    __ salq($dst$$Register, $shift$$constant);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\/\/ Shift Left by one\n-instruct salL_mem_1(memory dst, immI_1 shift, rFlagsReg cr)\n-%{\n-  match(Set dst (StoreL dst (LShiftL (LoadL dst) shift)));\n-  effect(KILL cr);\n-\n-  format %{ \"salq    $dst, $shift\" %}\n-  ins_encode %{\n-    __ salq($dst$$Address, $shift$$constant);\n-  %}\n-  ins_pipe(ialu_mem_imm);\n-%}\n-\n@@ -9569,26 +9318,0 @@\n-\/\/ Arithmetic shift right by one\n-instruct sarL_rReg_1(rRegL dst, immI_1 shift, rFlagsReg cr)\n-%{\n-  match(Set dst (RShiftL dst shift));\n-  effect(KILL cr);\n-\n-  format %{ \"sarq    $dst, $shift\" %}\n-  ins_encode %{\n-    __ sarq($dst$$Register, $shift$$constant);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\/\/ Arithmetic shift right by one\n-instruct sarL_mem_1(memory dst, immI_1 shift, rFlagsReg cr)\n-%{\n-  match(Set dst (StoreL dst (RShiftL (LoadL dst) shift)));\n-  effect(KILL cr);\n-\n-  format %{ \"sarq    $dst, $shift\" %}\n-  ins_encode %{\n-    __ sarq($dst$$Address, $shift$$constant);\n-  %}\n-  ins_pipe(ialu_mem_imm);\n-%}\n-\n@@ -9673,26 +9396,0 @@\n-\/\/ Logical shift right by one\n-instruct shrL_rReg_1(rRegL dst, immI_1 shift, rFlagsReg cr)\n-%{\n-  match(Set dst (URShiftL dst shift));\n-  effect(KILL cr);\n-\n-  format %{ \"shrq    $dst, $shift\" %}\n-  ins_encode %{\n-    __ shrq($dst$$Register, $shift$$constant);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\/\/ Logical shift right by one\n-instruct shrL_mem_1(memory dst, immI_1 shift, rFlagsReg cr)\n-%{\n-  match(Set dst (StoreL dst (URShiftL (LoadL dst) shift)));\n-  effect(KILL cr);\n-\n-  format %{ \"shrq    $dst, $shift\" %}\n-  ins_encode %{\n-    __ shrq($dst$$Address, $shift$$constant);\n-  %}\n-  ins_pipe(ialu_mem_imm);\n-%}\n-\n@@ -10069,1 +9766,1 @@\n-instruct andI_rReg_imm255(rRegI dst, immI_255 src)\n+instruct andI_rReg_imm255(rRegI dst, rRegI src, immI_255 mask)\n@@ -10071,1 +9768,1 @@\n-  match(Set dst (AndI dst src));\n+  match(Set dst (AndI src mask));\n@@ -10073,1 +9770,1 @@\n-  format %{ \"movzbl  $dst, $dst\\t# int & 0xFF\" %}\n+  format %{ \"movzbl  $dst, $src\\t# int & 0xFF\" %}\n@@ -10075,1 +9772,1 @@\n-    __ movzbl($dst$$Register, $dst$$Register);\n+    __ movzbl($dst$$Register, $src$$Register);\n@@ -10093,1 +9790,1 @@\n-instruct andI_rReg_imm65535(rRegI dst, immI_65535 src)\n+instruct andI_rReg_imm65535(rRegI dst, rRegI src, immI_65535 mask)\n@@ -10095,1 +9792,1 @@\n-  match(Set dst (AndI dst src));\n+  match(Set dst (AndI src mask));\n@@ -10097,1 +9794,1 @@\n-  format %{ \"movzwl  $dst, $dst\\t# int & 0xFFFF\" %}\n+  format %{ \"movzwl  $dst, $src\\t# int & 0xFFFF\" %}\n@@ -10099,1 +9796,1 @@\n-    __ movzwl($dst$$Register, $dst$$Register);\n+    __ movzwl($dst$$Register, $src$$Register);\n@@ -10150,1 +9847,1 @@\n-  ins_cost(125);\n+  ins_cost(150);\n@@ -10348,1 +10045,1 @@\n-  ins_cost(125);\n+  ins_cost(150);\n@@ -10441,1 +10138,1 @@\n-  ins_cost(125);\n+  ins_cost(150);\n@@ -10508,1 +10205,1 @@\n-instruct andL_rReg_imm255(rRegL dst, immL_255 src)\n+instruct andL_rReg_imm255(rRegL dst, rRegL src, immL_255 mask)\n@@ -10510,1 +10207,1 @@\n-  match(Set dst (AndL dst src));\n+  match(Set dst (AndL src mask));\n@@ -10512,1 +10209,1 @@\n-  format %{ \"movzbq  $dst, $dst\\t# long & 0xFF\" %}\n+  format %{ \"movzbl  $dst, $src\\t# long & 0xFF\" %}\n@@ -10514,1 +10211,2 @@\n-    __ movzbq($dst$$Register, $dst$$Register);\n+    \/\/ movzbl zeroes out the upper 32-bit and does not need REX.W\n+    __ movzbl($dst$$Register, $src$$Register);\n@@ -10520,1 +10218,1 @@\n-instruct andL_rReg_imm65535(rRegL dst, immL_65535 src)\n+instruct andL_rReg_imm65535(rRegL dst, rRegL src, immL_65535 mask)\n@@ -10522,1 +10220,1 @@\n-  match(Set dst (AndL dst src));\n+  match(Set dst (AndL src mask));\n@@ -10524,1 +10222,1 @@\n-  format %{ \"movzwq  $dst, $dst\\t# long & 0xFFFF\" %}\n+  format %{ \"movzwl  $dst, $src\\t# long & 0xFFFF\" %}\n@@ -10526,1 +10224,2 @@\n-    __ movzwq($dst$$Register, $dst$$Register);\n+    \/\/ movzwl zeroes out the upper 32-bit and does not need REX.W\n+    __ movzwl($dst$$Register, $src$$Register);\n@@ -10550,1 +10249,1 @@\n-  ins_cost(125);\n+  ins_cost(150);\n@@ -10765,1 +10464,1 @@\n-  ins_cost(125);\n+  ins_cost(150);\n@@ -10862,1 +10561,1 @@\n-  ins_cost(125);\n+  ins_cost(150);\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":75,"deletions":376,"binary":false,"changes":451,"status":"modified"},{"patch":"@@ -1584,1 +1584,1 @@\n-    default:       out->print(\"%3d:0x\" UINT64_FORMAT_X, type(), (uint64_t)as_jlong()); break;\n+    default:       out->print(\"%3d:\" UINT64_FORMAT_X, type(), (uint64_t)as_jlong()); break;\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"compiler\/compilerDefinitions.inline.hpp\"\n@@ -2686,4 +2687,0 @@\n-  if (compilation()->age_code()) {\n-    CodeEmitInfo* info = new CodeEmitInfo(scope()->start()->state()->copy(ValueStack::StateBefore, 0), NULL, false);\n-    decrement_age(info);\n-  }\n@@ -3023,4 +3020,0 @@\n-  case vmIntrinsics::_Continuation_doYield:\n-    do_continuation_doYield(x);\n-    break;\n-\n@@ -3256,21 +3249,0 @@\n-void LIRGenerator::decrement_age(CodeEmitInfo* info) {\n-  ciMethod* method = info->scope()->method();\n-  MethodCounters* mc_adr = method->ensure_method_counters();\n-  if (mc_adr != NULL) {\n-    LIR_Opr mc = new_pointer_register();\n-    __ move(LIR_OprFact::intptrConst(mc_adr), mc);\n-    int offset = in_bytes(MethodCounters::nmethod_age_offset());\n-    LIR_Address* counter = new LIR_Address(mc, offset, T_INT);\n-    LIR_Opr result = new_register(T_INT);\n-    __ load(counter, result);\n-    __ sub(result, LIR_OprFact::intConst(1), result);\n-    __ store(result, counter);\n-    \/\/ DeoptimizeStub will reexecute from the current state in code info.\n-    CodeStub* deopt = new DeoptimizeStub(info, Deoptimization::Reason_tenured,\n-                                         Deoptimization::Action_make_not_entrant);\n-    __ cmp(lir_cond_lessEqual, result, LIR_OprFact::intConst(0));\n-    __ branch(lir_cond_lessEqual, deopt);\n-  }\n-}\n-\n-\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.cpp","additions":1,"deletions":29,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -345,1 +345,0 @@\n-  FUNCTION_CASE(entry, StubRoutines::cont_doYield());\n@@ -558,1 +557,1 @@\n-    ResourceMark rm;\n+    ResourceMark rm; \/\/ print_value_string\n@@ -564,1 +563,1 @@\n-    Exceptions::log_exception(exception, tempst.as_string());\n+    Exceptions::log_exception(exception, tempst.freeze());\n@@ -1287,0 +1286,31 @@\n+static bool is_patching_needed(JavaThread* current, Runtime1::StubID stub_id) {\n+  if (stub_id == Runtime1::load_klass_patching_id ||\n+      stub_id == Runtime1::load_mirror_patching_id) {\n+    \/\/ last java frame on stack\n+    vframeStream vfst(current, true);\n+    assert(!vfst.at_end(), \"Java frame must exist\");\n+\n+    methodHandle caller_method(current, vfst.method());\n+    int bci = vfst.bci();\n+    Bytecodes::Code code = caller_method()->java_code_at(bci);\n+\n+    switch (code) {\n+      case Bytecodes::_new:\n+      case Bytecodes::_anewarray:\n+      case Bytecodes::_multianewarray:\n+      case Bytecodes::_instanceof:\n+      case Bytecodes::_checkcast: {\n+        Bytecode bc(caller_method(), caller_method->bcp_from(bci));\n+        constantTag tag = caller_method->constants()->tag_at(bc.get_index_u2(code));\n+        if (tag.is_unresolved_klass_in_error()) {\n+          return false; \/\/ throws resolution error\n+        }\n+        break;\n+      }\n+\n+      default: break;\n+    }\n+  }\n+  return true;\n+}\n+\n@@ -1311,4 +1341,6 @@\n-  \/\/ Make sure the nmethod is invalidated, i.e. made not entrant.\n-  nmethod* nm = CodeCache::find_nmethod(caller_frame.pc());\n-  if (nm != NULL) {\n-    nm->make_not_entrant();\n+  if (is_patching_needed(current, stub_id)) {\n+    \/\/ Make sure the nmethod is invalidated, i.e. made not entrant.\n+    nmethod* nm = CodeCache::find_nmethod(caller_frame.pc());\n+    if (nm != NULL) {\n+      nm->make_not_entrant();\n+    }\n@@ -1468,1 +1500,1 @@\n-    tty->print_cr(\"Predicate failed trap in method %s at bci %d inlined in %s at pc \" INTPTR_FORMAT, ss1.as_string(), vfst.bci(), ss2.as_string(), p2i(caller_frame.pc()));\n+    tty->print_cr(\"Predicate failed trap in method %s at bci %d inlined in %s at pc \" INTPTR_FORMAT, ss1.freeze(), vfst.bci(), ss2.freeze(), p2i(caller_frame.pc()));\n","filename":"src\/hotspot\/share\/c1\/c1_Runtime1.cpp","additions":40,"deletions":8,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -55,1 +55,0 @@\n-#include \"utilities\/hashtable.inline.hpp\"\n@@ -115,1 +114,1 @@\n-  address _dumped_obj;\n+  address _buffered_obj;\n@@ -118,2 +117,2 @@\n-  RelocateEmbeddedPointers(ArchiveBuilder* builder, address dumped_obj, BitMap::idx_t start_idx) :\n-    _builder(builder), _dumped_obj(dumped_obj), _start_idx(start_idx) {}\n+  RelocateEmbeddedPointers(ArchiveBuilder* builder, address buffered_obj, BitMap::idx_t start_idx) :\n+    _builder(builder), _buffered_obj(buffered_obj), _start_idx(start_idx) {}\n@@ -123,1 +122,1 @@\n-    address* ptr_loc = (address*)(_dumped_obj + field_offset);\n+    address* ptr_loc = (address*)(_buffered_obj + field_offset);\n@@ -126,1 +125,1 @@\n-    address new_p = _builder->get_dumped_addr(old_p);\n+    address new_p = _builder->get_buffered_addr(old_p);\n@@ -142,1 +141,1 @@\n-  RelocateEmbeddedPointers relocator(builder, src_info->dumped_addr(), start);\n+  RelocateEmbeddedPointers relocator(builder, src_info->buffered_addr(), start);\n@@ -164,0 +163,1 @@\n+  _buffered_to_src_table(INITIAL_TABLE_SIZE, MAX_TABLE_SIZE),\n@@ -640,0 +640,8 @@\n+  {\n+    bool created;\n+    _buffered_to_src_table.put_if_absent((address)dest, src, &created);\n+    assert(created, \"must be\");\n+    if (_buffered_to_src_table.maybe_grow()) {\n+      log_info(cds, hashtables)(\"Expanded _buffered_to_src_table table to %d\", _buffered_to_src_table.table_size());\n+    }\n+  }\n@@ -649,1 +657,1 @@\n-  src_info->set_dumped_addr((address)dest);\n+  src_info->set_buffered_addr((address)dest);\n@@ -656,2 +664,2 @@\n-address ArchiveBuilder::get_dumped_addr(address src_obj) const {\n-  SourceObjInfo* p = _src_obj_table.get(src_obj);\n+address ArchiveBuilder::get_buffered_addr(address src_addr) const {\n+  SourceObjInfo* p = _src_obj_table.get(src_addr);\n@@ -660,1 +668,8 @@\n-  return p->dumped_addr();\n+  return p->buffered_addr();\n+}\n+\n+address ArchiveBuilder::get_source_addr(address buffered_addr) const {\n+  assert(is_in_buffer_space(buffered_addr), \"must be\");\n+  address* src_p = _buffered_to_src_table.get(buffered_addr);\n+  assert(src_p != NULL && *src_p != NULL, \"must be\");\n+  return *src_p;\n@@ -674,1 +689,1 @@\n-    address dst_obj = get_dumped_addr(src_obj);\n+    address dst_obj = get_buffered_addr(src_obj);\n@@ -692,1 +707,1 @@\n-      ref->update(_builder->get_dumped_addr(ref->obj()));\n+      ref->update(_builder->get_buffered_addr(ref->obj()));\n@@ -834,3 +849,3 @@\n-\/\/ Update a Java object to point its Klass* to the new location after\n-\/\/ shared archive has been compacted.\n-void ArchiveBuilder::relocate_klass_ptr(oop o) {\n+\/\/ Update a Java object to point its Klass* to the address whene\n+\/\/ the class would be mapped at runtime.\n+void ArchiveBuilder::relocate_klass_ptr_of_oop(oop o) {\n@@ -838,1 +853,1 @@\n-  Klass* k = get_relocated_klass(o->klass());\n+  Klass* k = get_buffered_klass(o->klass());\n@@ -988,2 +1003,2 @@\n-      address src = src_info->orig_obj();\n-      address dest = src_info->dumped_addr();\n+      address src = src_info->source_addr();\n+      address dest = src_info->buffered_addr();\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.cpp","additions":34,"deletions":19,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -75,0 +75,28 @@\n+\/\/\n+\/\/ \"source\" vs \"buffered\" vs \"requested\"\n+\/\/\n+\/\/ The ArchiveBuilder deals with three types of addresses.\n+\/\/\n+\/\/ \"source\":    These are the addresses of objects created in step [1] above. They are the actual\n+\/\/              InstanceKlass*, Method*, etc, of the Java classes that are loaded for executing\n+\/\/              Java bytecodes in the JVM process that's dumping the CDS archive.\n+\/\/\n+\/\/              It may be necessary to contiue Java execution after ArchiveBuilder is finished.\n+\/\/              Therefore, we don't modify any of the \"source\" objects.\n+\/\/\n+\/\/ \"buffered\":  The \"source\" objects that are deemed archivable are copied into a temporary buffer.\n+\/\/              Objects in the buffer are modified in steps [2, 3, 4] (e.g., unshareable info is\n+\/\/              removed, pointers are relocated, etc) to prepare them to be loaded at runtime.\n+\/\/\n+\/\/ \"requested\": These are the addreses where the \"buffered\" objects should be loaded at runtime.\n+\/\/              When the \"buffered\" objects are written into the archive file, their addresses\n+\/\/              are adjusted in step [5] such that the lowest of these objects would be mapped\n+\/\/              at SharedBaseAddress.\n+\/\/\n+\/\/ Translation between \"source\" and \"buffered\" addresses is done with two hashtables:\n+\/\/     _src_obj_table          : \"source\"   -> \"buffered\"\n+\/\/     _buffered_to_src_table  : \"buffered\" -> \"source\"\n+\/\/\n+\/\/ Translation between \"buffered\" and \"requested\" addresses is done with a simple shift:\n+\/\/    buffered_address + _buffer_to_requested_delta == requested_address\n+\/\/\n@@ -126,1 +154,1 @@\n-    MetaspaceClosure::Ref* _ref;\n+    MetaspaceClosure::Ref* _ref; \/\/ The object that's copied into the buffer\n@@ -133,2 +161,1 @@\n-    address _dumped_addr;    \/\/ Address this->obj(), as used by the dumped archive.\n-    address _orig_obj;       \/\/ The value of the original object (_ref->obj()) when this\n+    address _source_addr;    \/\/ The value of the source object (_ref->obj()) when this\n@@ -137,1 +164,1 @@\n-\n+    address _buffered_addr;  \/\/ The copy of _ref->obj() insider the buffer.\n@@ -142,1 +169,1 @@\n-      _orig_obj(ref->obj()) {\n+      _source_addr(ref->obj()) {\n@@ -144,1 +171,1 @@\n-        _dumped_addr = ref->obj();\n+        _buffered_addr = ref->obj();\n@@ -146,1 +173,1 @@\n-        _dumped_addr = NULL;\n+        _buffered_addr = NULL;\n@@ -152,1 +179,1 @@\n-    void set_dumped_addr(address dumped_addr)  {\n+    void set_buffered_addr(address addr)  {\n@@ -154,3 +181,3 @@\n-      assert(_dumped_addr == NULL, \"cannot be copied twice\");\n-      assert(dumped_addr != NULL, \"must be a valid copy\");\n-      _dumped_addr = dumped_addr;\n+      assert(_buffered_addr == NULL, \"cannot be copied twice\");\n+      assert(addr != NULL, \"must be a valid copy\");\n+      _buffered_addr = addr;\n@@ -164,2 +191,2 @@\n-    address orig_obj()    const    { return _orig_obj; }\n-    address dumped_addr() const    { return _dumped_addr; }\n+    address source_addr() const    { return _source_addr; }\n+    address buffered_addr() const  { return _buffered_addr; }\n@@ -214,0 +241,1 @@\n+  ResizeableResourceHashtable<address, address, ResourceObj::C_HEAP, mtClassShared> _buffered_to_src_table;\n@@ -397,1 +425,5 @@\n-  address get_dumped_addr(address src_obj) const;\n+  address get_buffered_addr(address src_addr) const;\n+  address get_source_addr(address buffered_addr) const;\n+  template <typename T> T get_source_addr(T buffered_addr) const {\n+    return (T)get_source_addr((address)buffered_addr);\n+  }\n@@ -425,1 +457,1 @@\n-  void relocate_klass_ptr(oop o);\n+  void relocate_klass_ptr_of_oop(oop o);\n@@ -427,2 +459,2 @@\n-  static Klass* get_relocated_klass(Klass* orig_klass) {\n-    Klass* klass = (Klass*)current()->get_dumped_addr((address)orig_klass);\n+  static Klass* get_buffered_klass(Klass* src_klass) {\n+    Klass* klass = (Klass*)current()->get_buffered_addr((address)src_klass);\n@@ -433,2 +465,2 @@\n-  static Symbol* get_relocated_symbol(Symbol* orig_symbol) {\n-    return (Symbol*)current()->get_dumped_addr((address)orig_symbol);\n+  static Symbol* get_buffered_symbol(Symbol* src_symbol) {\n+    return (Symbol*)current()->get_buffered_addr((address)src_symbol);\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.hpp","additions":52,"deletions":20,"binary":false,"changes":72,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"cds\/archiveHeapLoader.inline.hpp\"\n@@ -32,1 +33,1 @@\n-#include \"cds\/heapShared.inline.hpp\"\n+#include \"cds\/heapShared.hpp\"\n@@ -332,1 +333,1 @@\n-    if (CompressedOops::is_null(o) || !HeapShared::is_fully_available()) {\n+    if (CompressedOops::is_null(o) || !ArchiveHeapLoader::is_fully_available()) {\n@@ -335,3 +336,3 @@\n-      assert(HeapShared::can_use(), \"sanity\");\n-      assert(HeapShared::is_fully_available(), \"must be\");\n-      *p = HeapShared::decode_from_archive(o);\n+      assert(ArchiveHeapLoader::can_use(), \"sanity\");\n+      assert(ArchiveHeapLoader::is_fully_available(), \"must be\");\n+      *p = ArchiveHeapLoader::decode_from_archive(o);\n@@ -341,1 +342,1 @@\n-    if (dumptime_oop == 0 || !HeapShared::is_fully_available()) {\n+    if (dumptime_oop == 0 || !ArchiveHeapLoader::is_fully_available()) {\n@@ -344,1 +345,1 @@\n-      intptr_t runtime_oop = dumptime_oop + HeapShared::runtime_delta();\n+      intptr_t runtime_oop = dumptime_oop + ArchiveHeapLoader::runtime_delta();\n","filename":"src\/hotspot\/share\/cds\/archiveUtils.cpp","additions":8,"deletions":7,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"cds\/archiveHeapLoader.hpp\"\n@@ -29,2 +30,1 @@\n-#include \"cds\/filemap.hpp\"\n-#include \"cds\/heapShared.inline.hpp\"\n+#include \"cds\/heapShared.hpp\"\n@@ -48,2 +48,0 @@\n-#include \"memory\/metadataFactory.hpp\"\n-#include \"memory\/metaspaceClosure.hpp\"\n@@ -59,2 +57,0 @@\n-#include \"runtime\/globals_extension.hpp\"\n-#include \"runtime\/java.hpp\"\n@@ -73,3 +69,15 @@\n-bool HeapShared::_closed_regions_mapped = false;\n-bool HeapShared::_open_regions_mapped = false;\n-bool HeapShared::_is_loaded = false;\n+struct ArchivableStaticFieldInfo {\n+  const char* klass_name;\n+  const char* field_name;\n+  InstanceKlass* klass;\n+  int offset;\n+  BasicType type;\n+\n+  ArchivableStaticFieldInfo(const char* k, const char* f)\n+  : klass_name(k), field_name(f), klass(NULL), offset(0), type(T_ILLEGAL) {}\n+\n+  bool valid() {\n+    return klass_name != NULL;\n+  }\n+};\n+\n@@ -77,2 +85,0 @@\n-address   HeapShared::_narrow_oop_base;\n-int       HeapShared::_narrow_oop_shift;\n@@ -81,16 +87,8 @@\n-\/\/ Support for loaded heap.\n-uintptr_t HeapShared::_loaded_heap_bottom = 0;\n-uintptr_t HeapShared::_loaded_heap_top = 0;\n-uintptr_t HeapShared::_dumptime_base_0 = UINTPTR_MAX;\n-uintptr_t HeapShared::_dumptime_base_1 = UINTPTR_MAX;\n-uintptr_t HeapShared::_dumptime_base_2 = UINTPTR_MAX;\n-uintptr_t HeapShared::_dumptime_base_3 = UINTPTR_MAX;\n-uintptr_t HeapShared::_dumptime_top    = 0;\n-intx HeapShared::_runtime_offset_0 = 0;\n-intx HeapShared::_runtime_offset_1 = 0;\n-intx HeapShared::_runtime_offset_2 = 0;\n-intx HeapShared::_runtime_offset_3 = 0;\n-bool HeapShared::_loading_failed = false;\n-\n-\/\/ Support for mapped heap (!UseCompressedOops only)\n-ptrdiff_t HeapShared::_runtime_delta = 0;\n+#ifndef PRODUCT\n+#define ARCHIVE_TEST_FIELD_NAME \"archivedObjects\"\n+static Array<char>* _archived_ArchiveHeapTestClass = NULL;\n+static const char* _test_class_name = NULL;\n+static const Klass* _test_class = NULL;\n+static const ArchivedKlassSubGraphInfoRecord* _test_class_record = NULL;\n+#endif\n+\n@@ -113,0 +111,1 @@\n+  {NULL, NULL},\n@@ -121,0 +120,4 @@\n+#ifndef PRODUCT\n+  {NULL, NULL}, \/\/ Extra slot for -XX:ArchiveHeapTestClass\n+#endif\n+  {NULL, NULL},\n@@ -128,0 +131,1 @@\n+  {NULL, NULL},\n@@ -130,7 +134,0 @@\n-const static int num_closed_archive_subgraph_entry_fields =\n-  sizeof(closed_archive_subgraph_entry_fields) \/ sizeof(ArchivableStaticFieldInfo);\n-const static int num_open_archive_subgraph_entry_fields =\n-  sizeof(open_archive_subgraph_entry_fields) \/ sizeof(ArchivableStaticFieldInfo);\n-const static int num_fmg_open_archive_subgraph_entry_fields =\n-  sizeof(fmg_open_archive_subgraph_entry_fields) \/ sizeof(ArchivableStaticFieldInfo);\n-\n@@ -148,2 +145,2 @@\n-static bool is_subgraph_root_class_of(ArchivableStaticFieldInfo fields[], int num, InstanceKlass* ik) {\n-  for (int i = 0; i < num; i++) {\n+static bool is_subgraph_root_class_of(ArchivableStaticFieldInfo fields[], InstanceKlass* ik) {\n+  for (int i = 0; fields[i].valid(); i++) {\n@@ -158,22 +155,3 @@\n-  return is_subgraph_root_class_of(closed_archive_subgraph_entry_fields,\n-                                   num_closed_archive_subgraph_entry_fields, ik) ||\n-         is_subgraph_root_class_of(open_archive_subgraph_entry_fields,\n-                                   num_open_archive_subgraph_entry_fields, ik) ||\n-         is_subgraph_root_class_of(fmg_open_archive_subgraph_entry_fields,\n-                                   num_fmg_open_archive_subgraph_entry_fields, ik);\n-}\n-\n-void HeapShared::fixup_regions() {\n-  FileMapInfo* mapinfo = FileMapInfo::current_info();\n-  if (is_mapped()) {\n-    mapinfo->fixup_mapped_heap_regions();\n-  } else if (_loading_failed) {\n-    fill_failed_loaded_region();\n-  }\n-  if (is_fully_available()) {\n-    if (!MetaspaceShared::use_full_module_graph()) {\n-      \/\/ Need to remove all the archived java.lang.Module objects from HeapShared::roots().\n-      ClassLoaderDataShared::clear_archived_oops();\n-    }\n-  }\n-  SystemDictionaryShared::update_archived_mirror_native_pointers();\n+  return is_subgraph_root_class_of(closed_archive_subgraph_entry_fields, ik) ||\n+         is_subgraph_root_class_of(open_archive_subgraph_entry_fields, ik) ||\n+         is_subgraph_root_class_of(fmg_open_archive_subgraph_entry_fields, ik);\n@@ -297,1 +275,1 @@\n-  if (is_fully_available()) {\n+  if (ArchiveHeapLoader::is_fully_available()) {\n@@ -373,1 +351,1 @@\n-    Klass* k = ArchiveBuilder::get_relocated_klass(klasses->at(i));\n+    Klass* k = ArchiveBuilder::get_buffered_klass(klasses->at(i));\n@@ -410,1 +388,1 @@\n-  Klass* relocated_k = ArchiveBuilder::get_relocated_klass(k);\n+  Klass* buffered_k = ArchiveBuilder::get_buffered_klass(k);\n@@ -418,1 +396,1 @@\n-    relocated_k->set_has_archived_enum_objs();\n+    buffered_k->set_has_archived_enum_objs();\n@@ -449,1 +427,1 @@\n-  if (!is_fully_available()) {\n+  if (!ArchiveHeapLoader::is_fully_available()) {\n@@ -530,1 +508,0 @@\n-                           num_closed_archive_subgraph_entry_fields,\n@@ -548,1 +525,0 @@\n-                           num_open_archive_subgraph_entry_fields,\n@@ -553,1 +529,0 @@\n-                             num_fmg_open_archive_subgraph_entry_fields,\n@@ -598,5 +573,0 @@\n-void HeapShared::init_narrow_oop_decoding(address base, int shift) {\n-  _narrow_oop_base = base;\n-  _narrow_oop_shift = shift;\n-}\n-\n@@ -610,2 +580,2 @@\n-\/\/ there is no existing one for k. The subgraph_info records the relocated\n-\/\/ Klass* of the original k.\n+\/\/ there is no existing one for k. The subgraph_info records the \"buffered\"\n+\/\/ address of the class.\n@@ -615,1 +585,1 @@\n-  Klass* relocated_k = ArchiveBuilder::get_relocated_klass(k);\n+  Klass* buffered_k = ArchiveBuilder::get_buffered_klass(k);\n@@ -617,1 +587,1 @@\n-    _dump_time_subgraph_info_table->put_if_absent(k, KlassSubGraphInfo(relocated_k, is_full_module_graph),\n+    _dump_time_subgraph_info_table->put_if_absent(k, KlassSubGraphInfo(buffered_k, is_full_module_graph),\n@@ -646,1 +616,1 @@\n-  Klass* relocated_k = ArchiveBuilder::get_relocated_klass(orig_k);\n+  Klass* buffered_k = ArchiveBuilder::get_buffered_klass(orig_k);\n@@ -653,1 +623,1 @@\n-  assert(ArchiveBuilder::current()->is_in_buffer_space(relocated_k), \"must be a shared class\");\n+  assert(ArchiveBuilder::current()->is_in_buffer_space(buffered_k), \"must be a shared class\");\n@@ -655,1 +625,1 @@\n-  if (_k == relocated_k) {\n+  if (_k == buffered_k) {\n@@ -661,2 +631,2 @@\n-  if (relocated_k->is_instance_klass()) {\n-    assert(InstanceKlass::cast(relocated_k)->is_shared_boot_class(),\n+  if (buffered_k->is_instance_klass()) {\n+    assert(InstanceKlass::cast(buffered_k)->is_shared_boot_class(),\n@@ -672,2 +642,3 @@\n-  } else if (relocated_k->is_objArray_klass()) {\n-    Klass* abk = ObjArrayKlass::cast(relocated_k)->bottom_klass();\n+    check_allowed_klass(InstanceKlass::cast(orig_k));\n+  } else if (buffered_k->is_objArray_klass()) {\n+    Klass* abk = ObjArrayKlass::cast(buffered_k)->bottom_klass();\n@@ -677,0 +648,1 @@\n+      check_allowed_klass(InstanceKlass::cast(ObjArrayKlass::cast(orig_k)->bottom_klass()));\n@@ -678,1 +650,1 @@\n-    if (relocated_k == Universe::objectArrayKlassObj()) {\n+    if (buffered_k == Universe::objectArrayKlassObj()) {\n@@ -684,1 +656,1 @@\n-    assert(relocated_k->is_typeArray_klass(), \"must be\");\n+    assert(buffered_k->is_typeArray_klass(), \"must be\");\n@@ -690,1 +662,1 @@\n-    if (!_subgraph_object_klasses->contains(relocated_k)) {\n+    if (!_subgraph_object_klasses->contains(buffered_k)) {\n@@ -696,1 +668,1 @@\n-  _subgraph_object_klasses->append_if_missing(relocated_k);\n+  _subgraph_object_klasses->append_if_missing(buffered_k);\n@@ -700,0 +672,22 @@\n+void KlassSubGraphInfo::check_allowed_klass(InstanceKlass* ik) {\n+  if (ik->module()->name() == vmSymbols::java_base()) {\n+    assert(ik->package() != NULL, \"classes in java.base cannot be in unnamed package\");\n+    return;\n+  }\n+\n+#ifndef PRODUCT\n+  if (!ik->module()->is_named() && ik->package() == NULL) {\n+    \/\/ This class is loaded by ArchiveHeapTestClass\n+    return;\n+  }\n+  const char* extra_msg = \", or in an unnamed package of an unnamed module\";\n+#else\n+  const char* extra_msg = \"\";\n+#endif\n+\n+  ResourceMark rm;\n+  log_error(cds, heap)(\"Class %s not allowed in archive heap. Must be in java.base%s\",\n+                       ik->external_name(), extra_msg);\n+  os::_exit(1);\n+}\n+\n@@ -786,2 +780,2 @@\n-      Klass* relocated_k = ArchiveBuilder::get_relocated_klass(klass);\n-      unsigned int hash = SystemDictionaryShared::hash_for_shared_dictionary((address)relocated_k);\n+      Klass* buffered_k = ArchiveBuilder::get_buffered_klass(klass);\n+      unsigned int hash = SystemDictionaryShared::hash_for_shared_dictionary((address)buffered_k);\n@@ -813,0 +807,9 @@\n+\n+#ifndef PRODUCT\n+  if (ArchiveHeapTestClass != NULL) {\n+    size_t len = strlen(ArchiveHeapTestClass) + 1;\n+    Array<char>* array = ArchiveBuilder::new_ro_array<char>((int)len);\n+    strncpy(array->adr_at(0), ArchiveHeapTestClass, len);\n+    _archived_ArchiveHeapTestClass = array;\n+  }\n+#endif\n@@ -823,1 +826,1 @@\n-      assert(HeapShared::is_fully_available(), \"must be\");\n+      assert(ArchiveHeapLoader::is_fully_available(), \"must be\");\n@@ -832,0 +835,8 @@\n+#ifndef PRODUCT\n+  soc->do_ptr((void**)&_archived_ArchiveHeapTestClass);\n+  if (soc->reading() && _archived_ArchiveHeapTestClass != NULL) {\n+    _test_class_name = _archived_ArchiveHeapTestClass->adr_at(0);\n+    setup_test_class(_test_class_name);\n+  }\n+#endif\n+\n@@ -869,1 +880,2 @@\n-  if (!is_fully_available()) {\n+  assert(UseSharedSpaces, \"runtime only!\");\n+  if (!ArchiveHeapLoader::is_fully_available()) {\n@@ -872,9 +884,3 @@\n-  resolve_classes_for_subgraphs(closed_archive_subgraph_entry_fields,\n-                                num_closed_archive_subgraph_entry_fields,\n-                                THREAD);\n-  resolve_classes_for_subgraphs(open_archive_subgraph_entry_fields,\n-                                num_open_archive_subgraph_entry_fields,\n-                                THREAD);\n-  resolve_classes_for_subgraphs(fmg_open_archive_subgraph_entry_fields,\n-                                num_fmg_open_archive_subgraph_entry_fields,\n-                                THREAD);\n+  resolve_classes_for_subgraphs(closed_archive_subgraph_entry_fields,   THREAD);\n+  resolve_classes_for_subgraphs(open_archive_subgraph_entry_fields,     THREAD);\n+  resolve_classes_for_subgraphs(fmg_open_archive_subgraph_entry_fields, THREAD);\n@@ -884,2 +890,2 @@\n-                                               int num, JavaThread* THREAD) {\n-  for (int i = 0; i < num; i++) {\n+                                               JavaThread* THREAD) {\n+  for (int i = 0; fields[i].valid(); i++) {\n@@ -907,1 +913,1 @@\n-  if (!is_fully_available()) {\n+  if (!ArchiveHeapLoader::is_fully_available()) {\n@@ -938,0 +944,7 @@\n+#ifndef PRODUCT\n+  if (_test_class_name != NULL && k->name()->equals(_test_class_name) && record != NULL) {\n+    _test_class = k;\n+    _test_class_record = record;\n+  }\n+#endif\n+\n@@ -959,0 +972,5 @@\n+    if (log_is_enabled(Info, cds, heap)) {\n+      ResourceMark rm;\n+      log_info(cds, heap)(\"%s subgraph %s \", do_init ? \"init\" : \"resolve\", k->external_name());\n+    }\n+\n@@ -1460,4 +1478,5 @@\n-      assert(!_found, \"fields cannot be overloaded\");\n-      assert(is_reference_type(fd->field_type()), \"can archive only fields that are references\");\n-      _found = true;\n-      _offset = fd->offset();\n+      assert(!_found, \"fields can never be overloaded\");\n+      if (is_reference_type(fd->field_type())) {\n+        _found = true;\n+        _offset = fd->offset();\n+      }\n@@ -1471,2 +1490,2 @@\n-                                            int num, TRAPS) {\n-  for (int i = 0; i < num; i++) {\n+                                            TRAPS) {\n+  for (int i = 0; fields[i].valid(); i++) {\n@@ -1476,0 +1495,25 @@\n+    ResourceMark rm; \/\/ for stringStream::as_string() etc.\n+\n+#ifndef PRODUCT\n+    bool is_test_class = (ArchiveHeapTestClass != NULL) && (strcmp(info->klass_name, ArchiveHeapTestClass) == 0);\n+#else\n+    bool is_test_class = false;\n+#endif\n+\n+    if (is_test_class) {\n+      log_warning(cds)(\"Loading ArchiveHeapTestClass %s ...\", ArchiveHeapTestClass);\n+    }\n+\n+    Klass* k = SystemDictionary::resolve_or_fail(klass_name, true, THREAD);\n+    if (HAS_PENDING_EXCEPTION) {\n+      CLEAR_PENDING_EXCEPTION;\n+      stringStream st;\n+      st.print(\"Fail to initialize archive heap: %s cannot be loaded by the boot loader\", info->klass_name);\n+      THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(), st.as_string());\n+    }\n+\n+    if (!k->is_instance_klass()) {\n+      stringStream st;\n+      st.print(\"Fail to initialize archive heap: %s is not an instance class\", info->klass_name);\n+      THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(), st.as_string());\n+    }\n@@ -1477,1 +1521,0 @@\n-    Klass* k = SystemDictionary::resolve_or_fail(klass_name, true, CHECK);\n@@ -1481,0 +1524,30 @@\n+\n+    if (is_test_class) {\n+      if (ik->module()->is_named()) {\n+        \/\/ We don't want ArchiveHeapTestClass to be abused to easily load\/initialize arbitrary\n+        \/\/ core-lib classes. You need to at least append to the bootclasspath.\n+        stringStream st;\n+        st.print(\"ArchiveHeapTestClass %s is not in unnamed module\", ArchiveHeapTestClass);\n+        THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(), st.as_string());\n+      }\n+\n+      if (ik->package() != NULL) {\n+        \/\/ This restriction makes HeapShared::is_a_test_class_in_unnamed_module() easy.\n+        stringStream st;\n+        st.print(\"ArchiveHeapTestClass %s is not in unnamed package\", ArchiveHeapTestClass);\n+        THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(), st.as_string());\n+      }\n+    } else {\n+      if (ik->module()->name() != vmSymbols::java_base()) {\n+        \/\/ We don't want to deal with cases when a module is unavailable at runtime.\n+        \/\/ FUTURE -- load from archived heap only when module graph has not changed\n+        \/\/           between dump and runtime.\n+        stringStream st;\n+        st.print(\"%s is not in java.base module\", info->klass_name);\n+        THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(), st.as_string());\n+      }\n+    }\n+\n+    if (is_test_class) {\n+      log_warning(cds)(\"Initializing ArchiveHeapTestClass %s ...\", ArchiveHeapTestClass);\n+    }\n@@ -1485,1 +1558,5 @@\n-    assert(finder.found(), \"field must exist\");\n+    if (!finder.found()) {\n+      stringStream st;\n+      st.print(\"Unable to find the static T_OBJECT field %s::%s\", info->klass_name, info->field_name);\n+      THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(), st.as_string());\n+    }\n@@ -1495,6 +1572,2 @@\n-  init_subgraph_entry_fields(closed_archive_subgraph_entry_fields,\n-                             num_closed_archive_subgraph_entry_fields,\n-                             CHECK);\n-  init_subgraph_entry_fields(open_archive_subgraph_entry_fields,\n-                             num_open_archive_subgraph_entry_fields,\n-                             CHECK);\n+  init_subgraph_entry_fields(closed_archive_subgraph_entry_fields, CHECK);\n+  init_subgraph_entry_fields(open_archive_subgraph_entry_fields, CHECK);\n@@ -1502,3 +1575,14 @@\n-    init_subgraph_entry_fields(fmg_open_archive_subgraph_entry_fields,\n-                               num_fmg_open_archive_subgraph_entry_fields,\n-                               CHECK);\n+    init_subgraph_entry_fields(fmg_open_archive_subgraph_entry_fields, CHECK);\n+  }\n+}\n+\n+#ifndef PRODUCT\n+void HeapShared::setup_test_class(const char* test_class_name) {\n+  ArchivableStaticFieldInfo* p = open_archive_subgraph_entry_fields;\n+  int num_slots = sizeof(open_archive_subgraph_entry_fields) \/ sizeof(ArchivableStaticFieldInfo);\n+  assert(p[num_slots - 2].klass_name == NULL, \"must have empty slot that's patched below\");\n+  assert(p[num_slots - 1].klass_name == NULL, \"must have empty slot that marks the end of the list\");\n+\n+  if (test_class_name != NULL) {\n+    p[num_slots - 2].klass_name = test_class_name;\n+    p[num_slots - 2].field_name = ARCHIVE_TEST_FIELD_NAME;\n@@ -1508,0 +1592,48 @@\n+\/\/ See if ik is one of the test classes that are pulled in by -XX:ArchiveHeapTestClass\n+\/\/ during runtime. This may be called before the module system is initialized so\n+\/\/ we cannot rely on InstanceKlass::module(), etc.\n+bool HeapShared::is_a_test_class_in_unnamed_module(Klass* ik) {\n+  if (_test_class != NULL) {\n+    if (ik == _test_class) {\n+      return true;\n+    }\n+    Array<Klass*>* klasses = _test_class_record->subgraph_object_klasses();\n+    if (klasses == NULL) {\n+      return false;\n+    }\n+\n+    for (int i = 0; i < klasses->length(); i++) {\n+      Klass* k = klasses->at(i);\n+      if (k == ik) {\n+        Symbol* name;\n+        if (k->is_instance_klass()) {\n+          name = InstanceKlass::cast(k)->name();\n+        } else if (k->is_objArray_klass()) {\n+          Klass* bk = ObjArrayKlass::cast(k)->bottom_klass();\n+          if (!bk->is_instance_klass()) {\n+            return false;\n+          }\n+          name = bk->name();\n+        } else {\n+          return false;\n+        }\n+\n+        \/\/ See KlassSubGraphInfo::check_allowed_klass() - only two types of\n+        \/\/ classes are allowed:\n+        \/\/   (A) java.base classes (which must not be in the unnamed module)\n+        \/\/   (B) test classes which must be in the unnamed package of the unnamed module.\n+        \/\/ So if we see a '\/' character in the class name, it must be in (A);\n+        \/\/ otherwise it must be in (B).\n+        if (name->index_of_at(0, \"\/\", 1)  >= 0) {\n+          return false; \/\/ (A)\n+        }\n+\n+        return true; \/\/ (B)\n+      }\n+    }\n+  }\n+\n+  return false;\n+}\n+#endif\n+\n@@ -1510,0 +1642,1 @@\n+    setup_test_class(ArchiveHeapTestClass);\n@@ -1516,1 +1649,1 @@\n-                                          int num, bool is_closed_archive,\n+                                          bool is_closed_archive,\n@@ -1531,1 +1664,1 @@\n-  for (i = 0; i < num; ) {\n+  for (int i = 0; fields[i].valid(); ) {\n@@ -1540,1 +1673,1 @@\n-    for (; i < num; i++) {\n+    for (; fields[i].valid(); i++) {\n@@ -1561,1 +1694,1 @@\n-  for (int i = 0; i < num; i++) {\n+  for (int i = 0; fields[i].valid(); i++) {\n@@ -1666,1 +1799,1 @@\n-      builder->relocate_klass_ptr(o);\n+      builder->relocate_klass_ptr_of_oop(o);\n@@ -1676,347 +1809,0 @@\n-\/\/ Patch all the embedded oop pointers inside an archived heap region,\n-\/\/ to be consistent with the runtime oop encoding.\n-class PatchCompressedEmbeddedPointers: public BitMapClosure {\n-  narrowOop* _start;\n-\n- public:\n-  PatchCompressedEmbeddedPointers(narrowOop* start) : _start(start) {}\n-\n-  bool do_bit(size_t offset) {\n-    narrowOop* p = _start + offset;\n-    narrowOop v = *p;\n-    assert(!CompressedOops::is_null(v), \"null oops should have been filtered out at dump time\");\n-    oop o = HeapShared::decode_from_archive(v);\n-    RawAccess<IS_NOT_NULL>::oop_store(p, o);\n-    return true;\n-  }\n-};\n-\n-class PatchUncompressedEmbeddedPointers: public BitMapClosure {\n-  oop* _start;\n-\n- public:\n-  PatchUncompressedEmbeddedPointers(oop* start) : _start(start) {}\n-\n-  bool do_bit(size_t offset) {\n-    oop* p = _start + offset;\n-    intptr_t dumptime_oop = (intptr_t)((void*)*p);\n-    assert(dumptime_oop != 0, \"null oops should have been filtered out at dump time\");\n-    intptr_t runtime_oop = dumptime_oop + HeapShared::runtime_delta();\n-    RawAccess<IS_NOT_NULL>::oop_store(p, cast_to_oop(runtime_oop));\n-    return true;\n-  }\n-};\n-\n-\/\/ Patch all the non-null pointers that are embedded in the archived heap objects\n-\/\/ in this region\n-void HeapShared::patch_embedded_pointers(MemRegion region, address oopmap,\n-                                         size_t oopmap_size_in_bits) {\n-  BitMapView bm((BitMap::bm_word_t*)oopmap, oopmap_size_in_bits);\n-\n-#ifndef PRODUCT\n-  ResourceMark rm;\n-  ResourceBitMap checkBm = calculate_oopmap(region);\n-  assert(bm.is_same(checkBm), \"sanity\");\n-#endif\n-\n-  if (UseCompressedOops) {\n-    PatchCompressedEmbeddedPointers patcher((narrowOop*)region.start());\n-    bm.iterate(&patcher);\n-  } else {\n-    PatchUncompressedEmbeddedPointers patcher((oop*)region.start());\n-    bm.iterate(&patcher);\n-  }\n-}\n-\n-\/\/ The CDS archive remembers each heap object by its address at dump time, but\n-\/\/ the heap object may be loaded at a different address at run time. This structure is used\n-\/\/ to translate the dump time addresses for all objects in FileMapInfo::space_at(region_index)\n-\/\/ to their runtime addresses.\n-struct LoadedArchiveHeapRegion {\n-  int       _region_index;   \/\/ index for FileMapInfo::space_at(index)\n-  size_t    _region_size;    \/\/ number of bytes in this region\n-  uintptr_t _dumptime_base;  \/\/ The dump-time (decoded) address of the first object in this region\n-  intx      _runtime_offset; \/\/ If an object's dump time address P is within in this region, its\n-                             \/\/ runtime address is P + _runtime_offset\n-\n-  static int comparator(const void* a, const void* b) {\n-    LoadedArchiveHeapRegion* reg_a = (LoadedArchiveHeapRegion*)a;\n-    LoadedArchiveHeapRegion* reg_b = (LoadedArchiveHeapRegion*)b;\n-    if (reg_a->_dumptime_base < reg_b->_dumptime_base) {\n-      return -1;\n-    } else if (reg_a->_dumptime_base == reg_b->_dumptime_base) {\n-      return 0;\n-    } else {\n-      return 1;\n-    }\n-  }\n-\n-  uintptr_t top() {\n-    return _dumptime_base + _region_size;\n-  }\n-};\n-\n-void HeapShared::init_loaded_heap_relocation(LoadedArchiveHeapRegion* loaded_regions,\n-                                             int num_loaded_regions) {\n-  _dumptime_base_0 = loaded_regions[0]._dumptime_base;\n-  _dumptime_base_1 = loaded_regions[1]._dumptime_base;\n-  _dumptime_base_2 = loaded_regions[2]._dumptime_base;\n-  _dumptime_base_3 = loaded_regions[3]._dumptime_base;\n-  _dumptime_top = loaded_regions[num_loaded_regions-1].top();\n-\n-  _runtime_offset_0 = loaded_regions[0]._runtime_offset;\n-  _runtime_offset_1 = loaded_regions[1]._runtime_offset;\n-  _runtime_offset_2 = loaded_regions[2]._runtime_offset;\n-  _runtime_offset_3 = loaded_regions[3]._runtime_offset;\n-\n-  assert(2 <= num_loaded_regions && num_loaded_regions <= 4, \"must be\");\n-  if (num_loaded_regions < 4) {\n-    _dumptime_base_3 = UINTPTR_MAX;\n-  }\n-  if (num_loaded_regions < 3) {\n-    _dumptime_base_2 = UINTPTR_MAX;\n-  }\n-}\n-\n-bool HeapShared::can_load() {\n-  return Universe::heap()->can_load_archived_objects();\n-}\n-\n-template <int NUM_LOADED_REGIONS>\n-class PatchLoadedRegionPointers: public BitMapClosure {\n-  narrowOop* _start;\n-  intx _offset_0;\n-  intx _offset_1;\n-  intx _offset_2;\n-  intx _offset_3;\n-  uintptr_t _base_0;\n-  uintptr_t _base_1;\n-  uintptr_t _base_2;\n-  uintptr_t _base_3;\n-  uintptr_t _top;\n-\n-  static_assert(MetaspaceShared::max_num_heap_regions == 4, \"can't handle more than 4 regions\");\n-  static_assert(NUM_LOADED_REGIONS >= 2, \"we have at least 2 loaded regions\");\n-  static_assert(NUM_LOADED_REGIONS <= 4, \"we have at most 4 loaded regions\");\n-\n- public:\n-  PatchLoadedRegionPointers(narrowOop* start, LoadedArchiveHeapRegion* loaded_regions)\n-    : _start(start),\n-      _offset_0(loaded_regions[0]._runtime_offset),\n-      _offset_1(loaded_regions[1]._runtime_offset),\n-      _offset_2(loaded_regions[2]._runtime_offset),\n-      _offset_3(loaded_regions[3]._runtime_offset),\n-      _base_0(loaded_regions[0]._dumptime_base),\n-      _base_1(loaded_regions[1]._dumptime_base),\n-      _base_2(loaded_regions[2]._dumptime_base),\n-      _base_3(loaded_regions[3]._dumptime_base) {\n-    _top = loaded_regions[NUM_LOADED_REGIONS-1].top();\n-  }\n-\n-  bool do_bit(size_t offset) {\n-    narrowOop* p = _start + offset;\n-    narrowOop v = *p;\n-    assert(!CompressedOops::is_null(v), \"null oops should have been filtered out at dump time\");\n-    uintptr_t o = cast_from_oop<uintptr_t>(HeapShared::decode_from_archive(v));\n-    assert(_base_0 <= o && o < _top, \"must be\");\n-\n-\n-    \/\/ We usually have only 2 regions for the default archive. Use template to avoid unnecessary comparisons.\n-    if (NUM_LOADED_REGIONS > 3 && o >= _base_3) {\n-      o += _offset_3;\n-    } else if (NUM_LOADED_REGIONS > 2 && o >= _base_2) {\n-      o += _offset_2;\n-    } else if (o >= _base_1) {\n-      o += _offset_1;\n-    } else {\n-      o += _offset_0;\n-    }\n-    HeapShared::assert_in_loaded_heap(o);\n-    RawAccess<IS_NOT_NULL>::oop_store(p, cast_to_oop(o));\n-    return true;\n-  }\n-};\n-\n-int HeapShared::init_loaded_regions(FileMapInfo* mapinfo, LoadedArchiveHeapRegion* loaded_regions,\n-                                    MemRegion& archive_space) {\n-  size_t total_bytes = 0;\n-  int num_loaded_regions = 0;\n-  for (int i = MetaspaceShared::first_archive_heap_region;\n-       i <= MetaspaceShared::last_archive_heap_region; i++) {\n-    FileMapRegion* r = mapinfo->space_at(i);\n-    r->assert_is_heap_region();\n-    if (r->used() > 0) {\n-      assert(is_aligned(r->used(), HeapWordSize), \"must be\");\n-      total_bytes += r->used();\n-      LoadedArchiveHeapRegion* ri = &loaded_regions[num_loaded_regions++];\n-      ri->_region_index = i;\n-      ri->_region_size = r->used();\n-      ri->_dumptime_base = (uintptr_t)mapinfo->start_address_as_decoded_from_archive(r);\n-    }\n-  }\n-\n-  assert(is_aligned(total_bytes, HeapWordSize), \"must be\");\n-  size_t word_size = total_bytes \/ HeapWordSize;\n-  HeapWord* buffer = Universe::heap()->allocate_loaded_archive_space(word_size);\n-  if (buffer == nullptr) {\n-    return 0;\n-  }\n-\n-  archive_space = MemRegion(buffer, word_size);\n-  _loaded_heap_bottom = (uintptr_t)archive_space.start();\n-  _loaded_heap_top    = _loaded_heap_bottom + total_bytes;\n-\n-  return num_loaded_regions;\n-}\n-\n-void HeapShared::sort_loaded_regions(LoadedArchiveHeapRegion* loaded_regions, int num_loaded_regions,\n-                                     uintptr_t buffer) {\n-  \/\/ Find the relocation offset of the pointers in each region\n-  qsort(loaded_regions, num_loaded_regions, sizeof(LoadedArchiveHeapRegion),\n-        LoadedArchiveHeapRegion::comparator);\n-\n-  uintptr_t p = buffer;\n-  for (int i = 0; i < num_loaded_regions; i++) {\n-    \/\/ This region will be loaded at p, so all objects inside this\n-    \/\/ region will be shifted by ri->offset\n-    LoadedArchiveHeapRegion* ri = &loaded_regions[i];\n-    ri->_runtime_offset = p - ri->_dumptime_base;\n-    p += ri->_region_size;\n-  }\n-  assert(p == _loaded_heap_top, \"must be\");\n-}\n-\n-bool HeapShared::load_regions(FileMapInfo* mapinfo, LoadedArchiveHeapRegion* loaded_regions,\n-                              int num_loaded_regions, uintptr_t buffer) {\n-  uintptr_t bitmap_base = (uintptr_t)mapinfo->map_bitmap_region();\n-  if (bitmap_base == 0) {\n-    _loading_failed = true;\n-    return false; \/\/ OOM or CRC error\n-  }\n-  uintptr_t load_address = buffer;\n-  for (int i = 0; i < num_loaded_regions; i++) {\n-    LoadedArchiveHeapRegion* ri = &loaded_regions[i];\n-    FileMapRegion* r = mapinfo->space_at(ri->_region_index);\n-\n-    if (!mapinfo->read_region(ri->_region_index, (char*)load_address, r->used(), \/* do_commit = *\/ false)) {\n-      \/\/ There's no easy way to free the buffer, so we will fill it with zero later\n-      \/\/ in fill_failed_loaded_region(), and it will eventually be GC'ed.\n-      log_warning(cds)(\"Loading of heap region %d has failed. Archived objects are disabled\", i);\n-      _loading_failed = true;\n-      return false;\n-    }\n-    log_info(cds)(\"Loaded heap    region #%d at base \" INTPTR_FORMAT \" top \" INTPTR_FORMAT\n-                  \" size \" SIZE_FORMAT_W(6) \" delta \" INTX_FORMAT,\n-                  ri->_region_index, load_address, load_address + ri->_region_size,\n-                  ri->_region_size, ri->_runtime_offset);\n-\n-    uintptr_t oopmap = bitmap_base + r->oopmap_offset();\n-    BitMapView bm((BitMap::bm_word_t*)oopmap, r->oopmap_size_in_bits());\n-\n-    if (num_loaded_regions == 4) {\n-      PatchLoadedRegionPointers<4> patcher((narrowOop*)load_address, loaded_regions);\n-      bm.iterate(&patcher);\n-    } else if (num_loaded_regions == 3) {\n-      PatchLoadedRegionPointers<3> patcher((narrowOop*)load_address, loaded_regions);\n-      bm.iterate(&patcher);\n-    } else {\n-      assert(num_loaded_regions == 2, \"must be\");\n-      PatchLoadedRegionPointers<2> patcher((narrowOop*)load_address, loaded_regions);\n-      bm.iterate(&patcher);\n-    }\n-\n-    load_address += r->used();\n-  }\n-\n-  return true;\n-}\n-\n-bool HeapShared::load_heap_regions(FileMapInfo* mapinfo) {\n-  init_narrow_oop_decoding(mapinfo->narrow_oop_base(), mapinfo->narrow_oop_shift());\n-\n-  LoadedArchiveHeapRegion loaded_regions[MetaspaceShared::max_num_heap_regions];\n-  memset(loaded_regions, 0, sizeof(loaded_regions));\n-\n-  MemRegion archive_space;\n-  int num_loaded_regions = init_loaded_regions(mapinfo, loaded_regions, archive_space);\n-  if (num_loaded_regions <= 0) {\n-    return false;\n-  }\n-  sort_loaded_regions(loaded_regions, num_loaded_regions, (uintptr_t)archive_space.start());\n-  if (!load_regions(mapinfo, loaded_regions, num_loaded_regions, (uintptr_t)archive_space.start())) {\n-    assert(_loading_failed, \"must be\");\n-    return false;\n-  }\n-\n-  init_loaded_heap_relocation(loaded_regions, num_loaded_regions);\n-  _is_loaded = true;\n-\n-  return true;\n-}\n-\n-class VerifyLoadedHeapEmbeddedPointers: public BasicOopIterateClosure {\n-  ResourceHashtable<uintptr_t, bool>* _table;\n-\n- public:\n-  VerifyLoadedHeapEmbeddedPointers(ResourceHashtable<uintptr_t, bool>* table) : _table(table) {}\n-\n-  virtual void do_oop(narrowOop* p) {\n-    \/\/ This should be called before the loaded regions are modified, so all the embedded pointers\n-    \/\/ must be NULL, or must point to a valid object in the loaded regions.\n-    narrowOop v = *p;\n-    if (!CompressedOops::is_null(v)) {\n-      oop o = CompressedOops::decode_not_null(v);\n-      uintptr_t u = cast_from_oop<uintptr_t>(o);\n-      HeapShared::assert_in_loaded_heap(u);\n-      guarantee(_table->contains(u), \"must point to beginning of object in loaded archived regions\");\n-    }\n-  }\n-  virtual void do_oop(oop* p) {\n-    ShouldNotReachHere();\n-  }\n-};\n-\n-void HeapShared::finish_initialization() {\n-  if (is_loaded()) {\n-    HeapWord* bottom = (HeapWord*)_loaded_heap_bottom;\n-    HeapWord* top    = (HeapWord*)_loaded_heap_top;\n-\n-    MemRegion archive_space = MemRegion(bottom, top);\n-    Universe::heap()->complete_loaded_archive_space(archive_space);\n-  }\n-\n-  if (VerifyArchivedFields <= 0 || !is_loaded()) {\n-    return;\n-  }\n-\n-  log_info(cds, heap)(\"Verify all oops and pointers in loaded heap\");\n-\n-  ResourceMark rm;\n-  ResourceHashtable<uintptr_t, bool> table;\n-  VerifyLoadedHeapEmbeddedPointers verifier(&table);\n-  HeapWord* bottom = (HeapWord*)_loaded_heap_bottom;\n-  HeapWord* top    = (HeapWord*)_loaded_heap_top;\n-\n-  for (HeapWord* p = bottom; p < top; ) {\n-    oop o = cast_to_oop(p);\n-    table.put(cast_from_oop<uintptr_t>(o), true);\n-    p += o->size();\n-  }\n-\n-  for (HeapWord* p = bottom; p < top; ) {\n-    oop o = cast_to_oop(p);\n-    o->oop_iterate(&verifier);\n-    p += o->size();\n-  }\n-}\n-\n-void HeapShared::fill_failed_loaded_region() {\n-  assert(_loading_failed, \"must be\");\n-  if (_loaded_heap_bottom != 0) {\n-    assert(_loaded_heap_top != 0, \"must be\");\n-    HeapWord* bottom = (HeapWord*)_loaded_heap_bottom;\n-    HeapWord* top = (HeapWord*)_loaded_heap_top;\n-    Universe::heap()->fill_with_objects(bottom, top - bottom);\n-  }\n-}\n-\n","filename":"src\/hotspot\/share\/cds\/heapShared.cpp","additions":257,"deletions":471,"binary":false,"changes":728,"status":"modified"},{"patch":"@@ -26,1 +26,2 @@\n-#include \"cds\/archiveUtils.hpp\"\n+#include \"cds\/archiveHeapLoader.hpp\"\n+#include \"cds\/archiveUtils.hpp\"\n@@ -32,1 +33,0 @@\n-#include \"cds\/heapShared.hpp\"\n@@ -64,0 +64,1 @@\n+#include \"oops\/objArrayKlass.hpp\"\n@@ -73,1 +74,0 @@\n-#include \"utilities\/hashtable.inline.hpp\"\n@@ -85,2 +85,2 @@\n-DEBUG_ONLY(bool SystemDictionaryShared::_no_class_loading_should_happen = false;)\n-bool SystemDictionaryShared::_dump_in_progress = false;\n+\/\/ Used by NoClassLoadingMark\n+DEBUG_ONLY(bool SystemDictionaryShared::_class_loading_may_happen = true;)\n@@ -182,1 +182,3 @@\n-void SystemDictionaryShared::start_dumping() {\n+\/\/ Guaranteed to return non-NULL value for non-shared classes.\n+\/\/ k must not be a shared class.\n+DumpTimeClassInfo* SystemDictionaryShared::get_info(InstanceKlass* k) {\n@@ -184,1 +186,2 @@\n-  _dump_in_progress = true;\n+  assert(!k->is_shared(), \"sanity\");\n+  return get_info_locked(k);\n@@ -187,1 +190,1 @@\n-void SystemDictionaryShared::stop_dumping() {\n+DumpTimeClassInfo* SystemDictionaryShared::get_info_locked(InstanceKlass* k) {\n@@ -189,14 +192,4 @@\n-  _dump_in_progress = false;\n-}\n-\n-DumpTimeClassInfo* SystemDictionaryShared::find_or_allocate_info_for(InstanceKlass* k) {\n-  MutexLocker ml(DumpTimeTable_lock, Mutex::_no_safepoint_check_flag);\n-  return find_or_allocate_info_for_locked(k);\n-}\n-\n-DumpTimeClassInfo* SystemDictionaryShared::find_or_allocate_info_for_locked(InstanceKlass* k) {\n-  assert_lock_strong(DumpTimeTable_lock);\n-  if (_dumptime_table == NULL) {\n-    _dumptime_table = new (ResourceObj::C_HEAP, mtClass) DumpTimeSharedClassTable;\n-  }\n-  return _dumptime_table->find_or_allocate_info_for(k, _dump_in_progress);\n+  assert(!k->is_shared(), \"sanity\");\n+  DumpTimeClassInfo* info = _dumptime_table->get_info(k);\n+  assert(info != NULL, \"must be\");\n+  return info;\n@@ -413,1 +406,0 @@\n-      unsigned int d_hash = dictionary->compute_hash(name);\n@@ -421,1 +413,1 @@\n-        InstanceKlass* check = dictionary->find_class(d_hash, name);\n+        InstanceKlass* check = dictionary->find_class(THREAD, name);\n@@ -505,4 +497,10 @@\n-  DumpTimeClassInfo* info = find_or_allocate_info_for(k);\n-  if (info != NULL) {\n-    info->_clsfile_size  = cfs->length();\n-    info->_clsfile_crc32 = ClassLoader::crc32(0, (const char*)cfs->buffer(), cfs->length());\n+  DumpTimeClassInfo* info = get_info(k);\n+  info->_clsfile_size  = cfs->length();\n+  info->_clsfile_crc32 = ClassLoader::crc32(0, (const char*)cfs->buffer(), cfs->length());\n+}\n+\n+void SystemDictionaryShared::initialize() {\n+  if (Arguments::is_dumping_archive()) {\n+    _dumptime_table = new (ResourceObj::C_HEAP, mtClass) DumpTimeSharedClassTable;\n+    _dumptime_lambda_proxy_class_dictionary =\n+                      new (ResourceObj::C_HEAP, mtClass) DumpTimeLambdaProxyClassDictionary;\n@@ -513,1 +511,3 @@\n-  (void)find_or_allocate_info_for(k);\n+  MutexLocker ml(DumpTimeTable_lock, Mutex::_no_safepoint_check_flag);\n+  assert(SystemDictionaryShared::class_loading_may_happen(), \"sanity\");\n+  _dumptime_table->allocate_info(k);\n@@ -518,29 +518,0 @@\n-  DumpTimeClassInfo* p = _dumptime_table->get(k);\n-  if (p == NULL) {\n-    return;\n-  }\n-  if (p->_verifier_constraints != NULL) {\n-    for (int i = 0; i < p->_verifier_constraints->length(); i++) {\n-      DumpTimeClassInfo::DTVerifierConstraint constraint = p->_verifier_constraints->at(i);\n-      if (constraint._name != NULL ) {\n-        constraint._name->decrement_refcount();\n-      }\n-      if (constraint._from_name != NULL ) {\n-        constraint._from_name->decrement_refcount();\n-      }\n-    }\n-    FREE_C_HEAP_ARRAY(DumpTimeClassInfo::DTVerifierConstraint, p->_verifier_constraints);\n-    p->_verifier_constraints = NULL;\n-    FREE_C_HEAP_ARRAY(char, p->_verifier_constraint_flags);\n-    p->_verifier_constraint_flags = NULL;\n-  }\n-  if (p->_loader_constraints != NULL) {\n-    for (int i = 0; i < p->_loader_constraints->length(); i++) {\n-      DumpTimeClassInfo::DTLoaderConstraint ld =  p->_loader_constraints->at(i);\n-      if (ld._name != NULL) {\n-        ld._name->decrement_refcount();\n-      }\n-    }\n-    FREE_C_HEAP_ARRAY(DumpTimeClassInfo::DTLoaderConstraint, p->_loader_constraints);\n-    p->_loader_constraints = NULL;\n-  }\n@@ -595,1 +566,1 @@\n-  assert(_no_class_loading_should_happen, \"class loading must be disabled\");\n+  assert(!class_loading_may_happen(), \"class loading must be disabled\");\n@@ -653,1 +624,1 @@\n-  assert(no_class_loading_should_happen(), \"sanity\");\n+  assert(!class_loading_may_happen(), \"class loading must be disabled\");\n@@ -675,1 +646,1 @@\n-  assert(_no_class_loading_should_happen, \"sanity\");\n+  assert(!class_loading_may_happen(), \"class loading must be disabled\");\n@@ -678,2 +649,2 @@\n-  DumpTimeClassInfo* p = find_or_allocate_info_for_locked(k);\n-  return (p == NULL) ? true : p->is_excluded();\n+  DumpTimeClassInfo* p = get_info_locked(k);\n+  return p->is_excluded();\n@@ -685,4 +656,2 @@\n-  DumpTimeClassInfo* info = find_or_allocate_info_for_locked(k);\n-  if (info != NULL) {\n-    info->set_excluded();\n-  }\n+  DumpTimeClassInfo* info = get_info_locked(k);\n+  info->set_excluded();\n@@ -693,4 +662,2 @@\n-  DumpTimeClassInfo* info = find_or_allocate_info_for(k);\n-  if (info != NULL) {\n-    info->set_excluded();\n-  }\n+  DumpTimeClassInfo* info = get_info(k);\n+  info->set_excluded();\n@@ -701,4 +668,2 @@\n-  DumpTimeClassInfo* p = find_or_allocate_info_for(ik);\n-  if (p != NULL) {\n-    p->set_failed_verification();\n-  }\n+  DumpTimeClassInfo* p = get_info(ik);\n+  p->set_failed_verification();\n@@ -709,5 +674,0 @@\n-  if (_dumptime_table == NULL) {\n-    assert(DynamicDumpSharedSpaces, \"sanity\");\n-    assert(ik->is_shared(), \"must be a shared class in the static archive\");\n-    return false;\n-  }\n@@ -728,9 +688,7 @@\n-  if (_dumptime_lambda_proxy_class_dictionary != NULL) {\n-    auto do_lambda = [&] (LambdaProxyClassKey& key, DumpTimeLambdaProxyClassInfo& info) {\n-      if (key.caller_ik()->is_loader_alive()) {\n-        info.metaspace_pointers_do(it);\n-        key.metaspace_pointers_do(it);\n-      }\n-    };\n-    _dumptime_lambda_proxy_class_dictionary->iterate_all(do_lambda);\n-  }\n+  auto do_lambda = [&] (LambdaProxyClassKey& key, DumpTimeLambdaProxyClassInfo& info) {\n+    if (key.caller_ik()->is_loader_alive()) {\n+      info.metaspace_pointers_do(it);\n+      key.metaspace_pointers_do(it);\n+    }\n+  };\n+  _dumptime_lambda_proxy_class_dictionary->iterate_all(do_lambda);\n@@ -742,7 +700,4 @@\n-  DumpTimeClassInfo* info = find_or_allocate_info_for(k);\n-  if (info != NULL) {\n-    info->add_verification_constraint(k, name, from_name, from_field_is_protected,\n-                                      from_is_array, from_is_object);\n-  } else {\n-    return true;\n-  }\n+  DumpTimeClassInfo* info = get_info(k);\n+  info->add_verification_constraint(k, name, from_name, from_field_is_protected,\n+                                    from_is_array, from_is_object);\n+\n@@ -770,2 +725,1 @@\n-  DumpTimeClassInfo* info = SystemDictionaryShared::find_or_allocate_info_for_locked(ik);\n-  assert(info != NULL, \"must be\");\n+  DumpTimeClassInfo* info = get_info_locked(ik);\n@@ -778,11 +732,5 @@\n-  if (_dumptime_lambda_proxy_class_dictionary == NULL) {\n-    _dumptime_lambda_proxy_class_dictionary =\n-      new (ResourceObj::C_HEAP, mtClass) DumpTimeLambdaProxyClassDictionary;\n-  }\n-  DumpTimeLambdaProxyClassInfo* lambda_info = _dumptime_lambda_proxy_class_dictionary->get(key);\n-  if (lambda_info == NULL) {\n-    DumpTimeLambdaProxyClassInfo info;\n-    info.add_proxy_klass(proxy_klass);\n-    _dumptime_lambda_proxy_class_dictionary->put(key, info);\n-    \/\/lambda_info = _dumptime_lambda_proxy_class_dictionary->get(key);\n-    \/\/assert(lambda_info->_proxy_klass == proxy_klass, \"must be\"); \/\/ debug only -- remove\n+\n+  bool created;\n+  DumpTimeLambdaProxyClassInfo* info = _dumptime_lambda_proxy_class_dictionary->put_if_absent(key, &created);\n+  info->add_proxy_klass(proxy_klass);\n+  if (created) {\n@@ -790,2 +738,0 @@\n-  } else {\n-    lambda_info->add_proxy_klass(proxy_klass);\n@@ -1034,4 +980,2 @@\n-  DumpTimeClassInfo* info = find_or_allocate_info_for(klass);\n-  if (info != NULL) {\n-    info->record_linking_constraint(name, loader1, loader2);\n-  }\n+  DumpTimeClassInfo* info = get_info(klass);\n+  info->record_linking_constraint(name, loader1, loader2);\n@@ -1164,3 +1108,3 @@\n-  if (_dumptime_lambda_proxy_class_dictionary != NULL) {\n-    size_t bytesize = align_up(sizeof(RunTimeLambdaProxyClassInfo), SharedSpaceObjectAlignment);\n-    total_size +=\n+\n+  size_t bytesize = align_up(sizeof(RunTimeLambdaProxyClassInfo), SharedSpaceObjectAlignment);\n+  total_size +=\n@@ -1169,3 +1113,1 @@\n-  } else {\n-    total_size += CompactHashtableWriter::estimate_size(0);\n-  }\n+\n@@ -1304,7 +1246,4 @@\n-  if (_dumptime_table != NULL) {\n-    write_dictionary(&archive->_builtin_dictionary, true);\n-    write_dictionary(&archive->_unregistered_dictionary, false);\n-  }\n-  if (_dumptime_lambda_proxy_class_dictionary != NULL) {\n-    write_lambda_proxy_class_dictionary(&archive->_lambda_proxy_class_dictionary);\n-  }\n+  write_dictionary(&archive->_builtin_dictionary, true);\n+  write_dictionary(&archive->_unregistered_dictionary, false);\n+\n+  write_lambda_proxy_class_dictionary(&archive->_lambda_proxy_class_dictionary);\n@@ -1314,4 +1253,2 @@\n-  if (_dumptime_lambda_proxy_class_dictionary != NULL) {\n-    AdjustLambdaProxyClassInfo adjuster;\n-    _dumptime_lambda_proxy_class_dictionary->iterate(&adjuster);\n-  }\n+  AdjustLambdaProxyClassInfo adjuster;\n+  _dumptime_lambda_proxy_class_dictionary->iterate(&adjuster);\n@@ -1395,1 +1332,1 @@\n-  DumpTimeClassInfo* info = find_or_allocate_info_for(k);\n+  DumpTimeClassInfo* info = get_info(k);\n@@ -1501,3 +1438,0 @@\n-  if (_dumptime_table == NULL) {\n-    return true;\n-  }\n@@ -1523,1 +1457,2 @@\n-      _cloned_table->put_if_absent(k, info.clone(), &created);\n+      _cloned_table->put_if_absent(k, info, &created);\n+      assert(created, \"must be\");\n@@ -1544,1 +1479,2 @@\n-    _cloned_table->put_if_absent(keyCopy, info.clone(), &created);\n+    _cloned_table->put_if_absent(keyCopy, info, &created);\n+    assert(created, \"must be\");\n@@ -1550,0 +1486,12 @@\n+\/\/ When dumping the CDS archive, the ArchiveBuilder will irrecoverably modify the\n+\/\/ _dumptime_table and _dumptime_lambda_proxy_class_dictionary (e.g., metaspace\n+\/\/ pointers are changed to use \"buffer\" addresses.)\n+\/\/\n+\/\/ We save a copy of these tables and restore them after the dumping is finished.\n+\/\/ This makes it possible to repeat the dumping operation (e.g., use\n+\/\/ \"jcmd VM.cds dynamic_dump\" multiple times on the same JVM process).\n+\/\/\n+\/\/ We use the copy constructors to clone the values in these tables. The copy constructors\n+\/\/ must make a deep copy, as internal data structures such as the contents of\n+\/\/ DumpTimeClassInfo::_loader_constraints are also modified by the ArchiveBuilder.\n+\n@@ -1553,16 +1501,14 @@\n-  if (_dumptime_table != NULL) {\n-    assert(_cloned_dumptime_table == NULL, \"_cloned_dumptime_table must be cleaned\");\n-    _cloned_dumptime_table = new (ResourceObj::C_HEAP, mtClass) DumpTimeSharedClassTable;\n-    CloneDumpTimeClassTable copy_classes(_dumptime_table, _cloned_dumptime_table);\n-    _dumptime_table->iterate_all_live_classes(&copy_classes);\n-    _cloned_dumptime_table->update_counts();\n-  }\n-  if (_dumptime_lambda_proxy_class_dictionary != NULL) {\n-    assert(_cloned_dumptime_lambda_proxy_class_dictionary == NULL,\n-           \"_cloned_dumptime_lambda_proxy_class_dictionary must be cleaned\");\n-    _cloned_dumptime_lambda_proxy_class_dictionary =\n-                                          new (ResourceObj::C_HEAP, mtClass) DumpTimeLambdaProxyClassDictionary;\n-    CloneDumpTimeLambdaProxyClassTable copy_proxy_classes(_dumptime_lambda_proxy_class_dictionary,\n-                                                          _cloned_dumptime_lambda_proxy_class_dictionary);\n-    _dumptime_lambda_proxy_class_dictionary->iterate(&copy_proxy_classes);\n-  }\n+\n+  assert(_cloned_dumptime_table == NULL, \"_cloned_dumptime_table must be cleaned\");\n+  _cloned_dumptime_table = new (ResourceObj::C_HEAP, mtClass) DumpTimeSharedClassTable;\n+  CloneDumpTimeClassTable copy_classes(_dumptime_table, _cloned_dumptime_table);\n+  _dumptime_table->iterate_all_live_classes(&copy_classes);\n+  _cloned_dumptime_table->update_counts();\n+\n+  assert(_cloned_dumptime_lambda_proxy_class_dictionary == NULL,\n+         \"_cloned_dumptime_lambda_proxy_class_dictionary must be cleaned\");\n+  _cloned_dumptime_lambda_proxy_class_dictionary =\n+                                        new (ResourceObj::C_HEAP, mtClass) DumpTimeLambdaProxyClassDictionary;\n+  CloneDumpTimeLambdaProxyClassTable copy_proxy_classes(_dumptime_lambda_proxy_class_dictionary,\n+                                                        _cloned_dumptime_lambda_proxy_class_dictionary);\n+  _dumptime_lambda_proxy_class_dictionary->iterate(&copy_proxy_classes);\n@@ -1576,0 +1522,1 @@\n+\n@@ -1606,4 +1553,2 @@\n-  if (_dumptime_lambda_proxy_class_dictionary != NULL) {\n-    CleanupDumpTimeLambdaProxyClassTable cleanup_proxy_classes;\n-    _dumptime_lambda_proxy_class_dictionary->unlink(&cleanup_proxy_classes);\n-  }\n+  CleanupDumpTimeLambdaProxyClassTable cleanup_proxy_classes;\n+  _dumptime_lambda_proxy_class_dictionary->unlink(&cleanup_proxy_classes);\n@@ -1663,1 +1608,1 @@\n-  if (!HeapShared::are_archived_mirrors_available()) {\n+  if (!ArchiveHeapLoader::are_archived_mirrors_available()) {\n","filename":"src\/hotspot\/share\/classfile\/systemDictionaryShared.cpp","additions":104,"deletions":159,"binary":false,"changes":263,"status":"modified"},{"patch":"@@ -149,0 +149,10 @@\n+uint G1CollectedHeap::get_chunks_per_region() {\n+  uint log_region_size = HeapRegion::LogOfHRGrainBytes;\n+  \/\/ Limit the expected input values to current known possible values of the\n+  \/\/ (log) region size. Adjust as necessary after testing if changing the permissible\n+  \/\/ values for region size.\n+  assert(log_region_size >= 20 && log_region_size <= 29,\n+         \"expected value in [20,29], but got %u\", log_region_size);\n+  return 1u << (log_region_size \/ 2 - 4);\n+}\n+\n@@ -989,8 +999,0 @@\n-  \/\/ If we start the compaction before the CM threads finish\n-  \/\/ scanning the root regions we might trip them over as we'll\n-  \/\/ be moving objects \/ updating references. So let's wait until\n-  \/\/ they are done. By telling them to abort, they should complete\n-  \/\/ early.\n-  _cm->root_regions()->abort();\n-  _cm->root_regions()->wait_until_scan_finished();\n-\n@@ -1061,1 +1063,1 @@\n-  if (_hot_card_cache->use_cache()) {\n+  if (G1HotCardCache::use_cache()) {\n@@ -1112,1 +1114,1 @@\n-  G1FullCollector collector(this, explicit_gc, do_clear_all_soft_refs, do_maximal_compaction);\n+  G1FullCollector collector(this, explicit_gc, do_clear_all_soft_refs, do_maximal_compaction, gc_mark.tracer());\n@@ -1895,0 +1897,1 @@\n+    case GCCause::_codecache_GC_aggressive: return true;\n@@ -2233,1 +2236,1 @@\n-  return is_in_reserved(p) && _hrm.is_available(addr_to_region((HeapWord*)p));\n+  return is_in_reserved(p) && _hrm.is_available(addr_to_region(p));\n@@ -2289,0 +2292,4 @@\n+void G1CollectedHeap::heap_region_iterate(HeapRegionIndexClosure* cl) const {\n+  _hrm.iterate(cl);\n+}\n+\n@@ -2527,1 +2534,1 @@\n-      hrrs->print();\n+      tty->print_cr(\"hrrs \" PTR_FORMAT, p2i(hrrs));\n@@ -2579,1 +2586,2 @@\n-                       stats->regions_filled(), stats->direct_allocated(),\n+                       stats->regions_filled(), stats->num_plab_filled(),\n+                       stats->direct_allocated(), stats->num_direct_allocated(),\n@@ -2844,2 +2852,2 @@\n-  _survivor_evac_stats.adjust_desired_plab_sz();\n-  _old_evac_stats.adjust_desired_plab_sz();\n+  _survivor_evac_stats.adjust_desired_plab_size();\n+  _old_evac_stats.adjust_desired_plab_size();\n@@ -2867,1 +2875,1 @@\n-  GCTraceCPUTime tcpu;\n+  GCTraceCPUTime tcpu(_gc_tracer_stw);\n@@ -2896,2 +2904,1 @@\n-void G1CollectedHeap::complete_cleaning(BoolObjectClosure* is_alive,\n-                                        bool class_unloading_occurred) {\n+void G1CollectedHeap::complete_cleaning(bool class_unloading_occurred) {\n@@ -2899,1 +2906,1 @@\n-  G1ParallelCleaningTask unlink_task(is_alive, num_workers, class_unloading_occurred);\n+  G1ParallelCleaningTask unlink_task(num_workers, class_unloading_occurred);\n@@ -3285,1 +3292,1 @@\n-    _cm->root_regions()->add(alloc_region->top_at_mark_start(), alloc_region->top());\n+    _cm->add_root_region(alloc_region);\n@@ -3304,4 +3311,3 @@\n-void G1CollectedHeap::mark_evac_failure_object(const oop obj) const {\n-  \/\/ All objects failing evacuation are live. What we'll do is\n-  \/\/ that we'll update the marking info so that they are\n-  \/\/ all below TAMS and explicitly marked.\n+void G1CollectedHeap::mark_evac_failure_object(uint worker_id, const oop obj, size_t obj_size) const {\n+  assert(!_cm->is_marked_in_bitmap(obj), \"must be\");\n+\n@@ -3309,0 +3315,3 @@\n+  if (collector_state()->in_concurrent_start_gc()) {\n+    _cm->add_to_liveness(worker_id, obj, obj_size);\n+  }\n@@ -3392,1 +3401,0 @@\n-  _hot_card_cache->set_use_cache(true);\n@@ -3443,1 +3451,1 @@\n-  if (!Continuations::is_gc_marking_cycle_active()) {\n+  if (!CodeCache::is_gc_marking_cycle_active()) {\n@@ -3450,2 +3458,2 @@\n-    Continuations::on_gc_marking_cycle_start();\n-    Continuations::arm_all_nmethods();\n+    CodeCache::on_gc_marking_cycle_start();\n+    CodeCache::arm_all_nmethods();\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":36,"deletions":28,"binary":false,"changes":64,"status":"modified"},{"patch":"@@ -533,0 +533,8 @@\n+  \/\/ Return \"optimal\" number of chunks per region we want to use for claiming areas\n+  \/\/ within a region to claim.\n+  \/\/ The returned value is a trade-off between granularity of work distribution and\n+  \/\/ memory usage and maintenance costs of that table.\n+  \/\/ Testing showed that 64 for 1M\/2M region, 128 for 4M\/8M regions, 256 for 16\/32M regions,\n+  \/\/ and so on seems to be such a good trade-off.\n+  static uint get_chunks_per_region();\n+\n@@ -570,0 +578,6 @@\n+  \/\/ Clamp the given PLAB word size to allowed values. Prevents humongous PLAB sizes\n+  \/\/ for two reasons:\n+  \/\/ * PLABs are allocated using a similar paths as oops, but should\n+  \/\/   never be in a humongous region\n+  \/\/ * Allowing humongous PLABs needlessly churns the region free lists\n+  inline size_t clamp_plab_size(size_t value) const;\n@@ -585,1 +599,1 @@\n-  inline void register_humongous_region_with_region_attr(uint index);\n+  inline void register_humongous_candidate_region_with_region_attr(uint index);\n@@ -1034,1 +1048,1 @@\n-  inline bool is_in_cset_or_humongous(const oop obj);\n+  inline bool is_in_cset_or_humongous_candidate(const oop obj);\n@@ -1076,0 +1090,1 @@\n+  void heap_region_iterate(HeapRegionIndexClosure* blk) const;\n@@ -1087,1 +1102,1 @@\n-  inline uint addr_to_region(HeapWord* addr) const;\n+  inline uint addr_to_region(const void* addr) const;\n@@ -1131,3 +1146,2 @@\n-  \/\/ Returns the HeapRegion that contains addr. addr must not be NULL.\n-  template <class T>\n-  inline HeapRegion* heap_region_containing(const T addr) const;\n+  \/\/ Returns the HeapRegion that contains addr. addr must not be nullptr.\n+  inline HeapRegion* heap_region_containing(const void* addr) const;\n@@ -1135,4 +1149,3 @@\n-  \/\/ Returns the HeapRegion that contains addr, or NULL if that is an uncommitted\n-  \/\/ region. addr must not be NULL.\n-  template <class T>\n-  inline HeapRegion* heap_region_containing_or_null(const T addr) const;\n+  \/\/ Returns the HeapRegion that contains addr, or nullptr if that is an uncommitted\n+  \/\/ region. addr must not be nullptr.\n+  inline HeapRegion* heap_region_containing_or_null(const void* addr) const;\n@@ -1222,0 +1235,1 @@\n+  inline static bool is_obj_filler(const oop obj);\n@@ -1235,2 +1249,2 @@\n-  \/\/ Mark the live object that failed evacuation in the prev bitmap.\n-  void mark_evac_failure_object(oop obj) const;\n+  \/\/ Mark the live object that failed evacuation in the bitmap.\n+  void mark_evac_failure_object(uint worker_id, oop obj, size_t obj_size) const;\n@@ -1252,3 +1266,0 @@\n-  \/\/ No nmethod flushing needed.\n-  void flush_nmethod(nmethod* nm) override {}\n-\n@@ -1273,1 +1284,1 @@\n-  void complete_cleaning(BoolObjectClosure* is_alive, bool class_unloading_occurred);\n+  void complete_cleaning(bool class_unloading_occurred);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.hpp","additions":27,"deletions":16,"binary":false,"changes":43,"status":"modified"},{"patch":"@@ -48,1 +48,0 @@\n-#include \"runtime\/continuation.hpp\"\n@@ -116,1 +115,2 @@\n-                                 bool do_maximal_compaction) :\n+                                 bool do_maximal_compaction,\n+                                 G1FullGCTracer* tracer) :\n@@ -118,1 +118,1 @@\n-    _scope(heap->monitoring_support(), explicit_gc, clear_soft_refs, do_maximal_compaction),\n+    _scope(heap->monitoring_support(), explicit_gc, clear_soft_refs, do_maximal_compaction, tracer),\n@@ -123,1 +123,1 @@\n-    _serial_compaction_point(),\n+    _serial_compaction_point(this),\n@@ -136,0 +136,1 @@\n+  _compaction_tops = NEW_C_HEAP_ARRAY(HeapWord*, _heap->max_regions(), mtGC);\n@@ -138,0 +139,1 @@\n+    _compaction_tops[j] = nullptr;\n@@ -142,1 +144,1 @@\n-    _compaction_points[i] = new G1FullGCCompactionPoint();\n+    _compaction_points[i] = new G1FullGCCompactionPoint(this);\n@@ -156,0 +158,1 @@\n+  FREE_C_HEAP_ARRAY(HeapWord*, _compaction_tops);\n@@ -211,2 +214,2 @@\n-  Continuations::on_gc_marking_cycle_finish();\n-  Continuations::arm_all_nmethods();\n+  CodeCache::on_gc_marking_cycle_finish();\n+  CodeCache::arm_all_nmethods();\n@@ -302,0 +305,1 @@\n+    CodeCache::UnloadingScope unloading_scope(&_is_alive);\n@@ -304,1 +308,1 @@\n-    _heap->complete_cleaning(&_is_alive, purged_class);\n+    _heap->complete_cleaning(purged_class);\n@@ -350,30 +354,34 @@\n-\/\/  GCTraceTime(Debug, gc, phases) debug(\"Phase 2: Prepare serial compaction\", scope()->timer());\n-\/\/  \/\/ At this point we know that after parallel compaction there will be no\n-\/\/  \/\/ completely free regions. That means that the last region of\n-\/\/  \/\/ all compaction queues still have data in them. We try to compact\n-\/\/  \/\/ these regions in serial to avoid a premature OOM when the mutator wants\n-\/\/  \/\/ to allocate the first eden region after gc.\n-\/\/  for (uint i = 0; i < workers(); i++) {\n-\/\/    G1FullGCCompactionPoint* cp = compaction_point(i);\n-\/\/    if (cp->has_regions()) {\n-\/\/      serial_compaction_point()->add(cp->remove_last());\n-\/\/    }\n-\/\/  }\n-\/\/\n-\/\/  \/\/ Update the forwarding information for the regions in the serial\n-\/\/  \/\/ compaction point.\n-\/\/  G1FullGCCompactionPoint* cp = serial_compaction_point();\n-\/\/  for (GrowableArrayIterator<HeapRegion*> it = cp->regions()->begin(); it != cp->regions()->end(); ++it) {\n-\/\/    HeapRegion* current = *it;\n-\/\/    if (!cp->is_initialized()) {\n-\/\/      \/\/ Initialize the compaction point. Nothing more is needed for the first heap region\n-\/\/      \/\/ since it is already prepared for compaction.\n-\/\/      cp->initialize(current);\n-\/\/    } else {\n-\/\/      assert(!current->is_humongous(), \"Should be no humongous regions in compaction queue\");\n-\/\/      G1SerialRePrepareClosure re_prepare(cp, current);\n-\/\/      current->set_compaction_top(current->bottom());\n-\/\/      current->apply_to_marked_objects(mark_bitmap(), &re_prepare);\n-\/\/    }\n-\/\/  }\n-\/\/  cp->update();\n+  \/\/GCTraceTime(Debug, gc, phases) debug(\"Phase 2: Prepare serial compaction\", scope()->timer());\n+  \/\/ At this point we know that after parallel compaction there will be no\n+  \/\/ completely free regions. That means that the last region of\n+  \/\/ all compaction queues still have data in them. We try to compact\n+  \/\/ these regions in serial to avoid a premature OOM when the mutator wants\n+  \/\/ to allocate the first eden region after gc.\n+  \/*\n+  for (uint i = 0; i < workers(); i++) {\n+    G1FullGCCompactionPoint* cp = compaction_point(i);\n+    if (cp->has_regions()) {\n+      serial_compaction_point()->add(cp->remove_last());\n+    }\n+  }\n+  *\/\n+\n+  \/\/ Update the forwarding information for the regions in the serial\n+  \/\/ compaction point.\n+  \/*\n+  G1FullGCCompactionPoint* cp = serial_compaction_point();\n+  for (GrowableArrayIterator<HeapRegion*> it = cp->regions()->begin(); it != cp->regions()->end(); ++it) {\n+    HeapRegion* current = *it;\n+    if (!cp->is_initialized()) {\n+      \/\/ Initialize the compaction point. Nothing more is needed for the first heap region\n+      \/\/ since it is already prepared for compaction.\n+      cp->initialize(current);\n+    } else {\n+      assert(!current->is_humongous(), \"Should be no humongous regions in compaction queue\");\n+      G1SerialRePrepareClosure re_prepare(cp, current);\n+      set_compaction_top(current, current->bottom());\n+      current->apply_to_marked_objects(mark_bitmap(), &re_prepare);\n+    }\n+  }\n+  cp->update();\n+  *\/\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullCollector.cpp","additions":46,"deletions":38,"binary":false,"changes":84,"status":"modified"},{"patch":"@@ -28,1 +28,0 @@\n-#include \"gc\/g1\/g1FullCollector.hpp\"\n@@ -67,0 +66,4 @@\n+    assert(_collector->compaction_top(r) == nullptr,\n+           \"region %u compaction_top \" PTR_FORMAT \" must not be different from bottom \" PTR_FORMAT,\n+           r->hrm_index(), p2i(_collector->compaction_top(r)), p2i(r->bottom()));\n+\n@@ -114,1 +117,1 @@\n-  hr->reset_compacted_after_full_gc();\n+  hr->reset_compacted_after_full_gc(_collector->compaction_top(hr));\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactTask.cpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+class G1FullCollector;\n@@ -39,1 +40,1 @@\n-protected:\n+  G1FullCollector* _collector;\n@@ -42,1 +43,0 @@\n-private:\n@@ -48,0 +48,1 @@\n+    _collector(collector),\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactTask.hpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -26,0 +26,1 @@\n+#include \"gc\/g1\/g1FullCollector.inline.hpp\"\n@@ -32,3 +33,4 @@\n-G1FullGCCompactionPoint::G1FullGCCompactionPoint() :\n-    _current_region(NULL),\n-    _compaction_top(NULL) {\n+G1FullGCCompactionPoint::G1FullGCCompactionPoint(G1FullCollector* collector) :\n+    _collector(collector),\n+    _current_region(nullptr),\n+    _compaction_top(nullptr) {\n@@ -45,1 +47,1 @@\n-    _current_region->set_compaction_top(_compaction_top);\n+    _collector->set_compaction_top(_current_region, _compaction_top);\n@@ -50,1 +52,1 @@\n-  _compaction_top = _current_region->compaction_top();\n+  _compaction_top = _collector->compaction_top(_current_region);\n@@ -87,1 +89,1 @@\n-  _current_region->set_compaction_top(_compaction_top);\n+  _collector->set_compaction_top(_current_region, _compaction_top);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactionPoint.cpp","additions":9,"deletions":7,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -32,0 +32,1 @@\n+class G1FullCollector;\n@@ -36,0 +37,1 @@\n+  G1FullCollector* _collector;\n@@ -47,1 +49,1 @@\n-  G1FullGCCompactionPoint();\n+  G1FullGCCompactionPoint(G1FullCollector* collector);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactionPoint.hpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -125,1 +125,1 @@\n-  if (hcc->use_cache()) {\n+  if (G1HotCardCache::use_cache()) {\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCPrepareTask.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -70,1 +70,1 @@\n-  hr->set_compaction_top(hr->bottom());\n+  _collector->set_compaction_top(hr, hr->bottom());\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCPrepareTask.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -67,1 +67,1 @@\n-  if (region_attr.is_humongous()) {\n+  if (region_attr.is_humongous_candidate()) {\n@@ -125,1 +125,0 @@\n-  assert(from != NULL, \"from region must be non-NULL\");\n@@ -176,1 +175,1 @@\n-         p2i(p), _g1h->addr_to_region((HeapWord*)p));\n+         p2i(p), _g1h->addr_to_region(p));\n@@ -249,1 +248,1 @@\n-    if (state.is_humongous()) {\n+    if (state.is_humongous_candidate()) {\n","filename":"src\/hotspot\/share\/gc\/g1\/g1OopClosures.inline.hpp","additions":3,"deletions":4,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -114,1 +114,1 @@\n-size_t G1ParScanThreadState::flush(size_t* surviving_young_words) {\n+size_t G1ParScanThreadState::flush_stats(size_t* surviving_young_words, uint num_workers) {\n@@ -118,1 +118,1 @@\n-  _plab_allocator->flush_and_retire_stats();\n+  _plab_allocator->flush_and_retire_stats(num_workers);\n@@ -198,1 +198,1 @@\n-  assert(!region_attr.is_humongous(),\n+  assert(!region_attr.is_humongous_candidate(),\n@@ -200,1 +200,1 @@\n-         p2i(obj), _g1h->addr_to_region(cast_from_oop<HeapWord*>(obj)), p2i(p));\n+         p2i(obj), _g1h->addr_to_region(obj), p2i(p));\n@@ -328,1 +328,1 @@\n-  assert(dest->is_in_cset_or_humongous(), \"Unexpected dest: %s region attr\", dest->get_type_str());\n+  assert(dest->is_in_cset_or_humongous_candidate(), \"Unexpected dest: %s region attr\", dest->get_type_str());\n@@ -589,1 +589,1 @@\n-void G1ParScanThreadStateSet::flush() {\n+void G1ParScanThreadStateSet::flush_stats() {\n@@ -602,1 +602,1 @@\n-    size_t copied_bytes = pss->flush(_surviving_young_words_total) * HeapWordSize;\n+    size_t copied_bytes = pss->flush_stats(_surviving_young_words_total, _n_workers) * HeapWordSize;\n@@ -633,5 +633,0 @@\n-    \/\/ Objects failing evacuation will turn into old objects since the regions\n-    \/\/ are relabeled as such. We mark the failing objects in the marking bitmap\n-    \/\/ and later use it to handle all failed objects.\n-    _g1h->mark_evac_failure_object(old);\n-\n@@ -642,0 +637,4 @@\n+    \/\/ Mark the failing object in the marking bitmap and later use the bitmap to handle\n+    \/\/ evacuation failure recovery.\n+    _g1h->mark_evac_failure_object(_worker_id, old, word_sz);\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.cpp","additions":11,"deletions":12,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -155,1 +155,1 @@\n-  size_t flush(size_t* surviving_young_words);\n+  size_t flush_stats(size_t* surviving_young_words, uint num_workers);\n@@ -256,1 +256,1 @@\n-  void flush();\n+  void flush_stats();\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -371,1 +371,1 @@\n-  st->print_cr(\" [\" INTPTR_FORMAT \", \" INTPTR_FORMAT \", \" INTPTR_FORMAT \")\",\n+  st->print_cr(\" [\" PTR_FORMAT \", \" PTR_FORMAT \", \" PTR_FORMAT \")\",\n","filename":"src\/hotspot\/share\/gc\/parallel\/psOldGen.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -189,0 +189,2 @@\n+\n+  _gc_tracer = new (ResourceObj::C_HEAP, mtGC) DefNewTracer();\n@@ -531,2 +533,1 @@\n-  DefNewTracer gc_tracer;\n-  gc_tracer.report_gc_start(heap->gc_cause(), _gc_timer->gc_start());\n+  _gc_tracer->report_gc_start(heap->gc_cause(), _gc_timer->gc_start());\n@@ -550,1 +551,1 @@\n-  heap->trace_heap_before_gc(&gc_tracer);\n+  heap->trace_heap_before_gc(_gc_tracer);\n@@ -593,2 +594,2 @@\n-  gc_tracer.report_gc_reference_stats(stats);\n-  gc_tracer.report_tenuring_threshold(tenuring_threshold());\n+  _gc_tracer->report_gc_reference_stats(stats);\n+  _gc_tracer->report_tenuring_threshold(tenuring_threshold());\n@@ -648,1 +649,1 @@\n-    gc_tracer.report_promotion_failed(_promotion_failed_info);\n+    _gc_tracer->report_promotion_failed(_promotion_failed_info);\n@@ -656,1 +657,1 @@\n-  heap->trace_heap_after_gc(&gc_tracer);\n+  heap->trace_heap_after_gc(_gc_tracer);\n@@ -660,1 +661,1 @@\n-  gc_tracer.report_gc_end(_gc_timer->gc_end(), _gc_timer->time_partitions());\n+  _gc_tracer->report_gc_end(_gc_timer->gc_end(), _gc_timer->time_partitions());\n@@ -881,3 +882,0 @@\n-      assert(gch->gc_cause() == GCCause::_scavenge_alot ||\n-             !gch->incremental_collection_failed(),\n-             \"Twice in a row\");\n","filename":"src\/hotspot\/share\/gc\/serial\/defNewGeneration.cpp","additions":9,"deletions":11,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -218,0 +218,1 @@\n+    CodeCache::UnloadingScope scope(&is_alive);\n@@ -223,1 +224,1 @@\n-    CodeCache::do_unloading(&is_alive, purged_class);\n+    CodeCache::do_unloading(purged_class);\n","filename":"src\/hotspot\/share\/gc\/serial\/genMarkSweep.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -291,0 +291,1 @@\n+    case GCCause::_codecache_GC_aggressive:\n@@ -293,1 +294,1 @@\n-    case GCCause::_metadata_GC_threshold : {\n+    case GCCause::_metadata_GC_threshold: {\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -481,2 +481,0 @@\n-  \/\/ Callback for when nmethod is about to be deleted.\n-  virtual void flush_nmethod(nmethod* nm) = 0;\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n-#include \"classfile\/symbolTable.hpp\"\n+#include \"classfile\/symbolTable.hpp\"\n@@ -34,0 +34,2 @@\n+#include \"gc\/serial\/genMarkSweep.hpp\"\n+#include \"gc\/serial\/markSweep.hpp\"\n@@ -41,0 +43,1 @@\n+#include \"gc\/shared\/gcInitLogger.hpp\"\n@@ -45,1 +48,1 @@\n-#include \"gc\/shared\/genArguments.hpp\"\n+#include \"gc\/shared\/genArguments.hpp\"\n@@ -48,2 +51,1 @@\n-#include \"gc\/shared\/genOopClosures.inline.hpp\"\n-#include \"gc\/shared\/gcInitLogger.hpp\"\n+#include \"gc\/shared\/genOopClosures.inline.hpp\"\n@@ -53,1 +55,1 @@\n-#include \"gc\/shared\/oopStorageSet.inline.hpp\"\n+#include \"gc\/shared\/oopStorageSet.inline.hpp\"\n@@ -67,1 +69,0 @@\n-#include \"runtime\/continuation.hpp\"\n@@ -540,1 +541,1 @@\n-    GCTraceCPUTime tcpu;\n+    GCTraceCPUTime tcpu(((DefNewGeneration*)_young_gen)->gc_tracer());\n@@ -590,1 +591,1 @@\n-    GCTraceCPUTime tcpu;\n+    GCTraceCPUTime tcpu(GenMarkSweep::gc_tracer());\n@@ -612,2 +613,2 @@\n-    Continuations::on_gc_marking_cycle_start();\n-    Continuations::arm_all_nmethods();\n+    CodeCache::on_gc_marking_cycle_start();\n+    CodeCache::arm_all_nmethods();\n@@ -622,2 +623,2 @@\n-    Continuations::on_gc_marking_cycle_finish();\n-    Continuations::arm_all_nmethods();\n+    CodeCache::on_gc_marking_cycle_finish();\n+    CodeCache::arm_all_nmethods();\n@@ -666,4 +667,0 @@\n-void GenCollectedHeap::flush_nmethod(nmethod* nm) {\n-  \/\/ Do nothing.\n-}\n-\n@@ -891,2 +888,2 @@\n-bool GenCollectedHeap::is_in_young(oop p) const {\n-  bool result = cast_from_oop<HeapWord*>(p) < _old_gen->reserved().start();\n+bool GenCollectedHeap::is_in_young(const void* p) const {\n+  bool result = p < _old_gen->reserved().start();\n@@ -894,1 +891,1 @@\n-         \"incorrect test - result=%d, p=\" INTPTR_FORMAT, result, p2i((void*)p));\n+         \"incorrect test - result=%d, p=\" PTR_FORMAT, result, p2i(p));\n","filename":"src\/hotspot\/share\/gc\/shared\/genCollectedHeap.cpp","additions":16,"deletions":19,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -202,2 +202,1 @@\n-  \/\/ Returns true if the reference is to an object in the reserved space\n-  \/\/ for the young generation.\n+  \/\/ Returns true if p points into the reserved space for the young generation.\n@@ -205,1 +204,1 @@\n-  bool is_in_young(oop p) const;\n+  bool is_in_young(const void* p) const;\n@@ -217,1 +216,0 @@\n-  virtual void flush_nmethod(nmethod* nm);\n@@ -395,9 +393,0 @@\n-  \/\/ Promotion of obj into gen failed.  Try to promote obj to higher\n-  \/\/ gens in ascending order; return the new location of obj if successful.\n-  \/\/ Otherwise, try expand-and-allocate for obj in both the young and old\n-  \/\/ generation; return the new location of obj if successful.  Otherwise, return NULL.\n-  oop handle_failed_promotion(Generation* old_gen,\n-                              oop obj,\n-                              size_t obj_size);\n-\n-\n","filename":"src\/hotspot\/share\/gc\/shared\/genCollectedHeap.hpp","additions":2,"deletions":13,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -28,1 +28,0 @@\n-#include \"gc\/shared\/blockOffsetTable.inline.hpp\"\n@@ -48,0 +47,1 @@\n+#include \"gc\/serial\/serialBlockOffsetTable.inline.hpp\"\n@@ -293,0 +293,1 @@\n+#if INCLUDE_SERIALGC\n@@ -309,0 +310,1 @@\n+#endif \/\/ INCLUDE_SERIALGC\n@@ -591,1 +593,1 @@\n-  st->print_cr(\" [\" INTPTR_FORMAT \", \" INTPTR_FORMAT \")\",\n+  st->print_cr(\" [\" PTR_FORMAT \", \" PTR_FORMAT \")\",\n@@ -597,1 +599,1 @@\n-  st->print_cr(\" [\" INTPTR_FORMAT \", \" INTPTR_FORMAT \", \" INTPTR_FORMAT \")\",\n+  st->print_cr(\" [\" PTR_FORMAT \", \" PTR_FORMAT \", \" PTR_FORMAT \")\",\n@@ -601,0 +603,1 @@\n+#if INCLUDE_SERIALGC\n@@ -603,2 +606,2 @@\n-  st->print_cr(\" [\" INTPTR_FORMAT \", \" INTPTR_FORMAT \", \"\n-                INTPTR_FORMAT \", \" INTPTR_FORMAT \")\",\n+  st->print_cr(\" [\" PTR_FORMAT \", \" PTR_FORMAT \", \"\n+                PTR_FORMAT \", \" PTR_FORMAT \")\",\n@@ -607,0 +610,1 @@\n+#endif\n@@ -742,0 +746,1 @@\n+#if INCLUDE_SERIALGC\n@@ -800,0 +805,1 @@\n+#endif \/\/ INCLUDE_SERIALGC\n","filename":"src\/hotspot\/share\/gc\/shared\/space.cpp","additions":11,"deletions":5,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -38,0 +38,3 @@\n+#if INCLUDE_SERIALGC\n+#include \"gc\/serial\/serialBlockOffsetTable.hpp\"\n+#endif\n@@ -46,0 +49,2 @@\n+class ContiguousSpace;\n+#if INCLUDE_SERIALGC\n@@ -48,0 +53,2 @@\n+class BlockOffsetTable;\n+#endif\n@@ -50,1 +57,0 @@\n-class BlockOffsetTable;\n@@ -604,0 +610,1 @@\n+#if INCLUDE_SERIALGC\n@@ -650,0 +657,2 @@\n+#endif \/\/INCLUDE_SERIALGC\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/space.hpp","additions":10,"deletions":1,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,1 +30,0 @@\n-#include \"gc\/shared\/blockOffsetTable.inline.hpp\"\n@@ -35,1 +34,1 @@\n-#include \"oops\/oopsHierarchy.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n@@ -40,0 +39,1 @@\n+#include \"gc\/serial\/serialBlockOffsetTable.inline.hpp\"\n@@ -47,0 +47,1 @@\n+#if INCLUDE_SERIALGC\n@@ -80,2 +81,0 @@\n-#if INCLUDE_SERIALGC\n-\n","filename":"src\/hotspot\/share\/gc\/shared\/space.inline.hpp","additions":4,"deletions":5,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -52,1 +52,1 @@\n-    msg.append(\"Raw heap memory:\\n%s\", ss.as_string());\n+    msg.append(\"Raw heap memory:\\n%s\", ss.freeze());\n@@ -75,2 +75,2 @@\n-  msg.append(\"  mark:%s\\n\", mw_ss.as_string());\n-  msg.append(\"  region: %s\", ss.as_string());\n+  msg.append(\"  mark:%s\\n\", mw_ss.freeze());\n+  msg.append(\"  region: %s\", ss.freeze());\n@@ -88,1 +88,1 @@\n-    msg.append(\"  region: %s\", ss.as_string());\n+    msg.append(\"  region: %s\", ss.freeze());\n@@ -93,1 +93,1 @@\n-    msg.append(\"  %s\", ss.as_string());\n+    msg.append(\"  %s\", ss.freeze());\n@@ -105,1 +105,1 @@\n-      msg.append(\"  region: %s\", ss.as_string());\n+      msg.append(\"  region: %s\", ss.freeze());\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahAsserts.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"runtime\/synchronizer.hpp\"\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahForwarding.inline.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -77,0 +77,1 @@\n+#include \"code\/codeCache.hpp\"\n@@ -1185,0 +1186,1 @@\n+  tcl->do_thread(_control_thread);\n@@ -1242,3 +1244,0 @@\n-\n-      \/\/ We must not expose from-space oops to the rest of runtime, or else it\n-      \/\/ will call klass() on it, which might fail because of unexpected header.\n@@ -1356,1 +1355,1 @@\n-      obj = ShenandoahBarrierSet::resolve_forwarded_not_null(obj);\n+      obj = ShenandoahBarrierSet::barrier_set()->load_reference_barrier(obj);\n@@ -1800,0 +1799,2 @@\n+    ShenandoahIsAliveSelector is_alive;\n+    CodeCache::UnloadingScope scope(is_alive.is_alive_closure());\n@@ -1804,2 +1805,1 @@\n-    ShenandoahIsAliveSelector is_alive;\n-    ShenandoahClassUnloadingTask unlink_task(phase, is_alive.is_alive_closure(), num_workers, purged_class);\n+    ShenandoahClassUnloadingTask unlink_task(phase, num_workers, purged_class);\n@@ -1923,4 +1923,0 @@\n-void ShenandoahHeap::flush_nmethod(nmethod* nm) {\n-  ShenandoahCodeRoots::flush_nmethod(nm);\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":6,"deletions":10,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -509,1 +509,0 @@\n-  void flush_nmethod(nmethod* nm);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -770,1 +770,0 @@\n-        ResourceMark rm;\n@@ -774,1 +773,1 @@\n-              label, reg_live, verf_live, ss.as_string());\n+              label, reg_live, verf_live, ss.freeze());\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -51,0 +51,1 @@\n+#include \"runtime\/arguments.hpp\"\n@@ -591,2 +592,2 @@\n-                                &&opc_invokehandle,     &&opc_default,        &&opc_default,\n-\/* 0xEC *\/ &&opc_default,       &&opc_default,          &&opc_default,        &&opc_default,\n+                                &&opc_invokehandle,     &&opc_nofast_getfield,&&opc_nofast_putfield,\n+\/* 0xEC *\/ &&opc_nofast_aload_0,&&opc_nofast_iload,     &&opc_default,        &&opc_default,\n@@ -865,0 +866,7 @@\n+      CASE(_nofast_iload):\n+      {\n+        \/\/ Normal, non-rewritable iload handling.\n+        SET_STACK_SLOT(LOCALS_SLOT(pc[1]), 0);\n+        UPDATE_PC_AND_TOS_AND_CONTINUE(2, 1);\n+      }\n+\n@@ -925,2 +933,3 @@\n-            case Bytecodes::_getfield: {\n-              \/* Otherwise, do nothing here, wait until it gets rewritten to _fast_Xgetfield.\n+            case Bytecodes::_getfield:\n+            case Bytecodes::_nofast_getfield: {\n+              \/* Otherwise, do nothing here, wait until\/if it gets rewritten to _fast_Xgetfield.\n@@ -936,0 +945,9 @@\n+        \/\/ Normal aload_0 handling.\n+        VERIFY_OOP(LOCALS_OBJECT(0));\n+        SET_STACK_OBJECT(LOCALS_OBJECT(0), 0);\n+        UPDATE_PC_AND_TOS_AND_CONTINUE(1, 1);\n+      }\n+\n+      CASE(_nofast_aload_0):\n+      {\n+        \/\/ Normal, non-rewritable aload_0 handling.\n@@ -1704,0 +1722,1 @@\n+      CASE(_nofast_getfield):\n@@ -1714,0 +1733,7 @@\n+          \/\/ Interpreter runtime does not expect \"nofast\" opcodes,\n+          \/\/ prepare the vanilla opcode for it.\n+          Bytecodes::Code code = (Bytecodes::Code)opcode;\n+          if (code == Bytecodes::_nofast_getfield) {\n+            code = Bytecodes::_getfield;\n+          }\n+\n@@ -1715,2 +1741,2 @@\n-          if (!cache->is_resolved((Bytecodes::Code)opcode)) {\n-            CALL_VM(InterpreterRuntime::resolve_from_cache(THREAD, (Bytecodes::Code)opcode),\n+          if (!cache->is_resolved(code)) {\n+            CALL_VM(InterpreterRuntime::resolve_from_cache(THREAD, code),\n@@ -1730,1 +1756,2 @@\n-            if (REWRITE_BYTECODES && !cache->is_volatile()) {\n+            if (REWRITE_BYTECODES && !cache->is_volatile() &&\n+                  ((Bytecodes::Code)opcode != Bytecodes::_nofast_getfield)) {\n@@ -1822,0 +1849,1 @@\n+      CASE(_nofast_putfield):\n@@ -1826,2 +1854,10 @@\n-          if (!cache->is_resolved((Bytecodes::Code)opcode)) {\n-            CALL_VM(InterpreterRuntime::resolve_from_cache(THREAD, (Bytecodes::Code)opcode),\n+\n+          \/\/ Interpreter runtime does not expect \"nofast\" opcodes,\n+          \/\/ prepare the vanilla opcode for it.\n+          Bytecodes::Code code = (Bytecodes::Code)opcode;\n+          if (code == Bytecodes::_nofast_putfield) {\n+            code = Bytecodes::_putfield;\n+          }\n+\n+          if (!cache->is_resolved(code)) {\n+            CALL_VM(InterpreterRuntime::resolve_from_cache(THREAD, code),\n@@ -1852,1 +1888,2 @@\n-            if (REWRITE_BYTECODES && !cache->is_volatile()) {\n+            if (REWRITE_BYTECODES && !cache->is_volatile() &&\n+                  ((Bytecodes::Code)opcode != Bytecodes::_nofast_putfield)) {\n@@ -2420,1 +2457,1 @@\n-              if (REWRITE_BYTECODES) {\n+              if (REWRITE_BYTECODES && !UseSharedSpaces && !Arguments::is_dumping_archive()) {\n","filename":"src\/hotspot\/share\/interpreter\/zero\/bytecodeInterpreter.cpp","additions":48,"deletions":11,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -66,0 +66,3 @@\n+#if INCLUDE_JFR\n+#include \"jfr\/jfr.hpp\"\n+#endif\n@@ -516,1 +519,1 @@\n-      resolved_klass = SystemDictionary::find_instance_klass(strippedsym,\n+      resolved_klass = SystemDictionary::find_instance_klass(THREAD, strippedsym,\n@@ -524,1 +527,1 @@\n-        resolved_klass = SystemDictionary::find_instance_klass(strippedsym,\n+        resolved_klass = SystemDictionary::find_instance_klass(THREAD, strippedsym,\n@@ -534,1 +537,1 @@\n-      resolved_klass = SystemDictionary::find_instance_klass(class_name,\n+      resolved_klass = SystemDictionary::find_instance_klass(THREAD, class_name,\n@@ -757,1 +760,1 @@\n-C2V_VMENTRY_NULL(jobject, lookupMethodInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint index, jbyte opcode))\n+C2V_VMENTRY_NULL(jobject, lookupMethodInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint index, jbyte opcode, ARGUMENT_PAIR(caller)))\n@@ -759,0 +762,1 @@\n+  methodHandle caller(THREAD, UNPACK_PAIR(Method, caller));\n@@ -762,0 +766,1 @@\n+  JFR_ONLY(if (method.not_null()) Jfr::on_resolution(caller(), method(), CHECK_NULL);)\n@@ -939,1 +944,1 @@\n-  nmethodLocker nmethod_handle;\n+\n@@ -947,1 +952,0 @@\n-      nmethod_handle,\n@@ -962,1 +966,1 @@\n-    tty->print_raw_cr(s.as_string());\n+    tty->print_raw_cr(s.freeze());\n@@ -974,1 +978,1 @@\n-        JVMCIENV->invalidate_nmethod_mirror(nmethod_mirror, JVMCI_CHECK_0);\n+        JVMCIENV->invalidate_nmethod_mirror(nmethod_mirror, true, JVMCI_CHECK_0);\n@@ -1000,2 +1004,1 @@\n-  nmethodLocker locker;\n-  CodeBlob* cb = JVMCIENV->get_code_blob(installedCodeObject, locker);\n+  CodeBlob* cb = JVMCIENV->get_code_blob(installedCodeObject);\n@@ -1015,6 +1018,0 @@\n-  if (cb->is_nmethod()) {\n-    nmethod* nm = (nmethod*) cb;\n-    if (!nm->is_alive()) {\n-      return NULL;\n-    }\n-  }\n@@ -1046,2 +1043,1 @@\n-  nmethodLocker locker;\n-  nmethod* nm = JVMCIENV->get_nmethod(nmethod_mirror, locker);\n+  nmethod* nm = JVMCIENV->get_nmethod(nmethod_mirror);\n@@ -1152,1 +1148,1 @@\n-C2V_VMENTRY(void, invalidateHotSpotNmethod, (JNIEnv* env, jobject, jobject hs_nmethod))\n+C2V_VMENTRY(void, invalidateHotSpotNmethod, (JNIEnv* env, jobject, jobject hs_nmethod, jboolean deoptimize))\n@@ -1154,1 +1150,1 @@\n-  JVMCIENV->invalidate_nmethod_mirror(nmethod_mirror, JVMCI_CHECK);\n+  JVMCIENV->invalidate_nmethod_mirror(nmethod_mirror, deoptimize, JVMCI_CHECK);\n@@ -2215,5 +2211,3 @@\n-C2V_VMENTRY(void, deleteGlobalHandle, (JNIEnv* env, jobject, jlong handle))\n-  if (handle != 0) {\n-    JVMCIENV->runtime()->destroy_oop_handle(handle);\n-  }\n-}\n+C2V_VMENTRY(void, releaseClearedOopHandles, (JNIEnv* env, jobject))\n+  JVMCIENV->runtime()->release_cleared_oop_handles();\n+C2V_END\n@@ -2517,6 +2511,5 @@\n-    nmethodLocker locker;\n-    nmethod* nm = JVMCIENV->get_nmethod(obj, locker);\n-    if (nm != NULL) {\n-      JVMCINMethodData* data = nm->jvmci_nmethod_data();\n-      if (data != NULL) {\n-        if (peerEnv->is_hotspot()) {\n+    if (peerEnv->is_hotspot()) {\n+      nmethod* nm = JVMCIENV->get_nmethod(obj);\n+      if (nm != NULL) {\n+        JVMCINMethodData* data = nm->jvmci_nmethod_data();\n+        if (data != NULL) {\n@@ -2532,0 +2525,1 @@\n+\n@@ -2541,0 +2535,1 @@\n+      nmethod* nm = JVMCIENV->get_nmethod(obj);\n@@ -2592,2 +2587,1 @@\n-  nmethodLocker locker;\n-  JVMCIENV->get_nmethod(code, locker);\n+  JVMCIENV->get_nmethod(code);\n@@ -2598,2 +2592,1 @@\n-  nmethodLocker locker;\n-  CodeBlob* cb = JVMCIENV->get_code_blob(code, locker);\n+  CodeBlob* cb = JVMCIENV->get_code_blob(code);\n@@ -2603,0 +2596,1 @@\n+  \/\/ Make a resource copy of code before the allocation causes a safepoint\n@@ -2604,0 +2598,3 @@\n+  jbyte* code_bytes = NEW_RESOURCE_ARRAY(jbyte, code_size);\n+  memcpy(code_bytes, (jbyte*) cb->code_begin(), code_size);\n+\n@@ -2605,1 +2602,1 @@\n-  JVMCIENV->copy_bytes_from((jbyte*) cb->code_begin(), result, 0, code_size);\n+  JVMCIENV->copy_bytes_from(code_bytes, result, 0, code_size);\n@@ -2837,1 +2834,1 @@\n-  {CC \"lookupMethodInPool\",                           CC \"(\" HS_CONSTANT_POOL2 \"IB)\" HS_METHOD,                                             FN_PTR(lookupMethodInPool)},\n+  {CC \"lookupMethodInPool\",                           CC \"(\" HS_CONSTANT_POOL2 \"IB\" HS_METHOD2 \")\" HS_METHOD,                               FN_PTR(lookupMethodInPool)},\n@@ -2866,1 +2863,1 @@\n-  {CC \"invalidateHotSpotNmethod\",                     CC \"(\" HS_NMETHOD \")V\",                                                               FN_PTR(invalidateHotSpotNmethod)},\n+  {CC \"invalidateHotSpotNmethod\",                     CC \"(\" HS_NMETHOD \"Z)V\",                                                              FN_PTR(invalidateHotSpotNmethod)},\n@@ -2907,1 +2904,1 @@\n-  {CC \"deleteGlobalHandle\",                           CC \"(J)V\",                                                                            FN_PTR(deleteGlobalHandle)},\n+  {CC \"releaseClearedOopHandles\",                     CC \"()V\",                                                                             FN_PTR(releaseClearedOopHandles)},\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":36,"deletions":39,"binary":false,"changes":75,"status":"modified"},{"patch":"@@ -103,0 +103,2 @@\n+  static_field(CompilerToVM::Data,             data_section_item_alignment,            int)                                          \\\n+                                                                                                                                     \\\n@@ -231,1 +233,0 @@\n-  nonstatic_field(MethodCounters,              _nmethod_age,                                  int)                                   \\\n@@ -258,1 +259,1 @@\n-  nonstatic_field(nmethod,                     _comp_level,                                   int)                                   \\\n+  nonstatic_field(nmethod,                     _comp_level,                                   CompLevel)                             \\\n@@ -339,1 +340,0 @@\n-  static_field(StubRoutines,                _cont_doYield,                                    address)                               \\\n@@ -368,0 +368,1 @@\n+  declare_integer_type(CompLevel)                                         \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -775,1 +775,1 @@\n-                    \"(must be aligned to \" SIZE_FORMAT_HEX \").\",\n+                    \"(must be aligned to \" SIZE_FORMAT_X \").\",\n@@ -1013,1 +1013,1 @@\n-void Metaspace::purge() {\n+void Metaspace::purge(bool classes_unloaded) {\n@@ -1018,6 +1018,2 @@\n-  ChunkManager* cm = ChunkManager::chunkmanager_nonclass();\n-  if (cm != NULL) {\n-    cm->purge();\n-  }\n-  if (using_class_space()) {\n-    cm = ChunkManager::chunkmanager_class();\n+  if (classes_unloaded) {\n+    ChunkManager* cm = ChunkManager::chunkmanager_nonclass();\n@@ -1027,0 +1023,6 @@\n+    if (using_class_space()) {\n+      cm = ChunkManager::chunkmanager_class();\n+      if (cm != NULL) {\n+        cm->purge();\n+      }\n+    }\n@@ -1029,1 +1031,6 @@\n-  MetaspaceCriticalAllocation::satisfy();\n+  \/\/ Try to satisfy queued metaspace allocation requests.\n+  \/\/\n+  \/\/ It might seem unnecessary to try to process allocation requests if no\n+  \/\/ classes have been unloaded. However, this call is required for the code\n+  \/\/ in MetaspaceCriticalAllocation::try_allocate_critical to work.\n+  MetaspaceCriticalAllocation::process();\n","filename":"src\/hotspot\/share\/memory\/metaspace.cpp","additions":16,"deletions":9,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -127,1 +127,1 @@\n-  static void purge();\n+  static void purge(bool classes_unloaded);\n","filename":"src\/hotspot\/share\/memory\/metaspace.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -60,2 +60,2 @@\n-         SIZE_FORMAT_HEX \" is not aligned to \"               \\\n-         SIZE_FORMAT_HEX, (size_t)(uintptr_t)value, (size_t)(alignment))\n+         SIZE_FORMAT_X \" is not aligned to \"                 \\\n+         SIZE_FORMAT_X, (size_t)(uintptr_t)value, (size_t)(alignment))\n","filename":"src\/hotspot\/share\/memory\/metaspace\/metaspaceCommon.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -197,1 +197,1 @@\n-           \" alignment: \" SIZE_FORMAT_HEX,\n+           \" alignment: \" SIZE_FORMAT_X,\n@@ -407,1 +407,1 @@\n-                             \" heap of size \" SIZE_FORMAT_HEX,\n+                             \" heap of size \" SIZE_FORMAT_X,\n@@ -604,1 +604,1 @@\n-      log_trace(gc, heap, coops)(\"Trying to allocate at address NULL heap of size \" SIZE_FORMAT_HEX, size + noaccess_prefix);\n+      log_trace(gc, heap, coops)(\"Trying to allocate at address NULL heap of size \" SIZE_FORMAT_X, size + noaccess_prefix);\n@@ -1069,2 +1069,2 @@\n-  out->print_cr(\" - [low, high]:     [\" INTPTR_FORMAT \", \" INTPTR_FORMAT \"]\",  p2i(low()), p2i(high()));\n-  out->print_cr(\" - [low_b, high_b]: [\" INTPTR_FORMAT \", \" INTPTR_FORMAT \"]\",  p2i(low_boundary()), p2i(high_boundary()));\n+  out->print_cr(\" - [low, high]:     [\" PTR_FORMAT \", \" PTR_FORMAT \"]\",  p2i(low()), p2i(high()));\n+  out->print_cr(\" - [low_b, high_b]: [\" PTR_FORMAT \", \" PTR_FORMAT \"]\",  p2i(low_boundary()), p2i(high_boundary()));\n","filename":"src\/hotspot\/share\/memory\/virtualspace.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -36,0 +36,4 @@\n+static inline bool check_alignment(Klass* v) {\n+  return (intptr_t)v % KlassAlignmentInBytes == 0;\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/compressedKlass.inline.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -180,1 +180,0 @@\n-\n","filename":"src\/hotspot\/share\/oops\/compressedOops.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -97,1 +97,1 @@\n-  static address* ptrs_base_addr()           { return &_narrow_oop._base; }\n+  static address  ptrs_base_addr()           { return (address)&_narrow_oop._base; }\n","filename":"src\/hotspot\/share\/oops\/compressedOops.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -57,2 +57,2 @@\n-  assert(is_object_aligned(result), \"address not aligned: \" INTPTR_FORMAT, p2i((void*) result));\n-  assert(Universe::is_in_heap(result), \"object not in heap \" PTR_FORMAT, p2i((void*) result));\n+  assert(is_object_aligned(result), \"address not aligned: \" PTR_FORMAT, p2i(result));\n+  assert(Universe::is_in_heap(result), \"object not in heap \" PTR_FORMAT, p2i(result));\n@@ -68,2 +68,2 @@\n-  assert(is_object_aligned(v), \"address not aligned: \" PTR_FORMAT, p2i((void*)v));\n-  assert(is_in(v), \"address not in heap range: \" PTR_FORMAT, p2i((void*)v));\n+  assert(is_object_aligned(v), \"address not aligned: \" PTR_FORMAT, p2i(v));\n+  assert(is_in(v), \"address not in heap range: \" PTR_FORMAT, p2i(v));\n@@ -82,1 +82,1 @@\n-  assert(Universe::is_in_heap(v), \"object not in heap \" PTR_FORMAT, p2i((void*) v));\n+  assert(Universe::is_in_heap(v), \"object not in heap \" PTR_FORMAT, p2i(v));\n@@ -87,1 +87,1 @@\n-  assert(Universe::is_in_heap_or_null(v), \"object not in heap \" PTR_FORMAT, p2i((void*) v));\n+  assert(Universe::is_in_heap_or_null(v), \"object not in heap \" PTR_FORMAT, p2i(v));\n","filename":"src\/hotspot\/share\/oops\/compressedOops.inline.hpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"cds\/archiveHeapLoader.hpp\"\n@@ -46,0 +47,1 @@\n+#include \"oops\/objArrayKlass.hpp\"\n@@ -545,0 +547,1 @@\n+#if INCLUDE_CDS\n@@ -612,1 +615,1 @@\n-    if (HeapShared::are_archived_mirrors_available()) {\n+    if (ArchiveHeapLoader::are_archived_mirrors_available()) {\n@@ -635,0 +638,1 @@\n+#endif \/\/ INCLUDE_CDS\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -550,0 +550,1 @@\n+#if INCLUDE_CDS\n@@ -551,0 +552,1 @@\n+#endif\n@@ -559,0 +561,1 @@\n+#if INCLUDE_CDS\n@@ -574,0 +577,1 @@\n+#endif \/\/ INCLUDE_CDS\n","filename":"src\/hotspot\/share\/oops\/klass.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -40,0 +40,1 @@\n+#include \"runtime\/synchronizer.hpp\"\n@@ -53,1 +54,1 @@\n-  st->print(\"{\" INTPTR_FORMAT \"}\", p2i(this));\n+  st->print(\"{\" PTR_FORMAT \"}\", p2i(this));\n@@ -137,1 +138,1 @@\n-  guarantee(oopDesc::is_oop_or_null(obj), \"invalid oop: \" INTPTR_FORMAT, p2i((oopDesc*) obj));\n+  guarantee(oopDesc::is_oop_or_null(obj), \"invalid oop: \" PTR_FORMAT, p2i(obj));\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -149,1 +149,1 @@\n-  template <DecoratorSet decorator>\n+  template<DecoratorSet decorators>\n@@ -152,0 +152,1 @@\n+\n@@ -155,0 +156,2 @@\n+  template<DecoratorSet decorators>\n+  void obj_field_put_access(int offset, oop value);\n","filename":"src\/hotspot\/share\/oops\/oop.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -230,0 +230,2 @@\n+template <DecoratorSet decorators>\n+inline void oopDesc::obj_field_put_access(int offset, oop value)    { HeapAccess<decorators>::oop_store_at(as_oop(), offset, value); }\n","filename":"src\/hotspot\/share\/oops\/oop.inline.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -141,7 +141,0 @@\n-\n-\/\/ For a ParmNode, all immediate inputs and outputs are considered relevant\n-\/\/ both in compact and standard representation.\n-void ParmNode::related(GrowableArray<Node*> *in_rel, GrowableArray<Node*> *out_rel, bool compact) const {\n-  this->collect_nodes(in_rel, 1, false, false);\n-  this->collect_nodes(out_rel, -1, false, false);\n-}\n@@ -1376,13 +1369,0 @@\n-\n-\/\/ The related nodes of a SafepointNode are all data inputs, excluding the\n-\/\/ control boundary, as well as all outputs till level 2 (to include projection\n-\/\/ nodes and targets). In compact mode, just include inputs till level 1 and\n-\/\/ outputs as before.\n-void SafePointNode::related(GrowableArray<Node*> *in_rel, GrowableArray<Node*> *out_rel, bool compact) const {\n-  if (compact) {\n-    this->collect_nodes(in_rel, 1, false, false);\n-  } else {\n-    this->collect_nodes_in_all_data(in_rel, false);\n-  }\n-  this->collect_nodes(out_rel, -2, false, false);\n-}\n@@ -1477,1 +1457,1 @@\n-    igvn->C->root()->rm_prec(nb);\n+    igvn->delete_precedence_of(igvn->C->root(), nb);\n@@ -1574,1 +1554,0 @@\n-  init_req( ValidLengthTest    , topnode);\n@@ -1601,0 +1580,48 @@\n+\/\/=============================================================================\n+Node* AllocateArrayNode::Ideal(PhaseGVN *phase, bool can_reshape) {\n+  if (remove_dead_region(phase, can_reshape))  return this;\n+  \/\/ Don't bother trying to transform a dead node\n+  if (in(0) && in(0)->is_top())  return NULL;\n+\n+  const Type* type = phase->type(Ideal_length());\n+  if (type->isa_int() && type->is_int()->_hi < 0) {\n+    if (can_reshape) {\n+      PhaseIterGVN *igvn = phase->is_IterGVN();\n+      \/\/ Unreachable fall through path (negative array length),\n+      \/\/ the allocation can only throw so disconnect it.\n+      Node* proj = proj_out_or_null(TypeFunc::Control);\n+      Node* catchproj = NULL;\n+      if (proj != NULL) {\n+        for (DUIterator_Fast imax, i = proj->fast_outs(imax); i < imax; i++) {\n+          Node *cn = proj->fast_out(i);\n+          if (cn->is_Catch()) {\n+            catchproj = cn->as_Multi()->proj_out_or_null(CatchProjNode::fall_through_index);\n+            break;\n+          }\n+        }\n+      }\n+      if (catchproj != NULL && catchproj->outcnt() > 0 &&\n+          (catchproj->outcnt() > 1 ||\n+           catchproj->unique_out()->Opcode() != Op_Halt)) {\n+        assert(catchproj->is_CatchProj(), \"must be a CatchProjNode\");\n+        Node* nproj = catchproj->clone();\n+        igvn->register_new_node_with_optimizer(nproj);\n+\n+        Node *frame = new ParmNode( phase->C->start(), TypeFunc::FramePtr );\n+        frame = phase->transform(frame);\n+        \/\/ Halt & Catch Fire\n+        Node* halt = new HaltNode(nproj, frame, \"unexpected negative array length\");\n+        igvn->add_input_to(phase->C->root(), halt);\n+        phase->transform(halt);\n+\n+        igvn->replace_node(catchproj, phase->C->top());\n+        return this;\n+      }\n+    } else {\n+      \/\/ Can't correct it during regular GVN so register for IGVN\n+      phase->C->record_for_igvn(this);\n+    }\n+  }\n+  return NULL;\n+}\n+\n@@ -1961,10 +1988,0 @@\n-\n-\/\/ The related set of lock nodes includes the control boundary.\n-void AbstractLockNode::related(GrowableArray<Node*> *in_rel, GrowableArray<Node*> *out_rel, bool compact) const {\n-  if (compact) {\n-      this->collect_nodes(in_rel, 1, false, false);\n-    } else {\n-      this->collect_nodes_in_all_data(in_rel, true);\n-    }\n-    this->collect_nodes(out_rel, -2, false, false);\n-}\n","filename":"src\/hotspot\/share\/opto\/callnode.cpp","additions":49,"deletions":32,"binary":false,"changes":81,"status":"modified"},{"patch":"@@ -1195,2 +1195,1 @@\n-            address slow_call_address,  \/\/ Address of slow call\n-            Node* valid_length_test \/\/ whether length is valid or not\n+            address slow_call_address  \/\/ Address of slow call\n@@ -1381,6 +1380,0 @@\n-  \/\/ For array allocations, copy the valid length check to the call node so Compile::final_graph_reshaping() can verify\n-  \/\/ that the call has the expected number of CatchProj nodes (in case the allocation always fails and the fallthrough\n-  \/\/ path dies).\n-  if (valid_length_test != NULL) {\n-    call->add_req(valid_length_test);\n-  }\n@@ -1875,1 +1868,1 @@\n-                         OptoRuntime::new_instance_Java(), NULL);\n+                         OptoRuntime::new_instance_Java());\n@@ -1880,1 +1873,0 @@\n-  Node* valid_length_test = alloc->in(AllocateNode::ValidLengthTest);\n@@ -1895,1 +1887,1 @@\n-                         slow_call_address, valid_length_test);\n+                         slow_call_address);\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":3,"deletions":11,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -74,0 +74,1 @@\n+#include \"runtime\/synchronizer.hpp\"\n@@ -733,31 +734,0 @@\n- const TypeFunc* OptoRuntime::continuation_doYield_Type() {\n-   \/\/ create input type (domain)\n-   const Type **fields = TypeTuple::fields(0);\n-   const TypeTuple *domain = TypeTuple::make(TypeFunc::Parms+0, fields);\n-\n-   \/\/ create result type (range)\n-   fields = TypeTuple::fields(1);\n-   fields[TypeFunc::Parms+0] = TypeInt::INT;\n-   const TypeTuple *range = TypeTuple::make(TypeFunc::Parms+1, fields);\n-\n-   return TypeFunc::make(domain, range);\n- }\n-\n- const TypeFunc* OptoRuntime::continuation_jump_Type() {\n-  \/\/ create input type (domain)\n-  const Type **fields = TypeTuple::fields(6);\n-  fields[TypeFunc::Parms+0] = TypeLong::LONG;\n-  fields[TypeFunc::Parms+1] = Type::HALF;\n-  fields[TypeFunc::Parms+2] = TypeLong::LONG;\n-  fields[TypeFunc::Parms+3] = Type::HALF;\n-  fields[TypeFunc::Parms+4] = TypeLong::LONG;\n-  fields[TypeFunc::Parms+5] = Type::HALF;\n-  const TypeTuple *domain = TypeTuple::make(TypeFunc::Parms+6, fields);\n-\n-  \/\/ create result type (range)\n-  fields = TypeTuple::fields(0);\n-  const TypeTuple *range = TypeTuple::make(TypeFunc::Parms+0, fields);\n-  return TypeFunc::make(domain, range);\n- }\n-\n-\n@@ -1754,1 +1724,1 @@\n-    c = new RTMLockingNamedCounter(st.as_string());\n+    c = new RTMLockingNamedCounter(st.freeze());\n@@ -1756,1 +1726,1 @@\n-    c = new NamedCounter(st.as_string(), tag);\n+    c = new NamedCounter(st.freeze(), tag);\n@@ -1790,1 +1760,1 @@\n-  st->print_raw_cr(tempst.as_string());\n+  st->print_raw_cr(tempst.freeze());\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":4,"deletions":34,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -1153,0 +1153,15 @@\n+\n+bool Type::has_category(Type::Category cat) const {\n+  if (category() == cat) {\n+    return true;\n+  }\n+  if (category() == Category::Mixed) {\n+    const TypeTuple* tuple = is_tuple();\n+    for (uint i = 0; i < tuple->cnt(); i++) {\n+      if (tuple->field_at(i)->has_category(cat)) {\n+        return true;\n+      }\n+    }\n+  }\n+  return false;\n+}\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"cds\/cds_globals.hpp\"\n@@ -56,0 +57,1 @@\n+#include \"runtime\/synchronizer.hpp\"\n@@ -551,0 +553,1 @@\n+  { \"UseCodeAging\",                 JDK_Version::undefined(), JDK_Version::jdk(20), JDK_Version::jdk(21) },\n@@ -3856,0 +3859,2 @@\n+#ifndef PRODUCT\n+  \/\/ UseDebuggerErgo is notproduct\n@@ -3859,0 +3864,1 @@\n+#endif\n@@ -3860,0 +3866,1 @@\n+#ifndef PRODUCT\n@@ -3865,0 +3872,1 @@\n+#endif\n@@ -4127,5 +4135,0 @@\n-\n-  if (LogTouchedMethods) {\n-    warning(\"LogTouchedMethods is not supported for Zero\");\n-    FLAG_SET_DEFAULT(LogTouchedMethods, false);\n-  }\n@@ -4390,0 +4393,75 @@\n+\n+bool Arguments::parse_malloc_limit_size(const char* s, size_t* out) {\n+  julong limit = 0;\n+  Arguments::ArgsRange range = parse_memory_size(s, &limit, 1, SIZE_MAX);\n+  switch (range) {\n+  case ArgsRange::arg_in_range:\n+    *out = (size_t)limit;\n+    return true;\n+  case ArgsRange::arg_too_big: \/\/ only possible on 32-bit\n+    vm_exit_during_initialization(\"MallocLimit: too large\", s);\n+    break;\n+  case ArgsRange::arg_too_small:\n+    vm_exit_during_initialization(\"MallocLimit: limit must be > 0\");\n+    break;\n+  default:\n+    break;\n+  }\n+  return false;\n+}\n+\n+\/\/ Helper for parse_malloc_limits\n+void Arguments::parse_single_category_limit(char* expression, size_t limits[mt_number_of_types]) {\n+  \/\/ <category>:<limit>\n+  char* colon = ::strchr(expression, ':');\n+  if (colon == nullptr) {\n+    vm_exit_during_initialization(\"MallocLimit: colon missing\", expression);\n+  }\n+  *colon = '\\0';\n+  MEMFLAGS f = NMTUtil::string_to_flag(expression);\n+  if (f == mtNone) {\n+    vm_exit_during_initialization(\"MallocLimit: invalid nmt category\", expression);\n+  }\n+  if (parse_malloc_limit_size(colon + 1, limits + (int)f) == false) {\n+    vm_exit_during_initialization(\"Invalid MallocLimit size\", colon + 1);\n+  }\n+}\n+\n+void Arguments::parse_malloc_limits(size_t* total_limit, size_t limits[mt_number_of_types]) {\n+\n+  \/\/ Reset output to 0\n+  *total_limit = 0;\n+  for (int i = 0; i < mt_number_of_types; i ++) {\n+    limits[i] = 0;\n+  }\n+\n+  \/\/ We are done if the option is not given.\n+  if (MallocLimit == nullptr) {\n+    return;\n+  }\n+\n+  \/\/ Global form?\n+  if (parse_malloc_limit_size(MallocLimit, total_limit)) {\n+    return;\n+  }\n+\n+  \/\/ No. So it must be in category-specific form: MallocLimit=<nmt category>:<size>[,<nmt category>:<size> ..]\n+  char* copy = os::strdup(MallocLimit);\n+  if (copy == nullptr) {\n+    vm_exit_out_of_memory(strlen(MallocLimit), OOM_MALLOC_ERROR, \"MallocLimit\");\n+  }\n+\n+  char* p = copy, *q;\n+  do {\n+    q = p;\n+    p = ::strchr(q, ',');\n+    if (p != nullptr) {\n+      *p = '\\0';\n+      p ++;\n+    }\n+    parse_single_category_limit(q, limits);\n+  } while (p != nullptr);\n+\n+  os::free(copy);\n+\n+}\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":83,"deletions":5,"binary":false,"changes":88,"status":"modified"},{"patch":"@@ -273,1 +273,1 @@\n-          \"Reclamation of zombie and not-entrant methods\")                  \\\n+          \"Reclamation of compiled methods\")                                \\\n@@ -382,1 +382,1 @@\n-          \"Create zombies (non-entrant) at exit from the runtime system\")   \\\n+          \"Create non-entrant nmethods at exit from the runtime system\")    \\\n@@ -514,0 +514,4 @@\n+  develop(intx, TraceDwarfLevel, 0,                                         \\\n+          \"Debug levels for the dwarf parser\")                              \\\n+          range(0, 4)                                                       \\\n+                                                                            \\\n@@ -976,6 +980,0 @@\n-  product(bool, LogTouchedMethods, false, DIAGNOSTIC,                       \\\n-          \"Log methods which have been ever touched in runtime\")            \\\n-                                                                            \\\n-  product(bool, PrintTouchedMethodsAtExit, false, DIAGNOSTIC,               \\\n-          \"Print all methods that have been ever touched in runtime\")       \\\n-                                                                            \\\n@@ -985,3 +983,0 @@\n-  develop(bool, PrintMethodFlushing, false,                                 \\\n-          \"Print the nmethods being flushed\")                               \\\n-                                                                            \\\n@@ -991,5 +986,0 @@\n-  product(intx, HotMethodDetectionLimit, 100000, DIAGNOSTIC,                \\\n-          \"Number of compiled code invocations after which \"                \\\n-          \"the method is considered as hot by the flusher\")                 \\\n-          range(1, max_jint)                                                \\\n-                                                                            \\\n@@ -1001,6 +991,0 @@\n-  product(bool, UseCodeAging, true,                                         \\\n-          \"Insert counter to detect warm methods\")                          \\\n-                                                                            \\\n-  product(bool, StressCodeAging, false, DIAGNOSTIC,                         \\\n-          \"Start with counters compiled in\")                                \\\n-                                                                            \\\n@@ -1097,3 +1081,0 @@\n-  develop(bool, TraceCreateZombies, false,                                  \\\n-          \"trace creation of zombie nmethods\")                              \\\n-                                                                            \\\n@@ -1316,1 +1297,4 @@\n-  product(intx, NmethodSweepActivity, 10,                                   \\\n+  product(bool, UseSystemMemoryBarrier, false, EXPERIMENTAL,                \\\n+          \"Try to enable system memory barrier\")                            \\\n+                                                                            \\\n+  product(intx, NmethodSweepActivity, 4,                                    \\\n@@ -1321,6 +1305,0 @@\n-  notproduct(bool, LogSweeper, false,                                       \\\n-          \"Keep a ring buffer of sweeper activity\")                         \\\n-                                                                            \\\n-  notproduct(intx, SweeperLogEntries, 1024,                                 \\\n-          \"Number of records in the ring buffer of sweeper activity\")       \\\n-                                                                            \\\n@@ -1368,0 +1346,10 @@\n+  product(ccstr, MallocLimit, nullptr, DIAGNOSTIC,                          \\\n+          \"Limit malloc allocation size from VM. Reaching the limit will \"  \\\n+          \"trigger a fatal error. This feature requires \"                   \\\n+          \"NativeMemoryTracking=summary or NativeMemoryTracking=detail.\"    \\\n+          \"Usage:\"                                                          \\\n+          \"- MallocLimit=<size> to set a total limit. \"                     \\\n+          \"- MallocLimit=<NMT category>:<size>[,<NMT category>:<size>...] \" \\\n+          \"  to set one or more category-specific limits.\"                  \\\n+          \"Example: -XX:MallocLimit=compiler:500m\")                         \\\n+                                                                            \\\n@@ -1587,2 +1575,2 @@\n-  product(double, SweeperThreshold, 0.5,                                    \\\n-          \"Threshold controlling when code cache sweeper is invoked.\"       \\\n+  product(double, SweeperThreshold, 15.0,                                   \\\n+          \"Threshold when a code cache unloading GC is invoked.\"            \\\n@@ -1800,33 +1788,0 @@\n-  \/* Shared spaces *\/                                                       \\\n-                                                                            \\\n-  product(bool, VerifySharedSpaces, false,                                  \\\n-          \"Verify integrity of shared spaces\")                              \\\n-                                                                            \\\n-  product(bool, RecordDynamicDumpInfo, false,                               \\\n-          \"Record class info for jcmd VM.cds dynamic_dump\")                 \\\n-                                                                            \\\n-  product(bool, AutoCreateSharedArchive, false,                             \\\n-          \"Create shared archive at exit if cds mapping failed\")            \\\n-                                                                            \\\n-  product(bool, PrintSharedArchiveAndExit, false,                           \\\n-          \"Print shared archive file contents\")                             \\\n-                                                                            \\\n-  product(bool, PrintSharedDictionary, false,                               \\\n-          \"If PrintSharedArchiveAndExit is true, also print the shared \"    \\\n-          \"dictionary\")                                                     \\\n-                                                                            \\\n-  product(size_t, SharedBaseAddress, LP64_ONLY(32*G)                        \\\n-          NOT_LP64(LINUX_ONLY(2*G) NOT_LINUX(0)),                           \\\n-          \"Address to allocate shared memory region for class data\")        \\\n-          range(0, SIZE_MAX)                                                \\\n-                                                                            \\\n-  product(ccstr, SharedArchiveConfigFile, NULL,                             \\\n-          \"Data to add to the CDS archive file\")                            \\\n-                                                                            \\\n-  product(uint, SharedSymbolTableBucketSize, 4,                             \\\n-          \"Average number of symbols per bucket in shared table\")           \\\n-          range(2, 246)                                                     \\\n-                                                                            \\\n-  product(bool, AllowArchivingWithJavaAgent, false, DIAGNOSTIC,             \\\n-          \"Allow Java agent to be run with CDS dumping\")                    \\\n-                                                                            \\\n@@ -1936,24 +1891,0 @@\n-  product(ccstr, DumpLoadedClassList, NULL,                                 \\\n-          \"Dump the names all loaded classes, that could be stored into \"   \\\n-          \"the CDS archive, in the specified file\")                         \\\n-                                                                            \\\n-  product(ccstr, SharedClassListFile, NULL,                                 \\\n-          \"Override the default CDS class list\")                            \\\n-                                                                            \\\n-  product(ccstr, SharedArchiveFile, NULL,                                   \\\n-          \"Override the default location of the CDS archive file\")          \\\n-                                                                            \\\n-  product(ccstr, ArchiveClassesAtExit, NULL,                                \\\n-          \"The path and name of the dynamic archive file\")                  \\\n-                                                                            \\\n-  product(ccstr, ExtraSharedClassListFile, NULL,                            \\\n-          \"Extra classlist for building the CDS archive file\")              \\\n-                                                                            \\\n-  product(int, ArchiveRelocationMode, 0, DIAGNOSTIC,                        \\\n-           \"(0) first map at preferred address, and if \"                    \\\n-           \"unsuccessful, map at alternative address (default); \"           \\\n-           \"(1) always map at alternative address; \"                        \\\n-           \"(2) always map at preferred address, and if unsuccessful, \"     \\\n-           \"do not map the archive\")                                        \\\n-           range(0, 2)                                                      \\\n-                                                                            \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":22,"deletions":91,"binary":false,"changes":113,"status":"modified"},{"patch":"@@ -687,1 +687,1 @@\n-    ss->print(\"owner=\" INTPTR_FORMAT, NULL);\n+    ss->print(\"owner=\" INTPTR_FORMAT, NULL_WORD);\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -478,3 +478,0 @@\n-\n-    \/\/ Handle ^BREAK\n-    os::signal(SIGBREAK, os::user_handler());\n@@ -942,0 +939,5 @@\n+void os::print_tos(outputStream* st, address sp) {\n+  st->print_cr(\"Top of Stack: (sp=\" PTR_FORMAT \")\", p2i(sp));\n+  print_hex_dump(st, sp, sp + 512, sizeof(intptr_t));\n+}\n+\n@@ -1083,1 +1085,1 @@\n-  CodeBlob* b = CodeCache::find_blob_unsafe(addr);\n+  CodeBlob* b = CodeCache::find_blob(addr);\n@@ -1908,1 +1910,1 @@\n-  assert(is_power_of_2(page_size), \"page_size must be a power of 2: \" SIZE_FORMAT_HEX, page_size);\n+  assert(is_power_of_2(page_size), \"page_size must be a power of 2: \" SIZE_FORMAT_X, page_size);\n@@ -1913,1 +1915,1 @@\n-  assert(is_power_of_2(page_size), \"page_size must be a power of 2: \" SIZE_FORMAT_HEX, page_size);\n+  assert(is_power_of_2(page_size), \"page_size must be a power of 2: \" SIZE_FORMAT_X, page_size);\n@@ -1918,1 +1920,1 @@\n-  assert(is_power_of_2(page_size), \"page_size must be a power of 2: \" SIZE_FORMAT_HEX, page_size);\n+  assert(is_power_of_2(page_size), \"page_size must be a power of 2: \" SIZE_FORMAT_X, page_size);\n@@ -1927,1 +1929,1 @@\n-  assert(is_power_of_2(page_size), \"page_size must be a power of 2: \" SIZE_FORMAT_HEX, page_size);\n+  assert(is_power_of_2(page_size), \"page_size must be a power of 2: \" SIZE_FORMAT_X, page_size);\n","filename":"src\/hotspot\/share\/runtime\/os.cpp","additions":10,"deletions":8,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -177,2 +177,1 @@\n-RuntimeStub* StubRoutines::_cont_doYield_stub = NULL;\n-address StubRoutines::_cont_doYield       = NULL;\n+\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -254,2 +254,0 @@\n-  static RuntimeStub* _cont_doYield_stub;\n-  static address _cont_doYield;\n@@ -436,2 +434,1 @@\n-  static RuntimeStub* cont_doYield_stub() { return _cont_doYield_stub; }\n-  static address cont_doYield()        { return _cont_doYield; }\n+\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.hpp","additions":1,"deletions":4,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -61,0 +61,1 @@\n+#include \"utilities\/linkedlist.hpp\"\n@@ -63,0 +64,5 @@\n+class ObjectMonitorsHashtable::PtrList :\n+  public LinkedListImpl<ObjectMonitor*,\n+                        ResourceObj::C_HEAP, mtThread,\n+                        AllocFailStrategy::RETURN_NULL> {};\n+\n@@ -82,1 +88,1 @@\n-    list = new (ResourceObj::C_HEAP, mtThread) ObjectMonitorsHashtable::PtrList();\n+    list = new (ResourceObj::C_HEAP, mtThread) ObjectMonitorsHashtable::PtrList;\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -32,2 +32,0 @@\n-#include \"utilities\/growableArray.hpp\"\n-#include \"utilities\/linkedlist.hpp\"\n@@ -36,0 +34,1 @@\n+template <typename T> class GrowableArray;\n@@ -52,3 +51,1 @@\n-  typedef LinkedListImpl<ObjectMonitor*,\n-                         ResourceObj::C_HEAP, mtThread,\n-                         AllocFailStrategy::RETURN_NULL> PtrList;\n+  class PtrList;\n@@ -56,0 +53,1 @@\n+ private:\n@@ -60,1 +58,0 @@\n- private:\n@@ -69,1 +66,1 @@\n-  ObjectMonitorsHashtable() : _ptrs(new (ResourceObj::C_HEAP, mtThread) PtrTable()), _key_count(0), _om_count(0) {}\n+  ObjectMonitorsHashtable() : _ptrs(new (ResourceObj::C_HEAP, mtThread) PtrTable), _key_count(0), _om_count(0) {}\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.hpp","additions":5,"deletions":8,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -101,1 +101,0 @@\n-  template(DumpTouchedMethods)                    \\\n","filename":"src\/hotspot\/share\/runtime\/vmOperation.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -110,1 +110,0 @@\n-#include \"utilities\/hashtable.hpp\"\n@@ -290,1 +289,0 @@\n-  nonstatic_field(MethodCounters,              _nmethod_age,                                  int)                                   \\\n@@ -664,3 +662,1 @@\n-  volatile_nonstatic_field(nmethod,            _lock_count,                                   jint)                                  \\\n-  volatile_nonstatic_field(nmethod,            _stack_traversal_mark,                         int64_t)                               \\\n-  nonstatic_field(nmethod,                     _comp_level,                                   int)                                   \\\n+  nonstatic_field(nmethod,                     _comp_level,                                   CompLevel)                             \\\n@@ -1320,1 +1316,0 @@\n-        declare_type(CodeCacheSweeperThread, JavaThread)                  \\\n@@ -1655,1 +1650,0 @@\n-  declare_c2_type(LoadPLockedNode, LoadPNode)                             \\\n@@ -1658,2 +1652,0 @@\n-  declare_c2_type(StorePConditionalNode, LoadStoreNode)                   \\\n-  declare_c2_type(StoreLConditionalNode, LoadStoreNode)                   \\\n@@ -1976,0 +1968,2 @@\n+                                                                          \\\n+  declare_integer_type(CompLevel)                                         \\\n@@ -2555,2 +2549,1 @@\n-  declare_constant(RegisterImpl::number_of_registers)                     \\\n-  declare_preprocessor_constant(\"REG_COUNT\", REG_COUNT)                \\\n+  declare_preprocessor_constant(\"REG_COUNT\", REG_COUNT)                   \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":4,"deletions":11,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -93,1 +93,9 @@\n-#define BOOL_TO_STR(_b_) ((_b_) ? \"true\" : \"false\")\n+\/\/ Guide to the suffixes used in the format specifiers for integers:\n+\/\/        - print the decimal value:                   745565\n+\/\/  _X    - print as hexadecimal, without leading 0s: 0x12345\n+\/\/  _X_0  - print as hexadecimal, with leading 0s: 0x00012345\n+\/\/  _W(w) - prints w sized string with the given value right\n+\/\/          adjusted. Use -w to print left adjusted.\n+\/\/\n+\/\/ Note that the PTR format specifiers print using 0x with leading zeros,\n+\/\/ just like the _X_0 version for integers.\n@@ -95,5 +103,3 @@\n-\/\/ Format 32-bit quantities.\n-#define INT32_FORMAT           \"%\" PRId32\n-#define UINT32_FORMAT          \"%\" PRIu32\n-#define INT32_FORMAT_W(width)  \"%\" #width PRId32\n-#define UINT32_FORMAT_W(width) \"%\" #width PRIu32\n+\/\/ Format 8-bit quantities.\n+#define INT8_FORMAT_X_0          \"0x%02\"      PRIx8\n+#define UINT8_FORMAT_X_0         \"0x%02\"      PRIx8\n@@ -101,2 +107,11 @@\n-#define PTR32_FORMAT           \"0x%08\" PRIx32\n-#define PTR32_FORMAT_W(width)  \"0x%\" #width PRIx32\n+\/\/ Format 16-bit quantities.\n+#define INT16_FORMAT_X_0         \"0x%04\"      PRIx16\n+#define UINT16_FORMAT_X_0        \"0x%04\"      PRIx16\n+\n+\/\/ Format 32-bit quantities.\n+#define INT32_FORMAT             \"%\"          PRId32\n+#define INT32_FORMAT_X_0         \"0x%08\"      PRIx32\n+#define INT32_FORMAT_W(width)    \"%\"   #width PRId32\n+#define UINT32_FORMAT            \"%\"          PRIu32\n+#define UINT32_FORMAT_X_0        \"0x%08\"      PRIx32\n+#define UINT32_FORMAT_W(width)   \"%\"   #width PRIu32\n@@ -105,6 +120,20 @@\n-#define INT64_FORMAT           \"%\" PRId64\n-#define UINT64_FORMAT          \"%\" PRIu64\n-#define UINT64_FORMAT_X        \"%\" PRIx64\n-#define INT64_FORMAT_W(width)  \"%\" #width PRId64\n-#define UINT64_FORMAT_W(width) \"%\" #width PRIu64\n-#define UINT64_FORMAT_X_W(width) \"%\" #width PRIx64\n+#define INT64_FORMAT             \"%\"          PRId64\n+#define INT64_FORMAT_X           \"0x%\"        PRIx64\n+#define INT64_FORMAT_X_0         \"0x%016\"     PRIx64\n+#define INT64_FORMAT_W(width)    \"%\"   #width PRId64\n+#define UINT64_FORMAT            \"%\"          PRIu64\n+#define UINT64_FORMAT_X          \"0x%\"        PRIx64\n+#define UINT64_FORMAT_X_0        \"0x%016\"     PRIx64\n+#define UINT64_FORMAT_W(width)   \"%\"   #width PRIu64\n+\n+\/\/ Format integers which change size between 32- and 64-bit.\n+#define SSIZE_FORMAT             \"%\"          PRIdPTR\n+#define SSIZE_FORMAT_W(width)    \"%\"   #width PRIdPTR\n+#define SIZE_FORMAT              \"%\"          PRIuPTR\n+#define SIZE_FORMAT_X            \"0x%\"        PRIxPTR\n+#ifdef _LP64\n+#define SIZE_FORMAT_X_0          \"0x%016\"     PRIxPTR\n+#else\n+#define SIZE_FORMAT_X_0          \"0x%08\"      PRIxPTR\n+#endif\n+#define SIZE_FORMAT_W(width)     \"%\"   #width PRIuPTR\n@@ -112,1 +141,6 @@\n-#define PTR64_FORMAT           \"0x%016\" PRIx64\n+#define INTX_FORMAT              \"%\"          PRIdPTR\n+#define INTX_FORMAT_X            \"0x%\"        PRIxPTR\n+#define INTX_FORMAT_W(width)     \"%\"   #width PRIdPTR\n+#define UINTX_FORMAT             \"%\"          PRIuPTR\n+#define UINTX_FORMAT_X           \"0x%\"        PRIxPTR\n+#define UINTX_FORMAT_W(width)    \"%\"   #width PRIuPTR\n@@ -116,1 +150,1 @@\n-#define JLONG_FORMAT           INT64_FORMAT\n+#define JLONG_FORMAT             INT64_FORMAT\n@@ -119,1 +153,1 @@\n-#define JLONG_FORMAT_W(width)  INT64_FORMAT_W(width)\n+#define JLONG_FORMAT_W(width)    INT64_FORMAT_W(width)\n@@ -122,1 +156,1 @@\n-#define JULONG_FORMAT          UINT64_FORMAT\n+#define JULONG_FORMAT            UINT64_FORMAT\n@@ -125,1 +159,1 @@\n-#define JULONG_FORMAT_X        UINT64_FORMAT_X\n+#define JULONG_FORMAT_X          UINT64_FORMAT_X\n@@ -130,2 +164,2 @@\n-#define INTPTR_FORMAT \"0x%016\" PRIxPTR\n-#define PTR_FORMAT    \"0x%016\" PRIxPTR\n+#define INTPTR_FORMAT            \"0x%016\"     PRIxPTR\n+#define PTR_FORMAT               \"0x%016\"     PRIxPTR\n@@ -133,2 +167,2 @@\n-#define INTPTR_FORMAT \"0x%08\"  PRIxPTR\n-#define PTR_FORMAT    \"0x%08\"  PRIxPTR\n+#define INTPTR_FORMAT            \"0x%08\"      PRIxPTR\n+#define PTR_FORMAT               \"0x%08\"      PRIxPTR\n@@ -137,17 +171,0 @@\n-\/\/ Format pointers without leading zeros\n-#define INTPTRNZ_FORMAT \"0x%\"  PRIxPTR\n-\n-#define INTPTR_FORMAT_W(width)   \"%\" #width PRIxPTR\n-\n-#define SSIZE_FORMAT             \"%\"   PRIdPTR\n-#define SIZE_FORMAT              \"%\"   PRIuPTR\n-#define SIZE_FORMAT_HEX          \"0x%\" PRIxPTR\n-#define SSIZE_FORMAT_W(width)    \"%\"   #width PRIdPTR\n-#define SIZE_FORMAT_W(width)     \"%\"   #width PRIuPTR\n-#define SIZE_FORMAT_HEX_W(width) \"0x%\" #width PRIxPTR\n-\n-#define INTX_FORMAT           \"%\" PRIdPTR\n-#define UINTX_FORMAT          \"%\" PRIuPTR\n-#define INTX_FORMAT_W(width)  \"%\" #width PRIdPTR\n-#define UINTX_FORMAT_W(width) \"%\" #width PRIuPTR\n-\n@@ -159,0 +176,1 @@\n+#define BOOL_TO_STR(_b_) ((_b_) ? \"true\" : \"false\")\n@@ -1126,1 +1144,0 @@\n-\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":58,"deletions":41,"binary":false,"changes":99,"status":"modified"},{"patch":"@@ -47,1 +47,1 @@\n-#include \"runtime\/os.hpp\"\n+#include \"runtime\/os.inline.hpp\"\n@@ -188,2 +188,0 @@\n-    char separator = os::file_separator()[0];\n-    const char *p = strrchr(_filename, separator);\n@@ -192,1 +190,1 @@\n-                         p ? p + 1 : _filename, _lineno,\n+                         get_filename_only(), _lineno,\n@@ -353,1 +351,1 @@\n-void VMError::print_native_stack(outputStream* st, frame fr, Thread* t, char* buf, int buf_size) {\n+void VMError::print_native_stack(outputStream* st, frame fr, Thread* t, bool print_source_info, char* buf, int buf_size) {\n@@ -363,1 +361,1 @@\n-        char buf[128];\n+        char filename[128];\n@@ -365,2 +363,6 @@\n-        if (Decoder::get_source_info(fr.pc(), buf, sizeof(buf), &line_no)) {\n-          st->print(\"  (%s:%d)\", buf, line_no);\n+        if (count == 1 && _lineno != 0) {\n+          \/\/ We have source information of the first frame for internal errors. There is no need to parse it from the symbols.\n+          st->print(\"  (%s:%d)\", get_filename_only(), _lineno);\n+        } else if (print_source_info &&\n+                   Decoder::get_source_info(fr.pc(), filename, sizeof(filename), &line_no, count != 1)) {\n+          st->print(\"  (%s:%d)\", filename, line_no);\n@@ -538,0 +540,2 @@\n+  static bool print_native_stack_succeeded = false;\n+\n@@ -674,4 +678,2 @@\n-         \/\/ In product mode chop off pathname?\n-         char separator = os::file_separator()[0];\n-         const char *p = strrchr(_filename, separator);\n-         const char *file = p ? p+1 : _filename;\n+         \/\/ In product mode chop off pathname\n+         const char *file = get_filename_only();\n@@ -837,1 +839,1 @@\n-  STEP(\"printing native stack\")\n+  STEP(\"printing native stack (with source info)\")\n@@ -847,1 +849,1 @@\n-       print_native_stack(st, fr, _thread, buf, sizeof(buf));\n+       print_native_stack(st, fr, _thread, true, buf, sizeof(buf));\n@@ -850,0 +852,11 @@\n+     print_native_stack_succeeded = true;\n+   }\n+\n+  STEP(\"retry printing native stack (no source info)\")\n+\n+   if (_verbose && !print_native_stack_succeeded) {\n+     st->cr();\n+     st->print_cr(\"Retrying call stack printing without source information...\");\n+     frame fr = _context ? os::fetch_frame_from_context(_context) : os::current_frame();\n+     print_native_stack(st, fr, _thread, false, buf, sizeof(buf));\n+     _print_native_stack_used = true;\n@@ -997,1 +1010,0 @@\n-#ifndef _WIN32\n@@ -1001,1 +1013,1 @@\n-       os::Posix::print_user_info(st);\n+       os::print_user_info(st);\n@@ -1003,1 +1015,0 @@\n-#endif\n@@ -1080,1 +1091,1 @@\n-         st->print_cr(\"Polling page: \" INTPTR_FORMAT, p2i(SafepointMechanism::get_polling_page()));\n+         st->print_cr(\"Polling page: \" PTR_FORMAT, p2i(SafepointMechanism::get_polling_page()));\n@@ -1162,1 +1173,0 @@\n-#ifndef _WIN32\n@@ -1166,1 +1176,1 @@\n-       os::Posix::print_active_locale(st);\n+       os::print_active_locale(st);\n@@ -1169,1 +1179,0 @@\n-#endif\n@@ -1299,1 +1308,1 @@\n-    st->print_cr(\"Polling page: \" INTPTR_FORMAT, p2i(SafepointMechanism::get_polling_page()));\n+    st->print_cr(\"Polling page: \" PTR_FORMAT, p2i(SafepointMechanism::get_polling_page()));\n@@ -1353,2 +1362,2 @@\n-#ifndef _WIN32\n-  os::Posix::print_active_locale(st);\n+\n+  os::print_active_locale(st);\n@@ -1356,1 +1365,1 @@\n-#endif\n+\n@@ -1707,1 +1716,1 @@\n-  static bool skip_replay = ReplayCompiles; \/\/ Do not overwrite file during replay\n+  static bool skip_replay = ReplayCompiles && !ReplayReduce; \/\/ Do not overwrite file during replay\n","filename":"src\/hotspot\/share\/utilities\/vmError.cpp","additions":35,"deletions":26,"binary":false,"changes":61,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -67,0 +67,1 @@\n+  private long pageSize;\n@@ -180,0 +181,1 @@\n+    this.pageSize = pageSize;\n@@ -235,5 +237,9 @@\n-  \/** May be called by subclasses directly but may not be overridden *\/\n-  protected final void writeBytes(long address, long numBytes, byte[] data)\n-    throws UnmappedAddressException, DebuggerException {\n-    if (cache != null) {\n-      cache.clear(address, numBytes);\n+  \/** If an address for a 64-bit value starts on the last 32-bit word of a\n+      page, then we can't use the page cache to read it because it will cause\n+      an ArrayIndexOutOfBoundsException when reading past the end of the page. *\/\n+  private boolean canUsePageCacheFor64bitRead(long address) {\n+    long pageMask = ~(pageSize - 1);\n+    if ((address & pageMask) != ((address + 4) & pageMask)) {\n+      \/\/ This address starts on the last 32-bit word of the page.\n+      \/\/ Cannot use the page cache in that case.\n+      return false;\n@@ -241,1 +247,1 @@\n-    writeBytesToProcess(address, numBytes, data);\n+    return true;\n@@ -268,2 +274,0 @@\n-  \/\/ NOTE: assumes value does not span pages (may be bad assumption on\n-  \/\/ Solaris\/x86; see unalignedAccessesOkay in DbxDebugger hierarchy)\n@@ -282,2 +286,0 @@\n-  \/\/ NOTE: assumes value does not span pages (may be bad assumption on\n-  \/\/ Solaris\/x86; see unalignedAccessesOkay in DbxDebugger hierarchy)\n@@ -288,1 +290,1 @@\n-    if (useFastAccessors) {\n+    if (useFastAccessors && canUsePageCacheFor64bitRead(address)) {\n@@ -296,2 +298,0 @@\n-  \/\/ NOTE: assumes value does not span pages (may be bad assumption on\n-  \/\/ Solaris\/x86; see unalignedAccessesOkay in DbxDebugger hierarchy)\n@@ -310,2 +310,0 @@\n-  \/\/ NOTE: assumes value does not span pages (may be bad assumption on\n-  \/\/ Solaris\/x86; see unalignedAccessesOkay in DbxDebugger hierarchy)\n@@ -324,2 +322,0 @@\n-  \/\/ NOTE: assumes value does not span pages (may be bad assumption on\n-  \/\/ Solaris\/x86; see unalignedAccessesOkay in DbxDebugger hierarchy)\n@@ -330,1 +326,1 @@\n-    if (useFastAccessors) {\n+    if (useFastAccessors && canUsePageCacheFor64bitRead(address)) {\n@@ -338,2 +334,0 @@\n-  \/\/ NOTE: assumes value does not span pages (may be bad assumption on\n-  \/\/ Solaris\/x86; see unalignedAccessesOkay in DbxDebugger hierarchy)\n@@ -352,2 +346,0 @@\n-  \/\/ NOTE: assumes value does not span pages (may be bad assumption on\n-  \/\/ Solaris\/x86; see unalignedAccessesOkay in DbxDebugger hierarchy)\n@@ -358,1 +350,1 @@\n-    if (useFastAccessors) {\n+    if (useFastAccessors && (numBytes != 8 || canUsePageCacheFor64bitRead(address))) {\n@@ -388,72 +380,0 @@\n-  public void writeJBoolean(long address, boolean value)\n-    throws UnmappedAddressException, UnalignedAddressException {\n-    checkJavaConfigured();\n-    utils.checkAlignment(address, jbooleanSize);\n-    byte[] data = utils.jbooleanToData(value);\n-    writeBytes(address, jbooleanSize, data);\n-  }\n-\n-  public void writeJByte(long address, byte value)\n-    throws UnmappedAddressException, UnalignedAddressException {\n-    checkJavaConfigured();\n-    utils.checkAlignment(address, jbyteSize);\n-    byte[] data = utils.jbyteToData(value);\n-    writeBytes(address, jbyteSize, data);\n-  }\n-\n-  public void writeJChar(long address, char value)\n-    throws UnmappedAddressException, UnalignedAddressException {\n-    checkJavaConfigured();\n-    utils.checkAlignment(address, jcharSize);\n-    byte[] data = utils.jcharToData(value);\n-    writeBytes(address, jcharSize, data);\n-  }\n-\n-  public void writeJDouble(long address, double value)\n-    throws UnmappedAddressException, UnalignedAddressException {\n-    checkJavaConfigured();\n-    utils.checkAlignment(address, jdoubleSize);\n-    byte[] data = utils.jdoubleToData(value);\n-    writeBytes(address, jdoubleSize, data);\n-  }\n-\n-  public void writeJFloat(long address, float value)\n-    throws UnmappedAddressException, UnalignedAddressException {\n-    checkJavaConfigured();\n-    utils.checkAlignment(address, jfloatSize);\n-    byte[] data = utils.jfloatToData(value);\n-    writeBytes(address, jfloatSize, data);\n-  }\n-\n-  public void writeJInt(long address, int value)\n-    throws UnmappedAddressException, UnalignedAddressException {\n-    checkJavaConfigured();\n-    utils.checkAlignment(address, jintSize);\n-    byte[] data = utils.jintToData(value);\n-    writeBytes(address, jintSize, data);\n-  }\n-\n-  public void writeJLong(long address, long value)\n-    throws UnmappedAddressException, UnalignedAddressException {\n-    checkJavaConfigured();\n-    utils.checkAlignment(address, jlongSize);\n-    byte[] data = utils.jlongToData(value);\n-    writeBytes(address, jlongSize, data);\n-  }\n-\n-  public void writeJShort(long address, short value)\n-    throws UnmappedAddressException, UnalignedAddressException {\n-    checkJavaConfigured();\n-    utils.checkAlignment(address, jshortSize);\n-    byte[] data = utils.jshortToData(value);\n-    writeBytes(address, jshortSize, data);\n-  }\n-\n-  public void writeCInteger(long address, long numBytes, long value)\n-    throws UnmappedAddressException, UnalignedAddressException {\n-    checkConfigured();\n-    utils.checkAlignment(address, numBytes);\n-    byte[] data = utils.cIntegerToData(numBytes, value);\n-    writeBytes(address, numBytes, data);\n-  }\n-\n@@ -489,5 +409,0 @@\n-  protected void writeAddressValue(long address, long value)\n-    throws UnmappedAddressException, UnalignedAddressException {\n-    writeCInteger(address, machDesc.getAddressSize(), value);\n-  }\n-\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/debugger\/DebuggerBase.java","additions":16,"deletions":101,"binary":false,"changes":117,"status":"modified"},{"patch":"@@ -186,1 +186,1 @@\n-    final int nmethodCompLevelOffset = getFieldOffset(\"nmethod::_comp_level\", Integer.class, \"int\");\n+    final int nmethodCompLevelOffset = getFieldOffset(\"nmethod::_comp_level\", Integer.class, \"CompLevel\");\n","filename":"src\/jdk.internal.vm.ci\/share\/classes\/jdk.vm.ci.hotspot\/src\/jdk\/vm\/ci\/hotspot\/HotSpotVMConfig.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -93,0 +93,1 @@\n+runtime\/handshake\/HandshakeSuspendExitTest.java 8294313 generic-all\n@@ -123,0 +124,1 @@\n+serviceability\/attach\/ConcAttachTest.java 8290043 linux-all\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -145,0 +145,2 @@\n+    public static final String CMOVEVF = START + \"CMoveVF\" + MID + END;\n+    public static final String CMOVEVD = START + \"CMoveVD\" + MID + END;\n@@ -179,0 +181,1 @@\n+    public static final String CMP_I = START + \"CmpI\" + MID + END;\n@@ -214,0 +217,2 @@\n+    public static final String Min_I = START + \"MinI\" + MID + END;\n+    public static final String Max_I = START + \"MaxI\" + MID + END;\n@@ -216,0 +221,6 @@\n+    public static final String MUL_VL = START + \"MulVL\" + MID + END;\n+\n+    public static final String ADD_REDUCTION_VF = START + \"AddReductionVF\" + MID + END;\n+    public static final String ADD_REDUCTION_VD = START + \"AddReductionVD\" + MID + END;\n+    public static final String MUL_REDUCTION_VF = START + \"MulReductionVF\" + MID + END;\n+    public static final String MUL_REDUCTION_VD = START + \"MulReductionVD\" + MID + END;\n@@ -220,0 +231,2 @@\n+    public static final String POPULATE_INDEX = START + \"PopulateIndex\" + MID + END;\n+\n","filename":"test\/hotspot\/jtreg\/compiler\/lib\/ir_framework\/IRNode.java","additions":13,"deletions":0,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -26,1 +26,1 @@\n-import java.lang.management.GarbageCollectorMXBean;\n+import java.lang.invoke.VarHandle;\n@@ -186,0 +186,35 @@\n+class PerfCounterSnapshot {\n+    private static long getMinCapacity(String ns) throws Exception {\n+        return PerfCounters.findByName(ns + \".minCapacity\").longValue();\n+    }\n+\n+    private static long getCapacity(String ns) throws Exception {\n+        return PerfCounters.findByName(ns + \".capacity\").longValue();\n+    }\n+\n+    private static long getMaxCapacity(String ns) throws Exception {\n+        return PerfCounters.findByName(ns + \".maxCapacity\").longValue();\n+    }\n+\n+    private static long getUsed(String ns) throws Exception {\n+        return PerfCounters.findByName(ns + \".used\").longValue();\n+    }\n+\n+    public long minCapacity;\n+    public long maxCapacity;\n+    public long capacity;\n+    public long used;\n+\n+    public void get(String ns) throws Exception {\n+        minCapacity = getMinCapacity(ns);\n+        maxCapacity = getMaxCapacity(ns);\n+        used = getUsed(ns);\n+        capacity = getCapacity(ns);\n+    }\n+\n+    public boolean consistentWith(PerfCounterSnapshot other) {\n+        return (minCapacity == other.minCapacity) && (maxCapacity == other.maxCapacity) &&\n+            (used == other.used) && (capacity == other.capacity);\n+    }\n+}\n+\n@@ -189,1 +224,0 @@\n-    private static final List<GarbageCollectorMXBean> gcBeans = ManagementFactoryHelper.getGarbageCollectorMXBeans();\n@@ -207,26 +241,22 @@\n-        long gcCountBefore;\n-        long gcCountAfter;\n-        long minCapacity;\n-        long maxCapacity;\n-        long capacity;\n-        long used;\n-\n-        \/\/ The perf counter values are updated during GC and to be able to\n-        \/\/ do the assertions below we need to ensure that the values are from\n-        \/\/ the same GC cycle.\n-        do {\n-            gcCountBefore = currentGCCount();\n-\n-            minCapacity = getMinCapacity(ns);\n-            maxCapacity = getMaxCapacity(ns);\n-            capacity = getCapacity(ns);\n-            used = getUsed(ns);\n-\n-            gcCountAfter = currentGCCount();\n-            assertGTE(gcCountAfter, gcCountBefore);\n-        } while(gcCountAfter > gcCountBefore);\n-\n-        assertGTE(minCapacity, 0L);\n-        assertGTE(used, minCapacity);\n-        assertGTE(capacity, used);\n-        assertGTE(maxCapacity, capacity);\n+        PerfCounterSnapshot snap1 = new PerfCounterSnapshot();\n+        PerfCounterSnapshot snap2 = new PerfCounterSnapshot();\n+\n+        final int MaxAttempts = 10;\n+\n+        for (int attempts = 0; ; attempts++) {\n+            snap1.get(ns);\n+            VarHandle.fullFence();\n+            snap2.get(ns);\n+\n+            if (snap1.consistentWith(snap2)) {\n+              \/\/ Got a consistent snapshot for examination.\n+              break;\n+            } else if (attempts == MaxAttempts) {\n+              throw new Exception(\"Failed to get stable reading of metaspace performance counters after \" + attempts + \" tries\");\n+            }\n+        }\n+\n+        assertGTE(snap1.minCapacity, 0L);\n+        assertGTE(snap1.used, snap1.minCapacity);\n+        assertGTE(snap1.capacity, snap1.used);\n+        assertGTE(snap1.maxCapacity, snap1.capacity);\n@@ -246,1 +276,2 @@\n-        long before = getUsed(ns);\n+        PerfCounterSnapshot before = new PerfCounterSnapshot();\n+        before.get(ns);\n@@ -249,1 +280,2 @@\n-        long after = getUsed(ns);\n+        PerfCounterSnapshot after = new PerfCounterSnapshot();\n+        after.get(ns);\n@@ -251,1 +283,1 @@\n-        assertGT(after, before);\n+        assertGT(after.used, before.used);\n@@ -270,24 +302,0 @@\n-\n-    private static long getMinCapacity(String ns) throws Exception {\n-        return PerfCounters.findByName(ns + \".minCapacity\").longValue();\n-    }\n-\n-    private static long getCapacity(String ns) throws Exception {\n-        return PerfCounters.findByName(ns + \".capacity\").longValue();\n-    }\n-\n-    private static long getMaxCapacity(String ns) throws Exception {\n-        return PerfCounters.findByName(ns + \".maxCapacity\").longValue();\n-    }\n-\n-    private static long getUsed(String ns) throws Exception {\n-        return PerfCounters.findByName(ns + \".used\").longValue();\n-    }\n-\n-    private static long currentGCCount() {\n-        long gcCount = 0;\n-        for (GarbageCollectorMXBean bean : gcBeans) {\n-            gcCount += bean.getCollectionCount();\n-        }\n-        return gcCount;\n-    }\n","filename":"test\/hotspot\/jtreg\/gc\/metaspace\/TestMetaspacePerfCounters.java","additions":63,"deletions":55,"binary":false,"changes":118,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -54,1 +54,1 @@\n-    String msg = \"OutOfMemoryError: Metaspace\";\n+    String msg = \"OutOfMemoryError: ((Metaspace)|(Compressed class space))\";\n@@ -56,1 +56,1 @@\n-    CDSTestUtils.executeAndLog(pb, \"dump\").shouldContain(msg).shouldHaveExitValue(1);\n+    CDSTestUtils.executeAndLog(pb, \"dump\").shouldMatch(msg).shouldHaveExitValue(1);\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/MaxMetaspaceSize.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -281,1 +281,1 @@\n- * @run main\/othervm -Xmx8g\n+ * @run main\/othervm\/timeout=240 -Xmx8g\n@@ -286,1 +286,1 @@\n- * @run main\/othervm -Xmx8g\n+ * @run main\/othervm\/timeout=240 -Xmx8g\n","filename":"test\/jdk\/java\/lang\/instrument\/GetObjectSizeIntrinsicsTest.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"}]}