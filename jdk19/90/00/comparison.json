{"files":[{"patch":"@@ -980,0 +980,73 @@\n+\/\/ Unpack an array argument into a pointer to the body and the length\n+\/\/ if the array is non-null, otherwise pass 0 for both.\n+static void unpack_array_argument(MacroAssembler* masm, VMRegPair reg, BasicType in_elem_type, VMRegPair body_arg, VMRegPair length_arg) { Unimplemented(); }\n+\n+\n+class ComputeMoveOrder: public StackObj {\n+  class MoveOperation: public ResourceObj {\n+    friend class ComputeMoveOrder;\n+   private:\n+    VMRegPair        _src;\n+    VMRegPair        _dst;\n+    int              _src_index;\n+    int              _dst_index;\n+    bool             _processed;\n+    MoveOperation*  _next;\n+    MoveOperation*  _prev;\n+\n+    static int get_id(VMRegPair r) { Unimplemented(); return 0; }\n+\n+   public:\n+    MoveOperation(int src_index, VMRegPair src, int dst_index, VMRegPair dst):\n+      _src(src)\n+    , _dst(dst)\n+    , _src_index(src_index)\n+    , _dst_index(dst_index)\n+    , _processed(false)\n+    , _next(NULL)\n+    , _prev(NULL) { Unimplemented(); }\n+\n+    VMRegPair src() const              { Unimplemented(); return _src; }\n+    int src_id() const                 { Unimplemented(); return 0; }\n+    int src_index() const              { Unimplemented(); return 0; }\n+    VMRegPair dst() const              { Unimplemented(); return _src; }\n+    void set_dst(int i, VMRegPair dst) { Unimplemented(); }\n+    int dst_index() const              { Unimplemented(); return 0; }\n+    int dst_id() const                 { Unimplemented(); return 0; }\n+    MoveOperation* next() const        { Unimplemented(); return 0; }\n+    MoveOperation* prev() const        { Unimplemented(); return 0; }\n+    void set_processed()               { Unimplemented(); }\n+    bool is_processed() const          { Unimplemented(); return 0; }\n+\n+    \/\/ insert\n+    void break_cycle(VMRegPair temp_register) { Unimplemented(); }\n+\n+    void link(GrowableArray<MoveOperation*>& killer) { Unimplemented(); }\n+  };\n+\n+ private:\n+  GrowableArray<MoveOperation*> edges;\n+\n+ public:\n+  ComputeMoveOrder(int total_in_args, VMRegPair* in_regs, int total_c_args, VMRegPair* out_regs,\n+                    BasicType* in_sig_bt, GrowableArray<int>& arg_order, VMRegPair tmp_vmreg) { Unimplemented(); }\n+\n+  \/\/ Collected all the move operations\n+  void add_edge(int src_index, VMRegPair src, int dst_index, VMRegPair dst) { Unimplemented(); }\n+\n+  \/\/ Walk the edges breaking cycles between moves.  The result list\n+  \/\/ can be walked in order to produce the proper set of loads\n+  GrowableArray<MoveOperation*>* get_store_order(VMRegPair temp_register) { Unimplemented(); return 0; }\n+};\n+\n+\n+static void rt_call(MacroAssembler* masm, address dest) {\n+  CodeBlob *cb = CodeCache::find_blob(dest);\n+  if (cb) {\n+    __ far_call(RuntimeAddress(dest));\n+  } else {\n+    __ lea(rscratch1, RuntimeAddress(dest));\n+    __ blr(rscratch1);\n+  }\n+}\n+\n@@ -1170,1 +1243,2 @@\n-                                                BasicType ret_type) {\n+                                                BasicType ret_type,\n+                                                address critical_entry) {\n@@ -1226,1 +1300,6 @@\n-  address native_func = method->native_function();\n+  bool is_critical_native = true;\n+  address native_func = critical_entry;\n+  if (native_func == NULL) {\n+    native_func = method->native_function();\n+    is_critical_native = false;\n+  }\n@@ -1240,1 +1319,13 @@\n-  int total_c_args = total_in_args + (method->is_static() ? 2 : 1);\n+  int total_c_args = total_in_args;\n+  if (!is_critical_native) {\n+    total_c_args += 1;\n+    if (method->is_static()) {\n+      total_c_args++;\n+    }\n+  } else {\n+    for (int i = 0; i < total_in_args; i++) {\n+      if (in_sig_bt[i] == T_ARRAY) {\n+        total_c_args++;\n+      }\n+    }\n+  }\n@@ -1247,4 +1338,5 @@\n-  out_sig_bt[argc++] = T_ADDRESS;\n-  if (method->is_static()) {\n-    out_sig_bt[argc++] = T_OBJECT;\n-  }\n+  if (!is_critical_native) {\n+    out_sig_bt[argc++] = T_ADDRESS;\n+    if (method->is_static()) {\n+      out_sig_bt[argc++] = T_OBJECT;\n+    }\n@@ -1252,2 +1344,24 @@\n-  for (int i = 0; i < total_in_args ; i++ ) {\n-    out_sig_bt[argc++] = in_sig_bt[i];\n+    for (int i = 0; i < total_in_args ; i++ ) {\n+      out_sig_bt[argc++] = in_sig_bt[i];\n+    }\n+  } else {\n+    in_elem_bt = NEW_RESOURCE_ARRAY(BasicType, total_in_args);\n+    SignatureStream ss(method->signature());\n+    for (int i = 0; i < total_in_args ; i++ ) {\n+      if (in_sig_bt[i] == T_ARRAY) {\n+        \/\/ Arrays are passed as int, elem* pair\n+        out_sig_bt[argc++] = T_INT;\n+        out_sig_bt[argc++] = T_ADDRESS;\n+        ss.skip_array_prefix(1);  \/\/ skip one '['\n+        assert(ss.is_primitive(), \"primitive type expected\");\n+        in_elem_bt[i] = ss.type();\n+      } else {\n+        out_sig_bt[argc++] = in_sig_bt[i];\n+        in_elem_bt[i] = T_VOID;\n+      }\n+      if (in_sig_bt[i] != T_VOID) {\n+        assert(in_sig_bt[i] == ss.type() ||\n+               in_sig_bt[i] == T_ARRAY, \"must match\");\n+        ss.next();\n+      }\n+    }\n@@ -1275,0 +1389,28 @@\n+  if (is_critical_native) {\n+    \/\/ Critical natives may have to call out so they need a save area\n+    \/\/ for register arguments.\n+    int double_slots = 0;\n+    int single_slots = 0;\n+    for ( int i = 0; i < total_in_args; i++) {\n+      if (in_regs[i].first()->is_Register()) {\n+        const Register reg = in_regs[i].first()->as_Register();\n+        switch (in_sig_bt[i]) {\n+          case T_BOOLEAN:\n+          case T_BYTE:\n+          case T_SHORT:\n+          case T_CHAR:\n+          case T_INT:  single_slots++; break;\n+          case T_ARRAY:  \/\/ specific to LP64 (7145024)\n+          case T_LONG: double_slots++; break;\n+          default:  ShouldNotReachHere();\n+        }\n+      } else if (in_regs[i].first()->is_FloatRegister()) {\n+        ShouldNotReachHere();\n+      }\n+    }\n+    total_save_slots = double_slots * 2 + single_slots;\n+    \/\/ align the save area\n+    if (double_slots != 0) {\n+      stack_slots = align_up(stack_slots, 2);\n+    }\n+  }\n@@ -1441,1 +1583,4 @@\n-  \/\/ For JNI natives the incoming and outgoing registers are offset upwards.\n+  \/\/ This may iterate in two different directions depending on the\n+  \/\/ kind of native it is.  The reason is that for regular JNI natives\n+  \/\/ the incoming and outgoing registers are offset upwards and for\n+  \/\/ critical natives they are offset down.\n@@ -1446,3 +1591,8 @@\n-  for (int i = total_in_args - 1, c_arg = total_c_args - 1; i >= 0; i--, c_arg--) {\n-    arg_order.push(i);\n-    arg_order.push(c_arg);\n+  if (!is_critical_native) {\n+    for (int i = total_in_args - 1, c_arg = total_c_args - 1; i >= 0; i--, c_arg--) {\n+      arg_order.push(i);\n+      arg_order.push(c_arg);\n+    }\n+  } else {\n+    \/\/ Compute a valid move order, using tmp_vmreg to break any cycles\n+    ComputeMoveOrder cmo(total_in_args, in_regs, total_c_args, out_regs, in_sig_bt, arg_order, tmp_vmreg);\n@@ -1456,1 +1606,14 @@\n-    assert(c_arg != -1 && i != -1, \"wrong order\");\n+    if (c_arg == -1) {\n+      assert(is_critical_native, \"should only be required for critical natives\");\n+      \/\/ This arg needs to be moved to a temporary\n+      __ mov(tmp_vmreg.first()->as_Register(), in_regs[i].first()->as_Register());\n+      in_regs[i] = tmp_vmreg;\n+      temploc = i;\n+      continue;\n+    } else if (i == -1) {\n+      assert(is_critical_native, \"should only be required for critical natives\");\n+      \/\/ Read from the temporary location\n+      assert(temploc != -1, \"must be valid\");\n+      i = temploc;\n+      temploc = -1;\n+    }\n@@ -1471,0 +1634,13 @@\n+        if (is_critical_native) {\n+          unpack_array_argument(masm, in_regs[i], in_elem_bt[i], out_regs[c_arg + 1], out_regs[c_arg]);\n+          c_arg++;\n+#ifdef ASSERT\n+          if (out_regs[c_arg].first()->is_Register()) {\n+            reg_destroyed[out_regs[c_arg].first()->as_Register()->encoding()] = true;\n+          } else if (out_regs[c_arg].first()->is_FloatRegister()) {\n+            freg_destroyed[out_regs[c_arg].first()->as_FloatRegister()->encoding()] = true;\n+          }\n+#endif\n+          int_args++;\n+          break;\n+        }\n@@ -1472,0 +1648,1 @@\n+        assert(!is_critical_native, \"no oop arguments\");\n@@ -1511,1 +1688,1 @@\n-  if (method->is_static()) {\n+  if (method->is_static() && !is_critical_native) {\n@@ -1569,0 +1746,1 @@\n+    assert(!is_critical_native, \"unhandled\");\n@@ -1625,1 +1803,2 @@\n-  __ lea(c_rarg0, Address(rthread, in_bytes(JavaThread::jni_environment_offset())));\n+  if (!is_critical_native) {\n+    __ lea(c_rarg0, Address(rthread, in_bytes(JavaThread::jni_environment_offset())));\n@@ -1627,4 +1806,5 @@\n-  \/\/ Now set thread in native\n-  __ mov(rscratch1, _thread_in_native);\n-  __ lea(rscratch2, Address(rthread, JavaThread::thread_state_offset()));\n-  __ stlrw(rscratch1, rscratch2);\n+    \/\/ Now set thread in native\n+    __ mov(rscratch1, _thread_in_native);\n+    __ lea(rscratch2, Address(rthread, JavaThread::thread_state_offset()));\n+    __ stlrw(rscratch1, rscratch2);\n+  }\n@@ -1661,0 +1841,12 @@\n+  \/\/ If this is a critical native, check for a safepoint or suspend request after the call.\n+  \/\/ If a safepoint is needed, transition to native, then to native_trans to handle\n+  \/\/ safepoints like the native methods that are not critical natives.\n+  if (is_critical_native) {\n+    Label needs_safepoint;\n+    __ safepoint_poll(needs_safepoint, false \/* at_return *\/, true \/* acquire *\/, false \/* in_nmethod *\/);\n+    __ ldrw(rscratch1, Address(rthread, JavaThread::suspend_flags_offset()));\n+    __ cbnzw(rscratch1, needs_safepoint);\n+    __ b(after_transition);\n+    __ bind(needs_safepoint);\n+  }\n+\n@@ -1774,3 +1966,5 @@\n-  \/\/ reset handle block\n-  __ ldr(r2, Address(rthread, JavaThread::active_handles_offset()));\n-  __ str(zr, Address(r2, JNIHandleBlock::top_offset_in_bytes()));\n+  if (!is_critical_native) {\n+    \/\/ reset handle block\n+    __ ldr(r2, Address(rthread, JavaThread::active_handles_offset()));\n+    __ str(zr, Address(r2, JNIHandleBlock::top_offset_in_bytes()));\n+  }\n@@ -1780,3 +1974,5 @@\n-  \/\/ Any exception pending?\n-  __ ldr(rscratch1, Address(rthread, in_bytes(Thread::pending_exception_offset())));\n-  __ cbnz(rscratch1, exception_pending);\n+  if (!is_critical_native) {\n+    \/\/ Any exception pending?\n+    __ ldr(rscratch1, Address(rthread, in_bytes(Thread::pending_exception_offset())));\n+    __ cbnz(rscratch1, exception_pending);\n+  }\n@@ -1789,2 +1985,3 @@\n-  \/\/ forward the exception\n-  __ bind(exception_pending);\n+  if (!is_critical_native) {\n+    \/\/ forward the exception\n+    __ bind(exception_pending);\n@@ -1792,2 +1989,3 @@\n-  \/\/ and forward the exception\n-  __ far_jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+    \/\/ and forward the exception\n+    __ far_jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+  }\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":228,"deletions":30,"binary":false,"changes":258,"status":"modified"},{"patch":"@@ -546,0 +546,2 @@\n+\n+  UNSUPPORTED_OPTION(CriticalJNINatives);\n","filename":"src\/hotspot\/cpu\/aarch64\/vm_version_aarch64.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -753,1 +753,2 @@\n-                                                BasicType ret_type) {\n+                                                BasicType ret_type,\n+                                                address critical_entry) {\n@@ -779,1 +780,1 @@\n-  bool method_is_static = method->is_static();\n+  bool is_static = method->is_static();\n@@ -782,1 +783,4 @@\n-  int total_c_args = total_in_args + (method_is_static ? 2 : 1);\n+  int total_c_args = total_in_args + 1;\n+  if (is_static) {\n+    total_c_args++;\n+  }\n@@ -789,1 +793,1 @@\n-  if (method_is_static) {\n+  if (is_static) {\n@@ -880,1 +884,1 @@\n-  const int extra_args = method_is_static ? 2 : 1;\n+  const int extra_args = is_static ? 2 : 1;\n@@ -903,1 +907,1 @@\n-        if ((i == 0) && (!method_is_static)) {\n+        if ((i == 0) && (!is_static)) {\n@@ -1115,1 +1119,1 @@\n-  if (method_is_static) {\n+  if (is_static) {\n@@ -1331,1 +1335,1 @@\n-                                     in_ByteSize(method_is_static ? klass_offset : receiver_offset),\n+                                     in_ByteSize(is_static ? klass_offset : receiver_offset),\n","filename":"src\/hotspot\/cpu\/arm\/sharedRuntime_arm.cpp","additions":12,"deletions":8,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2008, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2008, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -338,0 +338,1 @@\n+  UNSUPPORTED_OPTION(CriticalJNINatives);\n","filename":"src\/hotspot\/cpu\/arm\/vm_version_arm_32.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1537,0 +1537,51 @@\n+static void move_ptr(MacroAssembler* masm, VMRegPair src, VMRegPair dst, Register r_caller_sp, Register r_temp) {\n+  if (src.first()->is_stack()) {\n+    if (dst.first()->is_stack()) {\n+      \/\/ stack to stack\n+      __ ld(r_temp, reg2offset(src.first()), r_caller_sp);\n+      __ std(r_temp, reg2offset(dst.first()), R1_SP);\n+    } else {\n+      \/\/ stack to reg\n+      __ ld(dst.first()->as_Register(), reg2offset(src.first()), r_caller_sp);\n+    }\n+  } else if (dst.first()->is_stack()) {\n+    \/\/ reg to stack\n+    __ std(src.first()->as_Register(), reg2offset(dst.first()), R1_SP);\n+  } else {\n+    if (dst.first() != src.first()) {\n+      __ mr(dst.first()->as_Register(), src.first()->as_Register());\n+    }\n+  }\n+}\n+\n+\/\/ Unpack an array argument into a pointer to the body and the length\n+\/\/ if the array is non-null, otherwise pass 0 for both.\n+static void unpack_array_argument(MacroAssembler* masm, VMRegPair reg, BasicType in_elem_type,\n+                                  VMRegPair body_arg, VMRegPair length_arg, Register r_caller_sp,\n+                                  Register tmp_reg, Register tmp2_reg) {\n+  assert(!body_arg.first()->is_Register() || body_arg.first()->as_Register() != tmp_reg,\n+         \"possible collision\");\n+  assert(!length_arg.first()->is_Register() || length_arg.first()->as_Register() != tmp_reg,\n+         \"possible collision\");\n+\n+  \/\/ Pass the length, ptr pair.\n+  Label set_out_args;\n+  VMRegPair tmp, tmp2;\n+  tmp.set_ptr(tmp_reg->as_VMReg());\n+  tmp2.set_ptr(tmp2_reg->as_VMReg());\n+  if (reg.first()->is_stack()) {\n+    \/\/ Load the arg up from the stack.\n+    move_ptr(masm, reg, tmp, r_caller_sp, \/*unused*\/ R0);\n+    reg = tmp;\n+  }\n+  __ li(tmp2_reg, 0); \/\/ Pass zeros if Array=null.\n+  if (tmp_reg != reg.first()->as_Register()) __ li(tmp_reg, 0);\n+  __ cmpdi(CCR0, reg.first()->as_Register(), 0);\n+  __ beq(CCR0, set_out_args);\n+  __ lwa(tmp2_reg, arrayOopDesc::length_offset_in_bytes(), reg.first()->as_Register());\n+  __ addi(tmp_reg, reg.first()->as_Register(), arrayOopDesc::base_offset_in_bytes(in_elem_type));\n+  __ bind(set_out_args);\n+  move_ptr(masm, tmp, body_arg, r_caller_sp, \/*unused*\/ R0);\n+  move_ptr(masm, tmp2, length_arg, r_caller_sp, \/*unused*\/ R0); \/\/ Same as move32_64 on PPC64.\n+}\n+\n@@ -1641,1 +1692,2 @@\n-                                                BasicType ret_type) {\n+                                                BasicType ret_type,\n+                                                address critical_entry) {\n@@ -1664,1 +1716,6 @@\n-  address native_func = method->native_function();\n+  bool is_critical_native = true;\n+  address native_func = critical_entry;\n+  if (native_func == NULL) {\n+    native_func = method->native_function();\n+    is_critical_native = false;\n+  }\n@@ -1683,1 +1740,13 @@\n-  int  total_c_args     = total_in_args + (method_is_static ? 2 : 1);\n+  int  total_c_args     = total_in_args;\n+\n+  if (!is_critical_native) {\n+    int n_hidden_args = method_is_static ? 2 : 1;\n+    total_c_args += n_hidden_args;\n+  } else {\n+    \/\/ No JNIEnv*, no this*, but unpacked arrays (base+length).\n+    for (int i = 0; i < total_in_args; i++) {\n+      if (in_sig_bt[i] == T_ARRAY) {\n+        total_c_args++;\n+      }\n+    }\n+  }\n@@ -1697,4 +1766,28 @@\n-  out_sig_bt[argc++] = T_ADDRESS;\n-  if (method->is_static()) {\n-    out_sig_bt[argc++] = T_OBJECT;\n-  }\n+  if (!is_critical_native) {\n+    out_sig_bt[argc++] = T_ADDRESS;\n+    if (method->is_static()) {\n+      out_sig_bt[argc++] = T_OBJECT;\n+    }\n+\n+    for (int i = 0; i < total_in_args ; i++ ) {\n+      out_sig_bt[argc++] = in_sig_bt[i];\n+    }\n+  } else {\n+    in_elem_bt = NEW_RESOURCE_ARRAY(BasicType, total_c_args);\n+    SignatureStream ss(method->signature());\n+    int o = 0;\n+    for (int i = 0; i < total_in_args ; i++, o++) {\n+      if (in_sig_bt[i] == T_ARRAY) {\n+        \/\/ Arrays are passed as int, elem* pair\n+        ss.skip_array_prefix(1);  \/\/ skip one '['\n+        assert(ss.is_primitive(), \"primitive type expected\");\n+        in_elem_bt[o] = ss.type();\n+      } else {\n+        in_elem_bt[o] = T_VOID;\n+      }\n+      if (in_sig_bt[i] != T_VOID) {\n+        assert(in_sig_bt[i] == ss.type() ||\n+               in_sig_bt[i] == T_ARRAY, \"must match\");\n+        ss.next();\n+      }\n+    }\n@@ -1702,2 +1795,9 @@\n-  for (int i = 0; i < total_in_args ; i++ ) {\n-    out_sig_bt[argc++] = in_sig_bt[i];\n+    for (int i = 0; i < total_in_args ; i++ ) {\n+      if (in_sig_bt[i] == T_ARRAY) {\n+        \/\/ Arrays are passed as int, elem* pair.\n+        out_sig_bt[argc++] = T_INT;\n+        out_sig_bt[argc++] = T_ADDRESS;\n+      } else {\n+        out_sig_bt[argc++] = in_sig_bt[i];\n+      }\n+    }\n@@ -1730,1 +1830,1 @@\n-  \/\/        [oopHandle area]           <-- 3) R1_SP + oop_handle_offset\n+  \/\/        [oopHandle area]           <-- 3) R1_SP + oop_handle_offset (save area for critical natives)\n@@ -1745,0 +1845,29 @@\n+  if (is_critical_native) {\n+    \/\/ Critical natives may have to call out so they need a save area\n+    \/\/ for register arguments.\n+    int double_slots = 0;\n+    int single_slots = 0;\n+    for (int i = 0; i < total_in_args; i++) {\n+      if (in_regs[i].first()->is_Register()) {\n+        const Register reg = in_regs[i].first()->as_Register();\n+        switch (in_sig_bt[i]) {\n+          case T_BOOLEAN:\n+          case T_BYTE:\n+          case T_SHORT:\n+          case T_CHAR:\n+          case T_INT:\n+          \/\/ Fall through.\n+          case T_ARRAY:\n+          case T_LONG: double_slots++; break;\n+          default:  ShouldNotReachHere();\n+        }\n+      } else if (in_regs[i].first()->is_FloatRegister()) {\n+        switch (in_sig_bt[i]) {\n+          case T_FLOAT:  single_slots++; break;\n+          case T_DOUBLE: double_slots++; break;\n+          default:  ShouldNotReachHere();\n+        }\n+      }\n+    }\n+    total_save_slots = double_slots * 2 + align_up(single_slots, 2); \/\/ round to even\n+  }\n@@ -1751,1 +1880,1 @@\n-  if (method_is_static) {                                                         \/\/ 4)\n+  if (method_is_static && !is_critical_native) {                                  \/\/ 4)\n@@ -1797,2 +1926,4 @@\n-  r_carg1_jnienv        = out_regs[0].first()->as_Register();\n-  r_carg2_classorobject = out_regs[1].first()->as_Register();\n+  if (!is_critical_native) {\n+    r_carg1_jnienv        = out_regs[0].first()->as_Register();\n+    r_carg2_classorobject = out_regs[1].first()->as_Register();\n+  }\n@@ -1934,0 +2065,7 @@\n+        if (is_critical_native) {\n+          int body_arg = out;\n+          out -= 1; \/\/ Point to length arg.\n+          unpack_array_argument(masm, in_regs[in], in_elem_bt[in], out_regs[body_arg], out_regs[out],\n+                                r_callers_sp, r_temp_1, r_temp_2);\n+          break;\n+        }\n@@ -1935,0 +2073,1 @@\n+        assert(!is_critical_native, \"no oop arguments\");\n@@ -1966,1 +2105,1 @@\n-  if (method_is_static) {\n+  if (method_is_static && !is_critical_native) {\n@@ -1977,1 +2116,3 @@\n-  __ addi(r_carg1_jnienv, R16_thread, in_bytes(JavaThread::jni_environment_offset()));\n+  if (!is_critical_native) {\n+    __ addi(r_carg1_jnienv, R16_thread, in_bytes(JavaThread::jni_environment_offset()));\n+  }\n@@ -2006,0 +2147,1 @@\n+    assert(!is_critical_native, \"unhandled\");\n@@ -2050,2 +2192,3 @@\n-  \/\/ Publish thread state\n-  \/\/ --------------------------------------------------------------------------\n+  if (!is_critical_native) {\n+    \/\/ Publish thread state\n+    \/\/ --------------------------------------------------------------------------\n@@ -2053,5 +2196,6 @@\n-  \/\/ Transition from _thread_in_Java to _thread_in_native.\n-  __ li(R0, _thread_in_native);\n-  __ release();\n-  \/\/ TODO: PPC port assert(4 == JavaThread::sz_thread_state(), \"unexpected field size\");\n-  __ stw(R0, thread_(thread_state));\n+    \/\/ Transition from _thread_in_Java to _thread_in_native.\n+    __ li(R0, _thread_in_native);\n+    __ release();\n+    \/\/ TODO: PPC port assert(4 == JavaThread::sz_thread_state(), \"unexpected field size\");\n+    __ stw(R0, thread_(thread_state));\n+  }\n@@ -2119,0 +2263,18 @@\n+  \/\/ If this is a critical native, check for a safepoint or suspend request after the call.\n+  \/\/ If a safepoint is needed, transition to native, then to native_trans to handle\n+  \/\/ safepoints like the native methods that are not critical natives.\n+  if (is_critical_native) {\n+    Label needs_safepoint;\n+    Register sync_state      = r_temp_5;\n+    \/\/ Note: We should not reach here with active stack watermark. There's no safepoint between\n+    \/\/       start of the native wrapper and this check where it could have been added.\n+    \/\/       We don't check the watermark in the fast path.\n+    __ safepoint_poll(needs_safepoint, sync_state, false \/* at_return *\/, false \/* in_nmethod *\/);\n+\n+    Register suspend_flags   = r_temp_6;\n+    __ lwz(suspend_flags, thread_(suspend_flags));\n+    __ cmpwi(CCR1, suspend_flags, 0);\n+    __ beq(CCR1, after_transition);\n+    __ bind(needs_safepoint);\n+  }\n+\n@@ -2288,0 +2450,1 @@\n+  if (!is_critical_native) {\n@@ -2299,0 +2462,1 @@\n+  }\n@@ -2313,0 +2477,1 @@\n+  if (!is_critical_native) {\n@@ -2319,0 +2484,1 @@\n+  }\n","filename":"src\/hotspot\/cpu\/ppc\/sharedRuntime_ppc.cpp","additions":188,"deletions":22,"binary":false,"changes":210,"status":"modified"},{"patch":"@@ -1293,0 +1293,70 @@\n+static void move_ptr(MacroAssembler *masm,\n+                     VMRegPair src,\n+                     VMRegPair dst,\n+                     int framesize_in_slots) {\n+  int frame_offset = framesize_in_slots * VMRegImpl::stack_slot_size;\n+\n+  if (src.first()->is_stack()) {\n+    if (dst.first()->is_stack()) {\n+      \/\/ stack to stack\n+      __ mem2reg_opt(Z_R0_scratch, Address(Z_SP, reg2offset(src.first()) + frame_offset));\n+      __ reg2mem_opt(Z_R0_scratch, Address(Z_SP, reg2offset(dst.first())));\n+    } else {\n+      \/\/ stack to reg\n+      __ mem2reg_opt(dst.first()->as_Register(),\n+                     Address(Z_SP, reg2offset(src.first()) + frame_offset));\n+    }\n+  } else {\n+    if (dst.first()->is_stack()) {\n+      \/\/ reg to stack\n+    __ reg2mem_opt(src.first()->as_Register(), Address(Z_SP, reg2offset(dst.first())));\n+    } else {\n+    __ lgr_if_needed(dst.first()->as_Register(), src.first()->as_Register());\n+    }\n+  }\n+}\n+\n+\/\/ Unpack an array argument into a pointer to the body and the length\n+\/\/ if the array is non-null, otherwise pass 0 for both.\n+static void unpack_array_argument(MacroAssembler *masm,\n+                                   VMRegPair reg,\n+                                   BasicType in_elem_type,\n+                                   VMRegPair body_arg,\n+                                   VMRegPair length_arg,\n+                                   int framesize_in_slots) {\n+  Register tmp_reg = Z_tmp_2;\n+  Register tmp2_reg = Z_tmp_1;\n+\n+  assert(!body_arg.first()->is_Register() || body_arg.first()->as_Register() != tmp_reg,\n+         \"possible collision\");\n+  assert(!length_arg.first()->is_Register() || length_arg.first()->as_Register() != tmp_reg,\n+         \"possible collision\");\n+\n+  \/\/ Pass the length, ptr pair.\n+  NearLabel set_out_args;\n+  VMRegPair tmp, tmp2;\n+\n+  tmp.set_ptr(tmp_reg->as_VMReg());\n+  tmp2.set_ptr(tmp2_reg->as_VMReg());\n+  if (reg.first()->is_stack()) {\n+    \/\/ Load the arg up from the stack.\n+    move_ptr(masm, reg, tmp, framesize_in_slots);\n+    reg = tmp;\n+  }\n+\n+  const Register first = reg.first()->as_Register();\n+\n+  \/\/ Don't set CC, indicate unused result.\n+  (void) __ clear_reg(tmp2_reg, true, false);\n+  if (tmp_reg != first) {\n+    __ clear_reg(tmp_reg, true, false);  \/\/ Don't set CC.\n+  }\n+  __ compare64_and_branch(first, (RegisterOrConstant)0L, Assembler::bcondEqual, set_out_args);\n+  __ z_lgf(tmp2_reg, Address(first, arrayOopDesc::length_offset_in_bytes()));\n+  __ add2reg(tmp_reg, arrayOopDesc::base_offset_in_bytes(in_elem_type), first);\n+\n+  __ bind(set_out_args);\n+  move_ptr(masm, tmp, body_arg, framesize_in_slots);\n+  move32_64(masm, tmp2, length_arg, framesize_in_slots);\n+}\n+\n@@ -1302,1 +1372,2 @@\n-                                                BasicType ret_type) {\n+                                                BasicType ret_type,\n+                                                address critical_entry) {\n@@ -1336,1 +1407,6 @@\n-  address native_func = method->native_function();\n+  bool is_critical_native = true;\n+  address native_func = critical_entry;\n+  if (native_func == NULL) {\n+    native_func = method->native_function();\n+    is_critical_native = false;\n+  }\n@@ -1361,1 +1437,13 @@\n-  int  total_c_args     = total_in_args + (method_is_static ? 2 : 1);\n+  int  total_c_args     = total_in_args;\n+\n+  if (!is_critical_native) {\n+    int n_hidden_args = method_is_static ? 2 : 1;\n+    total_c_args += n_hidden_args;\n+  } else {\n+    \/\/ No JNIEnv*, no this*, but unpacked arrays (base+length).\n+    for (int i = 0; i < total_in_args; i++) {\n+      if (in_sig_bt[i] == T_ARRAY) {\n+        total_c_args ++;\n+      }\n+    }\n+  }\n@@ -1374,4 +1462,5 @@\n-  out_sig_bt[argc++] = T_ADDRESS;\n-  if (method->is_static()) {\n-    out_sig_bt[argc++] = T_OBJECT;\n-  }\n+  if (!is_critical_native) {\n+    out_sig_bt[argc++] = T_ADDRESS;\n+    if (method->is_static()) {\n+      out_sig_bt[argc++] = T_OBJECT;\n+    }\n@@ -1379,2 +1468,33 @@\n-  for (int i = 0; i < total_in_args; i++) {\n-    out_sig_bt[argc++] = in_sig_bt[i];\n+    for (int i = 0; i < total_in_args; i++) {\n+      out_sig_bt[argc++] = in_sig_bt[i];\n+    }\n+  } else {\n+    in_elem_bt = NEW_RESOURCE_ARRAY(BasicType, total_in_args);\n+    SignatureStream ss(method->signature());\n+    int o = 0;\n+    for (int i = 0; i < total_in_args; i++, o++) {\n+      if (in_sig_bt[i] == T_ARRAY) {\n+        \/\/ Arrays are passed as tuples (int, elem*).\n+        ss.skip_array_prefix(1);  \/\/ skip one '['\n+        assert(ss.is_primitive(), \"primitive type expected\");\n+        in_elem_bt[o] = ss.type();\n+      } else {\n+        in_elem_bt[o] = T_VOID;\n+      }\n+      if (in_sig_bt[i] != T_VOID) {\n+        assert(in_sig_bt[i] == ss.type() ||\n+               in_sig_bt[i] == T_ARRAY, \"must match\");\n+        ss.next();\n+      }\n+    }\n+    assert(total_in_args == o, \"must match\");\n+\n+    for (int i = 0; i < total_in_args; i++) {\n+      if (in_sig_bt[i] == T_ARRAY) {\n+        \/\/ Arrays are passed as tuples (int, elem*).\n+        out_sig_bt[argc++] = T_INT;\n+        out_sig_bt[argc++] = T_ADDRESS;\n+      } else {\n+        out_sig_bt[argc++] = in_sig_bt[i];\n+      }\n+    }\n@@ -1437,0 +1557,2 @@\n+  \/\/      | (save area for      |\n+  \/\/      |  critical natives)  |\n@@ -1464,0 +1586,31 @@\n+  if (is_critical_native) {\n+    \/\/ Critical natives may have to call out so they need a save area\n+    \/\/ for register arguments.\n+    int double_slots = 0;\n+    int single_slots = 0;\n+    for (int i = 0; i < total_in_args; i++) {\n+      if (in_regs[i].first()->is_Register()) {\n+        const Register reg = in_regs[i].first()->as_Register();\n+        switch (in_sig_bt[i]) {\n+          case T_BOOLEAN:\n+          case T_BYTE:\n+          case T_SHORT:\n+          case T_CHAR:\n+          case T_INT:\n+          \/\/ Fall through.\n+          case T_ARRAY:\n+          case T_LONG: double_slots++; break;\n+          default:  ShouldNotReachHere();\n+        }\n+      } else {\n+        if (in_regs[i].first()->is_FloatRegister()) {\n+          switch (in_sig_bt[i]) {\n+            case T_FLOAT:  single_slots++; break;\n+            case T_DOUBLE: double_slots++; break;\n+            default:  ShouldNotReachHere();\n+          }\n+        }\n+      }\n+    }  \/\/ for\n+    total_save_slots = double_slots * 2 + align_up(single_slots, 2); \/\/ Round to even.\n+  }\n@@ -1470,1 +1623,1 @@\n-  if (method_is_static) {                                                 \/\/ 4)\n+  if (method_is_static && !is_critical_native) {                          \/\/ 4)\n@@ -1637,0 +1790,7 @@\n+        if (is_critical_native) {\n+          int body_arg = cix;\n+          cix -= 1; \/\/ Point to length arg.\n+          unpack_array_argument(masm, in_regs[jix], in_elem_bt[jix], out_regs[body_arg], out_regs[cix], stack_slots);\n+          break;\n+        }\n+        \/\/ else fallthrough\n@@ -1638,0 +1798,1 @@\n+        assert(!is_critical_native, \"no oop arguments\");\n@@ -1667,1 +1828,1 @@\n-  if (method_is_static) {\n+  if (method_is_static && !is_critical_native) {\n@@ -1677,1 +1838,3 @@\n-  __ add2reg(Z_ARG1, in_bytes(JavaThread::jni_environment_offset()), Z_thread);\n+  if (!is_critical_native) {\n+    __ add2reg(Z_ARG1, in_bytes(JavaThread::jni_environment_offset()), Z_thread);\n+  }\n@@ -1699,0 +1862,1 @@\n+    assert(!is_critical_native, \"unhandled\");\n@@ -1766,2 +1930,4 @@\n-  \/\/ Transition from _thread_in_Java to _thread_in_native.\n-  __ set_thread_state(_thread_in_native);\n+  if (!is_critical_native) {\n+    \/\/ Transition from _thread_in_Java to _thread_in_native.\n+    __ set_thread_state(_thread_in_native);\n+  }\n@@ -1815,0 +1981,12 @@\n+  \/\/ If this is a critical native, check for a safepoint or suspend request after the call.\n+  \/\/ If a safepoint is needed, transition to native, then to native_trans to handle\n+  \/\/ safepoints like the native methods that are not critical natives.\n+  if (is_critical_native) {\n+    Label needs_safepoint;\n+    \/\/ Does this need to save_native_result and fences?\n+    __ safepoint_poll(needs_safepoint, Z_R1);\n+    __ load_and_test_int(Z_R0, Address(Z_thread, JavaThread::suspend_flags_offset()));\n+    __ z_bre(after_transition);\n+    __ bind(needs_safepoint);\n+  }\n+\n@@ -1985,2 +2163,3 @@\n-  __ z_lg(Z_R1_scratch, Address(Z_thread, JavaThread::active_handles_offset()));\n-  __ clear_mem(Address(Z_R1_scratch, JNIHandleBlock::top_offset_in_bytes()), 4);\n+  if (!is_critical_native) {\n+    __ z_lg(Z_R1_scratch, Address(Z_thread, JavaThread::active_handles_offset()));\n+    __ clear_mem(Address(Z_R1_scratch, JNIHandleBlock::top_offset_in_bytes()), 4);\n@@ -1988,3 +2167,4 @@\n-  \/\/ Check for pending exceptions.\n-  __ load_and_test_long(Z_R0, Address(Z_thread, Thread::pending_exception_offset()));\n-  __ z_brne(handle_pending_exception);\n+    \/\/ Check for pending exceptions.\n+    __ load_and_test_long(Z_R0, Address(Z_thread, Thread::pending_exception_offset()));\n+    __ z_brne(handle_pending_exception);\n+  }\n@@ -2012,17 +2192,20 @@\n-  \/\/---------------------------------------------------------------------\n-  \/\/ Handler for pending exceptions (out-of-line).\n-  \/\/---------------------------------------------------------------------\n-  \/\/ Since this is a native call, we know the proper exception handler\n-  \/\/ is the empty function. We just pop this frame and then jump to\n-  \/\/ forward_exception_entry. Z_R14 will contain the native caller's\n-  \/\/ return PC.\n-  __ bind(handle_pending_exception);\n-  __ pop_frame();\n-  __ load_const_optimized(Z_R1_scratch, StubRoutines::forward_exception_entry());\n-  __ restore_return_pc();\n-  __ z_br(Z_R1_scratch);\n-\n-  \/\/---------------------------------------------------------------------\n-  \/\/ Handler for a cache miss (out-of-line)\n-  \/\/---------------------------------------------------------------------\n-  __ call_ic_miss_handler(ic_miss, 0x77, 0, Z_R1_scratch);\n+  if (!is_critical_native) {\n+\n+    \/\/---------------------------------------------------------------------\n+    \/\/ Handler for pending exceptions (out-of-line).\n+    \/\/---------------------------------------------------------------------\n+    \/\/ Since this is a native call, we know the proper exception handler\n+    \/\/ is the empty function. We just pop this frame and then jump to\n+    \/\/ forward_exception_entry. Z_R14 will contain the native caller's\n+    \/\/ return PC.\n+    __ bind(handle_pending_exception);\n+    __ pop_frame();\n+    __ load_const_optimized(Z_R1_scratch, StubRoutines::forward_exception_entry());\n+    __ restore_return_pc();\n+    __ z_br(Z_R1_scratch);\n+\n+    \/\/---------------------------------------------------------------------\n+    \/\/ Handler for a cache miss (out-of-line)\n+    \/\/---------------------------------------------------------------------\n+    __ call_ic_miss_handler(ic_miss, 0x77, 0, Z_R1_scratch);\n+  }\n","filename":"src\/hotspot\/cpu\/s390\/sharedRuntime_s390.cpp","additions":219,"deletions":36,"binary":false,"changes":255,"status":"modified"},{"patch":"@@ -1239,0 +1239,34 @@\n+\/\/ Unpack an array argument into a pointer to the body and the length\n+\/\/ if the array is non-null, otherwise pass 0 for both.\n+static void unpack_array_argument(MacroAssembler* masm, VMRegPair reg, BasicType in_elem_type, VMRegPair body_arg, VMRegPair length_arg) {\n+  Register tmp_reg = rax;\n+  assert(!body_arg.first()->is_Register() || body_arg.first()->as_Register() != tmp_reg,\n+         \"possible collision\");\n+  assert(!length_arg.first()->is_Register() || length_arg.first()->as_Register() != tmp_reg,\n+         \"possible collision\");\n+\n+  \/\/ Pass the length, ptr pair\n+  Label is_null, done;\n+  VMRegPair tmp(tmp_reg->as_VMReg());\n+  if (reg.first()->is_stack()) {\n+    \/\/ Load the arg up from the stack\n+    simple_move32(masm, reg, tmp);\n+    reg = tmp;\n+  }\n+  __ testptr(reg.first()->as_Register(), reg.first()->as_Register());\n+  __ jccb(Assembler::equal, is_null);\n+  __ lea(tmp_reg, Address(reg.first()->as_Register(), arrayOopDesc::base_offset_in_bytes(in_elem_type)));\n+  simple_move32(masm, tmp, body_arg);\n+  \/\/ load the length relative to the body.\n+  __ movl(tmp_reg, Address(tmp_reg, arrayOopDesc::length_offset_in_bytes() -\n+                           arrayOopDesc::base_offset_in_bytes(in_elem_type)));\n+  simple_move32(masm, tmp, length_arg);\n+  __ jmpb(done);\n+  __ bind(is_null);\n+  \/\/ Pass zeros\n+  __ xorptr(tmp_reg, tmp_reg);\n+  simple_move32(masm, tmp, body_arg);\n+  simple_move32(masm, tmp, length_arg);\n+  __ bind(done);\n+}\n+\n@@ -1341,1 +1375,2 @@\n-                                                BasicType ret_type) {\n+                                                BasicType ret_type,\n+                                                address critical_entry) {\n@@ -1363,1 +1398,6 @@\n-  address native_func = method->native_function();\n+  bool is_critical_native = true;\n+  address native_func = critical_entry;\n+  if (native_func == NULL) {\n+    native_func = method->native_function();\n+    is_critical_native = false;\n+  }\n@@ -1376,1 +1416,13 @@\n-  int  total_c_args       = total_in_args + (method->is_static() ? 2 : 1);\n+  int total_c_args = total_in_args;\n+  if (!is_critical_native) {\n+    total_c_args += 1;\n+    if (method->is_static()) {\n+      total_c_args++;\n+    }\n+  } else {\n+    for (int i = 0; i < total_in_args; i++) {\n+      if (in_sig_bt[i] == T_ARRAY) {\n+        total_c_args++;\n+      }\n+    }\n+  }\n@@ -1383,4 +1435,5 @@\n-  out_sig_bt[argc++] = T_ADDRESS;\n-  if (method->is_static()) {\n-    out_sig_bt[argc++] = T_OBJECT;\n-  }\n+  if (!is_critical_native) {\n+    out_sig_bt[argc++] = T_ADDRESS;\n+    if (method->is_static()) {\n+      out_sig_bt[argc++] = T_OBJECT;\n+    }\n@@ -1388,2 +1441,24 @@\n-  for (int i = 0; i < total_in_args ; i++ ) {\n-    out_sig_bt[argc++] = in_sig_bt[i];\n+    for (int i = 0; i < total_in_args ; i++ ) {\n+      out_sig_bt[argc++] = in_sig_bt[i];\n+    }\n+  } else {\n+    in_elem_bt = NEW_RESOURCE_ARRAY(BasicType, total_in_args);\n+    SignatureStream ss(method->signature());\n+    for (int i = 0; i < total_in_args ; i++ ) {\n+      if (in_sig_bt[i] == T_ARRAY) {\n+        \/\/ Arrays are passed as int, elem* pair\n+        out_sig_bt[argc++] = T_INT;\n+        out_sig_bt[argc++] = T_ADDRESS;\n+        ss.skip_array_prefix(1);  \/\/ skip one '['\n+        assert(ss.is_primitive(), \"primitive type expected\");\n+        in_elem_bt[i] = ss.type();\n+      } else {\n+        out_sig_bt[argc++] = in_sig_bt[i];\n+        in_elem_bt[i] = T_VOID;\n+      }\n+      if (in_sig_bt[i] != T_VOID) {\n+        assert(in_sig_bt[i] == ss.type() ||\n+               in_sig_bt[i] == T_ARRAY, \"must match\");\n+        ss.next();\n+      }\n+    }\n@@ -1407,0 +1482,34 @@\n+  if (is_critical_native) {\n+    \/\/ Critical natives may have to call out so they need a save area\n+    \/\/ for register arguments.\n+    int double_slots = 0;\n+    int single_slots = 0;\n+    for ( int i = 0; i < total_in_args; i++) {\n+      if (in_regs[i].first()->is_Register()) {\n+        const Register reg = in_regs[i].first()->as_Register();\n+        switch (in_sig_bt[i]) {\n+          case T_ARRAY:  \/\/ critical array (uses 2 slots on LP64)\n+          case T_BOOLEAN:\n+          case T_BYTE:\n+          case T_SHORT:\n+          case T_CHAR:\n+          case T_INT:  single_slots++; break;\n+          case T_LONG: double_slots++; break;\n+          default:  ShouldNotReachHere();\n+        }\n+      } else if (in_regs[i].first()->is_XMMRegister()) {\n+        switch (in_sig_bt[i]) {\n+          case T_FLOAT:  single_slots++; break;\n+          case T_DOUBLE: double_slots++; break;\n+          default:  ShouldNotReachHere();\n+        }\n+      } else if (in_regs[i].first()->is_FloatRegister()) {\n+        ShouldNotReachHere();\n+      }\n+    }\n+    total_save_slots = double_slots * 2 + single_slots;\n+    \/\/ align the save area\n+    if (double_slots != 0) {\n+      stack_slots = align_up(stack_slots, 2);\n+    }\n+  }\n@@ -1585,1 +1694,1 @@\n-  int c_arg = method->is_static() ? 2 : 1;\n+  int c_arg = is_critical_native ? 0 : (method->is_static() ? 2 : 1 );\n@@ -1608,0 +1717,6 @@\n+        if (is_critical_native) {\n+          VMRegPair in_arg = in_regs[i];\n+          unpack_array_argument(masm, in_arg, in_elem_bt[i], out_regs[c_arg + 1], out_regs[c_arg]);\n+          c_arg++;\n+          break;\n+        }\n@@ -1609,0 +1724,1 @@\n+        assert(!is_critical_native, \"no oop arguments\");\n@@ -1640,1 +1756,1 @@\n-  if (method->is_static()) {\n+  if (method->is_static() && !is_critical_native) {\n@@ -1695,0 +1811,2 @@\n+    assert(!is_critical_native, \"unhandled\");\n+\n@@ -1751,2 +1869,3 @@\n-  __ lea(rdx, Address(thread, in_bytes(JavaThread::jni_environment_offset())));\n-  __ movptr(Address(rsp, 0), rdx);\n+  if (!is_critical_native) {\n+    __ lea(rdx, Address(thread, in_bytes(JavaThread::jni_environment_offset())));\n+    __ movptr(Address(rsp, 0), rdx);\n@@ -1754,2 +1873,3 @@\n-  \/\/ Now set thread in native\n-  __ movl(Address(thread, JavaThread::thread_state_offset()), _thread_in_native);\n+    \/\/ Now set thread in native\n+    __ movl(Address(thread, JavaThread::thread_state_offset()), _thread_in_native);\n+  }\n@@ -1788,0 +1908,11 @@\n+  \/\/ If this is a critical native, check for a safepoint or suspend request after the call.\n+  \/\/ If a safepoint is needed, transition to native, then to native_trans to handle\n+  \/\/ safepoints like the native methods that are not critical natives.\n+  if (is_critical_native) {\n+    Label needs_safepoint;\n+    __ safepoint_poll(needs_safepoint, thread, false \/* at_return *\/, false \/* in_nmethod *\/);\n+    __ cmpl(Address(thread, JavaThread::suspend_flags_offset()), 0);\n+    __ jcc(Assembler::equal, after_transition);\n+    __ bind(needs_safepoint);\n+  }\n+\n@@ -1926,3 +2057,4 @@\n-  \/\/ reset handle block\n-  __ movptr(rcx, Address(thread, JavaThread::active_handles_offset()));\n-  __ movl(Address(rcx, JNIHandleBlock::top_offset_in_bytes()), NULL_WORD);\n+  if (!is_critical_native) {\n+    \/\/ reset handle block\n+    __ movptr(rcx, Address(thread, JavaThread::active_handles_offset()));\n+    __ movl(Address(rcx, JNIHandleBlock::top_offset_in_bytes()), NULL_WORD);\n@@ -1930,3 +2062,4 @@\n-  \/\/ Any exception pending?\n-  __ cmpptr(Address(thread, in_bytes(Thread::pending_exception_offset())), (int32_t)NULL_WORD);\n-  __ jcc(Assembler::notEqual, exception_pending);\n+    \/\/ Any exception pending?\n+    __ cmpptr(Address(thread, in_bytes(Thread::pending_exception_offset())), (int32_t)NULL_WORD);\n+    __ jcc(Assembler::notEqual, exception_pending);\n+  }\n@@ -2046,2 +2179,3 @@\n-  \/\/ Forward  the exception\n-  __ bind(exception_pending);\n+  if (!is_critical_native) {\n+    \/\/ Forward  the exception\n+    __ bind(exception_pending);\n@@ -2049,2 +2183,2 @@\n-  \/\/ remove possible return value from FPU register stack\n-  __ empty_FPU_stack();\n+    \/\/ remove possible return value from FPU register stack\n+    __ empty_FPU_stack();\n@@ -2052,4 +2186,5 @@\n-  \/\/ pop our frame\n-  __ leave();\n-  \/\/ and forward the exception\n-  __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+    \/\/ pop our frame\n+    __ leave();\n+    \/\/ and forward the exception\n+    __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_32.cpp","additions":164,"deletions":29,"binary":false,"changes":193,"status":"modified"},{"patch":"@@ -1239,0 +1239,214 @@\n+\/\/ Unpack an array argument into a pointer to the body and the length\n+\/\/ if the array is non-null, otherwise pass 0 for both.\n+static void unpack_array_argument(MacroAssembler* masm, VMRegPair reg, BasicType in_elem_type, VMRegPair body_arg, VMRegPair length_arg) {\n+  Register tmp_reg = rax;\n+  assert(!body_arg.first()->is_Register() || body_arg.first()->as_Register() != tmp_reg,\n+         \"possible collision\");\n+  assert(!length_arg.first()->is_Register() || length_arg.first()->as_Register() != tmp_reg,\n+         \"possible collision\");\n+\n+  __ block_comment(\"unpack_array_argument {\");\n+\n+  \/\/ Pass the length, ptr pair\n+  Label is_null, done;\n+  VMRegPair tmp;\n+  tmp.set_ptr(tmp_reg->as_VMReg());\n+  if (reg.first()->is_stack()) {\n+    \/\/ Load the arg up from the stack\n+    __ move_ptr(reg, tmp);\n+    reg = tmp;\n+  }\n+  __ testptr(reg.first()->as_Register(), reg.first()->as_Register());\n+  __ jccb(Assembler::equal, is_null);\n+  __ lea(tmp_reg, Address(reg.first()->as_Register(), arrayOopDesc::base_offset_in_bytes(in_elem_type)));\n+  __ move_ptr(tmp, body_arg);\n+  \/\/ load the length relative to the body.\n+  __ movl(tmp_reg, Address(tmp_reg, arrayOopDesc::length_offset_in_bytes() -\n+                           arrayOopDesc::base_offset_in_bytes(in_elem_type)));\n+  __ move32_64(tmp, length_arg);\n+  __ jmpb(done);\n+  __ bind(is_null);\n+  \/\/ Pass zeros\n+  __ xorptr(tmp_reg, tmp_reg);\n+  __ move_ptr(tmp, body_arg);\n+  __ move32_64(tmp, length_arg);\n+  __ bind(done);\n+\n+  __ block_comment(\"} unpack_array_argument\");\n+}\n+\n+\n+\/\/ Different signatures may require very different orders for the move\n+\/\/ to avoid clobbering other arguments.  There's no simple way to\n+\/\/ order them safely.  Compute a safe order for issuing stores and\n+\/\/ break any cycles in those stores.  This code is fairly general but\n+\/\/ it's not necessary on the other platforms so we keep it in the\n+\/\/ platform dependent code instead of moving it into a shared file.\n+\/\/ (See bugs 7013347 & 7145024.)\n+\/\/ Note that this code is specific to LP64.\n+class ComputeMoveOrder: public StackObj {\n+  class MoveOperation: public ResourceObj {\n+    friend class ComputeMoveOrder;\n+   private:\n+    VMRegPair        _src;\n+    VMRegPair        _dst;\n+    int              _src_index;\n+    int              _dst_index;\n+    bool             _processed;\n+    MoveOperation*  _next;\n+    MoveOperation*  _prev;\n+\n+    static int get_id(VMRegPair r) {\n+      return r.first()->value();\n+    }\n+\n+   public:\n+    MoveOperation(int src_index, VMRegPair src, int dst_index, VMRegPair dst):\n+      _src(src)\n+    , _dst(dst)\n+    , _src_index(src_index)\n+    , _dst_index(dst_index)\n+    , _processed(false)\n+    , _next(NULL)\n+    , _prev(NULL) {\n+    }\n+\n+    VMRegPair src() const              { return _src; }\n+    int src_id() const                 { return get_id(src()); }\n+    int src_index() const              { return _src_index; }\n+    VMRegPair dst() const              { return _dst; }\n+    void set_dst(int i, VMRegPair dst) { _dst_index = i, _dst = dst; }\n+    int dst_index() const              { return _dst_index; }\n+    int dst_id() const                 { return get_id(dst()); }\n+    MoveOperation* next() const       { return _next; }\n+    MoveOperation* prev() const       { return _prev; }\n+    void set_processed()               { _processed = true; }\n+    bool is_processed() const          { return _processed; }\n+\n+    \/\/ insert\n+    void break_cycle(VMRegPair temp_register) {\n+      \/\/ create a new store following the last store\n+      \/\/ to move from the temp_register to the original\n+      MoveOperation* new_store = new MoveOperation(-1, temp_register, dst_index(), dst());\n+\n+      \/\/ break the cycle of links and insert new_store at the end\n+      \/\/ break the reverse link.\n+      MoveOperation* p = prev();\n+      assert(p->next() == this, \"must be\");\n+      _prev = NULL;\n+      p->_next = new_store;\n+      new_store->_prev = p;\n+\n+      \/\/ change the original store to save it's value in the temp.\n+      set_dst(-1, temp_register);\n+    }\n+\n+    void link(GrowableArray<MoveOperation*>& killer) {\n+      \/\/ link this store in front the store that it depends on\n+      MoveOperation* n = killer.at_grow(src_id(), NULL);\n+      if (n != NULL) {\n+        assert(_next == NULL && n->_prev == NULL, \"shouldn't have been set yet\");\n+        _next = n;\n+        n->_prev = this;\n+      }\n+    }\n+  };\n+\n+ private:\n+  GrowableArray<MoveOperation*> edges;\n+\n+ public:\n+  ComputeMoveOrder(int total_in_args, const VMRegPair* in_regs, int total_c_args, VMRegPair* out_regs,\n+                  const BasicType* in_sig_bt, GrowableArray<int>& arg_order, VMRegPair tmp_vmreg) {\n+    \/\/ Move operations where the dest is the stack can all be\n+    \/\/ scheduled first since they can't interfere with the other moves.\n+    for (int i = total_in_args - 1, c_arg = total_c_args - 1; i >= 0; i--, c_arg--) {\n+      if (in_sig_bt[i] == T_ARRAY) {\n+        c_arg--;\n+        if (out_regs[c_arg].first()->is_stack() &&\n+            out_regs[c_arg + 1].first()->is_stack()) {\n+          arg_order.push(i);\n+          arg_order.push(c_arg);\n+        } else {\n+          if (out_regs[c_arg].first()->is_stack() ||\n+              in_regs[i].first() == out_regs[c_arg].first()) {\n+            add_edge(i, in_regs[i].first(), c_arg, out_regs[c_arg + 1]);\n+          } else {\n+            add_edge(i, in_regs[i].first(), c_arg, out_regs[c_arg]);\n+          }\n+        }\n+      } else if (in_sig_bt[i] == T_VOID) {\n+        arg_order.push(i);\n+        arg_order.push(c_arg);\n+      } else {\n+        if (out_regs[c_arg].first()->is_stack() ||\n+            in_regs[i].first() == out_regs[c_arg].first()) {\n+          arg_order.push(i);\n+          arg_order.push(c_arg);\n+        } else {\n+          add_edge(i, in_regs[i].first(), c_arg, out_regs[c_arg]);\n+        }\n+      }\n+    }\n+    \/\/ Break any cycles in the register moves and emit the in the\n+    \/\/ proper order.\n+    GrowableArray<MoveOperation*>* stores = get_store_order(tmp_vmreg);\n+    for (int i = 0; i < stores->length(); i++) {\n+      arg_order.push(stores->at(i)->src_index());\n+      arg_order.push(stores->at(i)->dst_index());\n+    }\n+ }\n+\n+  \/\/ Collected all the move operations\n+  void add_edge(int src_index, VMRegPair src, int dst_index, VMRegPair dst) {\n+    if (src.first() == dst.first()) return;\n+    edges.append(new MoveOperation(src_index, src, dst_index, dst));\n+  }\n+\n+  \/\/ Walk the edges breaking cycles between moves.  The result list\n+  \/\/ can be walked in order to produce the proper set of loads\n+  GrowableArray<MoveOperation*>* get_store_order(VMRegPair temp_register) {\n+    \/\/ Record which moves kill which values\n+    GrowableArray<MoveOperation*> killer;\n+    for (int i = 0; i < edges.length(); i++) {\n+      MoveOperation* s = edges.at(i);\n+      assert(killer.at_grow(s->dst_id(), NULL) == NULL, \"only one killer\");\n+      killer.at_put_grow(s->dst_id(), s, NULL);\n+    }\n+    assert(killer.at_grow(MoveOperation::get_id(temp_register), NULL) == NULL,\n+           \"make sure temp isn't in the registers that are killed\");\n+\n+    \/\/ create links between loads and stores\n+    for (int i = 0; i < edges.length(); i++) {\n+      edges.at(i)->link(killer);\n+    }\n+\n+    \/\/ at this point, all the move operations are chained together\n+    \/\/ in a doubly linked list.  Processing it backwards finds\n+    \/\/ the beginning of the chain, forwards finds the end.  If there's\n+    \/\/ a cycle it can be broken at any point,  so pick an edge and walk\n+    \/\/ backward until the list ends or we end where we started.\n+    GrowableArray<MoveOperation*>* stores = new GrowableArray<MoveOperation*>();\n+    for (int e = 0; e < edges.length(); e++) {\n+      MoveOperation* s = edges.at(e);\n+      if (!s->is_processed()) {\n+        MoveOperation* start = s;\n+        \/\/ search for the beginning of the chain or cycle\n+        while (start->prev() != NULL && start->prev() != s) {\n+          start = start->prev();\n+        }\n+        if (start->prev() == s) {\n+          start->break_cycle(temp_register);\n+        }\n+        \/\/ walk the chain forward inserting to store list\n+        while (start != NULL) {\n+          stores->append(start);\n+          start->set_processed();\n+          start = start->next();\n+        }\n+      }\n+    }\n+    return stores;\n+  }\n+};\n+\n@@ -1470,1 +1684,2 @@\n-                                                BasicType ret_type) {\n+                                                BasicType ret_type,\n+                                                address critical_entry) {\n@@ -1521,1 +1736,6 @@\n-  address native_func = method->native_function();\n+  bool is_critical_native = true;\n+  address native_func = critical_entry;\n+  if (native_func == NULL) {\n+    native_func = method->native_function();\n+    is_critical_native = false;\n+  }\n@@ -1535,1 +1755,13 @@\n-  int total_c_args = total_in_args + (method->is_static() ? 2 : 1);\n+  int total_c_args = total_in_args;\n+  if (!is_critical_native) {\n+    total_c_args += 1;\n+    if (method->is_static()) {\n+      total_c_args++;\n+    }\n+  } else {\n+    for (int i = 0; i < total_in_args; i++) {\n+      if (in_sig_bt[i] == T_ARRAY) {\n+        total_c_args++;\n+      }\n+    }\n+  }\n@@ -1542,4 +1774,5 @@\n-  out_sig_bt[argc++] = T_ADDRESS;\n-  if (method->is_static()) {\n-    out_sig_bt[argc++] = T_OBJECT;\n-  }\n+  if (!is_critical_native) {\n+    out_sig_bt[argc++] = T_ADDRESS;\n+    if (method->is_static()) {\n+      out_sig_bt[argc++] = T_OBJECT;\n+    }\n@@ -1547,2 +1780,24 @@\n-  for (int i = 0; i < total_in_args ; i++ ) {\n-    out_sig_bt[argc++] = in_sig_bt[i];\n+    for (int i = 0; i < total_in_args ; i++ ) {\n+      out_sig_bt[argc++] = in_sig_bt[i];\n+    }\n+  } else {\n+    in_elem_bt = NEW_RESOURCE_ARRAY(BasicType, total_in_args);\n+    SignatureStream ss(method->signature());\n+    for (int i = 0; i < total_in_args ; i++ ) {\n+      if (in_sig_bt[i] == T_ARRAY) {\n+        \/\/ Arrays are passed as int, elem* pair\n+        out_sig_bt[argc++] = T_INT;\n+        out_sig_bt[argc++] = T_ADDRESS;\n+        ss.skip_array_prefix(1);  \/\/ skip one '['\n+        assert(ss.is_primitive(), \"primitive type expected\");\n+        in_elem_bt[i] = ss.type();\n+      } else {\n+        out_sig_bt[argc++] = in_sig_bt[i];\n+        in_elem_bt[i] = T_VOID;\n+      }\n+      if (in_sig_bt[i] != T_VOID) {\n+        assert(in_sig_bt[i] == ss.type() ||\n+               in_sig_bt[i] == T_ARRAY, \"must match\");\n+        ss.next();\n+      }\n+    }\n@@ -1566,0 +1821,34 @@\n+  if (is_critical_native) {\n+    \/\/ Critical natives may have to call out so they need a save area\n+    \/\/ for register arguments.\n+    int double_slots = 0;\n+    int single_slots = 0;\n+    for ( int i = 0; i < total_in_args; i++) {\n+      if (in_regs[i].first()->is_Register()) {\n+        const Register reg = in_regs[i].first()->as_Register();\n+        switch (in_sig_bt[i]) {\n+          case T_BOOLEAN:\n+          case T_BYTE:\n+          case T_SHORT:\n+          case T_CHAR:\n+          case T_INT:  single_slots++; break;\n+          case T_ARRAY:  \/\/ specific to LP64 (7145024)\n+          case T_LONG: double_slots++; break;\n+          default:  ShouldNotReachHere();\n+        }\n+      } else if (in_regs[i].first()->is_XMMRegister()) {\n+        switch (in_sig_bt[i]) {\n+          case T_FLOAT:  single_slots++; break;\n+          case T_DOUBLE: double_slots++; break;\n+          default:  ShouldNotReachHere();\n+        }\n+      } else if (in_regs[i].first()->is_FloatRegister()) {\n+        ShouldNotReachHere();\n+      }\n+    }\n+    total_save_slots = double_slots * 2 + single_slots;\n+    \/\/ align the save area\n+    if (double_slots != 0) {\n+      stack_slots = align_up(stack_slots, 2);\n+    }\n+  }\n@@ -1760,1 +2049,4 @@\n-  \/\/ For JNI natives the incoming and outgoing registers are offset upwards.\n+  \/\/ This may iterate in two different directions depending on the\n+  \/\/ kind of native it is.  The reason is that for regular JNI natives\n+  \/\/ the incoming and outgoing registers are offset upwards and for\n+  \/\/ critical natives they are offset down.\n@@ -1766,3 +2058,8 @@\n-  for (int i = total_in_args - 1, c_arg = total_c_args - 1; i >= 0; i--, c_arg--) {\n-    arg_order.push(i);\n-    arg_order.push(c_arg);\n+  if (!is_critical_native) {\n+    for (int i = total_in_args - 1, c_arg = total_c_args - 1; i >= 0; i--, c_arg--) {\n+      arg_order.push(i);\n+      arg_order.push(c_arg);\n+    }\n+  } else {\n+    \/\/ Compute a valid move order, using tmp_vmreg to break any cycles\n+    ComputeMoveOrder cmo(total_in_args, in_regs, total_c_args, out_regs, in_sig_bt, arg_order, tmp_vmreg);\n@@ -1776,0 +2073,14 @@\n+    if (c_arg == -1) {\n+      assert(is_critical_native, \"should only be required for critical natives\");\n+      \/\/ This arg needs to be moved to a temporary\n+      __ mov(tmp_vmreg.first()->as_Register(), in_regs[i].first()->as_Register());\n+      in_regs[i] = tmp_vmreg;\n+      temploc = i;\n+      continue;\n+    } else if (i == -1) {\n+      assert(is_critical_native, \"should only be required for critical natives\");\n+      \/\/ Read from the temporary location\n+      assert(temploc != -1, \"must be valid\");\n+      i = temploc;\n+      temploc = -1;\n+    }\n@@ -1790,0 +2101,12 @@\n+        if (is_critical_native) {\n+          unpack_array_argument(masm, in_regs[i], in_elem_bt[i], out_regs[c_arg + 1], out_regs[c_arg]);\n+          c_arg++;\n+#ifdef ASSERT\n+          if (out_regs[c_arg].first()->is_Register()) {\n+            reg_destroyed[out_regs[c_arg].first()->as_Register()->encoding()] = true;\n+          } else if (out_regs[c_arg].first()->is_XMMRegister()) {\n+            freg_destroyed[out_regs[c_arg].first()->as_XMMRegister()->encoding()] = true;\n+          }\n+#endif\n+          break;\n+        }\n@@ -1791,0 +2114,1 @@\n+        assert(!is_critical_native, \"no oop arguments\");\n@@ -1824,19 +2148,24 @@\n-  \/\/ point c_arg at the first arg that is already loaded in case we\n-  \/\/ need to spill before we call out\n-  c_arg = total_c_args - total_in_args;\n-\n-  if (method->is_static()) {\n-\n-    \/\/  load oop into a register\n-    __ movoop(oop_handle_reg, JNIHandles::make_local(method->method_holder()->java_mirror()));\n-\n-    \/\/ Now handlize the static class mirror it's known not-null.\n-    __ movptr(Address(rsp, klass_offset), oop_handle_reg);\n-    map->set_oop(VMRegImpl::stack2reg(klass_slot_offset));\n-\n-    \/\/ Now get the handle\n-    __ lea(oop_handle_reg, Address(rsp, klass_offset));\n-    \/\/ store the klass handle as second argument\n-    __ movptr(c_rarg1, oop_handle_reg);\n-    \/\/ and protect the arg if we must spill\n-    c_arg--;\n+  if (!is_critical_native) {\n+    \/\/ point c_arg at the first arg that is already loaded in case we\n+    \/\/ need to spill before we call out\n+    c_arg = total_c_args - total_in_args;\n+\n+    if (method->is_static()) {\n+\n+      \/\/  load oop into a register\n+      __ movoop(oop_handle_reg, JNIHandles::make_local(method->method_holder()->java_mirror()));\n+\n+      \/\/ Now handlize the static class mirror it's known not-null.\n+      __ movptr(Address(rsp, klass_offset), oop_handle_reg);\n+      map->set_oop(VMRegImpl::stack2reg(klass_slot_offset));\n+\n+      \/\/ Now get the handle\n+      __ lea(oop_handle_reg, Address(rsp, klass_offset));\n+      \/\/ store the klass handle as second argument\n+      __ movptr(c_rarg1, oop_handle_reg);\n+      \/\/ and protect the arg if we must spill\n+      c_arg--;\n+    }\n+  } else {\n+    \/\/ For JNI critical methods we need to save all registers in save_args.\n+    c_arg = 0;\n@@ -1894,0 +2223,2 @@\n+    assert(!is_critical_native, \"unhandled\");\n+\n@@ -1950,1 +2281,2 @@\n-  __ lea(c_rarg0, Address(r15_thread, in_bytes(JavaThread::jni_environment_offset())));\n+  if (!is_critical_native) {\n+    __ lea(c_rarg0, Address(r15_thread, in_bytes(JavaThread::jni_environment_offset())));\n@@ -1952,2 +2284,3 @@\n-  \/\/ Now set thread in native\n-  __ movl(Address(r15_thread, JavaThread::thread_state_offset()), _thread_in_native);\n+    \/\/ Now set thread in native\n+    __ movl(Address(r15_thread, JavaThread::thread_state_offset()), _thread_in_native);\n+  }\n@@ -1981,0 +2314,11 @@\n+  \/\/ If this is a critical native, check for a safepoint or suspend request after the call.\n+  \/\/ If a safepoint is needed, transition to native, then to native_trans to handle\n+  \/\/ safepoints like the native methods that are not critical natives.\n+  if (is_critical_native) {\n+    Label needs_safepoint;\n+    __ safepoint_poll(needs_safepoint, r15_thread, false \/* at_return *\/, false \/* in_nmethod *\/);\n+    __ cmpl(Address(r15_thread, JavaThread::suspend_flags_offset()), 0);\n+    __ jcc(Assembler::equal, after_transition);\n+    __ bind(needs_safepoint);\n+  }\n+\n@@ -2106,3 +2450,5 @@\n-  \/\/ reset handle block\n-  __ movptr(rcx, Address(r15_thread, JavaThread::active_handles_offset()));\n-  __ movl(Address(rcx, JNIHandleBlock::top_offset_in_bytes()), (int32_t)NULL_WORD);\n+  if (!is_critical_native) {\n+    \/\/ reset handle block\n+    __ movptr(rcx, Address(r15_thread, JavaThread::active_handles_offset()));\n+    __ movl(Address(rcx, JNIHandleBlock::top_offset_in_bytes()), (int32_t)NULL_WORD);\n+  }\n@@ -2114,3 +2460,5 @@\n-  \/\/ Any exception pending?\n-  __ cmpptr(Address(r15_thread, in_bytes(Thread::pending_exception_offset())), (int32_t)NULL_WORD);\n-  __ jcc(Assembler::notEqual, exception_pending);\n+  if (!is_critical_native) {\n+    \/\/ Any exception pending?\n+    __ cmpptr(Address(r15_thread, in_bytes(Thread::pending_exception_offset())), (int32_t)NULL_WORD);\n+    __ jcc(Assembler::notEqual, exception_pending);\n+  }\n@@ -2124,2 +2472,3 @@\n-  \/\/ forward the exception\n-  __ bind(exception_pending);\n+  if (!is_critical_native) {\n+    \/\/ forward the exception\n+    __ bind(exception_pending);\n@@ -2127,2 +2476,3 @@\n-  \/\/ and forward the exception\n-  __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+    \/\/ and forward the exception\n+    __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":395,"deletions":45,"binary":false,"changes":440,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -76,1 +76,2 @@\n-                                                BasicType ret_type) {\n+                                                BasicType ret_type,\n+                                                address critical_entry) {\n","filename":"src\/hotspot\/cpu\/zero\/sharedRuntime_zero.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -119,0 +119,1 @@\n+  UNSUPPORTED_OPTION(CriticalJNINatives);\n","filename":"src\/hotspot\/cpu\/zero\/vm_version_zero.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -185,0 +185,18 @@\n+\n+char* NativeLookup::critical_jni_name(const methodHandle& method) {\n+  stringStream st;\n+  \/\/ Prefix\n+  st.print(\"JavaCritical_\");\n+  \/\/ Klass name\n+  if (!map_escaped_name_on(&st, method->klass_name())) {\n+    return NULL;\n+  }\n+  st.print(\"_\");\n+  \/\/ Method name\n+  if (!map_escaped_name_on(&st, method->name())) {\n+    return NULL;\n+  }\n+  return st.as_string();\n+}\n+\n+\n@@ -315,0 +333,6 @@\n+address NativeLookup::lookup_critical_style(void* dll, const char* pure_name, const char* long_name, int args_size, bool os_style) {\n+  const char* jni_name = compute_complete_jni_name(pure_name, long_name, args_size, os_style);\n+  assert(dll != NULL, \"dll must be loaded\");\n+  return (address)os::dll_lookup(dll, jni_name);\n+}\n+\n@@ -358,0 +382,47 @@\n+\/\/ Check all the formats of native implementation name to see if there is one\n+\/\/ for the specified method.\n+address NativeLookup::lookup_critical_entry(const methodHandle& method) {\n+  assert(CriticalJNINatives, \"or should not be here\");\n+\n+  if (method->is_synchronized() ||\n+      !method->is_static()) {\n+    \/\/ Only static non-synchronized methods are allowed\n+    return NULL;\n+  }\n+\n+  ResourceMark rm;\n+\n+  Symbol* signature = method->signature();\n+  for (int end = 0; end < signature->utf8_length(); end++) {\n+    if (signature->char_at(end) == 'L') {\n+      \/\/ Don't allow object types\n+      return NULL;\n+    }\n+  }\n+\n+  \/\/ Compute argument size\n+  int args_size = method->size_of_parameters();\n+  for (SignatureStream ss(signature); !ss.at_return_type(); ss.next()) {\n+    if (ss.is_array()) {\n+      args_size += T_INT_size; \/\/ array length parameter\n+    }\n+  }\n+\n+  \/\/ dll handling requires I\/O. Don't do that while in _thread_in_vm (safepoint may get requested).\n+  ThreadToNativeFromVM thread_in_native(JavaThread::current());\n+\n+  void* dll = dll_load(method);\n+  address entry = NULL;\n+\n+  if (dll != NULL) {\n+    entry = lookup_critical_style(dll, method, args_size);\n+    \/\/ Close the handle to avoid keeping the library alive if the native method holder is unloaded.\n+    \/\/ This is fine because the library is still kept alive by JNI (see JVM_LoadLibrary). As soon\n+    \/\/ as the holder class and the library are unloaded (see JVM_UnloadLibrary), the native wrapper\n+    \/\/ that calls 'critical_entry' becomes unreachable and is unloaded as well.\n+    os::dll_unload(dll);\n+  }\n+\n+  return entry; \/\/ NULL indicates not found\n+}\n+\n@@ -376,0 +447,38 @@\n+address NativeLookup::lookup_critical_style(void* dll, const methodHandle& method, int args_size) {\n+  address entry = NULL;\n+  const char* critical_name = critical_jni_name(method);\n+  if (critical_name == NULL) {\n+    \/\/ JNI name mapping rejected this method so return\n+    \/\/ NULL to indicate UnsatisfiedLinkError should be thrown.\n+    return NULL;\n+  }\n+\n+  \/\/ 1) Try JNI short style\n+  entry = lookup_critical_style(dll, critical_name, \"\",        args_size, true);\n+  if (entry != NULL) {\n+    return entry;\n+  }\n+\n+  const char* long_name = long_jni_name(method);\n+  if (long_name == NULL) {\n+    \/\/ JNI name mapping rejected this method so return\n+    \/\/ NULL to indicate UnsatisfiedLinkError should be thrown.\n+    return NULL;\n+  }\n+\n+  \/\/ 2) Try JNI long style\n+  entry = lookup_critical_style(dll, critical_name, long_name, args_size, true);\n+  if (entry != NULL) {\n+    return entry;\n+  }\n+\n+  \/\/ 3) Try JNI short style without os prefix\/suffix\n+  entry = lookup_critical_style(dll, critical_name, \"\",        args_size, false);\n+  if (entry != NULL) {\n+    return entry;\n+  }\n+\n+  \/\/ 4) Try JNI long style without os prefix\/suffix\n+  return lookup_critical_style(dll, critical_name, long_name, args_size, false);\n+}\n+\n","filename":"src\/hotspot\/share\/prims\/nativeLookup.cpp","additions":109,"deletions":0,"binary":false,"changes":109,"status":"modified"},{"patch":"@@ -38,0 +38,2 @@\n+  static address lookup_critical_style(void* dll, const char* pure_name, const char* long_name, int args_size, bool os_style);\n+  static address lookup_critical_style(void* dll, const methodHandle& method, int args_size);\n@@ -48,0 +50,1 @@\n+  static char* critical_jni_name(const methodHandle& method);\n@@ -51,0 +54,1 @@\n+  static address lookup_critical_entry(const methodHandle& method);\n","filename":"src\/hotspot\/share\/prims\/nativeLookup.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -535,0 +535,1 @@\n+  { \"CriticalJNINatives\",           JDK_Version::jdk(17), JDK_Version::undefined(), JDK_Version::undefined() },\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -317,0 +317,3 @@\n+  product(bool, CriticalJNINatives, false,                                  \\\n+          \"(Deprecated) Check for critical JNI entry points\")               \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -3039,0 +3039,1 @@\n+  address critical_entry = NULL;\n@@ -3044,0 +3045,5 @@\n+  if (CriticalJNINatives && !method->is_method_handle_intrinsic()) {\n+    \/\/ We perform the I\/O with transition to native before acquiring AdapterHandlerLibrary_lock.\n+    critical_entry = NativeLookup::lookup_critical_entry(method);\n+  }\n+\n@@ -3093,1 +3099,1 @@\n-      nm = SharedRuntime::generate_native_wrapper(&_masm, method, compile_id, sig_bt, regs, ret_type);\n+      nm = SharedRuntime::generate_native_wrapper(&_masm, method, compile_id, sig_bt, regs, ret_type, critical_entry);\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.cpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -475,1 +475,2 @@\n-  \/\/ is a compiled method handle adapter, such as _invokeBasic, _linkToVirtual, etc.\n+  \/\/ is a JNI critical method, or a compiled method handle adapter,\n+  \/\/ such as _invokeBasic, _linkToVirtual, etc.\n@@ -481,1 +482,2 @@\n-                                          BasicType ret_type);\n+                                          BasicType ret_type,\n+                                          address critical_entry);\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.hpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -291,0 +291,1 @@\n+  -gc\/CriticalNativeArgs.java \\\n@@ -305,1 +306,3 @@\n-tier2_gc_epsilon =\n+tier2_gc_epsilon = \\\n+  gc\/CriticalNativeArgs.java \\\n+  gc\/stress\/CriticalNativeStress.java\n@@ -344,0 +347,2 @@\n+  gc\/CriticalNativeArgs.java \\\n+  gc\/stress\/CriticalNativeStress.java \\\n","filename":"test\/hotspot\/jtreg\/TEST.groups","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -60,0 +60,1 @@\n+            {\"CriticalJNINatives\", \"true\"},\n","filename":"test\/hotspot\/jtreg\/runtime\/CommandLine\/VMDeprecatedOptions.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"}]}