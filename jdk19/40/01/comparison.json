{"files":[{"patch":"@@ -4443,0 +4443,10 @@\n+operand immI_positive()\n+%{\n+  predicate(n->get_int() > 0);\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -4724,1 +4724,1 @@\n-instruct vsra8B_imm(vecD dst, vecD src, immI shift) %{\n+instruct vsra8B_imm(vecD dst, vecD src, immI_positive shift) %{\n@@ -4739,1 +4739,1 @@\n-instruct vsra16B_imm(vecX dst, vecX src, immI shift) %{\n+instruct vsra16B_imm(vecX dst, vecX src, immI_positive shift) %{\n@@ -4753,1 +4753,1 @@\n-instruct vsrl8B_imm(vecD dst, vecD src, immI shift) %{\n+instruct vsrl8B_imm(vecD dst, vecD src, immI_positive shift) %{\n@@ -4773,1 +4773,1 @@\n-instruct vsrl16B_imm(vecX dst, vecX src, immI shift) %{\n+instruct vsrl16B_imm(vecX dst, vecX src, immI_positive shift) %{\n@@ -4981,1 +4981,1 @@\n-instruct vsra4S_imm(vecD dst, vecD src, immI shift) %{\n+instruct vsra4S_imm(vecD dst, vecD src, immI_positive shift) %{\n@@ -4996,1 +4996,1 @@\n-instruct vsra8S_imm(vecX dst, vecX src, immI shift) %{\n+instruct vsra8S_imm(vecX dst, vecX src, immI_positive shift) %{\n@@ -5010,1 +5010,1 @@\n-instruct vsrl4S_imm(vecD dst, vecD src, immI shift) %{\n+instruct vsrl4S_imm(vecD dst, vecD src, immI_positive shift) %{\n@@ -5030,1 +5030,1 @@\n-instruct vsrl8S_imm(vecX dst, vecX src, immI shift) %{\n+instruct vsrl8S_imm(vecX dst, vecX src, immI_positive shift) %{\n@@ -5221,1 +5221,1 @@\n-instruct vsra2I_imm(vecD dst, vecD src, immI shift) %{\n+instruct vsra2I_imm(vecD dst, vecD src, immI_positive shift) %{\n@@ -5234,1 +5234,1 @@\n-instruct vsra4I_imm(vecX dst, vecX src, immI shift) %{\n+instruct vsra4I_imm(vecX dst, vecX src, immI_positive shift) %{\n@@ -5247,1 +5247,1 @@\n-instruct vsrl2I_imm(vecD dst, vecD src, immI shift) %{\n+instruct vsrl2I_imm(vecD dst, vecD src, immI_positive shift) %{\n@@ -5260,1 +5260,1 @@\n-instruct vsrl4I_imm(vecX dst, vecX src, immI shift) %{\n+instruct vsrl4I_imm(vecX dst, vecX src, immI_positive shift) %{\n@@ -5359,1 +5359,1 @@\n-instruct vsra2L_imm(vecX dst, vecX src, immI shift) %{\n+instruct vsra2L_imm(vecX dst, vecX src, immI_positive shift) %{\n@@ -5372,1 +5372,1 @@\n-instruct vsrl2L_imm(vecX dst, vecX src, immI shift) %{\n+instruct vsrl2L_imm(vecX dst, vecX src, immI_positive shift) %{\n@@ -5385,1 +5385,1 @@\n-instruct vsraa8B_imm(vecD dst, vecD src, immI shift) %{\n+instruct vsraa8B_imm(vecD dst, vecD src, immI_positive shift) %{\n@@ -5399,1 +5399,1 @@\n-instruct vsraa16B_imm(vecX dst, vecX src, immI shift) %{\n+instruct vsraa16B_imm(vecX dst, vecX src, immI_positive shift) %{\n@@ -5413,1 +5413,1 @@\n-instruct vsraa4S_imm(vecD dst, vecD src, immI shift) %{\n+instruct vsraa4S_imm(vecD dst, vecD src, immI_positive shift) %{\n@@ -5427,1 +5427,1 @@\n-instruct vsraa8S_imm(vecX dst, vecX src, immI shift) %{\n+instruct vsraa8S_imm(vecX dst, vecX src, immI_positive shift) %{\n@@ -5441,1 +5441,1 @@\n-instruct vsraa2I_imm(vecD dst, vecD src, immI shift) %{\n+instruct vsraa2I_imm(vecD dst, vecD src, immI_positive shift) %{\n@@ -5454,1 +5454,1 @@\n-instruct vsraa4I_imm(vecX dst, vecX src, immI shift) %{\n+instruct vsraa4I_imm(vecX dst, vecX src, immI_positive shift) %{\n@@ -5467,1 +5467,1 @@\n-instruct vsraa2L_imm(vecX dst, vecX src, immI shift) %{\n+instruct vsraa2L_imm(vecX dst, vecX src, immI_positive shift) %{\n@@ -5480,1 +5480,1 @@\n-instruct vsrla8B_imm(vecD dst, vecD src, immI shift) %{\n+instruct vsrla8B_imm(vecD dst, vecD src, immI_positive shift) %{\n@@ -5495,1 +5495,1 @@\n-instruct vsrla16B_imm(vecX dst, vecX src, immI shift) %{\n+instruct vsrla16B_imm(vecX dst, vecX src, immI_positive shift) %{\n@@ -5510,1 +5510,1 @@\n-instruct vsrla4S_imm(vecD dst, vecD src, immI shift) %{\n+instruct vsrla4S_imm(vecD dst, vecD src, immI_positive shift) %{\n@@ -5525,1 +5525,1 @@\n-instruct vsrla8S_imm(vecX dst, vecX src, immI shift) %{\n+instruct vsrla8S_imm(vecX dst, vecX src, immI_positive shift) %{\n@@ -5540,1 +5540,1 @@\n-instruct vsrla2I_imm(vecD dst, vecD src, immI shift) %{\n+instruct vsrla2I_imm(vecD dst, vecD src, immI_positive shift) %{\n@@ -5553,1 +5553,1 @@\n-instruct vsrla4I_imm(vecX dst, vecX src, immI shift) %{\n+instruct vsrla4I_imm(vecX dst, vecX src, immI_positive shift) %{\n@@ -5566,1 +5566,1 @@\n-instruct vsrla2L_imm(vecX dst, vecX src, immI shift) %{\n+instruct vsrla2L_imm(vecX dst, vecX src, immI_positive shift) %{\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_neon.ad","additions":28,"deletions":28,"binary":false,"changes":56,"status":"modified"},{"patch":"@@ -2202,1 +2202,1 @@\n-instruct vsra$1$2_imm`'(vec$4 dst, vec$4 src, immI shift) %{\n+instruct vsra$1$2_imm`'(vec$4 dst, vec$4 src, immI_positive shift) %{\n@@ -2224,1 +2224,1 @@\n-instruct vsrl$1$2_imm`'(vec$4 dst, vec$4 src, immI shift) %{\n+instruct vsrl$1$2_imm`'(vec$4 dst, vec$4 src, immI_positive shift) %{\n@@ -2256,1 +2256,1 @@\n-instruct vsrla$1$2_imm`'(vec$4 dst, vec$4 src, immI shift) %{\n+instruct vsrla$1$2_imm`'(vec$4 dst, vec$4 src, immI_positive shift) %{\n@@ -2280,1 +2280,1 @@\n-instruct vsraa$1$2_imm`'(vec$4 dst, vec$4 src, immI shift) %{\n+instruct vsraa$1$2_imm`'(vec$4 dst, vec$4 src, immI_positive shift) %{\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_neon_ad.m4","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -0,0 +1,73 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+\/*\n+ * @test\n+ * @bug 8288445\n+ * @summary Test shift by 0\n+ * @library \/test\/lib\n+ * @run main compiler.codegen.ShiftByZero\n+ * @run main\/othervm -Xbatch -XX:-TieredCompilation\n+ * compiler.codegen.ShiftByZero\n+ *\/\n+\n+package compiler.codegen;\n+\n+public class ShiftByZero {\n+\n+    public static final int N = 64;\n+\n+    public static int[] i32 = new int[N];\n+\n+    public static void bMeth() {\n+        int shift = i32[0];\n+        for (int i8 = 279; i8 > 1; --i8) {\n+            shift <<= 6;\n+        }\n+        \/\/ low 6 bits of shift are 0, so shift can be\n+        \/\/ simplified to constant 0\n+        {\n+            for (int i = 0; i < N; ++i) {\n+                i32[i] += i32[i] >>= shift;\n+            }\n+            for (int i = 0; i < N; ++i) {\n+                i32[i] += i32[i] >>>= shift;\n+            }\n+            for (int i = 0; i < N; ++i) {\n+                i32[i] >>>= shift;\n+            }\n+            for (int i = 0; i < N; ++i) {\n+                i32[i] >>= shift;\n+            }\n+            for (int i = 0; i < N; ++i) {\n+                i32[i] <<= shift;\n+            }\n+        }\n+    }\n+\n+    public static void main(String[] strArr) {\n+        for (int i = 0; i < 20_000; i++) {\n+            bMeth();\n+        }\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/codegen\/ShiftByZero.java","additions":73,"deletions":0,"binary":false,"changes":73,"status":"added"}]}