{"files":[{"patch":"@@ -3761,0 +3761,9 @@\n+void Assembler::vpermb(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {\n+  assert(VM_Version::supports_avx512_vbmi(), \"\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8((unsigned char)0x8D);\n+  emit_operand(dst, src);\n+}\n+\n@@ -3841,0 +3850,8 @@\n+void Assembler::evpmultishiftqb(XMMRegister dst, XMMRegister ctl, XMMRegister src, int vector_len) {\n+  assert(VM_Version::supports_avx512_vbmi(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(dst->encoding(), ctl->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0x83, (unsigned char)(0xC0 | encode));\n+}\n+\n@@ -4139,0 +4156,9 @@\n+void Assembler::vpmaskmovd(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {\n+  assert((VM_Version::supports_avx2() && vector_len == AVX_256bit), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8((unsigned char)0x8C);\n+  emit_operand(dst, src);\n+}\n+\n@@ -6568,0 +6594,7 @@\n+void Assembler::vpsubusb(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+  assert(UseAVX > 0, \"requires some form of AVX\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xD8, (0xC0 | encode));\n+}\n+\n@@ -6659,0 +6692,9 @@\n+void Assembler::vpmulhuw(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+  assert((vector_len == AVX_128bit && VM_Version::supports_avx()) ||\n+         (vector_len == AVX_256bit && VM_Version::supports_avx2()) ||\n+         (vector_len == AVX_512bit && VM_Version::supports_avx512bw()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xE4, (0xC0 | encode));\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":42,"deletions":0,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -1693,0 +1693,1 @@\n+  void vpermb(XMMRegister dst, XMMRegister nds, Address src, int vector_len);\n@@ -1703,0 +1704,1 @@\n+  void evpmultishiftqb(XMMRegister dst, XMMRegister ctl, XMMRegister src, int vector_len);\n@@ -1751,0 +1753,1 @@\n+  void vpmaskmovd(XMMRegister dst, XMMRegister nds, Address src, int vector_len);\n@@ -2253,0 +2256,1 @@\n+  void vpsubusb(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len);\n@@ -2273,0 +2277,1 @@\n+  void vpmulhuw(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len);\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -5290,2 +5290,93 @@\n-  \/\/base64 character set\n-  address base64_charset_addr() {\n+  address base64_shuffle_addr()\n+  {\n+    __ align(64, (unsigned long long)__ pc());\n+    StubCodeMark mark(this, \"StubRoutines\", \"shuffle_base64\");\n+    address start = __ pc();\n+    assert(((unsigned long long)start & 0x3f) == 0,\n+           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+    __ emit_data64(0x0405030401020001, relocInfo::none);\n+    __ emit_data64(0x0a0b090a07080607, relocInfo::none);\n+    __ emit_data64(0x10110f100d0e0c0d, relocInfo::none);\n+    __ emit_data64(0x1617151613141213, relocInfo::none);\n+    __ emit_data64(0x1c1d1b1c191a1819, relocInfo::none);\n+    __ emit_data64(0x222321221f201e1f, relocInfo::none);\n+    __ emit_data64(0x2829272825262425, relocInfo::none);\n+    __ emit_data64(0x2e2f2d2e2b2c2a2b, relocInfo::none);\n+    return start;\n+  }\n+\n+  address base64_avx2_shuffle_addr()\n+  {\n+    __ align(32);\n+    StubCodeMark mark(this, \"StubRoutines\", \"avx2_shuffle_base64\");\n+    address start = __ pc();\n+    __ emit_data64(0x0809070805060405, relocInfo::none);\n+    __ emit_data64(0x0e0f0d0e0b0c0a0b, relocInfo::none);\n+    __ emit_data64(0x0405030401020001, relocInfo::none);\n+    __ emit_data64(0x0a0b090a07080607, relocInfo::none);\n+    return start;\n+  }\n+\n+  address base64_avx2_input_mask_addr()\n+  {\n+    __ align(32);\n+    StubCodeMark mark(this, \"StubRoutines\", \"avx2_input_mask_base64\");\n+    address start = __ pc();\n+    __ emit_data64(0x8000000000000000, relocInfo::none);\n+    __ emit_data64(0x8000000080000000, relocInfo::none);\n+    __ emit_data64(0x8000000080000000, relocInfo::none);\n+    __ emit_data64(0x8000000080000000, relocInfo::none);\n+    return start;\n+  }\n+\n+  address base64_avx2_lut_addr()\n+  {\n+    __ align(32);\n+    StubCodeMark mark(this, \"StubRoutines\", \"avx2_lut_base64\");\n+    address start = __ pc();\n+    __ emit_data64(0xfcfcfcfcfcfc4741, relocInfo::none);\n+    __ emit_data64(0x0000f0edfcfcfcfc, relocInfo::none);\n+    __ emit_data64(0xfcfcfcfcfcfc4741, relocInfo::none);\n+    __ emit_data64(0x0000f0edfcfcfcfc, relocInfo::none);\n+\n+    \/\/ URL LUT\n+    __ emit_data64(0xfcfcfcfcfcfc4741, relocInfo::none);\n+    __ emit_data64(0x000020effcfcfcfc, relocInfo::none);\n+    __ emit_data64(0xfcfcfcfcfcfc4741, relocInfo::none);\n+    __ emit_data64(0x000020effcfcfcfc, relocInfo::none);\n+    return start;\n+  }\n+\n+  address base64_encoding_table_addr()\n+  {\n+    __ align(64, (unsigned long long)__ pc());\n+    StubCodeMark mark(this, \"StubRoutines\", \"encoding_table_base64\");\n+    address start = __ pc();\n+    assert(((unsigned long long)start & 0x3f) == 0, \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+    __ emit_data64(0x4847464544434241, relocInfo::none);\n+    __ emit_data64(0x504f4e4d4c4b4a49, relocInfo::none);\n+    __ emit_data64(0x5857565554535251, relocInfo::none);\n+    __ emit_data64(0x6665646362615a59, relocInfo::none);\n+    __ emit_data64(0x6e6d6c6b6a696867, relocInfo::none);\n+    __ emit_data64(0x767574737271706f, relocInfo::none);\n+    __ emit_data64(0x333231307a797877, relocInfo::none);\n+    __ emit_data64(0x2f2b393837363534, relocInfo::none);\n+\n+    \/\/ URL table\n+    __ emit_data64(0x4847464544434241, relocInfo::none);\n+    __ emit_data64(0x504f4e4d4c4b4a49, relocInfo::none);\n+    __ emit_data64(0x5857565554535251, relocInfo::none);\n+    __ emit_data64(0x6665646362615a59, relocInfo::none);\n+    __ emit_data64(0x6e6d6c6b6a696867, relocInfo::none);\n+    __ emit_data64(0x767574737271706f, relocInfo::none);\n+    __ emit_data64(0x333231307a797877, relocInfo::none);\n+    __ emit_data64(0x5f2d393837363534, relocInfo::none);\n+    return start;\n+  }\n+\n+  \/\/ Code for generating Base64 encoding.\n+  \/\/ Intrinsic function prototype in Base64.java:\n+  \/\/ private void encodeBlock(byte[] src, int sp, int sl, byte[] dst, int dp,\n+  \/\/ boolean isURL) {\n+  address generate_base64_encodeBlock()\n+  {\n@@ -5293,1 +5384,478 @@\n-    StubCodeMark mark(this, \"StubRoutines\", \"base64_charset\");\n+    StubCodeMark mark(this, \"StubRoutines\", \"implEncode\");\n+    address start = __ pc();\n+    __ enter();\n+\n+    \/\/ Save callee-saved registers before using them\n+    __ push(r12);\n+    __ push(r13);\n+    __ push(r14);\n+    __ push(r15);\n+\n+    \/\/ arguments\n+    const Register source = c_rarg0;       \/\/ Source Array\n+    const Register start_offset = c_rarg1; \/\/ start offset\n+    const Register end_offset = c_rarg2;   \/\/ end offset\n+    const Register dest = c_rarg3;   \/\/ destination array\n+\n+#ifndef _WIN64\n+    const Register dp = c_rarg4;    \/\/ Position for writing to dest array\n+    const Register isURL = c_rarg5; \/\/ Base64 or URL character set\n+#else\n+    const Address dp_mem(rbp, 6 * wordSize); \/\/ length is on stack on Win64\n+    const Address isURL_mem(rbp, 7 * wordSize);\n+    const Register isURL = r10; \/\/ pick the volatile windows register\n+    const Register dp = r12;\n+    __ movl(dp, dp_mem);\n+    __ movl(isURL, isURL_mem);\n+#endif\n+\n+    const Register length = r14;\n+    const Register encode_table = r13;\n+    Label L_process3, L_exit, L_processdata, L_vbmiLoop, L_not512, L_32byteLoop;\n+\n+    \/\/ calculate length from offsets\n+    __ movl(length, end_offset);\n+    __ subl(length, start_offset);\n+    __ cmpl(length, 0);\n+    __ jcc(Assembler::lessEqual, L_exit);\n+\n+    \/\/ Code for 512-bit VBMI encoding.  Encodes 48 input bytes into 64\n+    \/\/ output bytes. We read 64 input bytes and ignore the last 16, so be\n+    \/\/ sure not to read past the end of the input buffer.\n+    if (VM_Version::supports_avx512_vbmi()) {\n+      __ cmpl(length, 64); \/\/ Do not overrun input buffer.\n+      __ jcc(Assembler::below, L_not512);\n+\n+      __ shll(isURL, 6); \/\/ index into decode table based on isURL\n+      __ lea(encode_table, ExternalAddress(StubRoutines::x86::base64_encoding_table_addr()));\n+      __ addptr(encode_table, isURL);\n+      __ shrl(isURL, 6); \/\/ restore isURL\n+\n+      __ mov64(rax, 0x3036242a1016040aull); \/\/ Shifts\n+      __ evmovdquq(xmm3, ExternalAddress(StubRoutines::x86::base64_shuffle_addr()), Assembler::AVX_512bit, r15);\n+      __ evmovdquq(xmm2, Address(encode_table, 0), Assembler::AVX_512bit);\n+      __ evpbroadcastq(xmm1, rax, Assembler::AVX_512bit);\n+\n+      __ align(32);\n+      __ BIND(L_vbmiLoop);\n+\n+      __ vpermb(xmm0, xmm3, Address(source, start_offset), Assembler::AVX_512bit);\n+      __ subl(length, 48);\n+\n+      \/\/ Put the input bytes into the proper lanes for writing, then\n+      \/\/ encode them.\n+      __ evpmultishiftqb(xmm0, xmm1, xmm0, Assembler::AVX_512bit);\n+      __ vpermb(xmm0, xmm0, xmm2, Assembler::AVX_512bit);\n+\n+      \/\/ Write to destination\n+      __ evmovdquq(Address(dest, dp), xmm0, Assembler::AVX_512bit);\n+\n+      __ addptr(dest, 64);\n+      __ addptr(source, 48);\n+      __ cmpl(length, 64);\n+      __ jcc(Assembler::aboveEqual, L_vbmiLoop);\n+\n+      __ vzeroupper();\n+    }\n+\n+    __ BIND(L_not512);\n+    if (VM_Version::supports_avx2()\n+        && VM_Version::supports_avx512vlbw()) {\n+      \/*\n+      ** This AVX2 encoder is based off the paper at:\n+      **      https:\/\/dl.acm.org\/doi\/10.1145\/3132709\n+      **\n+      ** We use AVX2 SIMD instructions to encode 24 bytes into 32\n+      ** output bytes.\n+      **\n+      *\/\n+      \/\/ Lengths under 32 bytes are done with scalar routine\n+      __ cmpl(length, 31);\n+      __ jcc(Assembler::belowEqual, L_process3);\n+\n+      \/\/ Set up supporting constant table data\n+      __ vmovdqu(xmm9, ExternalAddress(StubRoutines::x86::base64_avx2_shuffle_addr()), rax);\n+      \/\/ 6-bit mask for 2nd and 4th (and multiples) 6-bit values\n+      __ movl(rax, 0x0fc0fc00);\n+      __ vmovdqu(xmm1, ExternalAddress(StubRoutines::x86::base64_avx2_input_mask_addr()), rax);\n+      __ evpbroadcastd(xmm8, rax, Assembler::AVX_256bit);\n+\n+      \/\/ Multiplication constant for \"shifting\" right by 6 and 10\n+      \/\/ bits\n+      __ movl(rax, 0x04000040);\n+\n+      __ subl(length, 24);\n+      __ evpbroadcastd(xmm7, rax, Assembler::AVX_256bit);\n+\n+      \/\/ For the first load, we mask off reading of the first 4\n+      \/\/ bytes into the register. This is so we can get 4 3-byte\n+      \/\/ chunks into each lane of the register, avoiding having to\n+      \/\/ handle end conditions.  We then shuffle these bytes into a\n+      \/\/ specific order so that manipulation is easier.\n+      \/\/\n+      \/\/ The initial read loads the XMM register like this:\n+      \/\/\n+      \/\/ Lower 128-bit lane:\n+      \/\/ +----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n+      \/\/ | XX | XX | XX | XX | A0 | A1 | A2 | B0 | B1 | B2 | C0 | C1\n+      \/\/ | C2 | D0 | D1 | D2 |\n+      \/\/ +----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n+      \/\/\n+      \/\/ Upper 128-bit lane:\n+      \/\/ +----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n+      \/\/ | E0 | E1 | E2 | F0 | F1 | F2 | G0 | G1 | G2 | H0 | H1 | H2\n+      \/\/ | XX | XX | XX | XX |\n+      \/\/ +----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n+      \/\/\n+      \/\/ Where A0 is the first input byte, B0 is the fourth, etc.\n+      \/\/ The alphabetical significance denotes the 3 bytes to be\n+      \/\/ consumed and encoded into 4 bytes.\n+      \/\/\n+      \/\/ We then shuffle the register so each 32-bit word contains\n+      \/\/ the sequence:\n+      \/\/    A1 A0 A2 A1, B1, B0, B2, B1, etc.\n+      \/\/ Each of these byte sequences are then manipulated into 4\n+      \/\/ 6-bit values ready for encoding.\n+      \/\/\n+      \/\/ If we focus on one set of 3-byte chunks, changing the\n+      \/\/ nomenclature such that A0 => a, A1 => b, and A2 => c, we\n+      \/\/ shuffle such that each 24-bit chunk contains:\n+      \/\/\n+      \/\/ b7 b6 b5 b4 b3 b2 b1 b0 | a7 a6 a5 a4 a3 a2 a1 a0 | c7 c6\n+      \/\/ c5 c4 c3 c2 c1 c0 | b7 b6 b5 b4 b3 b2 b1 b0\n+      \/\/ Explain this step.\n+      \/\/ b3 b2 b1 b0 c5 c4 c3 c2 | c1 c0 d5 d4 d3 d2 d1 d0 | a5 a4\n+      \/\/ a3 a2 a1 a0 b5 b4 | b3 b2 b1 b0 c5 c4 c3 c2\n+      \/\/\n+      \/\/ W first and off all but bits 4-9 and 16-21 (c5..c0 and\n+      \/\/ a5..a0) and shift them using a vector multiplication\n+      \/\/ operation (vpmulhuw) which effectively shifts c right by 6\n+      \/\/ bits and a right by 10 bits.  We similarly mask bits 10-15\n+      \/\/ (d5..d0) and 22-27 (b5..b0) and shift them left by 8 and 4\n+      \/\/ bits respecively.  This is done using vpmullw.  We end up\n+      \/\/ with 4 6-bit values, thus splitting the 3 input bytes,\n+      \/\/ ready for encoding:\n+      \/\/    0 0 d5..d0 0 0 c5..c0 0 0 b5..b0 0 0 a5..a0\n+      \/\/\n+      \/\/ For translation, we recognize that there are 5 distinct\n+      \/\/ ranges of legal Base64 characters as below:\n+      \/\/\n+      \/\/   +-------------+-------------+------------+\n+      \/\/   | 6-bit value | ASCII range |   offset   |\n+      \/\/   +-------------+-------------+------------+\n+      \/\/   |    0..25    |    A..Z     |     65     |\n+      \/\/   |   26..51    |    a..z     |     71     |\n+      \/\/   |   52..61    |    0..9     |     -4     |\n+      \/\/   |     62      |   + or -    | -19 or -17 |\n+      \/\/   |     63      |   \/ or _    | -16 or 32  |\n+      \/\/   +-------------+-------------+------------+\n+      \/\/\n+      \/\/ We note that vpshufb does a parallel lookup in a\n+      \/\/ destination register using the lower 4 bits of bytes from a\n+      \/\/ source register.  If we use a saturated subtraction and\n+      \/\/ subtract 51 from each 6-bit value, bytes from [0,51]\n+      \/\/ saturate to 0, and [52,63] map to a range of [1,12].  We\n+      \/\/ distinguish the [0,25] and [26,51] ranges by assigning a\n+      \/\/ value of 13 for all 6-bit values less than 26.  We end up\n+      \/\/ with:\n+      \/\/\n+      \/\/   +-------------+-------------+------------+\n+      \/\/   | 6-bit value |   Reduced   |   offset   |\n+      \/\/   +-------------+-------------+------------+\n+      \/\/   |    0..25    |     13      |     65     |\n+      \/\/   |   26..51    |      0      |     71     |\n+      \/\/   |   52..61    |    0..9     |     -4     |\n+      \/\/   |     62      |     11      | -19 or -17 |\n+      \/\/   |     63      |     12      | -16 or 32  |\n+      \/\/   +-------------+-------------+------------+\n+      \/\/\n+      \/\/ We then use a final vpshufb to add the appropriate offset,\n+      \/\/ translating the bytes.\n+      \/\/\n+      \/\/ Load input bytes - only 28 bytes.  Mask the first load to\n+      \/\/ not load into the full register.\n+      __ vpmaskmovd(xmm1, xmm1, Address(source, start_offset, Address::times_1, -4), Assembler::AVX_256bit);\n+\n+      \/\/ Move 3-byte chunks of input (12 bytes) into 16 bytes,\n+      \/\/ ordering by:\n+      \/\/   1, 0, 2, 1; 4, 3, 5, 4; etc.  This groups 6-bit chunks\n+      \/\/   for easy masking\n+      __ vpshufb(xmm1, xmm1, xmm9, Assembler::AVX_256bit);\n+\n+      __ addl(start_offset, 24);\n+\n+      \/\/ Load masking register for first and third (and multiples)\n+      \/\/ 6-bit values.\n+      __ movl(rax, 0x003f03f0);\n+      __ evpbroadcastd(xmm6, rax, Assembler::AVX_256bit);\n+      \/\/ Multiplication constant for \"shifting\" left by 4 and 8 bits\n+      __ movl(rax, 0x01000010);\n+      __ evpbroadcastd(xmm5, rax, Assembler::AVX_256bit);\n+\n+      \/\/ Isolate 6-bit chunks of interest\n+      __ vpand(xmm0, xmm8, xmm1, Assembler::AVX_256bit);\n+\n+      \/\/ Load constants for encoding\n+      __ movl(rax, 0x19191919);\n+      __ evpbroadcastd(xmm3, rax, Assembler::AVX_256bit);\n+      __ movl(rax, 0x33333333);\n+      __ evpbroadcastd(xmm4, rax, Assembler::AVX_256bit);\n+\n+      \/\/ Shift output bytes 0 and 2 into proper lanes\n+      __ vpmulhuw(xmm2, xmm0, xmm7, Assembler::AVX_256bit);\n+\n+      \/\/ Mask and shift output bytes 1 and 3 into proper lanes and\n+      \/\/ combine\n+      __ vpand(xmm0, xmm6, xmm1, Assembler::AVX_256bit);\n+      __ vpmullw(xmm0, xmm5, xmm0, Assembler::AVX_256bit);\n+      __ vpor(xmm0, xmm0, xmm2, Assembler::AVX_256bit);\n+\n+      \/\/ Find out which are 0..25.  This indicates which input\n+      \/\/ values fall in the range of 'A'-'Z', which require an\n+      \/\/ additional offset (see comments above)\n+      __ vpcmpgtb(xmm2, xmm0, xmm3, Assembler::AVX_256bit);\n+      __ vpsubusb(xmm1, xmm0, xmm4, Assembler::AVX_256bit);\n+      __ vpsubb(xmm1, xmm1, xmm2, Assembler::AVX_256bit);\n+\n+      \/\/ Load the proper lookup table\n+      __ lea(r11, ExternalAddress(StubRoutines::x86::base64_avx2_lut_addr()));\n+      __ movl(r15, isURL);\n+      __ shll(r15, 5);\n+      __ vmovdqu(xmm2, Address(r11, r15));\n+\n+      \/\/ Shuffle the offsets based on the range calculation done\n+      \/\/ above. This allows us to add the correct offset to the\n+      \/\/ 6-bit value corresponding to the range documented above.\n+      __ vpshufb(xmm1, xmm2, xmm1, Assembler::AVX_256bit);\n+      __ vpaddb(xmm0, xmm1, xmm0, Assembler::AVX_256bit);\n+\n+      \/\/ Store the encoded bytes\n+      __ vmovdqu(Address(dest, dp), xmm0);\n+      __ addl(dp, 32);\n+\n+      __ cmpl(length, 31);\n+      __ jcc(Assembler::belowEqual, L_process3);\n+\n+      __ align(32);\n+      __ BIND(L_32byteLoop);\n+\n+      \/\/ Get next 32 bytes\n+      __ vmovdqu(xmm1, Address(source, start_offset, Address::times_1, -4));\n+\n+      __ subl(length, 24);\n+      __ addl(start_offset, 24);\n+\n+      \/\/ This logic is identical to the above, with only constant\n+      \/\/ register loads removed.  Shuffle the input, mask off 6-bit\n+      \/\/ chunks, shift them into place, then add the offset to\n+      \/\/ encode.\n+      __ vpshufb(xmm1, xmm1, xmm9, Assembler::AVX_256bit);\n+\n+      __ vpand(xmm0, xmm8, xmm1, Assembler::AVX_256bit);\n+      __ vpmulhuw(xmm10, xmm0, xmm7, Assembler::AVX_256bit);\n+      __ vpand(xmm0, xmm6, xmm1, Assembler::AVX_256bit);\n+      __ vpmullw(xmm0, xmm5, xmm0, Assembler::AVX_256bit);\n+      __ vpor(xmm0, xmm0, xmm10, Assembler::AVX_256bit);\n+      __ vpcmpgtb(xmm10, xmm0, xmm3, Assembler::AVX_256bit);\n+      __ vpsubusb(xmm1, xmm0, xmm4, Assembler::AVX_256bit);\n+      __ vpsubb(xmm1, xmm1, xmm10, Assembler::AVX_256bit);\n+      __ vpshufb(xmm1, xmm2, xmm1, Assembler::AVX_256bit);\n+      __ vpaddb(xmm0, xmm1, xmm0, Assembler::AVX_256bit);\n+\n+      \/\/ Store the encoded bytes\n+      __ vmovdqu(Address(dest, dp), xmm0);\n+      __ addl(dp, 32);\n+\n+      __ cmpl(length, 31);\n+      __ jcc(Assembler::above, L_32byteLoop);\n+\n+      __ BIND(L_process3);\n+      __ vzeroupper();\n+    } else {\n+      __ BIND(L_process3);\n+    }\n+\n+    __ cmpl(length, 3);\n+    __ jcc(Assembler::below, L_exit);\n+\n+    \/\/ Load the encoding table based on isURL\n+    __ lea(r11, ExternalAddress(StubRoutines::x86::base64_encoding_table_addr()));\n+    __ movl(r15, isURL);\n+    __ shll(r15, 6);\n+    __ addptr(r11, r15);\n+\n+    __ BIND(L_processdata);\n+\n+    \/\/ Load 3 bytes\n+    __ load_unsigned_byte(r15, Address(source, start_offset));\n+    __ load_unsigned_byte(r10, Address(source, start_offset, Address::times_1, 1));\n+    __ load_unsigned_byte(r13, Address(source, start_offset, Address::times_1, 2));\n+\n+    \/\/ Build a 32-bit word with bytes 1, 2, 0, 1\n+    __ movl(rax, r10);\n+    __ shll(r10, 24);\n+    __ orl(rax, r10);\n+\n+    __ subl(length, 3);\n+\n+    __ shll(r15, 8);\n+    __ shll(r13, 16);\n+    __ orl(rax, r15);\n+\n+    __ addl(start_offset, 3);\n+\n+    __ orl(rax, r13);\n+    \/\/ At this point, rax contains | byte1 | byte2 | byte0 | byte1\n+    \/\/ r13 has byte2 << 16 - need low-order 6 bits to translate.\n+    \/\/ This translated byte is the fourth output byte.\n+    __ shrl(r13, 16);\n+    __ andl(r13, 0x3f);\n+\n+    \/\/ The high-order 6 bits of r15 (byte0) is translated.\n+    \/\/ The translated byte is the first output byte.\n+    __ shrl(r15, 10);\n+\n+    __ load_unsigned_byte(r13, Address(r11, r13));\n+    __ load_unsigned_byte(r15, Address(r11, r15));\n+\n+    __ movb(Address(dest, dp, Address::times_1, 3), r13);\n+\n+    \/\/ Extract high-order 4 bits of byte1 and low-order 2 bits of byte0.\n+    \/\/ This translated byte is the second output byte.\n+    __ shrl(rax, 4);\n+    __ movl(r10, rax);\n+    __ andl(rax, 0x3f);\n+\n+    __ movb(Address(dest, dp, Address::times_1, 0), r15);\n+\n+    __ load_unsigned_byte(rax, Address(r11, rax));\n+\n+    \/\/ Extract low-order 2 bits of byte1 and high-order 4 bits of byte2.\n+    \/\/ This translated byte is the third output byte.\n+    __ shrl(r10, 18);\n+    __ andl(r10, 0x3f);\n+\n+    __ load_unsigned_byte(r10, Address(r11, r10));\n+\n+    __ movb(Address(dest, dp, Address::times_1, 1), rax);\n+    __ movb(Address(dest, dp, Address::times_1, 2), r10);\n+\n+    __ addl(dp, 4);\n+    __ cmpl(length, 3);\n+    __ jcc(Assembler::aboveEqual, L_processdata);\n+\n+    __ BIND(L_exit);\n+    __ pop(r15);\n+    __ pop(r14);\n+    __ pop(r13);\n+    __ pop(r12);\n+    __ leave();\n+    __ ret(0);\n+    return start;\n+  }\n+\n+  \/\/ base64 AVX512vbmi tables\n+  address base64_vbmi_lookup_lo_addr() {\n+    __ align(64, (unsigned long long) __ pc());\n+    StubCodeMark mark(this, \"StubRoutines\", \"lookup_lo_base64\");\n+    address start = __ pc();\n+    assert(((unsigned long long)start & 0x3f) == 0,\n+           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+    __ emit_data64(0x8080808080808080, relocInfo::none);\n+    __ emit_data64(0x8080808080808080, relocInfo::none);\n+    __ emit_data64(0x8080808080808080, relocInfo::none);\n+    __ emit_data64(0x8080808080808080, relocInfo::none);\n+    __ emit_data64(0x8080808080808080, relocInfo::none);\n+    __ emit_data64(0x3f8080803e808080, relocInfo::none);\n+    __ emit_data64(0x3b3a393837363534, relocInfo::none);\n+    __ emit_data64(0x8080808080803d3c, relocInfo::none);\n+    return start;\n+  }\n+\n+  address base64_vbmi_lookup_hi_addr() {\n+    __ align(64, (unsigned long long) __ pc());\n+    StubCodeMark mark(this, \"StubRoutines\", \"lookup_hi_base64\");\n+    address start = __ pc();\n+    assert(((unsigned long long)start & 0x3f) == 0,\n+           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+    __ emit_data64(0x0605040302010080, relocInfo::none);\n+    __ emit_data64(0x0e0d0c0b0a090807, relocInfo::none);\n+    __ emit_data64(0x161514131211100f, relocInfo::none);\n+    __ emit_data64(0x8080808080191817, relocInfo::none);\n+    __ emit_data64(0x201f1e1d1c1b1a80, relocInfo::none);\n+    __ emit_data64(0x2827262524232221, relocInfo::none);\n+    __ emit_data64(0x302f2e2d2c2b2a29, relocInfo::none);\n+    __ emit_data64(0x8080808080333231, relocInfo::none);\n+    return start;\n+  }\n+  address base64_vbmi_lookup_lo_url_addr() {\n+    __ align(64, (unsigned long long) __ pc());\n+    StubCodeMark mark(this, \"StubRoutines\", \"lookup_lo_base64url\");\n+    address start = __ pc();\n+    assert(((unsigned long long)start & 0x3f) == 0,\n+           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+    __ emit_data64(0x8080808080808080, relocInfo::none);\n+    __ emit_data64(0x8080808080808080, relocInfo::none);\n+    __ emit_data64(0x8080808080808080, relocInfo::none);\n+    __ emit_data64(0x8080808080808080, relocInfo::none);\n+    __ emit_data64(0x8080808080808080, relocInfo::none);\n+    __ emit_data64(0x80803e8080808080, relocInfo::none);\n+    __ emit_data64(0x3b3a393837363534, relocInfo::none);\n+    __ emit_data64(0x8080808080803d3c, relocInfo::none);\n+    return start;\n+  }\n+\n+  address base64_vbmi_lookup_hi_url_addr() {\n+    __ align(64, (unsigned long long) __ pc());\n+    StubCodeMark mark(this, \"StubRoutines\", \"lookup_hi_base64url\");\n+    address start = __ pc();\n+    assert(((unsigned long long)start & 0x3f) == 0,\n+           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+    __ emit_data64(0x0605040302010080, relocInfo::none);\n+    __ emit_data64(0x0e0d0c0b0a090807, relocInfo::none);\n+    __ emit_data64(0x161514131211100f, relocInfo::none);\n+    __ emit_data64(0x3f80808080191817, relocInfo::none);\n+    __ emit_data64(0x201f1e1d1c1b1a80, relocInfo::none);\n+    __ emit_data64(0x2827262524232221, relocInfo::none);\n+    __ emit_data64(0x302f2e2d2c2b2a29, relocInfo::none);\n+    __ emit_data64(0x8080808080333231, relocInfo::none);\n+    return start;\n+  }\n+\n+  address base64_vbmi_pack_vec_addr() {\n+    __ align(64, (unsigned long long) __ pc());\n+    StubCodeMark mark(this, \"StubRoutines\", \"pack_vec_base64\");\n+    address start = __ pc();\n+    assert(((unsigned long long)start & 0x3f) == 0,\n+           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+    __ emit_data64(0x090a040506000102, relocInfo::none);\n+    __ emit_data64(0x161011120c0d0e08, relocInfo::none);\n+    __ emit_data64(0x1c1d1e18191a1415, relocInfo::none);\n+    __ emit_data64(0x292a242526202122, relocInfo::none);\n+    __ emit_data64(0x363031322c2d2e28, relocInfo::none);\n+    __ emit_data64(0x3c3d3e38393a3435, relocInfo::none);\n+    __ emit_data64(0x0000000000000000, relocInfo::none);\n+    __ emit_data64(0x0000000000000000, relocInfo::none);\n+    return start;\n+  }\n+\n+  address base64_vbmi_join_0_1_addr() {\n+    __ align(64, (unsigned long long) __ pc());\n+    StubCodeMark mark(this, \"StubRoutines\", \"join_0_1_base64\");\n+    address start = __ pc();\n+    assert(((unsigned long long)start & 0x3f) == 0,\n+           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+    __ emit_data64(0x090a040506000102, relocInfo::none);\n+    __ emit_data64(0x161011120c0d0e08, relocInfo::none);\n+    __ emit_data64(0x1c1d1e18191a1415, relocInfo::none);\n+    __ emit_data64(0x292a242526202122, relocInfo::none);\n+    __ emit_data64(0x363031322c2d2e28, relocInfo::none);\n+    __ emit_data64(0x3c3d3e38393a3435, relocInfo::none);\n+    __ emit_data64(0x494a444546404142, relocInfo::none);\n+    __ emit_data64(0x565051524c4d4e48, relocInfo::none);\n+    return start;\n+  }\n+\n+  address base64_vbmi_join_1_2_addr() {\n+    __ align(64, (unsigned long long) __ pc());\n+    StubCodeMark mark(this, \"StubRoutines\", \"join_1_2_base64\");\n@@ -5295,32 +5863,10 @@\n-    __ emit_data64(0x0000004200000041, relocInfo::none);\n-    __ emit_data64(0x0000004400000043, relocInfo::none);\n-    __ emit_data64(0x0000004600000045, relocInfo::none);\n-    __ emit_data64(0x0000004800000047, relocInfo::none);\n-    __ emit_data64(0x0000004a00000049, relocInfo::none);\n-    __ emit_data64(0x0000004c0000004b, relocInfo::none);\n-    __ emit_data64(0x0000004e0000004d, relocInfo::none);\n-    __ emit_data64(0x000000500000004f, relocInfo::none);\n-    __ emit_data64(0x0000005200000051, relocInfo::none);\n-    __ emit_data64(0x0000005400000053, relocInfo::none);\n-    __ emit_data64(0x0000005600000055, relocInfo::none);\n-    __ emit_data64(0x0000005800000057, relocInfo::none);\n-    __ emit_data64(0x0000005a00000059, relocInfo::none);\n-    __ emit_data64(0x0000006200000061, relocInfo::none);\n-    __ emit_data64(0x0000006400000063, relocInfo::none);\n-    __ emit_data64(0x0000006600000065, relocInfo::none);\n-    __ emit_data64(0x0000006800000067, relocInfo::none);\n-    __ emit_data64(0x0000006a00000069, relocInfo::none);\n-    __ emit_data64(0x0000006c0000006b, relocInfo::none);\n-    __ emit_data64(0x0000006e0000006d, relocInfo::none);\n-    __ emit_data64(0x000000700000006f, relocInfo::none);\n-    __ emit_data64(0x0000007200000071, relocInfo::none);\n-    __ emit_data64(0x0000007400000073, relocInfo::none);\n-    __ emit_data64(0x0000007600000075, relocInfo::none);\n-    __ emit_data64(0x0000007800000077, relocInfo::none);\n-    __ emit_data64(0x0000007a00000079, relocInfo::none);\n-    __ emit_data64(0x0000003100000030, relocInfo::none);\n-    __ emit_data64(0x0000003300000032, relocInfo::none);\n-    __ emit_data64(0x0000003500000034, relocInfo::none);\n-    __ emit_data64(0x0000003700000036, relocInfo::none);\n-    __ emit_data64(0x0000003900000038, relocInfo::none);\n-    __ emit_data64(0x0000002f0000002b, relocInfo::none);\n+    assert(((unsigned long long)start & 0x3f) == 0,\n+           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+    __ emit_data64(0x1c1d1e18191a1415, relocInfo::none);\n+    __ emit_data64(0x292a242526202122, relocInfo::none);\n+    __ emit_data64(0x363031322c2d2e28, relocInfo::none);\n+    __ emit_data64(0x3c3d3e38393a3435, relocInfo::none);\n+    __ emit_data64(0x494a444546404142, relocInfo::none);\n+    __ emit_data64(0x565051524c4d4e48, relocInfo::none);\n+    __ emit_data64(0x5c5d5e58595a5455, relocInfo::none);\n+    __ emit_data64(0x696a646566606162, relocInfo::none);\n@@ -5330,37 +5876,314 @@\n-  \/\/base64 url character set\n-  address base64url_charset_addr() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"base64url_charset\");\n-    address start = __ pc();\n-    __ emit_data64(0x0000004200000041, relocInfo::none);\n-    __ emit_data64(0x0000004400000043, relocInfo::none);\n-    __ emit_data64(0x0000004600000045, relocInfo::none);\n-    __ emit_data64(0x0000004800000047, relocInfo::none);\n-    __ emit_data64(0x0000004a00000049, relocInfo::none);\n-    __ emit_data64(0x0000004c0000004b, relocInfo::none);\n-    __ emit_data64(0x0000004e0000004d, relocInfo::none);\n-    __ emit_data64(0x000000500000004f, relocInfo::none);\n-    __ emit_data64(0x0000005200000051, relocInfo::none);\n-    __ emit_data64(0x0000005400000053, relocInfo::none);\n-    __ emit_data64(0x0000005600000055, relocInfo::none);\n-    __ emit_data64(0x0000005800000057, relocInfo::none);\n-    __ emit_data64(0x0000005a00000059, relocInfo::none);\n-    __ emit_data64(0x0000006200000061, relocInfo::none);\n-    __ emit_data64(0x0000006400000063, relocInfo::none);\n-    __ emit_data64(0x0000006600000065, relocInfo::none);\n-    __ emit_data64(0x0000006800000067, relocInfo::none);\n-    __ emit_data64(0x0000006a00000069, relocInfo::none);\n-    __ emit_data64(0x0000006c0000006b, relocInfo::none);\n-    __ emit_data64(0x0000006e0000006d, relocInfo::none);\n-    __ emit_data64(0x000000700000006f, relocInfo::none);\n-    __ emit_data64(0x0000007200000071, relocInfo::none);\n-    __ emit_data64(0x0000007400000073, relocInfo::none);\n-    __ emit_data64(0x0000007600000075, relocInfo::none);\n-    __ emit_data64(0x0000007800000077, relocInfo::none);\n-    __ emit_data64(0x0000007a00000079, relocInfo::none);\n-    __ emit_data64(0x0000003100000030, relocInfo::none);\n-    __ emit_data64(0x0000003300000032, relocInfo::none);\n-    __ emit_data64(0x0000003500000034, relocInfo::none);\n-    __ emit_data64(0x0000003700000036, relocInfo::none);\n-    __ emit_data64(0x0000003900000038, relocInfo::none);\n-    __ emit_data64(0x0000005f0000002d, relocInfo::none);\n+  address base64_vbmi_join_2_3_addr() {\n+    __ align(64, (unsigned long long) __ pc());\n+    StubCodeMark mark(this, \"StubRoutines\", \"join_2_3_base64\");\n+    address start = __ pc();\n+    assert(((unsigned long long)start & 0x3f) == 0,\n+           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+    __ emit_data64(0x363031322c2d2e28, relocInfo::none);\n+    __ emit_data64(0x3c3d3e38393a3435, relocInfo::none);\n+    __ emit_data64(0x494a444546404142, relocInfo::none);\n+    __ emit_data64(0x565051524c4d4e48, relocInfo::none);\n+    __ emit_data64(0x5c5d5e58595a5455, relocInfo::none);\n+    __ emit_data64(0x696a646566606162, relocInfo::none);\n+    __ emit_data64(0x767071726c6d6e68, relocInfo::none);\n+    __ emit_data64(0x7c7d7e78797a7475, relocInfo::none);\n+    return start;\n+  }\n+\n+  address base64_decoding_table_addr() {\n+    StubCodeMark mark(this, \"StubRoutines\", \"decoding_table_base64\");\n+    address start = __ pc();\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0x3fffffff3effffff, relocInfo::none);\n+    __ emit_data64(0x3b3a393837363534, relocInfo::none);\n+    __ emit_data64(0xffffffffffff3d3c, relocInfo::none);\n+    __ emit_data64(0x06050403020100ff, relocInfo::none);\n+    __ emit_data64(0x0e0d0c0b0a090807, relocInfo::none);\n+    __ emit_data64(0x161514131211100f, relocInfo::none);\n+    __ emit_data64(0xffffffffff191817, relocInfo::none);\n+    __ emit_data64(0x201f1e1d1c1b1aff, relocInfo::none);\n+    __ emit_data64(0x2827262524232221, relocInfo::none);\n+    __ emit_data64(0x302f2e2d2c2b2a29, relocInfo::none);\n+    __ emit_data64(0xffffffffff333231, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+\n+    \/\/ URL table\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffff3effffffffff, relocInfo::none);\n+    __ emit_data64(0x3b3a393837363534, relocInfo::none);\n+    __ emit_data64(0xffffffffffff3d3c, relocInfo::none);\n+    __ emit_data64(0x06050403020100ff, relocInfo::none);\n+    __ emit_data64(0x0e0d0c0b0a090807, relocInfo::none);\n+    __ emit_data64(0x161514131211100f, relocInfo::none);\n+    __ emit_data64(0x3fffffffff191817, relocInfo::none);\n+    __ emit_data64(0x201f1e1d1c1b1aff, relocInfo::none);\n+    __ emit_data64(0x2827262524232221, relocInfo::none);\n+    __ emit_data64(0x302f2e2d2c2b2a29, relocInfo::none);\n+    __ emit_data64(0xffffffffff333231, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+    return start;\n+  }\n+\n+\n+\/\/ Code for generating Base64 decoding.\n+\/\/\n+\/\/ Based on the article (and associated code) from https:\/\/arxiv.org\/abs\/1910.05109.\n+\/\/\n+\/\/ Intrinsic function prototype in Base64.java:\n+\/\/ private void decodeBlock(byte[] src, int sp, int sl, byte[] dst, int dp, boolean isURL, isMIME) {\n+  address generate_base64_decodeBlock() {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", \"implDecode\");\n+    address start = __ pc();\n+    __ enter();\n+\n+    \/\/ Save callee-saved registers before using them\n+    __ push(r12);\n+    __ push(r13);\n+    __ push(r14);\n+    __ push(r15);\n+    __ push(rbx);\n+\n+    \/\/ arguments\n+    const Register source = c_rarg0; \/\/ Source Array\n+    const Register start_offset = c_rarg1; \/\/ start offset\n+    const Register end_offset = c_rarg2; \/\/ end offset\n+    const Register dest = c_rarg3; \/\/ destination array\n+    const Register isMIME = rbx;\n+\n+#ifndef _WIN64\n+    const Register dp = c_rarg4;  \/\/ Position for writing to dest array\n+    const Register isURL = c_rarg5;\/\/ Base64 or URL character set\n+    __ movl(isMIME, Address(rbp, 2 * wordSize));\n+#else\n+    const Address  dp_mem(rbp, 6 * wordSize);  \/\/ length is on stack on Win64\n+    const Address isURL_mem(rbp, 7 * wordSize);\n+    const Register isURL = r10;      \/\/ pick the volatile windows register\n+    const Register dp = r12;\n+    __ movl(dp, dp_mem);\n+    __ movl(isURL, isURL_mem);\n+    __ movl(isMIME, Address(rbp, 8 * wordSize));\n+#endif\n+\n+    const XMMRegister lookup_lo = xmm5;\n+    const XMMRegister lookup_hi = xmm6;\n+    const XMMRegister errorvec = xmm7;\n+    const XMMRegister pack16_op = xmm9;\n+    const XMMRegister pack32_op = xmm8;\n+    const XMMRegister input0 = xmm3;\n+    const XMMRegister input1 = xmm20;\n+    const XMMRegister input2 = xmm21;\n+    const XMMRegister input3 = xmm19;\n+    const XMMRegister join01 = xmm12;\n+    const XMMRegister join12 = xmm11;\n+    const XMMRegister join23 = xmm10;\n+    const XMMRegister translated0 = xmm2;\n+    const XMMRegister translated1 = xmm1;\n+    const XMMRegister translated2 = xmm0;\n+    const XMMRegister translated3 = xmm4;\n+\n+    const XMMRegister merged0 = xmm2;\n+    const XMMRegister merged1 = xmm1;\n+    const XMMRegister merged2 = xmm0;\n+    const XMMRegister merged3 = xmm4;\n+    const XMMRegister merge_ab_bc0 = xmm2;\n+    const XMMRegister merge_ab_bc1 = xmm1;\n+    const XMMRegister merge_ab_bc2 = xmm0;\n+    const XMMRegister merge_ab_bc3 = xmm4;\n+\n+    const XMMRegister pack24bits = xmm4;\n+\n+    const Register length = r14;\n+    const Register output_size = r13;\n+    const Register output_mask = r15;\n+    const KRegister input_mask = k1;\n+\n+    const XMMRegister input_initial_valid_b64 = xmm0;\n+    const XMMRegister tmp = xmm10;\n+    const XMMRegister mask = xmm0;\n+    const XMMRegister invalid_b64 = xmm1;\n+\n+    Label L_process256, L_process64, L_process64Loop, L_exit, L_processdata, L_loadURL;\n+    Label L_continue, L_finalBit, L_padding, L_donePadding, L_bruteForce;\n+    Label L_forceLoop, L_bottomLoop, L_checkMIME, L_exit_no_vzero;\n+\n+    \/\/ calculate length from offsets\n+    __ movl(length, end_offset);\n+    __ subl(length, start_offset);\n+    __ push(dest);          \/\/ Save for return value calc\n+\n+    \/\/ If AVX512 VBMI not supported, just compile non-AVX code\n+    if(VM_Version::supports_avx512_vbmi() &&\n+       VM_Version::supports_avx512bw()) {\n+      __ cmpl(length, 128);     \/\/ 128-bytes is break-even for AVX-512\n+      __ jcc(Assembler::lessEqual, L_bruteForce);\n+\n+      __ cmpl(isMIME, 0);\n+      __ jcc(Assembler::notEqual, L_bruteForce);\n+\n+      \/\/ Load lookup tables based on isURL\n+      __ cmpl(isURL, 0);\n+      __ jcc(Assembler::notZero, L_loadURL);\n+\n+      __ evmovdquq(lookup_lo, ExternalAddress(StubRoutines::x86::base64_vbmi_lookup_lo_addr()), Assembler::AVX_512bit, r13);\n+      __ evmovdquq(lookup_hi, ExternalAddress(StubRoutines::x86::base64_vbmi_lookup_hi_addr()), Assembler::AVX_512bit, r13);\n+\n+      __ BIND(L_continue);\n+\n+      __ movl(r15, 0x01400140);\n+      __ evpbroadcastd(pack16_op, r15, Assembler::AVX_512bit);\n+\n+      __ movl(r15, 0x00011000);\n+      __ evpbroadcastd(pack32_op, r15, Assembler::AVX_512bit);\n+\n+      __ cmpl(length, 0xff);\n+      __ jcc(Assembler::lessEqual, L_process64);\n+\n+      \/\/ load masks required for decoding data\n+      __ BIND(L_processdata);\n+      __ evmovdquq(join01, ExternalAddress(StubRoutines::x86::base64_vbmi_join_0_1_addr()), Assembler::AVX_512bit,r13);\n+      __ evmovdquq(join12, ExternalAddress(StubRoutines::x86::base64_vbmi_join_1_2_addr()), Assembler::AVX_512bit, r13);\n+      __ evmovdquq(join23, ExternalAddress(StubRoutines::x86::base64_vbmi_join_2_3_addr()), Assembler::AVX_512bit, r13);\n+\n+      __ align(32);\n+      __ BIND(L_process256);\n+      \/\/ Grab input data\n+      __ evmovdquq(input0, Address(source, start_offset, Address::times_1, 0x00), Assembler::AVX_512bit);\n+      __ evmovdquq(input1, Address(source, start_offset, Address::times_1, 0x40), Assembler::AVX_512bit);\n+      __ evmovdquq(input2, Address(source, start_offset, Address::times_1, 0x80), Assembler::AVX_512bit);\n+      __ evmovdquq(input3, Address(source, start_offset, Address::times_1, 0xc0), Assembler::AVX_512bit);\n+\n+      \/\/ Copy the low part of the lookup table into the destination of the permutation\n+      __ evmovdquq(translated0, lookup_lo, Assembler::AVX_512bit);\n+      __ evmovdquq(translated1, lookup_lo, Assembler::AVX_512bit);\n+      __ evmovdquq(translated2, lookup_lo, Assembler::AVX_512bit);\n+      __ evmovdquq(translated3, lookup_lo, Assembler::AVX_512bit);\n+\n+      \/\/ Translate the base64 input into \"decoded\" bytes\n+      __ evpermt2b(translated0, input0, lookup_hi, Assembler::AVX_512bit);\n+      __ evpermt2b(translated1, input1, lookup_hi, Assembler::AVX_512bit);\n+      __ evpermt2b(translated2, input2, lookup_hi, Assembler::AVX_512bit);\n+      __ evpermt2b(translated3, input3, lookup_hi, Assembler::AVX_512bit);\n+\n+      \/\/ OR all of the translations together to check for errors (high-order bit of byte set)\n+      __ vpternlogd(input0, 0xfe, input1, input2, Assembler::AVX_512bit);\n+\n+      __ vpternlogd(input3, 0xfe, translated0, translated1, Assembler::AVX_512bit);\n+      __ vpternlogd(input0, 0xfe, translated2, translated3, Assembler::AVX_512bit);\n+      __ vpor(errorvec, input3, input0, Assembler::AVX_512bit);\n+\n+      \/\/ Check if there was an error - if so, try 64-byte chunks\n+      __ evpmovb2m(k3, errorvec, Assembler::AVX_512bit);\n+      __ kortestql(k3, k3);\n+      __ jcc(Assembler::notZero, L_process64);\n+\n+      \/\/ The merging and shuffling happens here\n+      \/\/ We multiply each byte pair [00dddddd | 00cccccc | 00bbbbbb | 00aaaaaa]\n+      \/\/ Multiply [00cccccc] by 2^6 added to [00dddddd] to get [0000cccc | ccdddddd]\n+      \/\/ The pack16_op is a vector of 0x01400140, so multiply D by 1 and C by 0x40\n+      __ vpmaddubsw(merge_ab_bc0, translated0, pack16_op, Assembler::AVX_512bit);\n+      __ vpmaddubsw(merge_ab_bc1, translated1, pack16_op, Assembler::AVX_512bit);\n+      __ vpmaddubsw(merge_ab_bc2, translated2, pack16_op, Assembler::AVX_512bit);\n+      __ vpmaddubsw(merge_ab_bc3, translated3, pack16_op, Assembler::AVX_512bit);\n+\n+      \/\/ Now do the same with packed 16-bit values.\n+      \/\/ We start with [0000cccc | ccdddddd | 0000aaaa | aabbbbbb]\n+      \/\/ pack32_op is 0x00011000 (2^12, 1), so this multiplies [0000aaaa | aabbbbbb] by 2^12\n+      \/\/ and adds [0000cccc | ccdddddd] to yield [00000000 | aaaaaabb | bbbbcccc | ccdddddd]\n+      __ vpmaddwd(merged0, merge_ab_bc0, pack32_op, Assembler::AVX_512bit);\n+      __ vpmaddwd(merged1, merge_ab_bc1, pack32_op, Assembler::AVX_512bit);\n+      __ vpmaddwd(merged2, merge_ab_bc2, pack32_op, Assembler::AVX_512bit);\n+      __ vpmaddwd(merged3, merge_ab_bc3, pack32_op, Assembler::AVX_512bit);\n+\n+      \/\/ The join vectors specify which byte from which vector goes into the outputs\n+      \/\/ One of every 4 bytes in the extended vector is zero, so we pack them into their\n+      \/\/ final positions in the register for storing (256 bytes in, 192 bytes out)\n+      __ evpermt2b(merged0, join01, merged1, Assembler::AVX_512bit);\n+      __ evpermt2b(merged1, join12, merged2, Assembler::AVX_512bit);\n+      __ evpermt2b(merged2, join23, merged3, Assembler::AVX_512bit);\n+\n+      \/\/ Store result\n+      __ evmovdquq(Address(dest, dp, Address::times_1, 0x00), merged0, Assembler::AVX_512bit);\n+      __ evmovdquq(Address(dest, dp, Address::times_1, 0x40), merged1, Assembler::AVX_512bit);\n+      __ evmovdquq(Address(dest, dp, Address::times_1, 0x80), merged2, Assembler::AVX_512bit);\n+\n+      __ addptr(source, 0x100);\n+      __ addptr(dest, 0xc0);\n+      __ subl(length, 0x100);\n+      __ cmpl(length, 64 * 4);\n+      __ jcc(Assembler::greaterEqual, L_process256);\n+\n+      \/\/ At this point, we've decoded 64 * 4 * n bytes.\n+      \/\/ The remaining length will be <= 64 * 4 - 1.\n+      \/\/ UNLESS there was an error decoding the first 256-byte chunk.  In this\n+      \/\/ case, the length will be arbitrarily long.\n+      \/\/\n+      \/\/ Note that this will be the path for MIME-encoded strings.\n+\n+      __ BIND(L_process64);\n+\n+      __ evmovdquq(pack24bits, ExternalAddress(StubRoutines::x86::base64_vbmi_pack_vec_addr()), Assembler::AVX_512bit, r13);\n+\n+      __ cmpl(length, 63);\n+      __ jcc(Assembler::lessEqual, L_finalBit);\n+\n+      __ align(32);\n+      __ BIND(L_process64Loop);\n+\n+      \/\/ Handle first 64-byte block\n+\n+      __ evmovdquq(input0, Address(source, start_offset), Assembler::AVX_512bit);\n+      __ evmovdquq(translated0, lookup_lo, Assembler::AVX_512bit);\n+      __ evpermt2b(translated0, input0, lookup_hi, Assembler::AVX_512bit);\n+\n+      __ vpor(errorvec, translated0, input0, Assembler::AVX_512bit);\n+\n+      \/\/ Check for error and bomb out before updating dest\n+      __ evpmovb2m(k3, errorvec, Assembler::AVX_512bit);\n+      __ kortestql(k3, k3);\n+      __ jcc(Assembler::notZero, L_exit);\n+\n+      \/\/ Pack output register, selecting correct byte ordering\n+      __ vpmaddubsw(merge_ab_bc0, translated0, pack16_op, Assembler::AVX_512bit);\n+      __ vpmaddwd(merged0, merge_ab_bc0, pack32_op, Assembler::AVX_512bit);\n+      __ vpermb(merged0, pack24bits, merged0, Assembler::AVX_512bit);\n+\n+      __ evmovdquq(Address(dest, dp), merged0, Assembler::AVX_512bit);\n@@ -5368,2 +6191,3 @@\n-    return start;\n-  }\n+      __ subl(length, 64);\n+      __ addptr(source, 64);\n+      __ addptr(dest, 48);\n@@ -5371,12 +6195,2 @@\n-  address base64_bswap_mask_addr() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"bswap_mask_base64\");\n-    address start = __ pc();\n-    __ emit_data64(0x0504038002010080, relocInfo::none);\n-    __ emit_data64(0x0b0a098008070680, relocInfo::none);\n-    __ emit_data64(0x0908078006050480, relocInfo::none);\n-    __ emit_data64(0x0f0e0d800c0b0a80, relocInfo::none);\n-    __ emit_data64(0x0605048003020180, relocInfo::none);\n-    __ emit_data64(0x0c0b0a8009080780, relocInfo::none);\n-    __ emit_data64(0x0504038002010080, relocInfo::none);\n-    __ emit_data64(0x0b0a098008070680, relocInfo::none);\n+      __ cmpl(length, 64);\n+      __ jcc(Assembler::greaterEqual, L_process64Loop);\n@@ -5384,2 +6198,2 @@\n-    return start;\n-  }\n+      __ cmpl(length, 0);\n+      __ jcc(Assembler::lessEqual, L_exit);\n@@ -5387,12 +6201,2 @@\n-  address base64_right_shift_mask_addr() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"right_shift_mask\");\n-    address start = __ pc();\n-    __ emit_data64(0x0006000400020000, relocInfo::none);\n-    __ emit_data64(0x0006000400020000, relocInfo::none);\n-    __ emit_data64(0x0006000400020000, relocInfo::none);\n-    __ emit_data64(0x0006000400020000, relocInfo::none);\n-    __ emit_data64(0x0006000400020000, relocInfo::none);\n-    __ emit_data64(0x0006000400020000, relocInfo::none);\n-    __ emit_data64(0x0006000400020000, relocInfo::none);\n-    __ emit_data64(0x0006000400020000, relocInfo::none);\n+      __ BIND(L_finalBit);\n+      \/\/ Now have 1 to 63 bytes left to decode\n@@ -5400,2 +6204,5 @@\n-    return start;\n-  }\n+      \/\/ I was going to let Java take care of the final fragment\n+      \/\/ however it will repeatedly call this routine for every 4 bytes\n+      \/\/ of input data, so handle the rest here.\n+      __ movq(rax, -1);\n+      __ bzhiq(rax, rax, length);    \/\/ Input mask in rax\n@@ -5403,12 +6210,4 @@\n-  address base64_left_shift_mask_addr() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"left_shift_mask\");\n-    address start = __ pc();\n-    __ emit_data64(0x0000000200040000, relocInfo::none);\n-    __ emit_data64(0x0000000200040000, relocInfo::none);\n-    __ emit_data64(0x0000000200040000, relocInfo::none);\n-    __ emit_data64(0x0000000200040000, relocInfo::none);\n-    __ emit_data64(0x0000000200040000, relocInfo::none);\n-    __ emit_data64(0x0000000200040000, relocInfo::none);\n-    __ emit_data64(0x0000000200040000, relocInfo::none);\n-    __ emit_data64(0x0000000200040000, relocInfo::none);\n+      __ movl(output_size, length);\n+      __ shrl(output_size, 2);   \/\/ Find (len \/ 4) * 3 (output length)\n+      __ lea(output_size, Address(output_size, output_size, Address::times_2, 0));\n+      \/\/ output_size in r13\n@@ -5416,2 +6215,3 @@\n-    return start;\n-  }\n+      \/\/ Strip pad characters, if any, and adjust length and mask\n+      __ cmpb(Address(source, length, Address::times_1, -1), '=');\n+      __ jcc(Assembler::equal, L_padding);\n@@ -5419,14 +6219,1 @@\n-  address base64_and_mask_addr() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"and_mask\");\n-    address start = __ pc();\n-    __ emit_data64(0x3f003f003f000000, relocInfo::none);\n-    __ emit_data64(0x3f003f003f000000, relocInfo::none);\n-    __ emit_data64(0x3f003f003f000000, relocInfo::none);\n-    __ emit_data64(0x3f003f003f000000, relocInfo::none);\n-    __ emit_data64(0x3f003f003f000000, relocInfo::none);\n-    __ emit_data64(0x3f003f003f000000, relocInfo::none);\n-    __ emit_data64(0x3f003f003f000000, relocInfo::none);\n-    __ emit_data64(0x3f003f003f000000, relocInfo::none);\n-    return start;\n-  }\n+      __ BIND(L_donePadding);\n@@ -5434,7 +6221,4 @@\n-  address base64_gather_mask_addr() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"gather_mask\");\n-    address start = __ pc();\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    return start;\n-  }\n+      \/\/ Output size is (64 - output_size), output mask is (all 1s >> output_size).\n+      __ kmovql(input_mask, rax);\n+      __ movq(output_mask, -1);\n+      __ bzhiq(output_mask, output_mask, output_size);\n@@ -5442,8 +6226,4 @@\n-\/\/ Code for generating Base64 encoding.\n-\/\/ Intrinsic function prototype in Base64.java:\n-\/\/ private void encodeBlock(byte[] src, int sp, int sl, byte[] dst, int dp, boolean isURL) {\n-  address generate_base64_encodeBlock() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"implEncode\");\n-    address start = __ pc();\n-    __ enter();\n+      \/\/ Load initial input with all valid base64 characters.  Will be used\n+      \/\/ in merging source bytes to avoid masking when determining if an error occurred.\n+      __ movl(rax, 0x61616161);\n+      __ evpbroadcastd(input_initial_valid_b64, rax, Assembler::AVX_512bit);\n@@ -5451,5 +6231,3 @@\n-    \/\/ Save callee-saved registers before using them\n-    __ push(r12);\n-    __ push(r13);\n-    __ push(r14);\n-    __ push(r15);\n+      \/\/ A register containing all invalid base64 decoded values\n+      __ movl(rax, 0x80808080);\n+      __ evpbroadcastd(invalid_b64, rax, Assembler::AVX_512bit);\n@@ -5457,5 +6235,13 @@\n-    \/\/ arguments\n-    const Register source = c_rarg0; \/\/ Source Array\n-    const Register start_offset = c_rarg1; \/\/ start offset\n-    const Register end_offset = c_rarg2; \/\/ end offset\n-    const Register dest = c_rarg3; \/\/ destination array\n+      \/\/ input_mask is in k1\n+      \/\/ output_size is in r13\n+      \/\/ output_mask is in r15\n+      \/\/ zmm0 - free\n+      \/\/ zmm1 - 0x00011000\n+      \/\/ zmm2 - 0x01400140\n+      \/\/ zmm3 - errorvec\n+      \/\/ zmm4 - pack vector\n+      \/\/ zmm5 - lookup_lo\n+      \/\/ zmm6 - lookup_hi\n+      \/\/ zmm7 - errorvec\n+      \/\/ zmm8 - 0x61616161\n+      \/\/ zmm9 - 0x80808080\n@@ -5463,11 +6249,2 @@\n-#ifndef _WIN64\n-    const Register dp = c_rarg4;  \/\/ Position for writing to dest array\n-    const Register isURL = c_rarg5;\/\/ Base64 or URL character set\n-#else\n-    const Address  dp_mem(rbp, 6 * wordSize);  \/\/ length is on stack on Win64\n-    const Address isURL_mem(rbp, 7 * wordSize);\n-    const Register isURL = r10;      \/\/ pick the volatile windows register\n-    const Register dp = r12;\n-    __ movl(dp, dp_mem);\n-    __ movl(isURL, isURL_mem);\n-#endif\n+      \/\/ Load only the bytes from source, merging into our \"fully-valid\" register\n+      __ evmovdqub(input_initial_valid_b64, input_mask, Address(source, start_offset, Address::times_1, 0x0), true, Assembler::AVX_512bit);\n@@ -5475,2 +6252,4 @@\n-    const Register length = r14;\n-    Label L_process80, L_process32, L_process3, L_exit, L_processdata;\n+      \/\/ Decode all bytes within our merged input\n+      __ evmovdquq(tmp, lookup_lo, Assembler::AVX_512bit);\n+      __ evpermt2b(tmp, input_initial_valid_b64, lookup_hi, Assembler::AVX_512bit);\n+      __ vporq(mask, tmp, input_initial_valid_b64, Assembler::AVX_512bit);\n@@ -5478,3 +6257,82 @@\n-    \/\/ calculate length from offsets\n-    __ movl(length, end_offset);\n-    __ subl(length, start_offset);\n+      \/\/ Check for error.  Compare (decoded | initial) to all invalid.\n+      \/\/ If any bytes have their high-order bit set, then we have an error.\n+      __ evptestmb(k2, mask, invalid_b64, Assembler::AVX_512bit);\n+      __ kortestql(k2, k2);\n+\n+      \/\/ If we have an error, use the brute force loop to decode what we can (4-byte chunks).\n+      __ jcc(Assembler::notZero, L_bruteForce);\n+\n+      \/\/ Shuffle output bytes\n+      __ vpmaddubsw(tmp, tmp, pack16_op, Assembler::AVX_512bit);\n+      __ vpmaddwd(tmp, tmp, pack32_op, Assembler::AVX_512bit);\n+\n+      __ vpermb(tmp, pack24bits, tmp, Assembler::AVX_512bit);\n+      __ kmovql(k1, output_mask);\n+      __ evmovdqub(Address(dest, dp), k1, tmp, true, Assembler::AVX_512bit);\n+\n+      __ addptr(dest, output_size);\n+\n+      __ BIND(L_exit);\n+      __ vzeroupper();\n+      __ pop(rax);             \/\/ Get original dest value\n+      __ subptr(dest, rax);      \/\/ Number of bytes converted\n+      __ movptr(rax, dest);\n+      __ pop(rbx);\n+      __ pop(r15);\n+      __ pop(r14);\n+      __ pop(r13);\n+      __ pop(r12);\n+      __ leave();\n+      __ ret(0);\n+\n+      __ BIND(L_loadURL);\n+      __ evmovdquq(lookup_lo, ExternalAddress(StubRoutines::x86::base64_vbmi_lookup_lo_url_addr()), Assembler::AVX_512bit, r13);\n+      __ evmovdquq(lookup_hi, ExternalAddress(StubRoutines::x86::base64_vbmi_lookup_hi_url_addr()), Assembler::AVX_512bit, r13);\n+      __ jmp(L_continue);\n+\n+      __ BIND(L_padding);\n+      __ decrementq(output_size, 1);\n+      __ shrq(rax, 1);\n+\n+      __ cmpb(Address(source, length, Address::times_1, -2), '=');\n+      __ jcc(Assembler::notEqual, L_donePadding);\n+\n+      __ decrementq(output_size, 1);\n+      __ shrq(rax, 1);\n+      __ jmp(L_donePadding);\n+\n+      __ align(32);\n+      __ BIND(L_bruteForce);\n+    }   \/\/ End of if(avx512_vbmi)\n+\n+    \/\/ Use non-AVX code to decode 4-byte chunks into 3 bytes of output\n+\n+    \/\/ Register state (Linux):\n+    \/\/ r12-15 - saved on stack\n+    \/\/ rdi - src\n+    \/\/ rsi - sp\n+    \/\/ rdx - sl\n+    \/\/ rcx - dst\n+    \/\/ r8 - dp\n+    \/\/ r9 - isURL\n+\n+    \/\/ Register state (Windows):\n+    \/\/ r12-15 - saved on stack\n+    \/\/ rcx - src\n+    \/\/ rdx - sp\n+    \/\/ r8 - sl\n+    \/\/ r9 - dst\n+    \/\/ r12 - dp\n+    \/\/ r10 - isURL\n+\n+    \/\/ Registers (common):\n+    \/\/ length (r14) - bytes in src\n+\n+    const Register decode_table = r11;\n+    const Register out_byte_count = rbx;\n+    const Register byte1 = r13;\n+    const Register byte2 = r15;\n+    const Register byte3 = WINDOWS_ONLY(r8) NOT_WINDOWS(rdx);\n+    const Register byte4 = WINDOWS_ONLY(r10) NOT_WINDOWS(r9);\n+\n+    __ shrl(length, 2);    \/\/ Multiple of 4 bytes only - length is # 4-byte chunks\n@@ -5482,1 +6340,1 @@\n-    __ jcc(Assembler::lessEqual, L_exit);\n+    __ jcc(Assembler::lessEqual, L_exit_no_vzero);\n@@ -5484,5 +6342,3 @@\n-    __ lea(r11, ExternalAddress(StubRoutines::x86::base64_charset_addr()));\n-    \/\/ check if base64 charset(isURL=0) or base64 url charset(isURL=1) needs to be loaded\n-    __ cmpl(isURL, 0);\n-    __ jcc(Assembler::equal, L_processdata);\n-    __ lea(r11, ExternalAddress(StubRoutines::x86::base64url_charset_addr()));\n+    __ shll(isURL, 8);    \/\/ index into decode table based on isURL\n+    __ lea(decode_table, ExternalAddress(StubRoutines::x86::base64_decoding_table_addr()));\n+    __ addptr(decode_table, isURL);\n@@ -5490,187 +6346,44 @@\n-    \/\/ load masks required for encoding data\n-    __ BIND(L_processdata);\n-    __ movdqu(xmm16, ExternalAddress(StubRoutines::x86::base64_gather_mask_addr()));\n-    \/\/ Set 64 bits of K register.\n-    __ evpcmpeqb(k3, xmm16, xmm16, Assembler::AVX_512bit);\n-    __ evmovdquq(xmm12, ExternalAddress(StubRoutines::x86::base64_bswap_mask_addr()), Assembler::AVX_256bit, r13);\n-    __ evmovdquq(xmm13, ExternalAddress(StubRoutines::x86::base64_right_shift_mask_addr()), Assembler::AVX_512bit, r13);\n-    __ evmovdquq(xmm14, ExternalAddress(StubRoutines::x86::base64_left_shift_mask_addr()), Assembler::AVX_512bit, r13);\n-    __ evmovdquq(xmm15, ExternalAddress(StubRoutines::x86::base64_and_mask_addr()), Assembler::AVX_512bit, r13);\n-\n-    \/\/ Vector Base64 implementation, producing 96 bytes of encoded data\n-    __ BIND(L_process80);\n-    __ cmpl(length, 80);\n-    __ jcc(Assembler::below, L_process32);\n-    __ evmovdquq(xmm0, Address(source, start_offset, Address::times_1, 0), Assembler::AVX_256bit);\n-    __ evmovdquq(xmm1, Address(source, start_offset, Address::times_1, 24), Assembler::AVX_256bit);\n-    __ evmovdquq(xmm2, Address(source, start_offset, Address::times_1, 48), Assembler::AVX_256bit);\n-\n-    \/\/permute the input data in such a manner that we have continuity of the source\n-    __ vpermq(xmm3, xmm0, 148, Assembler::AVX_256bit);\n-    __ vpermq(xmm4, xmm1, 148, Assembler::AVX_256bit);\n-    __ vpermq(xmm5, xmm2, 148, Assembler::AVX_256bit);\n-\n-    \/\/shuffle input and group 3 bytes of data and to it add 0 as the 4th byte.\n-    \/\/we can deal with 12 bytes at a time in a 128 bit register\n-    __ vpshufb(xmm3, xmm3, xmm12, Assembler::AVX_256bit);\n-    __ vpshufb(xmm4, xmm4, xmm12, Assembler::AVX_256bit);\n-    __ vpshufb(xmm5, xmm5, xmm12, Assembler::AVX_256bit);\n-\n-    \/\/convert byte to word. Each 128 bit register will have 6 bytes for processing\n-    __ vpmovzxbw(xmm3, xmm3, Assembler::AVX_512bit);\n-    __ vpmovzxbw(xmm4, xmm4, Assembler::AVX_512bit);\n-    __ vpmovzxbw(xmm5, xmm5, Assembler::AVX_512bit);\n-\n-    \/\/ Extract bits in the following pattern 6, 4+2, 2+4, 6 to convert 3, 8 bit numbers to 4, 6 bit numbers\n-    __ evpsrlvw(xmm0, xmm3, xmm13,  Assembler::AVX_512bit);\n-    __ evpsrlvw(xmm1, xmm4, xmm13, Assembler::AVX_512bit);\n-    __ evpsrlvw(xmm2, xmm5, xmm13, Assembler::AVX_512bit);\n-\n-    __ evpsllvw(xmm3, xmm3, xmm14, Assembler::AVX_512bit);\n-    __ evpsllvw(xmm4, xmm4, xmm14, Assembler::AVX_512bit);\n-    __ evpsllvw(xmm5, xmm5, xmm14, Assembler::AVX_512bit);\n-\n-    __ vpsrlq(xmm0, xmm0, 8, Assembler::AVX_512bit);\n-    __ vpsrlq(xmm1, xmm1, 8, Assembler::AVX_512bit);\n-    __ vpsrlq(xmm2, xmm2, 8, Assembler::AVX_512bit);\n-\n-    __ vpsllq(xmm3, xmm3, 8, Assembler::AVX_512bit);\n-    __ vpsllq(xmm4, xmm4, 8, Assembler::AVX_512bit);\n-    __ vpsllq(xmm5, xmm5, 8, Assembler::AVX_512bit);\n-\n-    __ vpandq(xmm3, xmm3, xmm15, Assembler::AVX_512bit);\n-    __ vpandq(xmm4, xmm4, xmm15, Assembler::AVX_512bit);\n-    __ vpandq(xmm5, xmm5, xmm15, Assembler::AVX_512bit);\n-\n-    \/\/ Get the final 4*6 bits base64 encoding\n-    __ vporq(xmm3, xmm3, xmm0, Assembler::AVX_512bit);\n-    __ vporq(xmm4, xmm4, xmm1, Assembler::AVX_512bit);\n-    __ vporq(xmm5, xmm5, xmm2, Assembler::AVX_512bit);\n-\n-    \/\/ Shift\n-    __ vpsrlq(xmm3, xmm3, 8, Assembler::AVX_512bit);\n-    __ vpsrlq(xmm4, xmm4, 8, Assembler::AVX_512bit);\n-    __ vpsrlq(xmm5, xmm5, 8, Assembler::AVX_512bit);\n-\n-    \/\/ look up 6 bits in the base64 character set to fetch the encoding\n-    \/\/ we are converting word to dword as gather instructions need dword indices for looking up encoding\n-    __ vextracti64x4(xmm6, xmm3, 0);\n-    __ vpmovzxwd(xmm0, xmm6, Assembler::AVX_512bit);\n-    __ vextracti64x4(xmm6, xmm3, 1);\n-    __ vpmovzxwd(xmm1, xmm6, Assembler::AVX_512bit);\n-\n-    __ vextracti64x4(xmm6, xmm4, 0);\n-    __ vpmovzxwd(xmm2, xmm6, Assembler::AVX_512bit);\n-    __ vextracti64x4(xmm6, xmm4, 1);\n-    __ vpmovzxwd(xmm3, xmm6, Assembler::AVX_512bit);\n-\n-    __ vextracti64x4(xmm4, xmm5, 0);\n-    __ vpmovzxwd(xmm6, xmm4, Assembler::AVX_512bit);\n-\n-    __ vextracti64x4(xmm4, xmm5, 1);\n-    __ vpmovzxwd(xmm7, xmm4, Assembler::AVX_512bit);\n-\n-    __ kmovql(k2, k3);\n-    __ evpgatherdd(xmm4, k2, Address(r11, xmm0, Address::times_4, 0), Assembler::AVX_512bit);\n-    __ kmovql(k2, k3);\n-    __ evpgatherdd(xmm5, k2, Address(r11, xmm1, Address::times_4, 0), Assembler::AVX_512bit);\n-    __ kmovql(k2, k3);\n-    __ evpgatherdd(xmm8, k2, Address(r11, xmm2, Address::times_4, 0), Assembler::AVX_512bit);\n-    __ kmovql(k2, k3);\n-    __ evpgatherdd(xmm9, k2, Address(r11, xmm3, Address::times_4, 0), Assembler::AVX_512bit);\n-    __ kmovql(k2, k3);\n-    __ evpgatherdd(xmm10, k2, Address(r11, xmm6, Address::times_4, 0), Assembler::AVX_512bit);\n-    __ kmovql(k2, k3);\n-    __ evpgatherdd(xmm11, k2, Address(r11, xmm7, Address::times_4, 0), Assembler::AVX_512bit);\n-\n-    \/\/Down convert dword to byte. Final output is 16*6 = 96 bytes long\n-    __ evpmovdb(Address(dest, dp, Address::times_1, 0), xmm4, Assembler::AVX_512bit);\n-    __ evpmovdb(Address(dest, dp, Address::times_1, 16), xmm5, Assembler::AVX_512bit);\n-    __ evpmovdb(Address(dest, dp, Address::times_1, 32), xmm8, Assembler::AVX_512bit);\n-    __ evpmovdb(Address(dest, dp, Address::times_1, 48), xmm9, Assembler::AVX_512bit);\n-    __ evpmovdb(Address(dest, dp, Address::times_1, 64), xmm10, Assembler::AVX_512bit);\n-    __ evpmovdb(Address(dest, dp, Address::times_1, 80), xmm11, Assembler::AVX_512bit);\n-\n-    __ addq(dest, 96);\n-    __ addq(source, 72);\n-    __ subq(length, 72);\n-    __ jmp(L_process80);\n-\n-    \/\/ Vector Base64 implementation generating 32 bytes of encoded data\n-    __ BIND(L_process32);\n-    __ cmpl(length, 32);\n-    __ jcc(Assembler::below, L_process3);\n-    __ evmovdquq(xmm0, Address(source, start_offset), Assembler::AVX_256bit);\n-    __ vpermq(xmm0, xmm0, 148, Assembler::AVX_256bit);\n-    __ vpshufb(xmm6, xmm0, xmm12, Assembler::AVX_256bit);\n-    __ vpmovzxbw(xmm6, xmm6, Assembler::AVX_512bit);\n-    __ evpsrlvw(xmm2, xmm6, xmm13, Assembler::AVX_512bit);\n-    __ evpsllvw(xmm3, xmm6, xmm14, Assembler::AVX_512bit);\n-\n-    __ vpsrlq(xmm2, xmm2, 8, Assembler::AVX_512bit);\n-    __ vpsllq(xmm3, xmm3, 8, Assembler::AVX_512bit);\n-    __ vpandq(xmm3, xmm3, xmm15, Assembler::AVX_512bit);\n-    __ vporq(xmm1, xmm2, xmm3, Assembler::AVX_512bit);\n-    __ vpsrlq(xmm1, xmm1, 8, Assembler::AVX_512bit);\n-    __ vextracti64x4(xmm9, xmm1, 0);\n-    __ vpmovzxwd(xmm6, xmm9, Assembler::AVX_512bit);\n-    __ vextracti64x4(xmm9, xmm1, 1);\n-    __ vpmovzxwd(xmm5, xmm9,  Assembler::AVX_512bit);\n-    __ kmovql(k2, k3);\n-    __ evpgatherdd(xmm8, k2, Address(r11, xmm6, Address::times_4, 0), Assembler::AVX_512bit);\n-    __ kmovql(k2, k3);\n-    __ evpgatherdd(xmm10, k2, Address(r11, xmm5, Address::times_4, 0), Assembler::AVX_512bit);\n-    __ evpmovdb(Address(dest, dp, Address::times_1, 0), xmm8, Assembler::AVX_512bit);\n-    __ evpmovdb(Address(dest, dp, Address::times_1, 16), xmm10, Assembler::AVX_512bit);\n-    __ subq(length, 24);\n-    __ addq(dest, 32);\n-    __ addq(source, 24);\n-    __ jmp(L_process32);\n-\n-    \/\/ Scalar data processing takes 3 bytes at a time and produces 4 bytes of encoded data\n-    \/* This code corresponds to the scalar version of the following snippet in Base64.java\n-    ** int bits = (src[sp0++] & 0xff) << 16 |(src[sp0++] & 0xff) << 8 |(src[sp0++] & 0xff);\n-    ** dst[dp0++] = (byte)base64[(bits >> > 18) & 0x3f];\n-    ** dst[dp0++] = (byte)base64[(bits >> > 12) & 0x3f];\n-    ** dst[dp0++] = (byte)base64[(bits >> > 6) & 0x3f];\n-    ** dst[dp0++] = (byte)base64[bits & 0x3f];*\/\n-    __ BIND(L_process3);\n-    __ cmpl(length, 3);\n-    __ jcc(Assembler::below, L_exit);\n-    \/\/ Read 1 byte at a time\n-    __ movzbl(rax, Address(source, start_offset));\n-    __ shll(rax, 0x10);\n-    __ movl(r15, rax);\n-    __ movzbl(rax, Address(source, start_offset, Address::times_1, 1));\n-    __ shll(rax, 0x8);\n-    __ movzwl(rax, rax);\n-    __ orl(r15, rax);\n-    __ movzbl(rax, Address(source, start_offset, Address::times_1, 2));\n-    __ orl(rax, r15);\n-    \/\/ Save 3 bytes read in r15\n-    __ movl(r15, rax);\n-    __ shrl(rax, 0x12);\n-    __ andl(rax, 0x3f);\n-    \/\/ rax contains the index, r11 contains base64 lookup table\n-    __ movb(rax, Address(r11, rax, Address::times_4));\n-    \/\/ Write the encoded byte to destination\n-    __ movb(Address(dest, dp, Address::times_1, 0), rax);\n-    __ movl(rax, r15);\n-    __ shrl(rax, 0xc);\n-    __ andl(rax, 0x3f);\n-    __ movb(rax, Address(r11, rax, Address::times_4));\n-    __ movb(Address(dest, dp, Address::times_1, 1), rax);\n-    __ movl(rax, r15);\n-    __ shrl(rax, 0x6);\n-    __ andl(rax, 0x3f);\n-    __ movb(rax, Address(r11, rax, Address::times_4));\n-    __ movb(Address(dest, dp, Address::times_1, 2), rax);\n-    __ movl(rax, r15);\n-    __ andl(rax, 0x3f);\n-    __ movb(rax, Address(r11, rax, Address::times_4));\n-    __ movb(Address(dest, dp, Address::times_1, 3), rax);\n-    __ subl(length, 3);\n-    __ addq(dest, 4);\n-    __ addq(source, 3);\n-    __ jmp(L_process3);\n-    __ BIND(L_exit);\n+    __ jmp(L_bottomLoop);\n+\n+    __ align(32);\n+    __ BIND(L_forceLoop);\n+    __ shll(byte1, 18);\n+    __ shll(byte2, 12);\n+    __ shll(byte3, 6);\n+    __ orl(byte1, byte2);\n+    __ orl(byte1, byte3);\n+    __ orl(byte1, byte4);\n+\n+    __ addptr(source, 4);\n+\n+    __ movb(Address(dest, dp, Address::times_1, 2), byte1);\n+    __ shrl(byte1, 8);\n+    __ movb(Address(dest, dp, Address::times_1, 1), byte1);\n+    __ shrl(byte1, 8);\n+    __ movb(Address(dest, dp, Address::times_1, 0), byte1);\n+\n+    __ addptr(dest, 3);\n+    __ decrementl(length, 1);\n+    __ jcc(Assembler::zero, L_exit_no_vzero);\n+\n+    __ BIND(L_bottomLoop);\n+    __ load_unsigned_byte(byte1, Address(source, start_offset, Address::times_1, 0x00));\n+    __ load_unsigned_byte(byte2, Address(source, start_offset, Address::times_1, 0x01));\n+    __ load_signed_byte(byte1, Address(decode_table, byte1));\n+    __ load_signed_byte(byte2, Address(decode_table, byte2));\n+    __ load_unsigned_byte(byte3, Address(source, start_offset, Address::times_1, 0x02));\n+    __ load_unsigned_byte(byte4, Address(source, start_offset, Address::times_1, 0x03));\n+    __ load_signed_byte(byte3, Address(decode_table, byte3));\n+    __ load_signed_byte(byte4, Address(decode_table, byte4));\n+\n+    __ mov(rax, byte1);\n+    __ orl(rax, byte2);\n+    __ orl(rax, byte3);\n+    __ orl(rax, byte4);\n+    __ jcc(Assembler::positive, L_forceLoop);\n+\n+    __ BIND(L_exit_no_vzero);\n+    __ pop(rax);             \/\/ Get original dest value\n+    __ subptr(dest, rax);      \/\/ Number of bytes converted\n+    __ movptr(rax, dest);\n+    __ pop(rbx);\n@@ -5683,0 +6396,1 @@\n+\n@@ -7620,0 +8334,1 @@\n+\n@@ -7621,7 +8336,20 @@\n-      StubRoutines::x86::_and_mask = base64_and_mask_addr();\n-      StubRoutines::x86::_bswap_mask = base64_bswap_mask_addr();\n-      StubRoutines::x86::_base64_charset = base64_charset_addr();\n-      StubRoutines::x86::_url_charset = base64url_charset_addr();\n-      StubRoutines::x86::_gather_mask = base64_gather_mask_addr();\n-      StubRoutines::x86::_left_shift_mask = base64_left_shift_mask_addr();\n-      StubRoutines::x86::_right_shift_mask = base64_right_shift_mask_addr();\n+      if(VM_Version::supports_avx2() &&\n+         VM_Version::supports_avx512bw() &&\n+         VM_Version::supports_avx512vl()) {\n+        StubRoutines::x86::_avx2_shuffle_base64 = base64_avx2_shuffle_addr();\n+        StubRoutines::x86::_avx2_input_mask_base64 = base64_avx2_input_mask_addr();\n+        StubRoutines::x86::_avx2_lut_base64 = base64_avx2_lut_addr();\n+      }\n+      StubRoutines::x86::_encoding_table_base64 = base64_encoding_table_addr();\n+      if (VM_Version::supports_avx512_vbmi()) {\n+        StubRoutines::x86::_shuffle_base64 = base64_shuffle_addr();\n+        StubRoutines::x86::_lookup_lo_base64 = base64_vbmi_lookup_lo_addr();\n+        StubRoutines::x86::_lookup_hi_base64 = base64_vbmi_lookup_hi_addr();\n+        StubRoutines::x86::_lookup_lo_base64url = base64_vbmi_lookup_lo_url_addr();\n+        StubRoutines::x86::_lookup_hi_base64url = base64_vbmi_lookup_hi_url_addr();\n+        StubRoutines::x86::_pack_vec_base64 = base64_vbmi_pack_vec_addr();\n+        StubRoutines::x86::_join_0_1_base64 = base64_vbmi_join_0_1_addr();\n+        StubRoutines::x86::_join_1_2_base64 = base64_vbmi_join_1_2_addr();\n+        StubRoutines::x86::_join_2_3_base64 = base64_vbmi_join_2_3_addr();\n+      }\n+      StubRoutines::x86::_decoding_table_base64 = base64_decoding_table_addr();\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":1099,"deletions":371,"binary":false,"changes":1470,"status":"modified"},{"patch":"@@ -68,7 +68,5 @@\n-address StubRoutines::x86::_bswap_mask = NULL;\n-address StubRoutines::x86::_base64_charset = NULL;\n-address StubRoutines::x86::_gather_mask = NULL;\n-address StubRoutines::x86::_right_shift_mask = NULL;\n-address StubRoutines::x86::_left_shift_mask = NULL;\n-address StubRoutines::x86::_and_mask = NULL;\n-address StubRoutines::x86::_url_charset = NULL;\n+address StubRoutines::x86::_encoding_table_base64 = NULL;\n+address StubRoutines::x86::_shuffle_base64 = NULL;\n+address StubRoutines::x86::_avx2_shuffle_base64 = NULL;\n+address StubRoutines::x86::_avx2_input_mask_base64 = NULL;\n+address StubRoutines::x86::_avx2_lut_base64 = NULL;\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.cpp","additions":5,"deletions":7,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -188,7 +188,5 @@\n-  static address _base64_charset;\n-  static address _bswap_mask;\n-  static address _gather_mask;\n-  static address _right_shift_mask;\n-  static address _left_shift_mask;\n-  static address _and_mask;\n-  static address _url_charset;\n+  static address _encoding_table_base64;\n+  static address _shuffle_base64;\n+  static address _avx2_shuffle_base64;\n+  static address _avx2_input_mask_base64;\n+  static address _avx2_lut_base64;\n@@ -342,7 +340,5 @@\n-  static address base64_charset_addr() { return _base64_charset; }\n-  static address base64url_charset_addr() { return _url_charset; }\n-  static address base64_bswap_mask_addr() { return _bswap_mask; }\n-  static address base64_gather_mask_addr() { return _gather_mask; }\n-  static address base64_right_shift_mask_addr() { return _right_shift_mask; }\n-  static address base64_left_shift_mask_addr() { return _left_shift_mask; }\n-  static address base64_and_mask_addr() { return _and_mask; }\n+  static address base64_encoding_table_addr() { return _encoding_table_base64; }\n+  static address base64_shuffle_addr() { return _shuffle_base64; }\n+  static address base64_avx2_shuffle_addr() { return _avx2_shuffle_base64; }\n+  static address base64_avx2_input_mask_addr() { return _avx2_input_mask_base64; }\n+  static address base64_avx2_lut_addr() { return _avx2_lut_base64; }\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.hpp","additions":10,"deletions":14,"binary":false,"changes":24,"status":"modified"}]}