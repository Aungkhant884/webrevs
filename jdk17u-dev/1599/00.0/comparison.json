{"files":[{"patch":"@@ -2967,0 +2967,16 @@\n+  \/\/ Big-endian 128-bit + 64-bit -> 128-bit addition.\n+  \/\/ Inputs: 128-bits. in is preserved.\n+  \/\/ The least-significant 64-bit word is in the upper dword of the vector\n+  \/\/ inc (the 64-bit increment) is preserved. Its lower dword must be zero\n+  \/\/ Output: result\n+  void be_add_128_64(FloatRegister result, FloatRegister in,\n+               FloatRegister inc, FloatRegister tmp) {\n+    assert_different_registers(result, tmp, inc);\n+\n+    __ addv(result, __ T2D, in, inc);      \/\/ Add inc to the least-significant dword of input\n+    __ cmhi(tmp, __ T2D, inc, result);     \/\/ Check for result overflowing\n+    __ ins(tmp, __ D, tmp, 0, 1);          \/\/ Move LSD of comparison result to MSD\n+    __ ins(tmp, __ D, inc, 1, 0);          \/\/ Move 0 to LSD of comparison result\n+    __ subv(result, __ T2D, result, tmp);  \/\/ Subtract -1 from MSD if there was an overflow\n+  }\n+\n@@ -3076,1 +3092,1 @@\n-      __ ins(v4, __ S, v5, 3, 3); \/\/ v4 contains { 0, 0, 0, 1 }\n+      __ ins(v4, __ S, v5, 2, 2); \/\/ v4 contains { 0, 1 }\n@@ -3078,5 +3094,8 @@\n-      __ ld1(v0, __ T16B, counter); \/\/ Load the counter into v0\n-      __ rev32(v16, __ T16B, v0);\n-      __ addv(v16, __ T4S, v16, v4);\n-      __ rev32(v16, __ T16B, v16);\n-      __ st1(v16, __ T16B, counter); \/\/ Save the incremented counter back\n+      \/\/ 128-bit big-endian increment\n+      __ ld1(v0, __ T16B, counter);\n+      __ rev64(v16, __ T16B, v0);\n+      be_add_128_64(v16, v16, v4, \/*tmp*\/v5);\n+      __ rev64(v16, __ T16B, v16);\n+      __ st1(v16, __ T16B, counter);\n+      \/\/ Previous counter value is in v0\n+      \/\/ v4 contains { 0, 1 }\n@@ -3114,3 +3133,3 @@\n-        __ rev32(v16, __ T16B, v16);\n-        __ addv(v16, __ T4S, v16, v4);\n-        __ rev32(v16, __ T16B, v16);\n+        __ rev64(v16, __ T16B, v16);\n+        be_add_128_64(v16, v16, v4, \/*tmp*\/v5);\n+        __ rev64(v16, __ T16B, v16);\n@@ -3164,1 +3183,1 @@\n-    __ rev32(v16, __ T16B, v0); \/\/ v16 contains byte-reversed counter\n+    __ rev64(v16, __ T16B, v0); \/\/ v16 contains byte-reversed counter\n@@ -3174,1 +3193,1 @@\n-      __ ins(v8, __ S, v9, 3, 3); \/\/ v8 contains { 0, 0, 0, 1 }\n+      __ ins(v8, __ S, v9, 2, 2); \/\/ v8 contains { 0, 1 }\n@@ -3177,2 +3196,2 @@\n-        __ rev32(f, __ T16B, v16);\n-        __ addv(v16, __ T4S, v16, v8);\n+        __ rev64(f, __ T16B, v16);\n+        be_add_128_64(v16, v16, v8, \/*tmp*\/v9);\n@@ -3206,1 +3225,1 @@\n-    __ rev32(v16, __ T16B, v16);\n+    __ rev64(v16, __ T16B, v16);\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":33,"deletions":14,"binary":false,"changes":47,"status":"modified"},{"patch":"@@ -2647,0 +2647,10 @@\n+\n+void Assembler::kshiftlbl(KRegister dst, KRegister src, int imm8) {\n+  assert(VM_Version::supports_avx512dq(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0 , src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int16(0x32, (0xC0 | encode));\n+  emit_int8(imm8);\n+}\n+\n+\n@@ -3962,0 +3972,8 @@\n+void Assembler::evpcmpuq(KRegister kdst, XMMRegister nds, XMMRegister src, ComparisonPredicate vcc, int vector_len) {\n+  assert(VM_Version::supports_avx512vlbw(), \"\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(kdst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int24(0x1E, (0xC0 | encode), vcc);\n+}\n+\n@@ -6588,0 +6606,13 @@\n+void Assembler::evpaddq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xD4, (0xC0 | encode));\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":31,"deletions":0,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -1494,0 +1494,2 @@\n+  void kshiftlbl(KRegister dst, KRegister src, int imm8);\n+\n@@ -1729,0 +1731,2 @@\n+  void evpcmpuq(KRegister kdst, XMMRegister nds, XMMRegister src, ComparisonPredicate vcc, int vector_len);\n+\n@@ -2251,0 +2255,4 @@\n+  \/\/ Leaf level assembler routines for masked operations.\n+  void evpaddq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+\/\/ void evpaddq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.hpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -4387,1 +4387,13 @@\n- \/\/ Vector AES Counter implementation\n+  \/\/ Vector AES Counter implementation\n+\n+  address counter_mask_ones_addr() {\n+    __ align64();\n+    StubCodeMark mark(this, \"StubRoutines\", \"counter_mask_addr\");\n+    address start = __ pc();\n+    for (int i = 0; i < 4; i ++) {\n+      __ emit_data64(0x0000000000000000, relocInfo::none);\n+      __ emit_data64(0x0000000000000001, relocInfo::none);\n+    }\n+    return start;\n+  }\n+\n@@ -7608,0 +7620,1 @@\n+        StubRoutines::x86::_counter_mask_ones_addr = counter_mask_ones_addr();\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":14,"deletions":1,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -2010,1 +2010,1 @@\n-  product(intx, ArchiveRelocationMode, 0, DIAGNOSTIC,                       \\\n+  product(intx, ArchiveRelocationMode, 1, DIAGNOSTIC,                       \\\n@@ -2012,2 +2012,2 @@\n-           \"unsuccessful, map at alternative address (default); \"           \\\n-           \"(1) always map at alternative address; \"                        \\\n+           \"unsuccessful, map at alternative address; \"                     \\\n+           \"(1) always map at alternative address (default); \"              \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"}]}