{"files":[{"patch":"@@ -4298,0 +4298,10 @@\n+operand immI_positive()\n+%{\n+  predicate(n->get_int() > 0);\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -4431,1 +4431,1 @@\n-instruct vsra8B_imm(vecD dst, vecD src, immI shift) %{\n+instruct vsra8B_imm(vecD dst, vecD src, immI_positive shift) %{\n@@ -4446,1 +4446,1 @@\n-instruct vsra16B_imm(vecX dst, vecX src, immI shift) %{\n+instruct vsra16B_imm(vecX dst, vecX src, immI_positive shift) %{\n@@ -4460,1 +4460,1 @@\n-instruct vsrl8B_imm(vecD dst, vecD src, immI shift) %{\n+instruct vsrl8B_imm(vecD dst, vecD src, immI_positive shift) %{\n@@ -4480,1 +4480,1 @@\n-instruct vsrl16B_imm(vecX dst, vecX src, immI shift) %{\n+instruct vsrl16B_imm(vecX dst, vecX src, immI_positive shift) %{\n@@ -4635,1 +4635,1 @@\n-instruct vsra4S_imm(vecD dst, vecD src, immI shift) %{\n+instruct vsra4S_imm(vecD dst, vecD src, immI_positive shift) %{\n@@ -4650,1 +4650,1 @@\n-instruct vsra8S_imm(vecX dst, vecX src, immI shift) %{\n+instruct vsra8S_imm(vecX dst, vecX src, immI_positive shift) %{\n@@ -4664,1 +4664,1 @@\n-instruct vsrl4S_imm(vecD dst, vecD src, immI shift) %{\n+instruct vsrl4S_imm(vecD dst, vecD src, immI_positive shift) %{\n@@ -4684,1 +4684,1 @@\n-instruct vsrl8S_imm(vecX dst, vecX src, immI shift) %{\n+instruct vsrl8S_imm(vecX dst, vecX src, immI_positive shift) %{\n@@ -4823,1 +4823,1 @@\n-instruct vsra2I_imm(vecD dst, vecD src, immI shift) %{\n+instruct vsra2I_imm(vecD dst, vecD src, immI_positive shift) %{\n@@ -4836,1 +4836,1 @@\n-instruct vsra4I_imm(vecX dst, vecX src, immI shift) %{\n+instruct vsra4I_imm(vecX dst, vecX src, immI_positive shift) %{\n@@ -4849,1 +4849,1 @@\n-instruct vsrl2I_imm(vecD dst, vecD src, immI shift) %{\n+instruct vsrl2I_imm(vecD dst, vecD src, immI_positive shift) %{\n@@ -4862,1 +4862,1 @@\n-instruct vsrl4I_imm(vecX dst, vecX src, immI shift) %{\n+instruct vsrl4I_imm(vecX dst, vecX src, immI_positive shift) %{\n@@ -4935,1 +4935,1 @@\n-instruct vsra2L_imm(vecX dst, vecX src, immI shift) %{\n+instruct vsra2L_imm(vecX dst, vecX src, immI_positive shift) %{\n@@ -4948,1 +4948,1 @@\n-instruct vsrl2L_imm(vecX dst, vecX src, immI shift) %{\n+instruct vsrl2L_imm(vecX dst, vecX src, immI_positive shift) %{\n@@ -4961,1 +4961,1 @@\n-instruct vsraa8B_imm(vecD dst, vecD src, immI shift) %{\n+instruct vsraa8B_imm(vecD dst, vecD src, immI_positive shift) %{\n@@ -4975,1 +4975,1 @@\n-instruct vsraa16B_imm(vecX dst, vecX src, immI shift) %{\n+instruct vsraa16B_imm(vecX dst, vecX src, immI_positive shift) %{\n@@ -4989,1 +4989,1 @@\n-instruct vsraa4S_imm(vecD dst, vecD src, immI shift) %{\n+instruct vsraa4S_imm(vecD dst, vecD src, immI_positive shift) %{\n@@ -5003,1 +5003,1 @@\n-instruct vsraa8S_imm(vecX dst, vecX src, immI shift) %{\n+instruct vsraa8S_imm(vecX dst, vecX src, immI_positive shift) %{\n@@ -5017,1 +5017,1 @@\n-instruct vsraa2I_imm(vecD dst, vecD src, immI shift) %{\n+instruct vsraa2I_imm(vecD dst, vecD src, immI_positive shift) %{\n@@ -5030,1 +5030,1 @@\n-instruct vsraa4I_imm(vecX dst, vecX src, immI shift) %{\n+instruct vsraa4I_imm(vecX dst, vecX src, immI_positive shift) %{\n@@ -5043,1 +5043,1 @@\n-instruct vsraa2L_imm(vecX dst, vecX src, immI shift) %{\n+instruct vsraa2L_imm(vecX dst, vecX src, immI_positive shift) %{\n@@ -5056,1 +5056,1 @@\n-instruct vsrla8B_imm(vecD dst, vecD src, immI shift) %{\n+instruct vsrla8B_imm(vecD dst, vecD src, immI_positive shift) %{\n@@ -5071,1 +5071,1 @@\n-instruct vsrla16B_imm(vecX dst, vecX src, immI shift) %{\n+instruct vsrla16B_imm(vecX dst, vecX src, immI_positive shift) %{\n@@ -5086,1 +5086,1 @@\n-instruct vsrla4S_imm(vecD dst, vecD src, immI shift) %{\n+instruct vsrla4S_imm(vecD dst, vecD src, immI_positive shift) %{\n@@ -5101,1 +5101,1 @@\n-instruct vsrla8S_imm(vecX dst, vecX src, immI shift) %{\n+instruct vsrla8S_imm(vecX dst, vecX src, immI_positive shift) %{\n@@ -5116,1 +5116,1 @@\n-instruct vsrla2I_imm(vecD dst, vecD src, immI shift) %{\n+instruct vsrla2I_imm(vecD dst, vecD src, immI_positive shift) %{\n@@ -5129,1 +5129,1 @@\n-instruct vsrla4I_imm(vecX dst, vecX src, immI shift) %{\n+instruct vsrla4I_imm(vecX dst, vecX src, immI_positive shift) %{\n@@ -5142,1 +5142,1 @@\n-instruct vsrla2L_imm(vecX dst, vecX src, immI shift) %{\n+instruct vsrla2L_imm(vecX dst, vecX src, immI_positive shift) %{\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_neon.ad","additions":28,"deletions":28,"binary":false,"changes":56,"status":"modified"},{"patch":"@@ -1995,1 +1995,1 @@\n-instruct vsra$3$4_imm`'(vec$6 dst, vec$6 src, immI shift) %{\n+instruct vsra$3$4_imm`'(vec$6 dst, vec$6 src, immI_positive shift) %{\n@@ -2020,1 +2020,1 @@\n-instruct vsrl$3$4_imm`'(vec$6 dst, vec$6 src, immI shift) %{\n+instruct vsrl$3$4_imm`'(vec$6 dst, vec$6 src, immI_positive shift) %{\n@@ -2055,1 +2055,1 @@\n-instruct vsrla$3$4_imm`'(vec$6 dst, vec$6 src, immI shift) %{\n+instruct vsrla$3$4_imm`'(vec$6 dst, vec$6 src, immI_positive shift) %{\n@@ -2079,1 +2079,1 @@\n-instruct vsraa$3$4_imm`'(vec$6 dst, vec$6 src, immI shift) %{\n+instruct vsraa$3$4_imm`'(vec$6 dst, vec$6 src, immI_positive shift) %{\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_neon_ad.m4","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -0,0 +1,73 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+\/*\n+ * @test\n+ * @bug 8288445\n+ * @summary Test shift by 0\n+ * @run main\/othervm -Xbatch -XX:-TieredCompilation\n+ * compiler.codegen.ShiftByZero\n+ *\/\n+\n+package compiler.codegen;\n+\n+public class ShiftByZero {\n+\n+    public static final int N = 64;\n+\n+    public static int[] i32 = new int[N];\n+\n+    public static void bMeth() {\n+        int shift = i32[0];\n+        \/\/ This loop is to confuse the optimizer, so that \"shift\" is\n+        \/\/ optimized to 0 only after loop vectorization.\n+        for (int i8 = 279; i8 > 1; --i8) {\n+            shift <<= 6;\n+        }\n+        \/\/ low 6 bits of shift are 0, so shift can be\n+        \/\/ simplified to constant 0\n+        {\n+            for (int i = 0; i < N; ++i) {\n+                i32[i] += i32[i] >>= shift;\n+            }\n+            for (int i = 0; i < N; ++i) {\n+                i32[i] += i32[i] >>>= shift;\n+            }\n+            for (int i = 0; i < N; ++i) {\n+                i32[i] >>>= shift;\n+            }\n+            for (int i = 0; i < N; ++i) {\n+                i32[i] >>= shift;\n+            }\n+            for (int i = 0; i < N; ++i) {\n+                i32[i] <<= shift;\n+            }\n+        }\n+    }\n+\n+    public static void main(String[] strArr) {\n+        for (int i = 0; i < 20_000; i++) {\n+            bMeth();\n+        }\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/codegen\/ShiftByZero.java","additions":73,"deletions":0,"binary":false,"changes":73,"status":"added"}]}